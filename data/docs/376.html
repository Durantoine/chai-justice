<!DOCTYPE html><html lang="en-US"><head><meta charSet="utf-8"/><title>Trustworthy AI - IBM Research</title><meta name="description" content="Our trust in technology relies on understanding how it works. It’s important to understand why AI makes the decisions it does. We’re developing tools to make AI more explainable, fair, robust, private, and transparent."/><meta name="robots" content="index,follow"/><meta name="viewport" content="width=device-width,initial-scale=1"/><link rel="canonical" href="https://research.ibm.com/topics/trustworthy-ai"/><link rel="icon" href="//www.ibm.com/favicon.ico"/><link rel="alternate" type="application/rss+xml" href="/rss"/><meta name="dcterms.date" content="2021-02-09"/><meta name="dcterms.rights" content="© Copyright IBM Corp. 2021"/><meta name="geo.country" content="US"/><meta name="google-site-verification" content="O1nsbg1J1iAeYJK6HneffI0_RiLebmSPxfs5ESYNnwI"/><meta property="og:title" content="Trustworthy AI"/><meta property="og:type" content="website"/><meta property="og:url" content="https://research.ibm.com/topics/trustworthy-ai"/><meta property="og:site_name" content="IBM Research"/><meta property="og:locale" content="en_US"/><meta property="og:description" content="Our trust in technology relies on understanding how it works. It’s important to understand why AI makes the decisions it does. We’re developing tools to make AI more explainable, fair, robust, private, and transparent."/><meta name="twitter:card" content="summary"/><meta name="twitter:creator" content="@IBMResearch"/><meta name="twitter:site" content="@IBMResearch"/><meta name="twitter:title" content="Trustworthy AI"/><meta name="twitter:description" content="Our trust in technology relies on understanding how it works. It’s important to understand why AI makes the decisions it does. We’re developing tools to make AI more explainable, fair, robust, private, and transparent."/><meta name="next-head-count" content="23"/><link rel="preload" href="/_next/static/css/a7b7807b13f738ad.css" as="style"/><link rel="stylesheet" href="/_next/static/css/a7b7807b13f738ad.css" data-n-g=""/><link rel="preload" href="/_next/static/css/d4696acdb9efe4e6.css" as="style"/><link rel="stylesheet" href="/_next/static/css/d4696acdb9efe4e6.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-78c92fac7aa8fdd8.js"></script><script src="/_next/static/chunks/webpack-3d89281636839a51.js" defer=""></script><script src="/_next/static/chunks/framework-6c089a70add278f0.js" defer=""></script><script src="/_next/static/chunks/main-8c0bc9c04ebd8d78.js" defer=""></script><script src="/_next/static/chunks/pages/_app-4fec7dbe510c9536.js" defer=""></script><script src="/_next/static/chunks/5c0b189e-ad5174a0e0ae7ecf.js" defer=""></script><script src="/_next/static/chunks/1664-ce91d37d67ef970d.js" defer=""></script><script src="/_next/static/chunks/6324-fdcf1845c74e74fa.js" defer=""></script><script src="/_next/static/chunks/6559-5bd237997a82a9cc.js" defer=""></script><script src="/_next/static/chunks/3970-0a691c442564e536.js" defer=""></script><script src="/_next/static/chunks/6994-33ac634f2921e727.js" defer=""></script><script src="/_next/static/chunks/4818-d04bced1922398d8.js" defer=""></script><script src="/_next/static/chunks/9755-b9fe035dbff059db.js" defer=""></script><script src="/_next/static/chunks/8115-35443899063471e0.js" defer=""></script><script src="/_next/static/chunks/5071-fcfc5bb73ea7078d.js" defer=""></script><script src="/_next/static/chunks/6811-0f0e1200e9904128.js" defer=""></script><script src="/_next/static/chunks/3176-cf81b599ab19c52e.js" defer=""></script><script src="/_next/static/chunks/5402-0553d338e2e0c2c1.js" defer=""></script><script src="/_next/static/chunks/1409-047360c9b09350cb.js" defer=""></script><script src="/_next/static/chunks/9375-a2535eddd424019f.js" defer=""></script><script src="/_next/static/chunks/4227-501ff477f4aa1e55.js" defer=""></script><script src="/_next/static/chunks/2327-f4b5378cd58fd2e9.js" defer=""></script><script src="/_next/static/chunks/3133-efad6aa7977afc30.js" defer=""></script><script src="/_next/static/chunks/4797-6b8e264679add27d.js" defer=""></script><script src="/_next/static/chunks/601-a58a41541b0641f8.js" defer=""></script><script src="/_next/static/chunks/5657-02d06d3e935a26ff.js" defer=""></script><script src="/_next/static/chunks/7112-569d26fba99261cd.js" defer=""></script><script src="/_next/static/chunks/4792-a10fde4ac3253dea.js" defer=""></script><script src="/_next/static/chunks/810-b408df84f5b7c521.js" defer=""></script><script src="/_next/static/chunks/4011-709596550a347c2a.js" defer=""></script><script src="/_next/static/chunks/7177-a8e70f731bc1fc51.js" defer=""></script><script src="/_next/static/chunks/8465-f288a83c29476110.js" defer=""></script><script src="/_next/static/chunks/pages/topics/%5Btid%5D-3d49d615e572d692.js" defer=""></script><script src="/_next/static/UZgKCVAyhOMQXLXugF_6V/_buildManifest.js" defer=""></script><script src="/_next/static/UZgKCVAyhOMQXLXugF_6V/_ssgManifest.js" defer=""></script></head><body><script>0</script><div id="__next"><main class="vmZu6 U_E6M" id="main-content"><div class="uoFvk FVYh0 Wh_ih cqWBn"><div class="kgkLd z23Vg yqhDV" style="--row:1"><header class="SAOCn"><div class="T82Jc"><a aria-label="Go to topic directory" class="vOtSk" href="/topics">View all topics</a></div><h1 class="v7KdG">Trustworthy AI</h1><p class="EIfJh">Our trust in technology relies on understanding how it works. It’s important to understand why AI makes the decisions it does. We’re developing tools to make AI more explainable, fair, robust, private, and transparent.</p><div class="foFsA"><a class="fFsRH ZXePn aFohb cds--btn cds--btn--secondary" type="button" href="/topics/trustworthy-ai#topics">Explore our topics<svg focusable="false" preserveAspectRatio="xMidYMid meet" fill="currentColor" aria-hidden="true" width="16" height="16" viewBox="0 0 16 16" class="cds--btn__icon" xmlns="http://www.w3.org/2000/svg"><path d="M12.3 9.3L8.5 13.1 8.5 1 7.5 1 7.5 13.1 3.7 9.3 3 10 8 15 13 10z"></path></svg></a></div></header><section class="cOi2S"><h2 class="q0JE9">Overview</h2><div class="YDOg0"><p class="eBWTD EihHw">Artificial intelligence systems have become increasingly prevalent in everyday life and enterprise settings, and they’re now often being used to support human decision-making. These systems have grown increasingly complex and efficient, and AI holds the promise of uncovering valuable insights across a wide range of applications. But broad adoption of AI systems will require humans to trust their output.</p><p class="eBWTD">When people understand how technology works, and we can assess that it’s safe and reliable, we’re far more inclined to trust it. Many AI systems to date have been black boxes, where data is fed in and results come out. To trust a decision made by an algorithm, we need to know that it is fair, that it’s reliable and can be accounted for, and that it will cause no harm. We need assurances that AI cannot be tampered with and that the system itself is secure. We need to be able to look inside AI systems, to understand the rationale behind the algorithmic outcome, and even ask it questions as to how it came to its decision.</p><p class="eBWTD DMxM7">At IBM Research, we’re working on a range of approaches to ensure that AI systems built in the future are fair, robust, explainable, account, and align with the values of the society they’re designed for. We’re ensuring that in the future, AI applications are as fair as they are efficient across their entire lifecycle.</p></div></section><section class="cOi2S"><div class="Zmp4_"><h2 class="KiFqI" id="our-work">Our work</h2><ul class="Di_2_ KqhlQ"><li class="ACYfb _8fTSM"><article class="sq5w6 Gs8Ke LPlid undefined"><h3 class="_0PBYm"><a class="LSzQx _61H0o XvVJd" href="/blog/map-measure-manage-gen-ai"><span class="_5fUZ">In AI, alignment is the goal. Steerability is how you get there</span><svg focusable="false" preserveAspectRatio="xMidYMid meet" fill="currentColor" width="32" height="32" viewBox="0 0 32 32" aria-hidden="true" class="_8CqAU" xmlns="http://www.w3.org/2000/svg"><path d="M18 6L16.57 7.393 24.15 15 4 15 4 17 24.15 17 16.57 24.573 18 26 28 16 18 6z"></path></svg></a></h3><div class="_5Cpod Wsf58"><svg focusable="false" preserveAspectRatio="xMidYMid meet" fill="currentColor" width="16" height="16" viewBox="0 0 32 32" aria-hidden="true" xmlns="http://www.w3.org/2000/svg"><path d="M27 25H21a3 3 0 00-3 3v2h2V28a1 1 0 011-1h6a1 1 0 011 1v2h2V28A3 3 0 0027 25zM20 20a4 4 0 104-4A4 4 0 0020 20zm6 0a2 2 0 11-2-2A2 2 0 0126 20zM6 21V20H4v1a7 7 0 007 7h3V26H11A5 5 0 016 21zM19 10H26V12H19zM19 6H29V8H19zM19 2H29V4H19zM11 11H5a3 3 0 00-3 3v2H4V14a1 1 0 011-1h6a1 1 0 011 1v2h2V14A3 3 0 0011 11zM8 10A4 4 0 104 6 4 4 0 008 10zM8 4A2 2 0 116 6 2 2 0 018 4z"></path></svg>Q &amp; A</div><div class="_2_FQZ"><div class="undefined"><div class="hr37V">Kim Martineau</div><div><time dateTime="2025-09-26T13:45:00.000Z">26 Sep 2025</time></div></div></div><ul class="XBsQU tIdYx H_THJ pFSjT _2bQNq sOAT8"><li class="vRTEX l8Im1 er023 EOn_C MtRtY"><a class="cds--tag cds--tag--operational EIIW1 Hm0oS cds--tag--green-10" id="tag-id-:R6ip9t6:" href="/artificial-intelligence"><span title="" class="" dir="auto"><span title="AI" class="cds--tag__label" dir="auto">AI</span></span></a></li><li class="vRTEX l8Im1 er023 EOn_C MtRtY"><a class="cds--tag cds--tag--operational EIIW1 Hm0oS cds--tag--green-10" id="tag-id-:Raip9t6:" href="/topics/fairness-accountability-transparency"><span title="" class="" dir="auto"><span title="Fairness, Accountability, Transparency" class="cds--tag__label" dir="auto">Fairness, Accountability, Transparency</span></span></a></li><li class="vRTEX l8Im1 er023 EOn_C MtRtY"><a class="cds--tag cds--tag--operational EIIW1 Hm0oS cds--tag--green-10" id="tag-id-:Reip9t6:" href="/topics/trustworthy-ai"><span title="" class="" dir="auto"><span title="Trustworthy AI" class="cds--tag__label" dir="auto">Trustworthy AI</span></span></a></li></ul></article></li><li class="ACYfb _8fTSM"><article class="sq5w6 Gs8Ke LPlid"><h3 class="_0PBYm"><a class="LSzQx" href="/blog/granite-hackerone-bug-bounty"><span class="_5fUZ">IBM further strengthens Granite for enterprise deployment with HackerOne</span><svg focusable="false" preserveAspectRatio="xMidYMid meet" fill="currentColor" width="32" height="32" viewBox="0 0 32 32" aria-hidden="true" class="_8CqAU" xmlns="http://www.w3.org/2000/svg"><path d="M18 6L16.57 7.393 24.15 15 4 15 4 17 24.15 17 16.57 24.573 18 26 28 16 18 6z"></path></svg></a></h3><div class="iRMH5"><span style="box-sizing:border-box;display:block;overflow:hidden;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0;position:absolute;top:0;left:0;bottom:0;right:0"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" decoding="async" data-nimg="fill" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%;object-fit:cover"/><noscript><img loading="lazy" decoding="async" data-nimg="fill" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%;object-fit:cover" sizes="(min-width: 99rem) calc((min(99rem, 100vw) - 33rem) * 0.25 + 6rem), (min-width: 82rem) calc((100vw - 32rem) * 0.25 + 6rem), (min-width: 66rem) calc((100vw - 32rem) * 0.5 + 14rem), (min-width: 42rem) calc((100vw - 16rem) * 0.5 + 6rem), calc((max(20rem, 100vw) - 6rem) * 1 + 6rem)" srcSet="https://research.ibm.com/_next/image?url=https%3A%2F%2Fresearch-website-prod-cms-uploads.s3.us.cloud-object-storage.appdomain.cloud%2Fblog_Art_hacker_One_1_8cfd5ce596.jpg&amp;w=640&amp;q=75 640w, https://research.ibm.com/_next/image?url=https%3A%2F%2Fresearch-website-prod-cms-uploads.s3.us.cloud-object-storage.appdomain.cloud%2Fblog_Art_hacker_One_1_8cfd5ce596.jpg&amp;w=750&amp;q=75 750w, https://research.ibm.com/_next/image?url=https%3A%2F%2Fresearch-website-prod-cms-uploads.s3.us.cloud-object-storage.appdomain.cloud%2Fblog_Art_hacker_One_1_8cfd5ce596.jpg&amp;w=828&amp;q=75 828w, https://research.ibm.com/_next/image?url=https%3A%2F%2Fresearch-website-prod-cms-uploads.s3.us.cloud-object-storage.appdomain.cloud%2Fblog_Art_hacker_One_1_8cfd5ce596.jpg&amp;w=1080&amp;q=75 1080w, https://research.ibm.com/_next/image?url=https%3A%2F%2Fresearch-website-prod-cms-uploads.s3.us.cloud-object-storage.appdomain.cloud%2Fblog_Art_hacker_One_1_8cfd5ce596.jpg&amp;w=1200&amp;q=75 1200w, https://research.ibm.com/_next/image?url=https%3A%2F%2Fresearch-website-prod-cms-uploads.s3.us.cloud-object-storage.appdomain.cloud%2Fblog_Art_hacker_One_1_8cfd5ce596.jpg&amp;w=1920&amp;q=75 1920w, https://research.ibm.com/_next/image?url=https%3A%2F%2Fresearch-website-prod-cms-uploads.s3.us.cloud-object-storage.appdomain.cloud%2Fblog_Art_hacker_One_1_8cfd5ce596.jpg&amp;w=2048&amp;q=75 2048w, https://research.ibm.com/_next/image?url=https%3A%2F%2Fresearch-website-prod-cms-uploads.s3.us.cloud-object-storage.appdomain.cloud%2Fblog_Art_hacker_One_1_8cfd5ce596.jpg&amp;w=3840&amp;q=75 3840w" src="https://research.ibm.com/_next/image?url=https%3A%2F%2Fresearch-website-prod-cms-uploads.s3.us.cloud-object-storage.appdomain.cloud%2Fblog_Art_hacker_One_1_8cfd5ce596.jpg&amp;w=3840&amp;q=75"/></noscript></span></div><div class="_5Cpod Wsf58"><svg focusable="false" preserveAspectRatio="xMidYMid meet" fill="currentColor" width="16" height="16" viewBox="0 0 32 32" aria-hidden="true" xmlns="http://www.w3.org/2000/svg"><path d="M19 10H26V12H19zM19 15H26V17H19zM19 20H26V22H19z"></path><path d="M28,5H4A2.002,2.002,0,0,0,2,7V25a2.0023,2.0023,0,0,0,2,2H28a2.0027,2.0027,0,0,0,2-2V7A2.0023,2.0023,0,0,0,28,5ZM4,7H15V25H4ZM17,25V7H28l.002,18Z"></path></svg>News</div><div class="_2_FQZ"><div class=""><div class="hr37V">Mike Murphy</div><div><time dateTime="2025-08-27T13:00:00.000Z">27 Aug 2025</time></div></div></div><ul class="XBsQU tIdYx H_THJ pFSjT _2bQNq sOAT8"><li class="vRTEX l8Im1 er023 EOn_C MtRtY"><a class="cds--tag cds--tag--operational EIIW1 Hm0oS cds--tag--green-10" id="tag-id-:R6kp9t6:" href="/topics/adversarial-robustness-and-privacy"><span title="" class="" dir="auto"><span title="Adversarial Robustness and Privacy" class="cds--tag__label" dir="auto">Adversarial Robustness and Privacy</span></span></a></li><li class="vRTEX l8Im1 er023 EOn_C MtRtY"><a class="cds--tag cds--tag--operational EIIW1 Hm0oS cds--tag--green-10" id="tag-id-:Rakp9t6:" href="/artificial-intelligence"><span title="" class="" dir="auto"><span title="AI" class="cds--tag__label" dir="auto">AI</span></span></a></li><li class="vRTEX l8Im1 er023 EOn_C MtRtY"><a class="cds--tag cds--tag--operational EIIW1 Hm0oS cds--tag--green-10" id="tag-id-:Rekp9t6:" href="/topics/data-and-ai-security"><span title="" class="" dir="auto"><span title="Data and AI Security" class="cds--tag__label" dir="auto">Data and AI Security</span></span></a></li></ul></article></li><li class="ACYfb _8fTSM"><article class="sq5w6 Gs8Ke LPlid"><h3 class="_0PBYm"><a class="LSzQx" href="/blog/debugging-LLMs-for-reliability"><span class="_5fUZ">Debugging LLMs to improve their credibility</span><svg focusable="false" preserveAspectRatio="xMidYMid meet" fill="currentColor" width="32" height="32" viewBox="0 0 32 32" aria-hidden="true" class="_8CqAU" xmlns="http://www.w3.org/2000/svg"><path d="M18 6L16.57 7.393 24.15 15 4 15 4 17 24.15 17 16.57 24.573 18 26 28 16 18 6z"></path></svg></a></h3><div class="iRMH5"><span style="box-sizing:border-box;display:block;overflow:hidden;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0;position:absolute;top:0;left:0;bottom:0;right:0"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" decoding="async" data-nimg="fill" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%;object-fit:cover"/><noscript><img loading="lazy" decoding="async" data-nimg="fill" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%;object-fit:cover" sizes="(min-width: 99rem) calc((min(99rem, 100vw) - 33rem) * 0.25 + 6rem), (min-width: 82rem) calc((100vw - 32rem) * 0.25 + 6rem), (min-width: 66rem) calc((100vw - 32rem) * 0.5 + 14rem), (min-width: 42rem) calc((100vw - 16rem) * 0.5 + 6rem), calc((max(20rem, 100vw) - 6rem) * 1 + 6rem)" srcSet="https://research.ibm.com/_next/image?url=https%3A%2F%2Fresearch-website-prod-cms-uploads.s3.us.cloud-object-storage.appdomain.cloud%2FDebugging_LL_Ms_V1_A_df6dc58b6e.jpg&amp;w=640&amp;q=75 640w, https://research.ibm.com/_next/image?url=https%3A%2F%2Fresearch-website-prod-cms-uploads.s3.us.cloud-object-storage.appdomain.cloud%2FDebugging_LL_Ms_V1_A_df6dc58b6e.jpg&amp;w=750&amp;q=75 750w, https://research.ibm.com/_next/image?url=https%3A%2F%2Fresearch-website-prod-cms-uploads.s3.us.cloud-object-storage.appdomain.cloud%2FDebugging_LL_Ms_V1_A_df6dc58b6e.jpg&amp;w=828&amp;q=75 828w, https://research.ibm.com/_next/image?url=https%3A%2F%2Fresearch-website-prod-cms-uploads.s3.us.cloud-object-storage.appdomain.cloud%2FDebugging_LL_Ms_V1_A_df6dc58b6e.jpg&amp;w=1080&amp;q=75 1080w, https://research.ibm.com/_next/image?url=https%3A%2F%2Fresearch-website-prod-cms-uploads.s3.us.cloud-object-storage.appdomain.cloud%2FDebugging_LL_Ms_V1_A_df6dc58b6e.jpg&amp;w=1200&amp;q=75 1200w, https://research.ibm.com/_next/image?url=https%3A%2F%2Fresearch-website-prod-cms-uploads.s3.us.cloud-object-storage.appdomain.cloud%2FDebugging_LL_Ms_V1_A_df6dc58b6e.jpg&amp;w=1920&amp;q=75 1920w, https://research.ibm.com/_next/image?url=https%3A%2F%2Fresearch-website-prod-cms-uploads.s3.us.cloud-object-storage.appdomain.cloud%2FDebugging_LL_Ms_V1_A_df6dc58b6e.jpg&amp;w=2048&amp;q=75 2048w, https://research.ibm.com/_next/image?url=https%3A%2F%2Fresearch-website-prod-cms-uploads.s3.us.cloud-object-storage.appdomain.cloud%2FDebugging_LL_Ms_V1_A_df6dc58b6e.jpg&amp;w=3840&amp;q=75 3840w" src="https://research.ibm.com/_next/image?url=https%3A%2F%2Fresearch-website-prod-cms-uploads.s3.us.cloud-object-storage.appdomain.cloud%2FDebugging_LL_Ms_V1_A_df6dc58b6e.jpg&amp;w=3840&amp;q=75"/></noscript></span></div><div class="_5Cpod Wsf58"><svg focusable="false" preserveAspectRatio="xMidYMid meet" fill="currentColor" width="16" height="16" viewBox="0 0 32 32" aria-hidden="true" xmlns="http://www.w3.org/2000/svg"><path d="M22 24H18V22h4V18h2v4A2.0021 2.0021 0 0122 24zM10 14H8V10a2.0022 2.0022 0 012-2h4v2H10z"></path><path d="M28,8H24V4a2.0023,2.0023,0,0,0-2-2H4A2.0023,2.0023,0,0,0,2,4V22a2.0023,2.0023,0,0,0,2,2H8v4a2.0023,2.0023,0,0,0,2,2H28a2.0023,2.0023,0,0,0,2-2V10A2.0023,2.0023,0,0,0,28,8Zm0,20H10V24h4V22H10V18H8v4H4V4H22V8H18v2h4v4h2V10h4Z"></path></svg>Research</div><div class="_2_FQZ"><div class=""><div class="hr37V">Kim Martineau</div><div><time dateTime="2025-07-30T14:30:00.000Z">30 Jul 2025</time></div></div></div><ul class="XBsQU tIdYx H_THJ pFSjT _2bQNq sOAT8"><li class="vRTEX l8Im1 er023 EOn_C MtRtY"><a class="cds--tag cds--tag--operational EIIW1 Hm0oS cds--tag--green-10" id="tag-id-:R6mp9t6:" href="/artificial-intelligence"><span title="" class="" dir="auto"><span title="AI" class="cds--tag__label" dir="auto">AI</span></span></a></li><li class="vRTEX l8Im1 er023 EOn_C MtRtY"><a class="cds--tag cds--tag--operational EIIW1 Hm0oS cds--tag--green-10" id="tag-id-:Ramp9t6:" href="/topics/explainable-ai"><span title="" class="" dir="auto"><span title="Explainable AI" class="cds--tag__label" dir="auto">Explainable AI</span></span></a></li><li class="vRTEX l8Im1 er023 EOn_C MtRtY"><a class="cds--tag cds--tag--operational EIIW1 Hm0oS cds--tag--green-10" id="tag-id-:Remp9t6:" href="/topics/generative-ai"><span title="" class="" dir="auto"><span title="Generative AI" class="cds--tag__label" dir="auto">Generative AI</span></span></a></li><li class="vRTEX l8Im1 er023 EOn_C MtRtY"><a class="cds--tag cds--tag--operational EIIW1 Hm0oS cds--tag--green-10" id="tag-id-:Rimp9t6:" href="/topics/natural-language-processing"><span title="" class="" dir="auto"><span title="Natural Language Processing" class="cds--tag__label" dir="auto">Natural Language Processing</span></span></a></li><li class="vRTEX l8Im1 er023 EOn_C MtRtY"><a class="cds--tag cds--tag--operational EIIW1 Hm0oS cds--tag--green-10" id="tag-id-:Rmmp9t6:" href="/artificial-intelligence"><span title="" class="" dir="auto"><span title="Open Source" class="cds--tag__label" dir="auto">Open Source</span></span></a></li><li class="vRTEX l8Im1 er023 EOn_C MtRtY"><a class="cds--tag cds--tag--operational EIIW1 Hm0oS cds--tag--green-10" id="tag-id-:Rqmp9t6:" href="/topics/trustworthy-ai"><span title="" class="" dir="auto"><span title="Trustworthy AI" class="cds--tag__label" dir="auto">Trustworthy AI</span></span></a></li><li class="vRTEX l8Im1 er023 EOn_C MtRtY"><a class="cds--tag cds--tag--operational EIIW1 Hm0oS cds--tag--green-10" id="tag-id-:Rump9t6:" href="/topics/trustworthy-generation"><span title="" class="" dir="auto"><span title="Trustworthy Generation" class="cds--tag__label" dir="auto">Trustworthy Generation</span></span></a></li></ul></article></li><li class="ACYfb _8fTSM"><article class="sq5w6 Gs8Ke LPlid"><h3 class="_0PBYm"><a class="LSzQx" href="/blog/kush-varshney-camera-man"><span class="_5fUZ">How IBM’s Kush Varshney became the face of the modern ‘camera man’ </span><svg focusable="false" preserveAspectRatio="xMidYMid meet" fill="currentColor" width="32" height="32" viewBox="0 0 32 32" aria-hidden="true" class="_8CqAU" xmlns="http://www.w3.org/2000/svg"><path d="M18 6L16.57 7.393 24.15 15 4 15 4 17 24.15 17 16.57 24.573 18 26 28 16 18 6z"></path></svg></a></h3><div class="iRMH5"><span style="box-sizing:border-box;display:block;overflow:hidden;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0;position:absolute;top:0;left:0;bottom:0;right:0"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" decoding="async" data-nimg="fill" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%;object-fit:cover"/><noscript><img loading="lazy" decoding="async" data-nimg="fill" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%;object-fit:cover" sizes="(min-width: 99rem) calc((min(99rem, 100vw) - 33rem) * 0.25 + 6rem), (min-width: 82rem) calc((100vw - 32rem) * 0.25 + 6rem), (min-width: 66rem) calc((100vw - 32rem) * 0.5 + 14rem), (min-width: 42rem) calc((100vw - 16rem) * 0.5 + 6rem), calc((max(20rem, 100vw) - 6rem) * 1 + 6rem)" srcSet="https://research.ibm.com/_next/image?url=https%3A%2F%2Fresearch-website-prod-cms-uploads.s3.us.cloud-object-storage.appdomain.cloud%2Fblog_Art_kush_inline_Image_62a370593c.jpg&amp;w=640&amp;q=75 640w, https://research.ibm.com/_next/image?url=https%3A%2F%2Fresearch-website-prod-cms-uploads.s3.us.cloud-object-storage.appdomain.cloud%2Fblog_Art_kush_inline_Image_62a370593c.jpg&amp;w=750&amp;q=75 750w, https://research.ibm.com/_next/image?url=https%3A%2F%2Fresearch-website-prod-cms-uploads.s3.us.cloud-object-storage.appdomain.cloud%2Fblog_Art_kush_inline_Image_62a370593c.jpg&amp;w=828&amp;q=75 828w, https://research.ibm.com/_next/image?url=https%3A%2F%2Fresearch-website-prod-cms-uploads.s3.us.cloud-object-storage.appdomain.cloud%2Fblog_Art_kush_inline_Image_62a370593c.jpg&amp;w=1080&amp;q=75 1080w, https://research.ibm.com/_next/image?url=https%3A%2F%2Fresearch-website-prod-cms-uploads.s3.us.cloud-object-storage.appdomain.cloud%2Fblog_Art_kush_inline_Image_62a370593c.jpg&amp;w=1200&amp;q=75 1200w, https://research.ibm.com/_next/image?url=https%3A%2F%2Fresearch-website-prod-cms-uploads.s3.us.cloud-object-storage.appdomain.cloud%2Fblog_Art_kush_inline_Image_62a370593c.jpg&amp;w=1920&amp;q=75 1920w, https://research.ibm.com/_next/image?url=https%3A%2F%2Fresearch-website-prod-cms-uploads.s3.us.cloud-object-storage.appdomain.cloud%2Fblog_Art_kush_inline_Image_62a370593c.jpg&amp;w=2048&amp;q=75 2048w, https://research.ibm.com/_next/image?url=https%3A%2F%2Fresearch-website-prod-cms-uploads.s3.us.cloud-object-storage.appdomain.cloud%2Fblog_Art_kush_inline_Image_62a370593c.jpg&amp;w=3840&amp;q=75 3840w" src="https://research.ibm.com/_next/image?url=https%3A%2F%2Fresearch-website-prod-cms-uploads.s3.us.cloud-object-storage.appdomain.cloud%2Fblog_Art_kush_inline_Image_62a370593c.jpg&amp;w=3840&amp;q=75"/></noscript></span></div><div class="_5Cpod Wsf58"><svg focusable="false" preserveAspectRatio="xMidYMid meet" fill="currentColor" width="16" height="16" viewBox="0 0 32 32" aria-hidden="true" xmlns="http://www.w3.org/2000/svg"><path d="M27 25H21a3 3 0 00-3 3v2h2V28a1 1 0 011-1h6a1 1 0 011 1v2h2V28A3 3 0 0027 25zM20 20a4 4 0 104-4A4 4 0 0020 20zm6 0a2 2 0 11-2-2A2 2 0 0126 20zM6 21V20H4v1a7 7 0 007 7h3V26H11A5 5 0 016 21zM19 10H26V12H19zM19 6H29V8H19zM19 2H29V4H19zM11 11H5a3 3 0 00-3 3v2H4V14a1 1 0 011-1h6a1 1 0 011 1v2h2V14A3 3 0 0011 11zM8 10A4 4 0 104 6 4 4 0 008 10zM8 4A2 2 0 116 6 2 2 0 018 4z"></path></svg>Q &amp; A</div><div class="_2_FQZ"><div class=""><div class="hr37V">Kim Martineau</div><div><time dateTime="2025-07-21T12:00:00.000Z">21 Jul 2025</time></div></div></div><ul class="XBsQU tIdYx H_THJ pFSjT _2bQNq sOAT8"><li class="vRTEX l8Im1 er023 EOn_C MtRtY"><a class="cds--tag cds--tag--operational EIIW1 Hm0oS cds--tag--green-10" id="tag-id-:R6op9t6:" href="/artificial-intelligence"><span title="" class="" dir="auto"><span title="AI" class="cds--tag__label" dir="auto">AI</span></span></a></li><li class="vRTEX l8Im1 er023 EOn_C MtRtY"><a class="cds--tag cds--tag--operational EIIW1 Hm0oS cds--tag--green-10" id="tag-id-:Raop9t6:" href="/topics/generative-ai"><span title="" class="" dir="auto"><span title="Generative AI" class="cds--tag__label" dir="auto">Generative AI</span></span></a></li><li class="vRTEX l8Im1 er023 EOn_C MtRtY"><a class="cds--tag cds--tag--operational EIIW1 Hm0oS cds--tag--green-10" id="tag-id-:Reop9t6:" href="/topics/trustworthy-ai"><span title="" class="" dir="auto"><span title="Trustworthy AI" class="cds--tag__label" dir="auto">Trustworthy AI</span></span></a></li></ul></article></li><li class="ACYfb _8fTSM"><article class="sq5w6 Gs8Ke LPlid"><h3 class="_0PBYm"><a class="LSzQx" href="/blog/AI-agent-benchmarks"><span class="_5fUZ">A 360 review of AI agent benchmarks</span><svg focusable="false" preserveAspectRatio="xMidYMid meet" fill="currentColor" width="32" height="32" viewBox="0 0 32 32" aria-hidden="true" class="_8CqAU" xmlns="http://www.w3.org/2000/svg"><path d="M18 6L16.57 7.393 24.15 15 4 15 4 17 24.15 17 16.57 24.573 18 26 28 16 18 6z"></path></svg></a></h3><div class="iRMH5"><span style="box-sizing:border-box;display:block;overflow:hidden;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0;position:absolute;top:0;left:0;bottom:0;right:0"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" decoding="async" data-nimg="fill" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%;object-fit:cover"/><noscript><img loading="lazy" decoding="async" data-nimg="fill" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%;object-fit:cover" sizes="(min-width: 99rem) calc((min(99rem, 100vw) - 33rem) * 0.25 + 6rem), (min-width: 82rem) calc((100vw - 32rem) * 0.25 + 6rem), (min-width: 66rem) calc((100vw - 32rem) * 0.5 + 14rem), (min-width: 42rem) calc((100vw - 16rem) * 0.5 + 6rem), calc((max(20rem, 100vw) - 6rem) * 1 + 6rem)" srcSet="https://research.ibm.com/_next/image?url=https%3A%2F%2Fresearch-website-prod-cms-uploads.s3.us.cloud-object-storage.appdomain.cloud%2Fevaluating_The_Evaluation_Of_AI_Agents_blue_f5616df572.jpg&amp;w=640&amp;q=75 640w, https://research.ibm.com/_next/image?url=https%3A%2F%2Fresearch-website-prod-cms-uploads.s3.us.cloud-object-storage.appdomain.cloud%2Fevaluating_The_Evaluation_Of_AI_Agents_blue_f5616df572.jpg&amp;w=750&amp;q=75 750w, https://research.ibm.com/_next/image?url=https%3A%2F%2Fresearch-website-prod-cms-uploads.s3.us.cloud-object-storage.appdomain.cloud%2Fevaluating_The_Evaluation_Of_AI_Agents_blue_f5616df572.jpg&amp;w=828&amp;q=75 828w, https://research.ibm.com/_next/image?url=https%3A%2F%2Fresearch-website-prod-cms-uploads.s3.us.cloud-object-storage.appdomain.cloud%2Fevaluating_The_Evaluation_Of_AI_Agents_blue_f5616df572.jpg&amp;w=1080&amp;q=75 1080w, https://research.ibm.com/_next/image?url=https%3A%2F%2Fresearch-website-prod-cms-uploads.s3.us.cloud-object-storage.appdomain.cloud%2Fevaluating_The_Evaluation_Of_AI_Agents_blue_f5616df572.jpg&amp;w=1200&amp;q=75 1200w, https://research.ibm.com/_next/image?url=https%3A%2F%2Fresearch-website-prod-cms-uploads.s3.us.cloud-object-storage.appdomain.cloud%2Fevaluating_The_Evaluation_Of_AI_Agents_blue_f5616df572.jpg&amp;w=1920&amp;q=75 1920w, https://research.ibm.com/_next/image?url=https%3A%2F%2Fresearch-website-prod-cms-uploads.s3.us.cloud-object-storage.appdomain.cloud%2Fevaluating_The_Evaluation_Of_AI_Agents_blue_f5616df572.jpg&amp;w=2048&amp;q=75 2048w, https://research.ibm.com/_next/image?url=https%3A%2F%2Fresearch-website-prod-cms-uploads.s3.us.cloud-object-storage.appdomain.cloud%2Fevaluating_The_Evaluation_Of_AI_Agents_blue_f5616df572.jpg&amp;w=3840&amp;q=75 3840w" src="https://research.ibm.com/_next/image?url=https%3A%2F%2Fresearch-website-prod-cms-uploads.s3.us.cloud-object-storage.appdomain.cloud%2Fevaluating_The_Evaluation_Of_AI_Agents_blue_f5616df572.jpg&amp;w=3840&amp;q=75"/></noscript></span></div><div class="_5Cpod Wsf58"><svg focusable="false" preserveAspectRatio="xMidYMid meet" fill="currentColor" width="16" height="16" viewBox="0 0 32 32" aria-hidden="true" xmlns="http://www.w3.org/2000/svg"><path d="M22 24H18V22h4V18h2v4A2.0021 2.0021 0 0122 24zM10 14H8V10a2.0022 2.0022 0 012-2h4v2H10z"></path><path d="M28,8H24V4a2.0023,2.0023,0,0,0-2-2H4A2.0023,2.0023,0,0,0,2,4V22a2.0023,2.0023,0,0,0,2,2H8v4a2.0023,2.0023,0,0,0,2,2H28a2.0023,2.0023,0,0,0,2-2V10A2.0023,2.0023,0,0,0,28,8Zm0,20H10V24h4V22H10V18H8v4H4V4H22V8H18v2h4v4h2V10h4Z"></path></svg>Research</div><div class="_2_FQZ"><div class=""><div class="hr37V">Kim Martineau</div><div><time dateTime="2025-06-04T13:30:00.000Z">04 Jun 2025</time></div></div></div><ul class="XBsQU tIdYx H_THJ pFSjT _2bQNq sOAT8"><li class="vRTEX l8Im1 er023 EOn_C MtRtY"><a class="cds--tag cds--tag--operational EIIW1 Hm0oS cds--tag--green-10" id="tag-id-:R6qp9t6:" href="/artificial-intelligence"><span title="" class="" dir="auto"><span title="AI" class="cds--tag__label" dir="auto">AI</span></span></a></li><li class="vRTEX l8Im1 er023 EOn_C MtRtY"><a class="cds--tag cds--tag--operational EIIW1 Hm0oS cds--tag--green-10" id="tag-id-:Raqp9t6:" href="/topics/generative-ai"><span title="" class="" dir="auto"><span title="Generative AI" class="cds--tag__label" dir="auto">Generative AI</span></span></a></li><li class="vRTEX l8Im1 er023 EOn_C MtRtY"><a class="cds--tag cds--tag--operational EIIW1 Hm0oS cds--tag--green-10" id="tag-id-:Reqp9t6:" href="/topics/natural-language-processing"><span title="" class="" dir="auto"><span title="Natural Language Processing" class="cds--tag__label" dir="auto">Natural Language Processing</span></span></a></li><li class="vRTEX l8Im1 er023 EOn_C MtRtY"><a class="cds--tag cds--tag--operational EIIW1 Hm0oS cds--tag--green-10" id="tag-id-:Riqp9t6:" href="/topics/trustworthy-generation"><span title="" class="" dir="auto"><span title="Trustworthy Generation" class="cds--tag__label" dir="auto">Trustworthy Generation</span></span></a></li></ul></article></li><li class="ACYfb _8fTSM"><article class="sq5w6 Gs8Ke LPlid undefined"><h3 class="_0PBYm"><a class="LSzQx _61H0o XvVJd" href="/blog/tabular-data-watermark"><span class="_5fUZ">An invisible watermark to keep tabs on tabular data</span><svg focusable="false" preserveAspectRatio="xMidYMid meet" fill="currentColor" width="32" height="32" viewBox="0 0 32 32" aria-hidden="true" class="_8CqAU" xmlns="http://www.w3.org/2000/svg"><path d="M18 6L16.57 7.393 24.15 15 4 15 4 17 24.15 17 16.57 24.573 18 26 28 16 18 6z"></path></svg></a></h3><div class="_5Cpod Wsf58"><svg focusable="false" preserveAspectRatio="xMidYMid meet" fill="currentColor" width="16" height="16" viewBox="0 0 32 32" aria-hidden="true" xmlns="http://www.w3.org/2000/svg"><path d="M22 24H18V22h4V18h2v4A2.0021 2.0021 0 0122 24zM10 14H8V10a2.0022 2.0022 0 012-2h4v2H10z"></path><path d="M28,8H24V4a2.0023,2.0023,0,0,0-2-2H4A2.0023,2.0023,0,0,0,2,4V22a2.0023,2.0023,0,0,0,2,2H8v4a2.0023,2.0023,0,0,0,2,2H28a2.0023,2.0023,0,0,0,2-2V10A2.0023,2.0023,0,0,0,28,8Zm0,20H10V24h4V22H10V18H8v4H4V4H22V8H18v2h4v4h2V10h4Z"></path></svg>Research</div><div class="_2_FQZ"><div class="undefined"><div class="hr37V">Kim Martineau</div><div><time dateTime="2025-05-19T10:00:00.000Z">19 May 2025</time></div></div></div><ul class="XBsQU tIdYx H_THJ pFSjT _2bQNq sOAT8"><li class="vRTEX l8Im1 er023 EOn_C MtRtY"><a class="cds--tag cds--tag--operational EIIW1 Hm0oS cds--tag--green-10" id="tag-id-:R6sp9t6:" href="/topics/adversarial-robustness-and-privacy"><span title="" class="" dir="auto"><span title="Adversarial Robustness and Privacy" class="cds--tag__label" dir="auto">Adversarial Robustness and Privacy</span></span></a></li><li class="vRTEX l8Im1 er023 EOn_C MtRtY"><a class="cds--tag cds--tag--operational EIIW1 Hm0oS cds--tag--green-10" id="tag-id-:Rasp9t6:" href="/artificial-intelligence"><span title="" class="" dir="auto"><span title="AI" class="cds--tag__label" dir="auto">AI</span></span></a></li><li class="vRTEX l8Im1 er023 EOn_C MtRtY"><a class="cds--tag cds--tag--operational EIIW1 Hm0oS cds--tag--green-10" id="tag-id-:Resp9t6:" href="/topics/generative-ai"><span title="" class="" dir="auto"><span title="Generative AI" class="cds--tag__label" dir="auto">Generative AI</span></span></a></li><li class="vRTEX l8Im1 er023 EOn_C MtRtY"><a class="cds--tag cds--tag--operational EIIW1 Hm0oS cds--tag--green-10" id="tag-id-:Risp9t6:" href="/topics/trustworthy-generation"><span title="" class="" dir="auto"><span title="Trustworthy Generation" class="cds--tag__label" dir="auto">Trustworthy Generation</span></span></a></li></ul></article></li><li class="ACYfb"><a class="_5iWUO sq5w6" href="/blog?tag=trustworthy-ai"><span>See more of our work on Trustworthy AI</span><svg focusable="false" preserveAspectRatio="xMidYMid meet" fill="currentColor" width="32" height="32" viewBox="0 0 32 32" aria-hidden="true" class="vEddu" xmlns="http://www.w3.org/2000/svg"><path d="M18 6L16.57 7.393 24.15 15 4 15 4 17 24.15 17 16.57 24.573 18 26 28 16 18 6z"></path></svg></a></li></ul></div></section><section class="cOi2S"><div class="HjjpU"><h2 class="_3uWl9" id="topics">Topics</h2><ul class="H99ob"><li class="P7iSy"><article class="N4G3l"><h3 class="YQwC6"><a class="f5NI8" href="/topics/ai-testing">AI Testing</a></h3><div class="xHVJx">We’re designing tools to help ensure that AI systems are trustworthy, reliable and can optimize business processes. </div><div class="NUKjD"><div class="RqPPz"><div><svg focusable="false" preserveAspectRatio="xMidYMid meet" fill="currentColor" width="24" height="24" viewBox="0 0 24 24" aria-hidden="true" xmlns="http://www.w3.org/2000/svg"><path d="M14 4L12.9 5.1 18.9 11.2 2 11.2 2 12.8 18.9 12.8 12.9 18.9 14 20 22 12z"></path></svg></div></div></div></article></li><li class="P7iSy"><article class="N4G3l"><h3 class="YQwC6"><a class="f5NI8" href="/topics/adversarial-robustness-and-privacy">Adversarial Robustness and Privacy</a></h3><div class="xHVJx">We’re making tools to protect AI and certify its robustness, and helping AI systems adhere to privacy requirements.</div><div class="NUKjD"><div class="RqPPz"><div><svg focusable="false" preserveAspectRatio="xMidYMid meet" fill="currentColor" width="24" height="24" viewBox="0 0 24 24" aria-hidden="true" xmlns="http://www.w3.org/2000/svg"><path d="M14 4L12.9 5.1 18.9 11.2 2 11.2 2 12.8 18.9 12.8 12.9 18.9 14 20 22 12z"></path></svg></div></div></div></article></li><li class="P7iSy"><article class="N4G3l"><h3 class="YQwC6"><a class="f5NI8" href="/topics/explainable-ai">Explainable AI</a></h3><div class="xHVJx">We’re creating tools to help AI systems explain why they made the decisions they did. </div><div class="NUKjD"><div class="RqPPz"><div><svg focusable="false" preserveAspectRatio="xMidYMid meet" fill="currentColor" width="24" height="24" viewBox="0 0 24 24" aria-hidden="true" xmlns="http://www.w3.org/2000/svg"><path d="M14 4L12.9 5.1 18.9 11.2 2 11.2 2 12.8 18.9 12.8 12.9 18.9 14 20 22 12z"></path></svg></div></div></div></article></li><li class="P7iSy"><article class="N4G3l"><h3 class="YQwC6"><a class="f5NI8" href="/topics/fairness-accountability-transparency">Fairness, Accountability, Transparency</a></h3><div class="xHVJx">We’re developing technologies to increase the end-to-end transparency and fairness of AI systems. </div><div class="NUKjD"><div class="RqPPz"><div><svg focusable="false" preserveAspectRatio="xMidYMid meet" fill="currentColor" width="24" height="24" viewBox="0 0 24 24" aria-hidden="true" xmlns="http://www.w3.org/2000/svg"><path d="M14 4L12.9 5.1 18.9 11.2 2 11.2 2 12.8 18.9 12.8 12.9 18.9 14 20 22 12z"></path></svg></div></div></div></article></li><li class="P7iSy"><article class="N4G3l"><h3 class="YQwC6"><a class="f5NI8" href="/topics/trustworthy-generation">Trustworthy Generation</a></h3><div class="xHVJx">We’re developing theoretical and algorithmic frameworks for generative AI to accelerate future scientific discoveries. </div><div class="NUKjD"><div class="RqPPz"><div><svg focusable="false" preserveAspectRatio="xMidYMid meet" fill="currentColor" width="24" height="24" viewBox="0 0 24 24" aria-hidden="true" xmlns="http://www.w3.org/2000/svg"><path d="M14 4L12.9 5.1 18.9 11.2 2 11.2 2 12.8 18.9 12.8 12.9 18.9 14 20 22 12z"></path></svg></div></div></div></article></li><li class="P7iSy"><article class="N4G3l"><h3 class="YQwC6"><a class="f5NI8" href="/topics/uncertainty-quantification">Uncertainty Quantification</a></h3><div class="xHVJx">We’re developing ways for AI to communicate when it&#x27;s unsure of a decision across the AI application development lifecycle.</div><div class="NUKjD"><div class="RqPPz"><div><svg focusable="false" preserveAspectRatio="xMidYMid meet" fill="currentColor" width="24" height="24" viewBox="0 0 24 24" aria-hidden="true" xmlns="http://www.w3.org/2000/svg"><path d="M14 4L12.9 5.1 18.9 11.2 2 11.2 2 12.8 18.9 12.8 12.9 18.9 14 20 22 12z"></path></svg></div></div></div></article></li></ul></div></section><section class="cOi2S"><div class="R5PeJ"><h2 class="SH63y" id="publications">Publications</h2><ul><li class="Ub1uU"><article class="PpV0z _4VvpY ywX4g"><h3 class="d0naJ jjmX_"><a class="_1kOEn" href="/publications/agentic-process-observability-discovering-behavioral-variability">Agentic Process Observability: Discovering Behavioral Variability</a></h3><ul class="VgSQD"><li class="EEJK9"><ul class="_8ydsY"><li class="_8MUnA"><a class="WvbPS" href="/people/fabiana-fournier">Fabiana Fournier</a></li><li class="_8MUnA"><a class="WvbPS" href="/people/lior-limonad">Lior Limonad</a></li><li class="_8MUnA">et al.</li></ul></li><li class="EEJK9">2025</li><li class="EEJK9 XuP8p">ECAI 2025</li></ul><div class="FN4MA"><svg focusable="false" preserveAspectRatio="xMidYMid meet" fill="currentColor" width="24" height="24" viewBox="0 0 24 24" aria-hidden="true" class="UI7HX QQGoa" xmlns="http://www.w3.org/2000/svg"><path d="M14 4L12.9 5.1 18.9 11.2 2 11.2 2 12.8 18.9 12.8 12.9 18.9 14 20 22 12z"></path></svg></div></article></li><li class="Ub1uU"><article class="PpV0z _4VvpY ywX4g"><h3 class="d0naJ jjmX_"><a class="_1kOEn" href="/publications/xabps-towards-explainable-autonomous-business-processes">XABPs: Towards eXplainable Autonomous Business Processes</a></h3><ul class="VgSQD"><li class="EEJK9"><ul class="_8ydsY"><li class="_8MUnA">Peter Fettke</li><li class="_8MUnA"><a class="WvbPS" href="/people/fabiana-fournier">Fabiana Fournier</a></li><li class="_8MUnA">et al.</li></ul></li><li class="EEJK9">2025</li><li class="EEJK9 XuP8p">ECAI 2025</li></ul><div class="FN4MA"><svg focusable="false" preserveAspectRatio="xMidYMid meet" fill="currentColor" width="24" height="24" viewBox="0 0 24 24" aria-hidden="true" class="UI7HX QQGoa" xmlns="http://www.w3.org/2000/svg"><path d="M14 4L12.9 5.1 18.9 11.2 2 11.2 2 12.8 18.9 12.8 12.9 18.9 14 20 22 12z"></path></svg></div></article></li><li class="Ub1uU"><article class="PpV0z _4VvpY ywX4g"><h3 class="d0naJ jjmX_"><a class="_1kOEn" href="/publications/when-in-doubt-cascade-towards-building-efficient-and-capable-guardrails">When in Doubt, Cascade: Towards Building Efficient and Capable Guardrails</a></h3><ul class="VgSQD"><li class="EEJK9"><ul class="_8ydsY"><li class="_8MUnA"><a class="WvbPS" href="/people/manish-nagireddy">Manish Nagireddy</a></li><li class="_8MUnA"><a class="WvbPS" href="/people/inkit-padhi">Inkit Padhi</a></li><li class="_8MUnA">et al.</li></ul></li><li class="EEJK9">2025</li><li class="EEJK9 XuP8p">AIES 2025</li></ul><div class="FN4MA"><svg focusable="false" preserveAspectRatio="xMidYMid meet" fill="currentColor" width="24" height="24" viewBox="0 0 24 24" aria-hidden="true" class="UI7HX QQGoa" xmlns="http://www.w3.org/2000/svg"><path d="M14 4L12.9 5.1 18.9 11.2 2 11.2 2 12.8 18.9 12.8 12.9 18.9 14 20 22 12z"></path></svg></div></article></li><li class="Ub1uU"><article class="PpV0z _4VvpY ywX4g"><h3 class="d0naJ jjmX_"><a class="_1kOEn" href="/publications/highlight-all-the-phrases-enhancing-llm-transparency-through-visual-factuality-indicators">Highlight All the Phrases: Enhancing LLM Transparency through Visual Factuality Indicators</a></h3><ul class="VgSQD"><li class="EEJK9"><ul class="_8ydsY"><li class="_8MUnA"><a class="WvbPS" href="/people/hyo-jin-do">Hyo Jin Do</a></li><li class="_8MUnA"><a class="WvbPS" href="/people/rachel-ostrand">Rachel Ostrand</a></li><li class="_8MUnA">et al.</li></ul></li><li class="EEJK9">2025</li><li class="EEJK9 XuP8p">AIES 2025</li></ul><div class="FN4MA"><svg focusable="false" preserveAspectRatio="xMidYMid meet" fill="currentColor" width="24" height="24" viewBox="0 0 24 24" aria-hidden="true" class="UI7HX QQGoa" xmlns="http://www.w3.org/2000/svg"><path d="M14 4L12.9 5.1 18.9 11.2 2 11.2 2 12.8 18.9 12.8 12.9 18.9 14 20 22 12z"></path></svg></div></article></li><li class="Ub1uU"><article class="PpV0z _4VvpY ywX4g"><h3 class="d0naJ jjmX_"><a class="_1kOEn" href="/publications/localizing-persona-representations-in-llms">Localizing Persona Representations in LLMs</a></h3><ul class="VgSQD"><li class="EEJK9"><ul class="_8ydsY"><li class="_8MUnA"><a class="WvbPS" href="/people/celia-cintas">Celia Cintas</a></li><li class="_8MUnA"><a class="WvbPS" href="/people/miriam-rateike">Miriam Rateike</a></li><li class="_8MUnA">et al.</li></ul></li><li class="EEJK9">2025</li><li class="EEJK9 XuP8p">AIES 2025</li></ul><div class="FN4MA"><svg focusable="false" preserveAspectRatio="xMidYMid meet" fill="currentColor" width="24" height="24" viewBox="0 0 24 24" aria-hidden="true" class="UI7HX QQGoa" xmlns="http://www.w3.org/2000/svg"><path d="M14 4L12.9 5.1 18.9 11.2 2 11.2 2 12.8 18.9 12.8 12.9 18.9 14 20 22 12z"></path></svg></div></article></li><li class="Ub1uU"><article class="PpV0z _4VvpY ywX4g"><h3 class="d0naJ jjmX_"><a class="_1kOEn" href="/publications/exposing-ai-bias-by-crowdsourcing-democratizing-critique-of-large-language-models">Exposing AI Bias by Crowdsourcing: Democratizing Critique of Large Language Models</a></h3><ul class="VgSQD"><li class="EEJK9"><ul class="_8ydsY"><li class="_8MUnA">Hangzhi Guo</li><li class="_8MUnA">Pranav Venkit</li><li class="_8MUnA">et al.</li></ul></li><li class="EEJK9">2025</li><li class="EEJK9 XuP8p">AIES 2025</li></ul><div class="FN4MA"><svg focusable="false" preserveAspectRatio="xMidYMid meet" fill="currentColor" width="24" height="24" viewBox="0 0 24 24" aria-hidden="true" class="UI7HX QQGoa" xmlns="http://www.w3.org/2000/svg"><path d="M14 4L12.9 5.1 18.9 11.2 2 11.2 2 12.8 18.9 12.8 12.9 18.9 14 20 22 12z"></path></svg></div></article></li></ul><div class="_8CPQE"><a class="bbALT fFsRH ZXePn aFohb cds--btn cds--btn--secondary" type="button" href="/publications?tag=trustworthy-ai">View all publications<svg focusable="false" preserveAspectRatio="xMidYMid meet" fill="currentColor" aria-hidden="true" width="16" height="16" viewBox="0 0 16 16" class="cds--btn__icon" xmlns="http://www.w3.org/2000/svg"><path d="M9.3 3.7L13.1 7.5 1 7.5 1 8.5 13.1 8.5 9.3 12.3 10 13 15 8 10 3z"></path></svg></a></div></div></section><div class="cOi2S"><div class="oUCnb"><article class="pNrSC pasLV"><div class="Q255D DEk9x LEHef _5o4qg"><div class="FMJU_"><span style="box-sizing:border-box;display:block;overflow:hidden;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0;position:absolute;top:0;left:0;bottom:0;right:0"><img alt="" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" decoding="async" data-nimg="fill" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%;object-fit:cover"/><noscript><img alt="" loading="lazy" decoding="async" data-nimg="fill" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%;object-fit:cover" sizes="(min-width: 99rem) calc((min(99rem, 100vw) - 33rem) * 0.375 + 10rem), (min-width: 82rem) calc((100vw - 32rem) * 0.375 + 10rem), (min-width: 66rem) calc((100vw - 32rem) * 0.375 + 10rem), (min-width: 42rem) calc((100vw - 16rem) * 0.5 + 6rem), 100vw" srcSet="https://research.ibm.com/_next/image?url=https%3A%2F%2Fresearch-website-prod-cms-uploads.s3.us.cloud-object-storage.appdomain.cloud%2Ftrustworthy_ai_f0b247c9d2.png&amp;w=640&amp;q=85 640w, https://research.ibm.com/_next/image?url=https%3A%2F%2Fresearch-website-prod-cms-uploads.s3.us.cloud-object-storage.appdomain.cloud%2Ftrustworthy_ai_f0b247c9d2.png&amp;w=750&amp;q=85 750w, https://research.ibm.com/_next/image?url=https%3A%2F%2Fresearch-website-prod-cms-uploads.s3.us.cloud-object-storage.appdomain.cloud%2Ftrustworthy_ai_f0b247c9d2.png&amp;w=828&amp;q=85 828w, https://research.ibm.com/_next/image?url=https%3A%2F%2Fresearch-website-prod-cms-uploads.s3.us.cloud-object-storage.appdomain.cloud%2Ftrustworthy_ai_f0b247c9d2.png&amp;w=1080&amp;q=85 1080w, https://research.ibm.com/_next/image?url=https%3A%2F%2Fresearch-website-prod-cms-uploads.s3.us.cloud-object-storage.appdomain.cloud%2Ftrustworthy_ai_f0b247c9d2.png&amp;w=1200&amp;q=85 1200w, https://research.ibm.com/_next/image?url=https%3A%2F%2Fresearch-website-prod-cms-uploads.s3.us.cloud-object-storage.appdomain.cloud%2Ftrustworthy_ai_f0b247c9d2.png&amp;w=1920&amp;q=85 1920w, https://research.ibm.com/_next/image?url=https%3A%2F%2Fresearch-website-prod-cms-uploads.s3.us.cloud-object-storage.appdomain.cloud%2Ftrustworthy_ai_f0b247c9d2.png&amp;w=2048&amp;q=85 2048w, https://research.ibm.com/_next/image?url=https%3A%2F%2Fresearch-website-prod-cms-uploads.s3.us.cloud-object-storage.appdomain.cloud%2Ftrustworthy_ai_f0b247c9d2.png&amp;w=3840&amp;q=85 3840w" src="https://research.ibm.com/_next/image?url=https%3A%2F%2Fresearch-website-prod-cms-uploads.s3.us.cloud-object-storage.appdomain.cloud%2Ftrustworthy_ai_f0b247c9d2.png&amp;w=3840&amp;q=85"/></noscript></span></div></div><div class="Q255D _40_fF qKE4t n_hMm"><div class="xvgCt"><h2 class="GrZa4"><a class="_5BPdc" href="https://www.ibm.com/watson/trustworthy-ai">Building trustworthy AI with Watson</a></h2><div class="zUqxz"><p class="LAkhf" style="--line-clamp:none">Our research is regularly integrated into Watson solutions to make IBM’s AI for business more transparent, explainable, robust, private, and fair.</p></div><div class="XIRD4"><span class="zrxh4">Learn more</span><svg focusable="false" preserveAspectRatio="xMidYMid meet" fill="currentColor" width="32" height="32" viewBox="0 0 32 32" aria-hidden="true" class="_fwOD" xmlns="http://www.w3.org/2000/svg"><path d="M18 6L16.57 7.393 24.15 15 4 15 4 17 24.15 17 16.57 24.573 18 26 28 16 18 6z"></path></svg></div></div></div></article></div></div></div><aside class="kgkLd _42woM" style="--row:1"><section class="k1mVB B3K0w"><nav aria-label="breadcrumbs" class="ZdBXE"><ol><li><a class="cds--link" href="/">Home</a></li><li><span class="bIiLY" aria-hidden="true">↳<!-- --> </span><a class="cds--link" href="/artificial-intelligence">AI</a></li><li><span class="bIiLY" aria-hidden="true">↳<!-- --> </span><a aria-current="location" href="/topics/trustworthy-ai">Trustworthy AI</a></li></ol></nav></section><div class="cds--form-item OgtcX"><div class="cds--select"><label for="" class="cds--label" dir="auto"></label><div class="cds--select-input__wrapper"><select id="" class="cds--select-input cds--select-input--lg" title="Overview"><option class="cds--select-option" value="main-content" selected="">Overview</option><option class="cds--select-option" value="our-work">Our work</option><option class="cds--select-option" value="topics">Topics</option><option class="cds--select-option" value="publications">Publications</option></select><svg focusable="false" preserveAspectRatio="xMidYMid meet" fill="currentColor" width="16" height="16" viewBox="0 0 16 16" aria-hidden="true" class="cds--select__arrow" xmlns="http://www.w3.org/2000/svg"><path d="M8 11L3 6 3.7 5.3 8 9.6 12.3 5.3 13 6z"></path></svg></div></div></div><nav class="jVbkJ KouHr Ps0cN"><ul><li><a class="irm9g zAVK_" href="#main-content">Overview</a></li><li><a class="irm9g" href="#our-work">Our work</a></li><li><a class="irm9g" href="#topics">Topics</a></li><li><a class="irm9g" href="#publications">Publications</a></li></ul></nav></aside></div></main></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"initialApolloState":{"Tag:10":{"__typename":"Tag","id":"10","name":"Trustworthy AI","slug":"trustworthy-ai","tag_parent":{"__typename":"Tag","focus_area":{"__typename":"FocusArea","name":"AI","slug":"artificial-intelligence"}},"topic":{"__typename":"TeamCollection","slug":"trustworthy-ai"}},"ComponentCommonTextBlockWithLink:1":{"__typename":"ComponentCommonTextBlockWithLink","id":"1","title":"Overview","text":"Artificial intelligence systems have become increasingly prevalent in everyday life and enterprise settings, and they’re now often being used to support human decision-making. These systems have grown increasingly complex and efficient, and AI holds the promise of uncovering valuable insights across a wide range of applications. But broad adoption of AI systems will require humans to trust their output.\n\nWhen people understand how technology works, and we can assess that it’s safe and reliable, we’re far more inclined to trust it. Many AI systems to date have been black boxes, where data is fed in and results come out. To trust a decision made by an algorithm, we need to know that it is fair, that it’s reliable and can be accounted for, and that it will cause no harm. We need assurances that AI cannot be tampered with and that the system itself is secure. We need to be able to look inside AI systems, to understand the rationale behind the algorithmic outcome, and even ask it questions as to how it came to its decision.\n\nAt IBM Research, we’re working on a range of approaches to ensure that AI systems built in the future are fair, robust, explainable, account, and align with the values of the society they’re designed for. We’re ensuring that in the future, AI applications are as fair as they are efficient across their entire lifecycle.","link":null},"BlogCategory:42":{"__typename":"BlogCategory","id":"42","name":"Q \u0026 A","slug":"questions-and-answers"},"UploadFile:6920":{"__typename":"UploadFile","id":"6920","url":"https://research-website-prod-cms-uploads.s3.us.cloud-object-storage.appdomain.cloud/AI_Safety3_7944ed0a31.png","alternativeText":null,"width":1920,"height":1080},"Tag:5":{"__typename":"Tag","id":"5","name":"AI","topic":{"__typename":"FocusArea","slug":"artificial-intelligence"}},"Tag:11":{"__typename":"Tag","id":"11","name":"Fairness, Accountability, Transparency","topic":{"__typename":"Team","slug":"fairness-accountability-transparency"}},"BlogAuthor:889":{"__typename":"BlogAuthor","id":"889","name":"Kim Martineau"},"BlogPost:4507":{"__typename":"BlogPost","id":"4507","slug":"map-measure-manage-gen-ai","title":"In AI, alignment is the goal. Steerability is how you get there","blog_category":{"__ref":"BlogCategory:42"},"cover_image":{"__ref":"UploadFile:6920"},"publish_at":"2025-09-26T13:45:00.000Z","read_duration":null,"tags":[{"__ref":"Tag:5"},{"__ref":"Tag:10"},{"__ref":"Tag:11"}],"blog_authors":[{"__ref":"BlogAuthor:889"}]},"BlogCategory:1":{"__typename":"BlogCategory","id":"1","name":"News","slug":"news"},"UploadFile:6802":{"__typename":"UploadFile","id":"6802","url":"https://research-website-prod-cms-uploads.s3.us.cloud-object-storage.appdomain.cloud/blog_Art_hacker_One_1_8cfd5ce596.jpg","alternativeText":null,"width":640,"height":360},"Tag:35":{"__typename":"Tag","id":"35","name":"Data and AI Security","topic":{"__typename":"Team","slug":"data-and-ai-security"}},"Tag:13":{"__typename":"Tag","id":"13","name":"Adversarial Robustness and Privacy","topic":{"__typename":"Team","slug":"adversarial-robustness-and-privacy"}},"BlogAuthor:514":{"__typename":"BlogAuthor","id":"514","name":"Mike Murphy"},"BlogPost:4466":{"__typename":"BlogPost","id":"4466","slug":"granite-hackerone-bug-bounty","title":"IBM further strengthens Granite for enterprise deployment with HackerOne","blog_category":{"__ref":"BlogCategory:1"},"cover_image":{"__ref":"UploadFile:6802"},"publish_at":"2025-08-27T13:00:00.000Z","read_duration":null,"tags":[{"__ref":"Tag:5"},{"__ref":"Tag:35"},{"__ref":"Tag:13"}],"blog_authors":[{"__ref":"BlogAuthor:514"}]},"BlogCategory:2":{"__typename":"BlogCategory","id":"2","name":"Research","slug":"research"},"UploadFile:6735":{"__typename":"UploadFile","id":"6735","url":"https://research-website-prod-cms-uploads.s3.us.cloud-object-storage.appdomain.cloud/Debugging_LL_Ms_V1_A_df6dc58b6e.jpg","alternativeText":null,"width":2520,"height":1418},"Tag:9":{"__typename":"Tag","id":"9","name":"Explainable AI","topic":{"__typename":"Team","slug":"explainable-ai"}},"Tag:100":{"__typename":"Tag","id":"100","name":"Trustworthy Generation","topic":{"__typename":"Team","slug":"trustworthy-generation"}},"Tag:270":{"__typename":"Tag","id":"270","name":"Generative AI","topic":{"__typename":"Team","slug":"generative-ai"}},"Tag:25":{"__typename":"Tag","id":"25","name":"Natural Language Processing","topic":{"__typename":"Team","slug":"natural-language-processing"}},"Tag:341":{"__typename":"Tag","id":"341","name":"Open Source","topic":{"__typename":"FocusArea","slug":"artificial-intelligence"}},"BlogPost:4421":{"__typename":"BlogPost","id":"4421","slug":"debugging-LLMs-for-reliability","title":"Debugging LLMs to improve their credibility","blog_category":{"__ref":"BlogCategory:2"},"cover_image":{"__ref":"UploadFile:6735"},"publish_at":"2025-07-30T14:30:00.000Z","read_duration":null,"tags":[{"__ref":"Tag:10"},{"__ref":"Tag:9"},{"__ref":"Tag:100"},{"__ref":"Tag:270"},{"__ref":"Tag:25"},{"__ref":"Tag:5"},{"__ref":"Tag:341"}],"blog_authors":[{"__ref":"BlogAuthor:889"}]},"UploadFile:6693":{"__typename":"UploadFile","id":"6693","url":"https://research-website-prod-cms-uploads.s3.us.cloud-object-storage.appdomain.cloud/blog_Art_kush_inline_Image_62a370593c.jpg","alternativeText":null,"width":1920,"height":1000},"BlogPost:4380":{"__typename":"BlogPost","id":"4380","slug":"kush-varshney-camera-man","title":"How IBM’s Kush Varshney became the face of the modern ‘camera man’ ","blog_category":{"__ref":"BlogCategory:42"},"cover_image":{"__ref":"UploadFile:6693"},"publish_at":"2025-07-21T12:00:00.000Z","read_duration":null,"tags":[{"__ref":"Tag:5"},{"__ref":"Tag:270"},{"__ref":"Tag:10"}],"blog_authors":[{"__ref":"BlogAuthor:889"}]},"UploadFile:6478":{"__typename":"UploadFile","id":"6478","url":"https://research-website-prod-cms-uploads.s3.us.cloud-object-storage.appdomain.cloud/evaluating_The_Evaluation_Of_AI_Agents_blue_f5616df572.jpg","alternativeText":null,"width":1923,"height":1080},"BlogPost:4321":{"__typename":"BlogPost","id":"4321","slug":"AI-agent-benchmarks","title":"A 360 review of AI agent benchmarks","blog_category":{"__ref":"BlogCategory:2"},"cover_image":{"__ref":"UploadFile:6478"},"publish_at":"2025-06-04T13:30:00.000Z","read_duration":null,"tags":[{"__ref":"Tag:5"},{"__ref":"Tag:25"},{"__ref":"Tag:100"},{"__ref":"Tag:270"}],"blog_authors":[{"__ref":"BlogAuthor:889"}]},"UploadFile:6422":{"__typename":"UploadFile","id":"6422","url":"https://research-website-prod-cms-uploads.s3.us.cloud-object-storage.appdomain.cloud/blog_watermark2_b2c10cc23c.jpg","alternativeText":null,"width":3840,"height":2160},"BlogPost:4283":{"__typename":"BlogPost","id":"4283","slug":"tabular-data-watermark","title":"An invisible watermark to keep tabs on tabular data","blog_category":{"__ref":"BlogCategory:2"},"cover_image":{"__ref":"UploadFile:6422"},"publish_at":"2025-05-19T10:00:00.000Z","read_duration":null,"tags":[{"__ref":"Tag:5"},{"__ref":"Tag:270"},{"__ref":"Tag:13"},{"__ref":"Tag:100"}],"blog_authors":[{"__ref":"BlogAuthor:889"}]},"UploadFile:6352":{"__typename":"UploadFile","id":"6352","url":"https://research-website-prod-cms-uploads.s3.us.cloud-object-storage.appdomain.cloud/AI_credits_a1cbc7035b.png","alternativeText":null,"width":5040,"height":2836},"Tag:14":{"__typename":"Tag","id":"14","name":"AI Transparency","topic":{"__typename":"TeamCollection","slug":"trustworthy-ai"}},"BlogPost:4214":{"__typename":"BlogPost","id":"4214","slug":"AI-attribution-toolkit","title":"AI is changing how we work — is it time to change how we credit AI’s involvement?","blog_category":{"__ref":"BlogCategory:2"},"cover_image":{"__ref":"UploadFile:6352"},"publish_at":"2025-05-13T10:00:00.000Z","read_duration":null,"tags":[{"__ref":"Tag:5"},{"__ref":"Tag:25"},{"__ref":"Tag:270"},{"__ref":"Tag:14"}],"blog_authors":[{"__ref":"BlogAuthor:889"}]},"ComponentTeamBlogPosts:86":{"__typename":"ComponentTeamBlogPosts","id":"86","blog_posts({\"limit\":7})":{"__typename":"BlogPostsSearchConnection","nodes":[{"__ref":"BlogPost:4507"},{"__ref":"BlogPost:4466"},{"__ref":"BlogPost:4421"},{"__ref":"BlogPost:4380"},{"__ref":"BlogPost:4321"},{"__ref":"BlogPost:4283"},{"__ref":"BlogPost:4214"}],"totalCount":69}},"ComponentTeamTeamCardCollection:2":{"__typename":"ComponentTeamTeamCardCollection","id":"2","max_count":null},"Source:43":{"__typename":"Source","id":"43","longName":"European Conference on Artificial Intelligence","shortName":"ECAI","type":"CONFERENCE"},"SourceInstance:24116":{"__typename":"SourceInstance","id":"24116","name":"ECAI 2025"},"Ibmer:820":{"__typename":"Ibmer","id":"820","slug":"fabiana-fournier","displayName":"Fabiana Fournier"},"Author:42875":{"__typename":"Author","id":"42875","ibmer":{"__ref":"Ibmer:820"}},"AuthorName:35170":{"__typename":"AuthorName","id":"35170","firstName":"Fabiana","firstNameInitials":null,"lastName":"Fournier","author":{"__ref":"Author:42875"}},"Affiliation:73":{"__typename":"Affiliation","id":"73","isIbm":true},"Ibmer:1648":{"__typename":"Ibmer","id":"1648","slug":"lior-limonad","displayName":"Lior Limonad"},"Author:41402":{"__typename":"Author","id":"41402","ibmer":{"__ref":"Ibmer:1648"}},"AuthorName:34623":{"__typename":"AuthorName","id":"34623","firstName":"Lior","firstNameInitials":null,"lastName":"Limonad","author":{"__ref":"Author:41402"}},"Author:184505":{"__typename":"Author","id":"184505","ibmer":null},"AuthorName:212115":{"__typename":"AuthorName","id":"212115","firstName":"Yuval","firstNameInitials":null,"lastName":"David","author":{"__ref":"Author:184505"}},"Publication:159502":{"__typename":"Publication","id":"159502","slug":"agentic-process-observability-discovering-behavioral-variability","title":"Agentic Process Observability: Discovering Behavioral Variability","type":{"__typename":"PublicationType","displayValue":"Workshop paper"},"published":"2025-10-25","publishedMeta":{"__typename":"PublishedMeta","source":"ECAI 2025","year":"2025"},"abstract":"AI agents that leverage Large Language Models (LLMs) are increasingly becoming core building blocks\nof modern software systems. A wide range of frameworks is now available to support the specification\nof such applications. These frameworks enable the definition of agent setups using natural language\nprompting, which specifies the roles, goals, and tools assigned to the various agents involved. Within\nsuch setups, agent behavior is non-deterministic for any given input, highlighting the critical need\nfor robust debugging and observability tools. In this work, we explore the use of process and causal\ndiscovery applied to agent execution trajectories as a means of enhancing developer observability. This\napproach aids in monitoring and understanding the emergent variability in agent behavior. Additionally,\nwe complement this with LLM-based static analysis techniques to distinguish between intended and\nunintended behavioral variability. We argue that such instrumentation is essential for giving developers\ngreater control over evolving specifications and for identifying aspects of functionality that may require\nmore precise and explicit definitions.","linkCode":null,"source":{"__ref":"Source:43"},"sourceInstance":{"__ref":"SourceInstance:24116"},"authors":[{"__typename":"PublicationHasAuthorName","authorName":{"__ref":"AuthorName:35170"},"affiliations":[{"__ref":"Affiliation:73"}]},{"__typename":"PublicationHasAuthorName","authorName":{"__ref":"AuthorName:34623"},"affiliations":[{"__ref":"Affiliation:73"}]},{"__typename":"PublicationHasAuthorName","authorName":{"__ref":"AuthorName:212115"},"affiliations":[{"__ref":"Affiliation:73"}]}]},"Author:194511":{"__typename":"Author","id":"194511","ibmer":null},"AuthorName:222840":{"__typename":"AuthorName","id":"222840","firstName":"Peter","firstNameInitials":null,"lastName":"Fettke","author":{"__ref":"Author:194511"}},"Affiliation:3020":{"__typename":"Affiliation","id":"3020","isIbm":false},"Author:194512":{"__typename":"Author","id":"194512","ibmer":null},"AuthorName:222841":{"__typename":"AuthorName","id":"222841","firstName":"Andreas","firstNameInitials":null,"lastName":"Metzger","author":{"__ref":"Author:194512"}},"Author:56220":{"__typename":"Author","id":"56220","ibmer":null},"AuthorName:48444":{"__typename":"AuthorName","id":"48444","firstName":"Stefanie","firstNameInitials":null,"lastName":"Rinderle-ma","author":{"__ref":"Author:56220"}},"Author:194513":{"__typename":"Author","id":"194513","ibmer":null},"AuthorName:222842":{"__typename":"AuthorName","id":"222842","firstName":"Barbara","firstNameInitials":null,"lastName":"Weber","author":{"__ref":"Author:194513"}},"Publication:159503":{"__typename":"Publication","id":"159503","slug":"xabps-towards-explainable-autonomous-business-processes","title":"XABPs: Towards eXplainable Autonomous Business Processes","type":{"__typename":"PublicationType","displayValue":"Workshop paper"},"published":"2025-10-25","publishedMeta":{"__typename":"PublishedMeta","source":"ECAI 2025","year":"2025"},"abstract":"Autonomous business processes (ABPs), i.e., self-executing workflows leveraging AI/ML, have the potential to improve operational efficiency, reduce errors, lower costs, improve response times, and free human workers for more strategic and creative work.\nHowever, ABPs may raise specific concerns including decreased stakeholder trust, difficulties in debugging, hindered accountability, risk of bias, and issues with regulatory compliance. We argue for eXplainable ABPs (XABPs) to address these concerns by enabling systems to articulate their rationale. \nThe paper outlines a systematic approach to XABPs, characterizing their forms, structuring explainability, and identifying key BPM research challenges towards XABPs.","linkCode":null,"source":{"__ref":"Source:43"},"sourceInstance":{"__ref":"SourceInstance:24116"},"authors":[{"__typename":"PublicationHasAuthorName","authorName":{"__ref":"AuthorName:222840"},"affiliations":[{"__ref":"Affiliation:3020"}]},{"__typename":"PublicationHasAuthorName","authorName":{"__ref":"AuthorName:35170"},"affiliations":[{"__ref":"Affiliation:73"}]},{"__typename":"PublicationHasAuthorName","authorName":{"__ref":"AuthorName:34623"},"affiliations":[{"__ref":"Affiliation:73"}]},{"__typename":"PublicationHasAuthorName","authorName":{"__ref":"AuthorName:222841"},"affiliations":[{"__ref":"Affiliation:3020"}]},{"__typename":"PublicationHasAuthorName","authorName":{"__ref":"AuthorName:48444"},"affiliations":[{"__ref":"Affiliation:3020"}]},{"__typename":"PublicationHasAuthorName","authorName":{"__ref":"AuthorName:222842"},"affiliations":[{"__ref":"Affiliation:3020"}]}]},"Source:371":{"__typename":"Source","id":"371","longName":"AAAI/ACM Conference on AI, Ethics, and Society","shortName":"AIES","type":"CONFERENCE"},"SourceInstance:23917":{"__typename":"SourceInstance","id":"23917","name":"AIES 2025"},"Ibmer:4343":{"__typename":"Ibmer","id":"4343","slug":"manish-nagireddy","displayName":"Manish Nagireddy"},"Author:68030":{"__typename":"Author","id":"68030","ibmer":{"__ref":"Ibmer:4343"}},"AuthorName:61267":{"__typename":"AuthorName","id":"61267","firstName":"Manish","firstNameInitials":null,"lastName":"Nagireddy","author":{"__ref":"Author:68030"}},"Ibmer:2580":{"__typename":"Ibmer","id":"2580","slug":"inkit-padhi","displayName":"Inkit Padhi"},"Author:19683":{"__typename":"Author","id":"19683","ibmer":{"__ref":"Ibmer:2580"}},"AuthorName:16470":{"__typename":"AuthorName","id":"16470","firstName":"Inkit","firstNameInitials":null,"lastName":"Padhi","author":{"__ref":"Author:19683"}},"Author:42291":{"__typename":"Author","id":"42291","ibmer":null},"AuthorName:34949":{"__typename":"AuthorName","id":"34949","firstName":"Soumya","firstNameInitials":null,"lastName":"Ghosh","author":{"__ref":"Author:42291"}},"Ibmer:4557":{"__typename":"Ibmer","id":"4557","slug":"prasanna-sattigeri","displayName":"Prasanna Sattigeri"},"Author:3590":{"__typename":"Author","id":"3590","ibmer":{"__ref":"Ibmer:4557"}},"AuthorName:11177":{"__typename":"AuthorName","id":"11177","firstName":"Prasanna","firstNameInitials":null,"lastName":"Sattigeri","author":{"__ref":"Author:3590"}},"Publication:159092":{"__typename":"Publication","id":"159092","slug":"when-in-doubt-cascade-towards-building-efficient-and-capable-guardrails","title":"When in Doubt, Cascade: Towards Building Efficient and Capable Guardrails","type":{"__typename":"PublicationType","displayValue":"Conference paper"},"published":"2025-10-20","publishedMeta":{"__typename":"PublishedMeta","source":"AIES 2025","year":"2025"},"abstract":"Large language models (LLMs) have convincing performance in a variety of downstream tasks. However, these systems are prone to generating undesirable outputs such as harmful and biased text. In order to remedy such generations, the development of guardrail (or detector) models has gained traction. Motivated by findings from developing a detector for social bias, we adopt the notion of a use-mention distinction - which we identified as the primary source of under-performance in the preliminary versions of our social bias detector. Armed with this information, we describe a fully extensible and reproducible synthetic data generation pipeline which leverages taxonomy-driven instructions to create targeted and labeled data. Using this pipeline, we generate over 300K unique contrastive samples and provide extensive experiments to systematically evaluate performance on a suite of open source datasets. We show that our method achieves competitive performance with a fraction of the cost in compute and offers insight into iteratively developing efficient and capable guardrail models.\n\nWarning: This paper contains examples of text which are toxic, biased, and potentially harmful.","linkCode":null,"source":{"__ref":"Source:371"},"sourceInstance":{"__ref":"SourceInstance:23917"},"authors":[{"__typename":"PublicationHasAuthorName","authorName":{"__ref":"AuthorName:61267"},"affiliations":[{"__ref":"Affiliation:73"}]},{"__typename":"PublicationHasAuthorName","authorName":{"__ref":"AuthorName:16470"},"affiliations":[{"__ref":"Affiliation:73"}]},{"__typename":"PublicationHasAuthorName","authorName":{"__ref":"AuthorName:34949"},"affiliations":[{"__ref":"Affiliation:73"}]},{"__typename":"PublicationHasAuthorName","authorName":{"__ref":"AuthorName:11177"},"affiliations":[{"__ref":"Affiliation:73"}]}]},"Ibmer:2310":{"__typename":"Ibmer","id":"2310","slug":"hyo-jin-do","displayName":"Hyo Jin Gina Do"},"Author:56026":{"__typename":"Author","id":"56026","ibmer":{"__ref":"Ibmer:2310"}},"AuthorName:48250":{"__typename":"AuthorName","id":"48250","firstName":"Hyo","firstNameInitials":null,"lastName":"Jin Do","author":{"__ref":"Author:56026"}},"Ibmer:2118":{"__typename":"Ibmer","id":"2118","slug":"rachel-ostrand","displayName":"Rachel Ostrand"},"Author:42456":{"__typename":"Author","id":"42456","ibmer":{"__ref":"Ibmer:2118"}},"AuthorName:34990":{"__typename":"AuthorName","id":"34990","firstName":"Rachel","firstNameInitials":null,"lastName":"Ostrand","author":{"__ref":"Author:42456"}},"Ibmer:1668":{"__typename":"Ibmer","id":"1668","slug":"werner-geyer","displayName":"Werner Geyer"},"Author:27206":{"__typename":"Author","id":"27206","ibmer":{"__ref":"Ibmer:1668"}},"AuthorName:22829":{"__typename":"AuthorName","id":"22829","firstName":"Werner","firstNameInitials":null,"lastName":"Geyer","author":{"__ref":"Author:27206"}},"Ibmer:2594":{"__typename":"Ibmer","id":"2594","slug":"keerthiram-murugesan","displayName":"Keerthiram Murugesan"},"Author:20760":{"__typename":"Author","id":"20760","ibmer":{"__ref":"Ibmer:2594"}},"AuthorName:16942":{"__typename":"AuthorName","id":"16942","firstName":"Keerthiram","firstNameInitials":null,"lastName":"Murugesan","author":{"__ref":"Author:20760"}},"Ibmer:819":{"__typename":"Ibmer","id":"819","slug":"dennis-wei","displayName":"Dennis Wei"},"Author:1976":{"__typename":"Author","id":"1976","ibmer":{"__ref":"Ibmer:819"}},"AuthorName:3601":{"__typename":"AuthorName","id":"3601","firstName":"Dennis","firstNameInitials":"D.","lastName":"Wei","author":{"__ref":"Author:1976"}},"Ibmer:3373":{"__typename":"Ibmer","id":"3373","slug":"justin-weisz","displayName":"Justin Weisz"},"Author:5555":{"__typename":"Author","id":"5555","ibmer":{"__ref":"Ibmer:3373"}},"AuthorName:6007":{"__typename":"AuthorName","id":"6007","firstName":"Justin","firstNameInitials":"J.","lastName":"Weisz","author":{"__ref":"Author:5555"}},"Publication:159197":{"__typename":"Publication","id":"159197","slug":"highlight-all-the-phrases-enhancing-llm-transparency-through-visual-factuality-indicators","title":"Highlight All the Phrases: Enhancing LLM Transparency through Visual Factuality Indicators","type":{"__typename":"PublicationType","displayValue":"Conference paper"},"published":"2025-10-20","publishedMeta":{"__typename":"PublishedMeta","source":"AIES 2025","year":"2025"},"abstract":"Large language models (LLMs) are susceptible to generating inaccurate or false information, often referred to as “hallucinations” or “confabulations.” While several technical advancements have been made to detect hallucinated content by assessing the factuality of the model’s responses, there is still limited research on how to effectively communicate this information to users. To address this gap, we conducted two scenario-based experiments with a total of 208 participants to systematically compare the effects of various design strategies for communicating factuality scores by assessing participants’ ratings of trust, ease in validating response accuracy, and preference. Our findings reveal that participants preferred and trusted a design in which all phrases within a response were color-coded based on factuality scores. Participants also found it easier to validate accuracy of the response in this style compared to a baseline with no style applied. Our study offers practical design guidelines for LLM application developers and designers, aimed at calibrating user trust, aligning with user preferences, and enhancing users’ ability to scrutinize LLM outputs.","linkCode":null,"source":{"__ref":"Source:371"},"sourceInstance":{"__ref":"SourceInstance:23917"},"authors":[{"__typename":"PublicationHasAuthorName","authorName":{"__ref":"AuthorName:48250"},"affiliations":[{"__ref":"Affiliation:73"}]},{"__typename":"PublicationHasAuthorName","authorName":{"__ref":"AuthorName:34990"},"affiliations":[{"__ref":"Affiliation:73"}]},{"__typename":"PublicationHasAuthorName","authorName":{"__ref":"AuthorName:22829"},"affiliations":[{"__ref":"Affiliation:73"}]},{"__typename":"PublicationHasAuthorName","authorName":{"__ref":"AuthorName:16942"},"affiliations":[{"__ref":"Affiliation:73"}]},{"__typename":"PublicationHasAuthorName","authorName":{"__ref":"AuthorName:3601"},"affiliations":[{"__ref":"Affiliation:73"}]},{"__typename":"PublicationHasAuthorName","authorName":{"__ref":"AuthorName:6007"},"affiliations":[{"__ref":"Affiliation:73"}]}]},"Ibmer:3030":{"__typename":"Ibmer","id":"3030","slug":"celia-cintas","displayName":"Celia Cintas"},"Author:3695":{"__typename":"Author","id":"3695","ibmer":{"__ref":"Ibmer:3030"}},"AuthorName:4156":{"__typename":"AuthorName","id":"4156","firstName":"Celia","firstNameInitials":null,"lastName":"Cintas","author":{"__ref":"Author:3695"}},"Ibmer:15124":{"__typename":"Ibmer","id":"15124","slug":"miriam-rateike","displayName":"Miriam Rateike"},"Author:74780":{"__typename":"Author","id":"74780","ibmer":{"__ref":"Ibmer:15124"}},"AuthorName:68665":{"__typename":"AuthorName","id":"68665","firstName":"Miriam","firstNameInitials":null,"lastName":"Rateike","author":{"__ref":"Author:74780"}},"Ibmer:4977":{"__typename":"Ibmer","id":"4977","slug":"erik-miehling","displayName":"Erik Miehling"},"Author:76951":{"__typename":"Author","id":"76951","ibmer":{"__ref":"Ibmer:4977"}},"AuthorName:71093":{"__typename":"AuthorName","id":"71093","firstName":"Erik","firstNameInitials":null,"lastName":"Miehling","author":{"__ref":"Author:76951"}},"Ibmer:3494":{"__typename":"Ibmer","id":"3494","slug":"elizabeth-daly","displayName":"Elizabeth Daly"},"Author:19200":{"__typename":"Author","id":"19200","ibmer":{"__ref":"Ibmer:3494"}},"AuthorName:16296":{"__typename":"AuthorName","id":"16296","firstName":"Elizabeth","firstNameInitials":"E.","lastName":"Daly","author":{"__ref":"Author:19200"}},"Ibmer:2837":{"__typename":"Ibmer","id":"2837","slug":"skyler-speakman","displayName":"Skyler Speakman"},"Author:19631":{"__typename":"Author","id":"19631","ibmer":{"__ref":"Ibmer:2837"}},"AuthorName:16459":{"__typename":"AuthorName","id":"16459","firstName":"Skyler","firstNameInitials":null,"lastName":"Speakman","author":{"__ref":"Author:19631"}},"Publication:159311":{"__typename":"Publication","id":"159311","slug":"localizing-persona-representations-in-llms","title":"Localizing Persona Representations in LLMs","type":{"__typename":"PublicationType","displayValue":"Conference paper"},"published":"2025-10-20","publishedMeta":{"__typename":"PublishedMeta","source":"AIES 2025","year":"2025"},"abstract":"We present a study on how and where personas---defined by distinct sets of human characteristics, values, and beliefs---are encoded in the representation space of large language models (LLMs).\nUsing a range of dimension reduction and pattern recognition methods, we first identify the model layers that show the greatest divergence in encoding these representations. We then analyze the activations within a selected layer to examine how specific personas are encoded relative to others, including their shared and distinct embedding spaces.\nWe find that, across multiple pre-trained decoder-only LLMs, a few personas exhibit early separation in representation space, while most personas show large differences in representation spaces only within the final third of the decoder layers.\nWhen we look at a single of these later layers, we observe overlapping activations for specific ethical perspectives---such as moral nihilism and utilitarianism---suggesting a degree of polysemy. In contrast, political ideologies like conservatism and liberalism appear to have more distinct regions.\nThese findings inform future efforts to improve the interpretability of representations and enhance control over human-centered concepts in LLM outputs.","linkCode":null,"source":{"__ref":"Source:371"},"sourceInstance":{"__ref":"SourceInstance:23917"},"authors":[{"__typename":"PublicationHasAuthorName","authorName":{"__ref":"AuthorName:4156"},"affiliations":[{"__ref":"Affiliation:73"}]},{"__typename":"PublicationHasAuthorName","authorName":{"__ref":"AuthorName:68665"},"affiliations":[{"__ref":"Affiliation:73"}]},{"__typename":"PublicationHasAuthorName","authorName":{"__ref":"AuthorName:71093"},"affiliations":[{"__ref":"Affiliation:73"}]},{"__typename":"PublicationHasAuthorName","authorName":{"__ref":"AuthorName:16296"},"affiliations":[{"__ref":"Affiliation:73"}]},{"__typename":"PublicationHasAuthorName","authorName":{"__ref":"AuthorName:16459"},"affiliations":[{"__ref":"Affiliation:73"}]}]},"Author:194539":{"__typename":"Author","id":"194539","ibmer":null},"AuthorName:222868":{"__typename":"AuthorName","id":"222868","firstName":"Hangzhi","firstNameInitials":null,"lastName":"Guo","author":{"__ref":"Author:194539"}},"Author:194540":{"__typename":"Author","id":"194540","ibmer":null},"AuthorName:222869":{"__typename":"AuthorName","id":"222869","firstName":"Pranav","firstNameInitials":null,"lastName":"Venkit","author":{"__ref":"Author:194540"}},"Author:194541":{"__typename":"Author","id":"194541","ibmer":null},"AuthorName:222870":{"__typename":"AuthorName","id":"222870","firstName":"Eunchae","firstNameInitials":null,"lastName":"Jang","author":{"__ref":"Author:194541"}},"Author:194542":{"__typename":"Author","id":"194542","ibmer":null},"AuthorName:222871":{"__typename":"AuthorName","id":"222871","firstName":"Mukund","firstNameInitials":null,"lastName":"Srinath","author":{"__ref":"Author:194542"}},"Author:194543":{"__typename":"Author","id":"194543","ibmer":null},"AuthorName:222872":{"__typename":"AuthorName","id":"222872","firstName":"Wenbo","firstNameInitials":null,"lastName":"Zhang","author":{"__ref":"Author:194543"}},"Author:194544":{"__typename":"Author","id":"194544","ibmer":null},"AuthorName:222873":{"__typename":"AuthorName","id":"222873","firstName":"Bonam","firstNameInitials":null,"lastName":"Mingole","author":{"__ref":"Author:194544"}},"Author:194545":{"__typename":"Author","id":"194545","ibmer":null},"AuthorName:222874":{"__typename":"AuthorName","id":"222874","firstName":"Vipul","firstNameInitials":null,"lastName":"Gupta","author":{"__ref":"Author:194545"}},"Ibmer:59":{"__typename":"Ibmer","id":"59","slug":"kush-varshney","displayName":"Kush Varshney"},"Author:18859":{"__typename":"Author","id":"18859","ibmer":{"__ref":"Ibmer:59"}},"AuthorName:16176":{"__typename":"AuthorName","id":"16176","firstName":"Kush","firstNameInitials":null,"lastName":"Varshney","author":{"__ref":"Author:18859"}},"Author:194546":{"__typename":"Author","id":"194546","ibmer":null},"AuthorName:222875":{"__typename":"AuthorName","id":"222875","firstName":"Shyam","firstNameInitials":null,"lastName":"Sundar","author":{"__ref":"Author:194546"}},"Author:194547":{"__typename":"Author","id":"194547","ibmer":null},"AuthorName:222876":{"__typename":"AuthorName","id":"222876","firstName":"Amulya","firstNameInitials":null,"lastName":"Yadav","author":{"__ref":"Author:194547"}},"Publication:159514":{"__typename":"Publication","id":"159514","slug":"exposing-ai-bias-by-crowdsourcing-democratizing-critique-of-large-language-models","title":"Exposing AI Bias by Crowdsourcing: Democratizing Critique of Large Language Models","type":{"__typename":"PublicationType","displayValue":"Conference paper"},"published":"2025-10-20","publishedMeta":{"__typename":"PublishedMeta","source":"AIES 2025","year":"2025"},"abstract":"The widespread adoption of large language models (LLMs) and generative AI (GenAI) tools across diverse real-world applications has amplified the importance of addressing societal biases inherent within these technologies. While the Natural Language Processing (NLP) community has extensively studied LLM bias, research investigating how non-expert users perceive and interact with biases from these systems remains limited. As these technologies become increasingly prevalent, understanding this question is crucial to inform model developers in their efforts to mitigate bias. To address this gap, this paper presents findings from a university-level competition that challenged participants to design prompts specifically for eliciting biased outputs from GenAI tools. We conducted a quantitative and qualitative analysis of the submitted prompts and the resulting GenAI outputs. This analysis led to the identification of reproducible biases across eight distinct categories within GenAI systems. Furthermore, we identified and categorized the various strategies employed by participants to successfully induce these biased responses. Our findings provide unique insights into how non-expert users understand, engage with, and attempt to manipulate biases in GenAI tools. This research contributes to a deeper understanding of the user-side experience of AI bias and offers actionable knowledge for developers and policymakers working towards creating fairer and more equitable AI systems.","linkCode":null,"source":{"__ref":"Source:371"},"sourceInstance":{"__ref":"SourceInstance:23917"},"authors":[{"__typename":"PublicationHasAuthorName","authorName":{"__ref":"AuthorName:222868"},"affiliations":[{"__ref":"Affiliation:3020"}]},{"__typename":"PublicationHasAuthorName","authorName":{"__ref":"AuthorName:222869"},"affiliations":[{"__ref":"Affiliation:3020"}]},{"__typename":"PublicationHasAuthorName","authorName":{"__ref":"AuthorName:222870"},"affiliations":[{"__ref":"Affiliation:3020"}]},{"__typename":"PublicationHasAuthorName","authorName":{"__ref":"AuthorName:222871"},"affiliations":[{"__ref":"Affiliation:3020"}]},{"__typename":"PublicationHasAuthorName","authorName":{"__ref":"AuthorName:222872"},"affiliations":[{"__ref":"Affiliation:3020"}]},{"__typename":"PublicationHasAuthorName","authorName":{"__ref":"AuthorName:222873"},"affiliations":[{"__ref":"Affiliation:3020"}]},{"__typename":"PublicationHasAuthorName","authorName":{"__ref":"AuthorName:222874"},"affiliations":[{"__ref":"Affiliation:3020"}]},{"__typename":"PublicationHasAuthorName","authorName":{"__ref":"AuthorName:16176"},"affiliations":[{"__ref":"Affiliation:73"}]},{"__typename":"PublicationHasAuthorName","authorName":{"__ref":"AuthorName:222875"},"affiliations":[{"__ref":"Affiliation:3020"}]},{"__typename":"PublicationHasAuthorName","authorName":{"__ref":"AuthorName:222876"},"affiliations":[{"__ref":"Affiliation:3020"}]}]},"ComponentTeamPublications:87":{"__typename":"ComponentTeamPublications","id":"87","publications":{"__typename":"PublicationsQuery","result({\"first\":6})":{"__typename":"PublicationsQueryResult_Connection","totalCount":838,"nodes":[{"__ref":"Publication:159502"},{"__ref":"Publication:159503"},{"__ref":"Publication:159092"},{"__ref":"Publication:159197"},{"__ref":"Publication:159311"},{"__ref":"Publication:159514"}]}}},"UploadFile:520":{"__typename":"UploadFile","id":"520","alternativeText":"","url":"https://research-website-prod-cms-uploads.s3.us.cloud-object-storage.appdomain.cloud/trustworthy_ai_f0b247c9d2.png","width":864,"height":864},"FeatureCard:46":{"__typename":"FeatureCard","card_color":"LIGHT","cta_text":"Learn more","id":"46","image":{"__ref":"UploadFile:520"},"intro":"Our research is regularly integrated into Watson solutions to make IBM’s AI for business more transparent, explainable, robust, private, and fair.","subject":null,"text_position":"RIGHT","title":"Building trustworthy AI with Watson","type":"COLOR_BG","url":"https://www.ibm.com/watson/trustworthy-ai","url_is_external":true},"ComponentCommonFeatureCard:62":{"__typename":"ComponentCommonFeatureCard","id":"62","feature_card":{"__ref":"FeatureCard:46"}},"Team:106":{"__typename":"Team","description":"We’re designing tools to help ensure that AI systems are trustworthy, reliable and can optimize business processes. We create tests to simulate real-life scenarios and localize the faults in AI systems. We’re working on automating testing, debugging, and repairing AI models across a wide range of scenarios.","id":"106","name":"AI Testing","short_description":"We’re designing tools to help ensure that AI systems are trustworthy, reliable and can optimize business processes. ","short_name":null,"slug":"ai-testing"},"Team:16":{"__typename":"Team","description":"Even advanced AI systems can be vulnerable to adversarial attacks. We’re making tools to protect AI and certify its robustness, including quantifying the vulnerability of neural networks and designing new attacks to make better defenses. And we’re helping AI systems adhere to privacy requirements.","id":"16","name":"Adversarial Robustness and Privacy","short_description":"We’re making tools to protect AI and certify its robustness, and helping AI systems adhere to privacy requirements.","short_name":null,"slug":"adversarial-robustness-and-privacy"},"Team:8":{"__typename":"Team","description":"To trust AI systems, explanations can go a long way. We’re creating tools to help debug AI, where systems can explain what they’re doing. This includes training highly optimized, directly interpretable models, as well as explanations of black-box models and visualizations of neural network information flows.","id":"8","name":"Explainable AI","short_description":"We’re creating tools to help AI systems explain why they made the decisions they did. ","short_name":null,"slug":"explainable-ai"},"Team:9":{"__typename":"Team","description":"Biases can lead to systematic disadvantages for marginalized individuals and groups — and they can arise in any point in the AI development lifecycle. To increase the accountability of high-risk AI systems, we're developing technologies to increase their end-to-end transparency and fairness.","id":"9","name":"Fairness, Accountability, Transparency","short_description":"We’re developing technologies to increase the end-to-end transparency and fairness of AI systems. ","short_name":null,"slug":"fairness-accountability-transparency"},"Team:104":{"__typename":"Team","description":"Data is key to technological innovations. We develop theoretical and algorithmic frameworks for generative AI to synthesize realistic, diverse, and targeted data. Our methods facilitate data augmentation for trustworthy machine learning and accelerate novel designs for drug and material discovery, and beyond.","id":"104","name":"Trustworthy Generation","short_description":"We’re developing theoretical and algorithmic frameworks for generative AI to accelerate future scientific discoveries. ","short_name":null,"slug":"trustworthy-generation"},"Team:103":{"__typename":"Team","description":"When AI can explain to us that it's unsure, it adds a critical layer of transparency for its safe deployment and use. We’re developing ways to foster and streamline the common practices of quantifying, evaluating, improving, and communicating uncertainty in the AI application development lifecycle.","id":"103","name":"Uncertainty Quantification","short_description":"We’re developing ways for AI to communicate when it's unsure of a decision across the AI application development lifecycle.","short_name":null,"slug":"uncertainty-quantification"},"TeamCollection:2":{"__typename":"TeamCollection","id":"2","name":"Trustworthy AI","description":"Our trust in technology relies on understanding how it works. It’s important to understand why AI makes the decisions it does. We’re developing tools to make AI more explainable, fair, robust, private, and transparent.","slug":"trustworthy-ai","tag":{"__ref":"Tag:10"},"modules":[{"__ref":"ComponentCommonTextBlockWithLink:1"},{"__ref":"ComponentTeamBlogPosts:86"},{"__ref":"ComponentTeamTeamCardCollection:2"},{"__ref":"ComponentTeamPublications:87"},{"__ref":"ComponentCommonFeatureCard:62"}],"teams({\"sort\":[\"name\"]})":[{"__ref":"Team:106"},{"__ref":"Team:16"},{"__ref":"Team:8"},{"__ref":"Team:9"},{"__ref":"Team:104"},{"__ref":"Team:103"}],"seo":null},"ROOT_QUERY":{"__typename":"Query","teamBySlug({\"slug\":\"trustworthy-ai\"})":null,"teamCollectionBySlug({\"slug\":\"trustworthy-ai\"})":{"__ref":"TeamCollection:2"}}},"ssrDateString":"2025-10-01T12:19:42.998Z"},"__N_SSP":true},"page":"/topics/[tid]","query":{"tid":"trustworthy-ai"},"buildId":"UZgKCVAyhOMQXLXugF_6V","isFallback":false,"gssp":true,"locale":"en-US","locales":["en-US"],"defaultLocale":"en-US","scriptLoader":[]}</script></body></html>