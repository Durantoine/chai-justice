
<!DOCTYPE html>
<html lang="en" >
    <head >

        <meta charset="UTF-8" />
        <meta http-equiv="X-UA-Compatible" content="IE=edge" />
        <meta name="HandheldFriendly" content="True" />
        <meta name="MobileOptimized" content="320" />
        <meta name="viewport" content="width=device-width, initial-scale=1.0" />

        
        

        
        
  <link rel="stylesheet" href="/static/assets/style-8258669d.css" />
<script type="module" crossorigin="" src="/static/assets/base_style-f4a99799.js"></script>

  <link rel="stylesheet" href="/static/assets/style-ef962842.css" />
<link rel="stylesheet" href="/static/assets/style-3ade8b5c.css" />
<script type="module" crossorigin="" src="/static/assets/article_style-d757a0dd.js"></script>

  
  
    <style>
  
  
  @media screen and (min-width: 64em) {
    div.pmc-wm {
      background: repeat-y;
      background-image: url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' width='20' height='350' xmlns:xlink='http://www.w3.org/1999/xlink'%3E%3Cdefs%3E%3Cfilter x='-.02' y='0' width='1.05' height='1' id='c'%3E%3CfeFlood flood-color='%23FFF'/%3E%3CfeComposite in='SourceGraphic'/%3E%3C/filter%3E%3Ctext id='b' font-family='Helvetica' font-size='11pt' style='opacity:1;fill:%23005ea2;stroke:none;text-anchor:middle' x='175' y='14'%3E%3C/text%3E%3Cpath id='a' style='fill:%23005ea2' d='M0 8h350v3H0z'/%3E%3C/defs%3E%3Cuse xlink:href='%23a' transform='rotate(90 10 10)'/%3E%3Cuse xlink:href='%23b' transform='rotate(90 10 10)' filter='url(%23c)'/%3E%3C/svg%3E");
      padding-left: 3rem;
    }
  }
</style>

  



        
            <link rel="apple-touch-icon"
                  sizes="180x180"
                  href="/static/img/favicons/apple-touch-icon.png" />
            <link rel="icon"
                  type="image/png"
                  sizes="48x48"
                  href="/static/img/favicons/favicon-48x48.png" />
            <link rel="icon"
                  type="image/png"
                  sizes="32x32"
                  href="/static/img/favicons/favicon-32x32.png" />
            <link rel="icon"
                  type="image/png"
                  sizes="16x16"
                  href="/static/img/favicons/favicon-16x16.png" />
            <link rel="manifest" href="/static/img/favicons/site.webmanifest" />
            <link rel="mask-icon"
                  href="/static/img/favicons/safari-pinned-tab.svg"
                  color="#0071bc" />
            <meta name="msapplication-config"
                  content="/static/img/favicons/browserconfig.xml" />
            <meta name="theme-color" content="#ffffff" />
        

        <title>
            Critiquing the Reasons for Making Artificial Moral Agents - PMC
        </title>

        
        
  
  <!-- Logging params: Pinger defaults -->
<meta name="ncbi_app" content="cloudpmc-viewer" />
<meta name="ncbi_db" content="pmc" />
<meta name="ncbi_phid" content="ABCB3B558D3E58E30D3B55001773696D.m_1" />
<meta name="ncbi_pinger_stat_url" content="https://pmc.ncbi.nlm.nih.gov/stat" />
<!-- Logging params: Pinger custom -->
<meta name="ncbi_pdid" content="article" />
  
    <link rel="preconnect" href="https://www.google-analytics.com" />

    
        <link rel="preconnect" href="https://cdn.ncbi.nlm.nih.gov" />
    

    <!-- Include USWDS Init Script -->
    <script src="/static/assets/uswds-init.js"></script>


    <meta name="ncbi_domain" content="springeropen">
<meta name="ncbi_type" content="fulltext">
<meta name="ncbi_pcid" content="journal">
<link rel="canonical" href="https://pmc.ncbi.nlm.nih.gov/articles/PMC6591188/">
<meta name="robots" content="INDEX,NOFOLLOW,NOARCHIVE">
<meta name="citation_journal_title" content="Science and Engineering Ethics">
<meta name="citation_title" content="Critiquing the Reasons for Making Artificial Moral Agents">
<meta name="citation_author" content="Aimee van Wynsberghe">
<meta name="citation_author_institution" content="Technical University of Delft, Jaffalaan 5, 2628 BX Delft, Netherlands">
<meta name="citation_author" content="Scott Robbins">
<meta name="citation_author_institution" content="Technical University of Delft, Jaffalaan 5, 2628 BX Delft, Netherlands">
<meta name="citation_publication_date" content="2018 Feb 19">
<meta name="citation_volume" content="25">
<meta name="citation_issue" content="3">
<meta name="citation_firstpage" content="719">
<meta name="citation_doi" content="10.1007/s11948-018-0030-8">
<meta name="citation_pmid" content="29460081">
<meta name="citation_abstract_html_url" content="https://pmc.ncbi.nlm.nih.gov/articles/PMC6591188/">
<meta name="citation_fulltext_html_url" content="https://pmc.ncbi.nlm.nih.gov/articles/PMC6591188/">
<meta name="citation_pdf_url" content="https://pmc.ncbi.nlm.nih.gov/articles/PMC6591188/pdf/11948_2018_Article_30.pdf">
<meta name="description" content="Many industry leaders and academics from the field of machine ethics would have us believe that the inevitability of robots coming to have a larger role in our lives demands that robots be endowed with moral reasoning capabilities. Robots endowed in ...">
<meta name="og:title" content="Critiquing the Reasons for Making Artificial Moral Agents">
<meta name="og:type" content="article">
<meta name="og:site_name" content="PubMed Central (PMC)">
<meta name="og:description" content="Many industry leaders and academics from the field of machine ethics would have us believe that the inevitability of robots coming to have a larger role in our lives demands that robots be endowed with moral reasoning capabilities. Robots endowed in ...">
<meta name="og:url" content="https://pmc.ncbi.nlm.nih.gov/articles/PMC6591188/">
<meta name="og:image" content="https://cdn.ncbi.nlm.nih.gov/pmc/cms/images/pmc-card-share.jpg?_=0">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:site" content="@ncbi">
    
    

    </head>
    <body >
        
    <a class="usa-skipnav " href="#main-content">
      Skip to main content
    </a>


        
            

<section class="usa-banner " aria-label="Official website of the United States government" >
    <div class="usa-accordion">
        <header class="usa-banner__header">
            <div class="usa-banner__inner">
                <div class="grid-col-auto">
                    <img aria-hidden="true"
                         class="usa-banner__header-flag"
                         src="/static/img/us_flag.svg"
                         alt="" />
                </div>

                <div class="grid-col-fill tablet:grid-col-auto" aria-hidden="true">
                    <p class="usa-banner__header-text">
                        An official website of the United States government
                    </p>
                    <span class="usa-banner__header-action">Here's how you know</span>
                </div>

                



















    
        <button
            type="button"
        
    
    class="usa-accordion__button usa-banner__button
           

           
               
               
               
               
            

           
           
           
           "
    aria-expanded="false"
    aria-controls="gov-banner-default"
    
    data-testid="storybook-django-banner"
    
    >
    
        

        
                    <span class="usa-banner__button-text">Here's how you know</span>
                

        
    
        
            </button>
        


            </div>
        </header>

        <div class="usa-banner__content usa-accordion__content"
             id="gov-banner-default"
             hidden>
            <div class="grid-row grid-gap-lg">
                <div class="usa-banner__guidance tablet:grid-col-6">
                    <img class="usa-banner__icon usa-media-block__img"
                         src="/static/img/icon-dot-gov.svg"
                         alt=""
                         aria-hidden="true" />
                    <div class="usa-media-block__body">
                        <p>
                            <strong>Official websites use .gov</strong>
                            <br />
                            A
                            <strong>.gov</strong> website belongs to an official
                            government organization in the United States.
                        </p>
                    </div>
                </div>

                <div class="usa-banner__guidance tablet:grid-col-6">
                    <img class="usa-banner__icon usa-media-block__img"
                         src="/static/img/icon-https.svg"
                         alt=""
                         aria-hidden="true" />

                    <div class="usa-media-block__body">
                        <p>
                            <strong>Secure .gov websites use HTTPS</strong>
                            <br />
                            A <strong>lock</strong> (
                            <span class="icon-lock">
                                <svg xmlns="http://www.w3.org/2000/svg"
                                     width="52"
                                     height="64"
                                     viewBox="0 0 52 64"
                                     class="usa-banner__lock-image"
                                     role="graphics-symbol"
                                     aria-labelledby="banner-lock-description"
                                     focusable="false">
                                    <title id="banner-lock-title">Lock</title>
                                    <desc id="banner-lock-description">
                                    Locked padlock icon
                                    </desc>
                                    <path fill="#000000"
                                          fill-rule="evenodd"
                                          d="M26 0c10.493 0 19 8.507 19 19v9h3a4 4 0 0 1 4 4v28a4 4 0 0 1-4 4H4a4 4 0 0 1-4-4V32a4 4 0 0 1 4-4h3v-9C7 8.507 15.507 0 26 0zm0 8c-5.979 0-10.843 4.77-10.996 10.712L15 19v9h22v-9c0-6.075-4.925-11-11-11z" />
                                </svg>
</span>) or <strong>https://</strong> means you've safely
                                connected to the .gov website. Share sensitive
                                information only on official, secure websites.
                            </p>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </section>

        

        
    
    
    

<div class="usa-overlay">
</div>



<header class="usa-header usa-header--extended usa-header--wide" data-header data-testid="header"    >
    <div class="ncbi-header">
        <div class="ncbi-header__container">
            <a class="ncbi-header__logo-container"
               href="https://www.ncbi.nlm.nih.gov/">
                <img alt="NCBI home page"
                     class="ncbi-header__logo-image"
                     src="/static/img/ncbi-logos/nih-nlm-ncbi--white.svg"
                     width="410"
                     height="100" />
            </a>

            <!-- Mobile menu hamburger button -->
            



















    
        <button
            type="button"
        
    
    class="usa-menu-btn ncbi-header__hamburger-button
           

           
               
               
               
               
            

           
           
           
           "
    
    
    aria-label="Show menu"
    data-testid="navMenuButton"
    
    >
    
        

        
                <svg aria-hidden="true"
                     class="ncbi-hamburger-icon"
                     fill="none"
                     focusable="false"
                     height="21"
                     viewBox="0 0 31 21"
                     width="31"
                     xmlns="http://www.w3.org/2000/svg">
                    <path clip-rule="evenodd"
                          d="M0.125 20.75H30.875V17.3333H0.125V20.75ZM0.125 12.2083H30.875V8.79167H0.125V12.2083ZM0.125 0.25V3.66667H30.875V0.25H0.125Z"
                          fill="#F1F1F1"
                          fill-rule="evenodd" />
                </svg>
            

        
    
        
            </button>
        



            
                <!-- Desktop buttons-->
                <div class="ncbi-header__desktop-buttons">
                    
                        <!-- Desktop search button -->
                        



















    
        <button
            type="button"
        
    
    class="usa-button
           

           
               
               
               
               
            

           
           
           usa-button--unstyled ncbi-header__desktop-button
           "
    aria-expanded="false"
    aria-controls="search-field-desktop-navigation"
    aria-label="Show search overlay"
    data-testid="toggleSearchPanelButton"
    data-toggle-search-panel-button
    >
    
        

        
                            



    <svg class="usa-icon " role="graphics-symbol" aria-hidden="true" >
        
        <use xlink:href="/static/img/sprite.svg#search" />
    </svg>


                            Search
                        

        
    
        
            </button>
        


                    

                    <!-- Desktop login dropdown -->
                    
                        <div class="ncbi-header__login-dropdown">
                            



















    
        <button
            type="button"
        
    
    class="usa-button
           

           
               
               
               
               
            

           
           
           usa-button--unstyled ncbi-header__desktop-button ncbi-header__login-dropdown-button
           "
    aria-expanded="false"
    aria-controls="login-dropdown-menu"
    aria-label="Show login menu"
    data-testid="toggleLoginMenuDropdown"
    data-desktop-login-button
    >
    
        

        
                                



    <svg class="usa-icon " role="graphics-symbol" aria-hidden="true" >
        
        <use xlink:href="/static/img/sprite.svg#person" />
    </svg>



                                <span data-login-dropdown-text>Log in</span>

                                <!-- Dropdown icon pointing up -->
                                



    <svg class="usa-icon ncbi-header__login-dropdown-icon ncbi-header__login-dropdown-icon--expand-less ncbi-header__login-dropdown-icon--hidden" role="graphics-symbol" aria-hidden="true" data-login-dropdown-up-arrow>
        
        <use xlink:href="/static/img/sprite.svg#expand_less" />
    </svg>



                                <!-- Dropdown icon pointing down -->
                                



    <svg class="usa-icon ncbi-header__login-dropdown-icon ncbi-header__login-dropdown-icon--expand-more ncbi-header__login-dropdown-icon--hidden" role="graphics-symbol" aria-hidden="true" data-login-dropdown-down-arrow>
        
        <use xlink:href="/static/img/sprite.svg#expand_more" />
    </svg>


                            

        
    
        
            </button>
        



                            <!-- Login dropdown menu -->
                            <ul class="usa-nav__submenu ncbi-header__login-dropdown-menu"
                                id="login-dropdown-menu"
                                data-desktop-login-menu-dropdown
                                hidden>
                                
                                    <li class="usa-nav__submenu-item">
                                        <!-- Uses custom style overrides to render external and document links. -->
                                        









    <a href="https://www.ncbi.nlm.nih.gov/myncbi/" class="usa-link  "  >
        

        
            Dashboard
        

        
    </a>


                                    </li>
                                
                                    <li class="usa-nav__submenu-item">
                                        <!-- Uses custom style overrides to render external and document links. -->
                                        









    <a href="https://www.ncbi.nlm.nih.gov/myncbi/collections/bibliography/" class="usa-link  "  >
        

        
            Publications
        

        
    </a>


                                    </li>
                                
                                    <li class="usa-nav__submenu-item">
                                        <!-- Uses custom style overrides to render external and document links. -->
                                        









    <a href="https://www.ncbi.nlm.nih.gov/account/settings/" class="usa-link  "  >
        

        
            Account settings
        

        
    </a>


                                    </li>
                                
                                <li class="usa-nav__submenu-item">
                                    



















    
        <button
            type="button"
        
    
    class="usa-button
           

           
               
               
               
               
            

           
           
           usa-button--outline ncbi-header__login-dropdown-logout-button
           "
    
    
    
    data-testid="desktopLogoutButton"
    data-desktop-logout-button
    >
    
        

        Log out

        
    
        
            </button>
        


                                </li>
                            </ul>
                        </div>
                    
                </div>
            
        </div>
    </div>

    <!-- Search panel -->
    
        <div class="ncbi-search-panel ncbi--show-only-at-desktop"
             data-header-search-panel
             hidden>
            <div class="ncbi-search-panel__container">
                <form action="https://www.ncbi.nlm.nih.gov/search/all/"
                      
                      autocomplete="off"
                      class="usa-search usa-search--big ncbi-search-panel__form"
                      data-testid="desktop-navigation-search-form"
                      method="GET"
                      role="search">
                    <label class="usa-sr-only" for="search-field-desktop-navigation">
                        Search…
                    </label>
                    <input class="usa-input"
                           id="search-field-desktop-navigation"
                           name="term"
                           
                               placeholder="Search NCBI"
                           
                           type="search"
                           value="" />
                    



















    
        <button
            type="submit"
        
    
    class="usa-button
           

           
               
               
               
               
            

           
           
           
           "
    
    
    
    
    
    >
    
        

        
                        <span class="usa-search__submit-text">
                            Search NCBI
                        </span>
                    

        
    
        
            </button>
        


                </form>

                
            </div>
        </div>
    

    <nav aria-label="Primary navigation" class="usa-nav">
        <p class="usa-sr-only" id="primary-navigation-sr-only-title">
            Primary site navigation
        </p>

        <!-- Mobile menu close button -->
        



















    
        <button
            type="button"
        
    
    class="usa-nav__close ncbi-nav__close-button
           

           
               
               
               
               
            

           
           
           
           "
    
    
    aria-label="Close navigation menu"
    data-testid="navCloseButton"
    
    >
    
        

        
            <img src="/static/img/usa-icons/close.svg" alt="Close" />
        

        
    
        
            </button>
        



        
            <!-- Mobile search component -->
            <form class="usa-search usa-search--small ncbi--hide-at-desktop margin-top-6"
                  action="https://www.ncbi.nlm.nih.gov/search/all/"
                  
                  autocomplete="off"
                  data-testid="mobile-navigation-search-form"
                  method="GET"
                  role="search">
                <label class="usa-sr-only" for="search-field-mobile-navigation">
                    Search
                </label>

                <input class="usa-input"
                       id="search-field-mobile-navigation"
                       type="search"
                       
                           placeholder="Search NCBI"
                       
                       name="term" />

                



















    
        <button
            type="submit"
        
    
    class="usa-button
           

           
               
               
               
               
            

           
           
           
           "
    
    
    
    
    
    >
    
        

        
                    <!-- This SVG should be kept inline and not replaced with a link to the icon as otherwise it will render in the wrong color -->
                    <img src="data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIGhlaWdodD0iMjQiIHZpZXdCb3g9IjAgMCAyNCAyNCIgd2lkdGg9IjI0Ij48cGF0aCBkPSJNMCAwaDI0djI0SDB6IiBmaWxsPSJub25lIi8+PHBhdGggZmlsbD0iI2ZmZiIgZD0iTTE1LjUgMTRoLS43OWwtLjI4LS4yN0E2LjQ3MSA2LjQ3MSAwIDAgMCAxNiA5LjUgNi41IDYuNSAwIDEgMCA5LjUgMTZjMS42MSAwIDMuMDktLjU5IDQuMjMtMS41N2wuMjcuMjh2Ljc5bDUgNC45OUwyMC40OSAxOWwtNC45OS01em0tNiAwQzcuMDEgMTQgNSAxMS45OSA1IDkuNVM3LjAxIDUgOS41IDUgMTQgNy4wMSAxNCA5LjUgMTEuOTkgMTQgOS41IDE0eiIvPjwvc3ZnPg=="
                         class="usa-search__submit-icon"
                         alt="Search" />
                

        
    
        
            </button>
        


            </form>

            
        

        <!-- Primary navigation menu items -->
        <!-- This usa-nav__inner wrapper is required to correctly style the navigation items on Desktop -->
        

        
            <div class="ncbi-nav__mobile-login-menu ncbi--hide-at-desktop"
                 data-mobile-login-menu
                 hidden>
                <p class="ncbi-nav__mobile-login-menu-status">
                    Logged in as:
                    <strong class="ncbi-nav__mobile-login-menu-email"
                            data-mobile-login-email-text></strong>
                </p>
                <ul class="usa-nav__primary usa-accordion">
                    
                        <li class="usa-nav__primary-item">
                            









    <a href="https://www.ncbi.nlm.nih.gov/myncbi/" class="usa-link  "  >
        

        
            Dashboard
        

        
    </a>


                        </li>
                    
                        <li class="usa-nav__primary-item">
                            









    <a href="https://www.ncbi.nlm.nih.gov/myncbi/collections/bibliography/" class="usa-link  "  >
        

        
            Publications
        

        
    </a>


                        </li>
                    
                        <li class="usa-nav__primary-item">
                            









    <a href="https://www.ncbi.nlm.nih.gov/account/settings/" class="usa-link  "  >
        

        
            Account settings
        

        
    </a>


                        </li>
                    
                </ul>
            </div>
        

        



















    
        <button
            type="button"
        
    
    class="usa-button
           

           
               
               
               
               
            

           
           
           ncbi-nav__mobile-login-button ncbi--hide-at-desktop
           "
    
    
    
    data-testid="mobileLoginButton"
    data-mobile-login-button
    >
    
        

        Log in

        
    
        
            </button>
        


    </nav>
</header>

    
    
        

<section class="pmc-header pmc-header--basic" aria-label="PMC Header with search box">
    <div class="pmc-nav-container">
        <div class="pmc-header__bar">
           <div class="pmc-header__logo">
               <a href="/" title="Home" aria-label="PMC Home"></a>
           </div>
            <button
                    type="button"
                    class="usa-button usa-button--unstyled pmc-header__search__button"
                    aria-label="Open search"
                    data-ga-category="search"
                    data-ga-action="PMC"
                    data-ga-label="pmc_search_panel_mobile"
            >
                <svg class="usa-icon width-4 height-4 pmc-icon__open" aria-hidden="true" focusable="false" role="img">
                    <use xlink:href="/static/img/sprite.svg#search"></use>
                </svg>
                <svg class="usa-icon width-4 height-4 pmc-icon__close" aria-hidden="true" focusable="false" role="img">
                    <use xlink:href="/static/img/sprite.svg#close"></use>
                </svg>
            </button>
        </div>
        <div class="pmc-header__search">
            


<form class="usa-search usa-search--extra usa-search--article-right-column  pmc-header__search__form" id="pmc-search-form" autocomplete="off" role="search">
<label class="usa-sr-only" for="pmc-search">Search PMC Full-Text Archive</label>
<span class="autoComplete_wrapper flex-1">
<input class="usa-input width-full maxw-none" required="required" placeholder="Search PMC Full-Text Archive" id="pmc-search"  type="search" name="term" data-autocomplete-url="https://pmc.ncbi.nlm.nih.gov/autocomp/search/autocomp/"/>
</span>
<button
class="usa-button"
type="submit"
formaction="https://pmc.ncbi.nlm.nih.gov/search/"
data-ga-category="search"
data-ga-action="PMC"
data-ga-label="PMC_search_button"
>
<span class="usa-search__submit-text">Search in PMC</span>
<img
src="/static/img/usa-icons-bg/search--white.svg"
class="usa-search__submit-icon"
alt="Search"
/>
</button>
</form>
            <div class="display-flex flex-column tablet:flex-row tablet:flex-justify flex-justify-center flex-align-center width-full desktop:maxw-44">
                <ul class="pmc-header__search__menu">
                    <li>
                        
                            <a class="usa-link" href="/journals/" data-ga-action="featured_link" data-ga-label="journal list">
                                Journal List
                            </a>
                        
                    </li>
                    <li>
                        
                            <a class="usa-link" href="/about/userguide/" data-ga-action="featured_link"
                            data-ga-label="user guide">
                                User Guide
                            </a>
                        
                    </li>
                </ul>
            </div>
        </div>
    </div>
</section>

    


        
        

       
  <div class="usa-section padding-top-0 desktop:padding-top-6 pmc-article-section" data-article-db="pmc" data-article-id="6591188">

    

   



<div class="grid-container pmc-actions-bar" aria-label="Actions bar" role="complementary">
    <div class="grid-row">
        <div class="grid-col-fill display-flex">
             <div class="display-flex">
                <ul class="usa-list usa-list--unstyled usa-list--horizontal">
                    <li class="margin-right-2 mobile-lg:margin-right-4 display-flex mob">
                        <button
                                type="button"
                                class="usa-button pmc-sidenav__container__open usa-button--unstyled width-auto display-flex"
                                aria-label="Open resources"
                                data-extra-class="is-visible-resources"
                                data-ga-category="resources_accordion"
                                data-ga-action="click"
                                data-ga-label="mobile_icon"
                        >
                            <svg class="usa-icon width-4 height-4" aria-hidden="true" focusable="false" role="img">
                                <use xlink:href="/static/img/sprite.svg#more_vert"></use>
                            </svg>
                        </button>
                    </li>
                    
                    <li class="margin-right-2 mobile-lg:margin-right-4 display-flex mob">
                        <a
                                href="https://doi.org/10.1007/s11948-018-0030-8"
                                class="usa-link display-flex usa-tooltip"
                                role="button"
                                target="_blank"
                                rel="noreferrer noopener"
                                title="View on publisher site"
                                data-position="bottom"
                                aria-label="View on publisher site"
                                data-ga-category="actions"
                                data-ga-action="click"
                                data-ga-label="publisher_link_mobile"
                        >
                                <svg class="usa-icon width-4 height-4" aria-hidden="true" focusable="false" role="img">
                                    <use xlink:href="/static/img/sprite.svg#launch"></use>
                                </svg>
                        </a>
                    </li>
                    
                    
                        <li class="margin-right-2 mobile-lg:margin-right-4 display-flex">
                             <a
                                     href="pdf/11948_2018_Article_30.pdf"
                                     class="usa-link display-flex usa-tooltip"
                                     role="button"
                                     title="Download PDF"
                                     data-position="bottom"
                                     aria-label="Download PDF"
                                     data-ga-category="actions"
                                     data-ga-action="click"
                                     data-ga-label="pdf_download_mobile"
                             >
                                <svg class="usa-icon width-4 height-4" aria-hidden="true" focusable="false" role="img">
                                    <use xlink:href="/static/img/sprite.svg#file_download"></use>
                                </svg>
                            </a>
                        </li>
                    
                    <li class="margin-right-2 mobile-lg:margin-right-4 display-flex">
                        <button
                            class="usa-button usa-button--unstyled  usa-tooltip collections-dialog-trigger collections-button display-flex collections-button-empty"
                            title="Add to Collections"
                            data-position="bottom"
                            aria-label="Save article in MyNCBI collections."
                            data-ga-category="actions"
                            data-ga-action="click"
                            data-ga-label="collections_button_mobile"
                            data-collections-open-dialog-enabled="false"
                            data-collections-open-dialog-url="https://account.ncbi.nlm.nih.gov/?back_url=https%3A%2F%2Fpmc.ncbi.nlm.nih.gov%2Farticles%2FPMC6591188%2F%23open-collections-dialog"
                            data-in-collections="false"
                        >
                            <svg class="usa-icon width-4 height-4 usa-icon--bookmark-full" aria-hidden="true" focusable="false" role="img" hidden>
                                <use xlink:href="/static/img/action-bookmark-full.svg#icon"></use>
                            </svg>
                            <svg class="usa-icon width-4 height-4 usa-icon--bookmark-empty" aria-hidden="true" focusable="false" role="img" hidden>
                                <use xlink:href="/static/img/action-bookmark-empty.svg#icon"></use>
                            </svg>
                        </button>
                    </li>
                    
                    <li class="margin-right-2 mobile-lg:margin-right-4 display-flex">
                        <button role="button" class="usa-button usa-button--unstyled usa-tooltip citation-dialog-trigger display-flex"
                            aria-label="Open dialog with citation text in different styles"
                            title="Cite"
                            data-position="bottom"
                            data-ga-category="actions"
                            data-ga-action="open"
                            data-ga-label="cite_mobile"
                            data-all-citations-url="/resources/citations/6591188/"
                            data-citation-style="nlm"
                            data-download-format-link="/resources/citations/6591188/export/"
                        >
                            <svg class="usa-icon width-4 height-4 usa-icon--bookmark-empty" aria-hidden="true" focusable="false" role="img" hidden>
                                <use xlink:href="/static/img/sprite.svg#format_quote"></use>
                            </svg>
                        </button>
                    </li>
                    
                    <li class="pmc-permalink display-flex" >
                         <button
                                 type="button"
                                 title="Permalink"
                                 data-position="bottom"
                                 class="usa-button usa-button--unstyled display-flex usa-tooltip"
                                 aria-label="Show article permalink"
                                 aria-expanded="false"
                                 aria-haspopup="true"
                                 data-ga-category="actions"
                                 data-ga-action="open"
                                 data-ga-label="permalink_mobile"
                         >
                            <svg class="usa-icon width-4 height-4" aria-hidden="true" focusable="false" role="img">
                                <use xlink:href="/static/img/sprite.svg#share"></use>
                            </svg>
                        </button>
                        

<div class="pmc-permalink__dropdown" hidden>
    <div class="pmc-permalink__dropdown__container">
          <h2 class="usa-modal__heading margin-top-0 margin-bottom-2 text-uppercase font-sans-xs">PERMALINK</h2>
          <div class="pmc-permalink__dropdown__content">
              <input type="text" class="usa-input" value="https://pmc.ncbi.nlm.nih.gov/articles/PMC6591188/" aria-label="Article permalink">
              <button class="usa-button display-inline-flex pmc-permalink__dropdown__copy__btn margin-right-0" title="Copy article permalink" data-ga-category="save_share" data-ga-action="link" data-ga-label="copy_link">
                  <svg class="usa-icon" aria-hidden="true" focusable="false" role="img">
                    <use xlink:href="/static/img/sprite.svg#content_copy"></use>
                  </svg>
                  <span class="margin-left-1">Copy</span>
              </button>
          </div>
    </div>
</div>
                    </li>
                </ul>
            </div>
            <button
                    type="button"
                    class="usa-button pmc-sidenav__container__open usa-button--unstyled width-auto display-flex"
                    aria-label="Open article navigation"
                    data-extra-class="is-visible-in-page"
                    data-ga-category="actions"
                    data-ga-action="open"
                    data-ga-label="article_nav_mobile"
            >
                <svg class="usa-icon width-4 height-4" aria-hidden="true" focusable="false" role="img">
                    <use xlink:href="/static/img/sprite.svg#list"></use>
                </svg>
            </button>
        </div>
    </div>
</div>
    <div class="grid-container desktop:padding-left-6">
      <div id="article-container" class="grid-row grid-gap">
        <div class="grid-col-12 desktop:grid-col-8 order-2 pmc-layout__content">
            <div class="grid-container padding-left-0 padding-right-0">
                <div class="grid-row desktop:margin-left-neg-6">
                    <div class="grid-col-12">
                        <div class="pmc-layout__disclaimer" role="complementary" aria-label="Disclaimer note">
    As a library, NLM provides access to scientific literature. Inclusion in an NLM database does not imply endorsement of, or agreement with,
    the contents by NLM or the National Institutes of Health.<br/>
    Learn more:
    <a class="usa-link" data-ga-category="Link click" data-ga-action="Disclaimer" data-ga-label="New disclaimer box" href="/about/disclaimer/">PMC Disclaimer</a>
    |
    <a class="usa-link" data-ga-category="Link click" data-ga-action="PMC Copyright Notice" data-ga-label="New disclaimer box" href="/about/copyright/">
        PMC Copyright Notice
    </a>
</div>
                    </div>
                </div>
                <div class="grid-row pmc-wm desktop:margin-left-neg-6">
                    <!-- Main content -->
                    <main
                      id="main-content"
                      class="usa-layout-docs__main usa-layout-docs grid-col-12 pmc-layout pmc-prose padding-0"
                    >

                      
                        <section class="pmc-journal-banner text-center line-height-none" aria-label="Journal banner"><img src="https://cdn.ncbi.nlm.nih.gov/pmc/banners/logo-springeropen.png" alt="Springer logo" usemap="#pmc-banner-imagemap" width="500" height="75"><map name="pmc-banner-imagemap"><area alt="Link to Springer" title="Link to Springer" shape="default" href="https://doi.org/10.1007/s11948-018-0030-8" target="_blank" rel="noopener noreferrer"></map></section><article lang="en"><section aria-label="Article citation and metadata"><section class="pmc-layout__citation font-secondary font-xs"><div>
<div class="display-inline-block"><button type="button" class="cursor-pointer text-no-underline bg-transparent border-0 padding-0 text-left margin-0 text-normal text-primary" aria-controls="journal_context_menu">Sci Eng Ethics</button></div>. 2018 Feb 19;25(3):719–735. doi: <a href="https://doi.org/10.1007/s11948-018-0030-8" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">10.1007/s11948-018-0030-8</a>
</div>
<nav id="journal_context_menu" hidden="hidden"><ul class="menu-list font-family-ui" role="menu">
<li role="presentation"><a href="https://pmc.ncbi.nlm.nih.gov/search/?term=%22Sci%20Eng%20Ethics%22%5Bjour%5D" class="usa-link" role="menuitem">Search in PMC</a></li>
<li role="presentation"><a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Sci%20Eng%20Ethics%22%5Bjour%5D" lang="en" class="usa-link" role="menuitem">Search in PubMed</a></li>
<li role="presentation"><a href="https://www.ncbi.nlm.nih.gov/nlmcatalog?term=%22Sci%20Eng%20Ethics%22%5BTitle%20Abbreviation%5D" class="usa-link" role="menuitem">View in NLM Catalog</a></li>
<li role="presentation"><a href="?term=%22Sci%20Eng%20Ethics%22%5Bjour%5D" class="usa-link" role="menuitem" data-add-to-search="true">Add to search</a></li>
</ul></nav></section><section class="front-matter"><div class="ameta p font-secondary font-xs">
<hgroup><h1>Critiquing the Reasons for Making Artificial Moral Agents</h1></hgroup><div class="cg p">
<a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22van%20Wynsberghe%20A%22%5BAuthor%5D" class="usa-link" aria-describedby="id1"><span class="name western">Aimee van Wynsberghe</span></a><div hidden="hidden" id="id1">
<h3><span class="name western">Aimee van Wynsberghe</span></h3>
<div class="p">
<sup>1</sup>Technical University of Delft, Jaffalaan 5, 2628 BX Delft, Netherlands </div>
<div class="p">Find articles by <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22van%20Wynsberghe%20A%22%5BAuthor%5D" class="usa-link"><span class="name western">Aimee van Wynsberghe</span></a>
</div>
</div>
<sup>1,</sup><sup>✉</sup>, <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Robbins%20S%22%5BAuthor%5D" class="usa-link" aria-describedby="id2"><span class="name western">Scott Robbins</span></a><div hidden="hidden" id="id2">
<h3><span class="name western">Scott Robbins</span></h3>
<div class="p">
<sup>1</sup>Technical University of Delft, Jaffalaan 5, 2628 BX Delft, Netherlands </div>
<div class="p">Find articles by <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Robbins%20S%22%5BAuthor%5D" class="usa-link"><span class="name western">Scott Robbins</span></a>
</div>
</div>
<sup>1</sup>
</div>
<ul class="d-buttons inline-list">
<li><button class="d-button" aria-controls="aip_a" aria-expanded="false">Author information</button></li>
<li><button class="d-button" aria-controls="anp_a" aria-expanded="false">Article notes</button></li>
<li><button class="d-button" aria-controls="clp_a" aria-expanded="false">Copyright and License information</button></li>
</ul>
<div class="d-panels font-secondary-light">
<div id="aip_a" class="d-panel p" style="display: none">
<div class="p" id="Aff1">
<sup>1</sup>Technical University of Delft, Jaffalaan 5, 2628 BX Delft, Netherlands </div>
<div class="author-notes p"><div class="fn" id="_fncrsp93pmc__">
<sup>✉</sup><p class="display-inline">Corresponding author.</p>
</div></div>
</div>
<div id="anp_a" class="d-panel p" style="display: none"><div class="notes p"><section id="historyarticle-meta1" class="history"><p>Received 2017 Nov 28; Accepted 2018 Feb 8; Issue date 2019.</p></section></div></div>
<div id="clp_a" class="d-panel p" style="display: none">
<div>© The Author(s) 2018</div>
<p><strong>Open Access</strong>This article is distributed under the terms of the Creative Commons Attribution 4.0 International License (<a href="http://creativecommons.org/licenses/by/4.0/" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">http://creativecommons.org/licenses/by/4.0/</a>), which permits unrestricted use, distribution, and reproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and indicate if changes were made.</p>
<div class="p"><a href="/about/copyright/" class="usa-link">PMC Copyright notice</a></div>
</div>
</div>
<div>PMCID: PMC6591188  PMID: <a href="https://pubmed.ncbi.nlm.nih.gov/29460081/" class="usa-link">29460081</a>
</div>
</div></section></section><section aria-label="Article content"><section class="body main-article-body"><section class="abstract" id="Abs1"><h2>Abstract</h2>
<p id="Par1">Many industry leaders and academics from the field of machine ethics would have us believe that the inevitability of robots coming to have a larger role in our lives demands that robots be endowed with moral reasoning capabilities. Robots endowed in this way may be referred to as artificial moral agents (AMA). Reasons often given for developing AMAs are: the prevention of harm, the necessity for public trust, the prevention of immoral use, such machines are better moral reasoners than humans, and building these machines would lead to a better understanding of human morality. Although some scholars have challenged the very initiative to develop AMAs, what is currently missing from the debate is a closer examination of the reasons offered by machine ethicists to justify the development of AMAs. This closer examination is especially needed because of the amount of funding currently being allocated to the development of AMAs (from funders like Elon Musk) coupled with the amount of attention researchers and industry leaders receive in the media for their efforts in this direction. The stakes in this debate are high because moral robots would make demands on society; answers to a host of pending questions about what counts as an AMA and whether they are morally responsible for their behavior or not. This paper shifts the burden of proof back to the machine ethicists demanding that they give good reasons to build AMAs. The paper argues that until this is done, the development of commercially available AMAs should not proceed further.</p>
<section id="kwd-group1" lang="en" class="kwd-group"><p><strong>Keywords:</strong> Artificial moral agents, Robot ethics, Machine ethics</p></section></section><section id="Sec1"><h2 class="pmc_sec_title">Introduction</h2>
<p id="Par2">Robots perform exceptionally well at clearly defined tasks like playing chess, assembling a car, classifying images, or vacuuming your floor. Increasingly, however, robots are being assigned more general tasks which require more than one skill. A driverless car, for example is supposed to get you from point A to point B following the rules of the road and reacting to unforeseen situations—like a child running out into the middle of the road after a ball. In order for robots to execute their function they require algorithms. These algorithms controlling robots are becoming increasingly autonomous and often require artificial intelligence (AI). As autonomy in robots and AI increases so does the likelihood that they encounter situations which are <em>morally salient</em>. As of 2017 robots are and will continue to be designed, developed and deployed in morally salient contexts; from robots in the hospital lifting and/or bathing patients to robots in the military assisting with bomb disposal or intelligence gathering.</p>
<p id="Par3">The Executive Summary of the International Federation of Robotics<a href="#Fn1" class="usa-link">1</a> shows a marked increase in robot sales across every sector from 1 year to the next; including a 25% increase in the total number of service robots sold in 2015 alone. These robots can be used to save lives, to assist in dangerous activities, and/or to enhance the proficiency of human workers. Many industry leaders and academics from the field of machine ethics—the study of endowing machines with ethical reasoning—would have us believe that robots in these and other morally charged contexts will inevitably demand that these machines be endowed with moral reasoning capabilities. Such robots are often referred to as artificial moral agents (AMAs). In this paper the variety of reasons offered by machine ethicists in favor of AMAs are challenged. This paper asks: are the given reasons adequate justification for the design and development of AMAs?</p>
<p id="Par5">From the academic domain a variety of scholars in the fields of ethics and technology and/or robot ethics have argued against the development of AMAs (Bryson <a href="#CR13" class="usa-link" aria-describedby="CR13">2008</a>; Johnson and Miller <a href="#CR32" class="usa-link" aria-describedby="CR32">2008</a>; Sharkey <a href="#CR50" class="usa-link" aria-describedby="CR50">2017</a>; Tonkens <a href="#CR58" class="usa-link" aria-describedby="CR58">2009</a>). What is currently missing from the debate on AMAs is a closer look at the reasons offered (to society, academics, the media) by machine ethicists to justify the development of AMAs. This closer inspection is particularly compulsory given the amount of funding allocated to the development of AMAs (from funders like Elon Musk) coupled with the amount of attention researchers and industry leaders receive in the media for their efforts in this direction.<a href="#Fn2" class="usa-link">2</a> Moreover, the stakes are high because the resulting technology could create novel demands on society; questions about what counts as an AMA, whether they are deserving of citizenship,<a href="#Fn3" class="usa-link">3</a> and/or whether they are morally responsible for their behavior or not. In other words, a machine with moral reasoning capabilities might be thought to deserve moral consideration in the form of rights and/or protections (Coeckelbergh <a href="#CR15" class="usa-link" aria-describedby="CR15">2010</a>; Darling <a href="#CR16" class="usa-link" aria-describedby="CR16">2012</a>; Gunkel <a href="#CR26" class="usa-link" aria-describedby="CR26">2014</a>).</p>
<p id="Par8">In order to examine the justifications for AMAs, this paper begins with a description of the field of machine ethics: what it is, the terminology used, and the response to machine ethics found in the literature by robot ethicists and scholars in the field of ethics and technology. In subsequent sections, the reasons offered in favor of developing robots with moral reasoning capabilities are evaluated. It is argued that each of the reasons lack both empirical and intuitive support. The burden of proof is thereby shifted to machine ethicists to justify their pursuits.</p></section><section id="Sec2"><h2 class="pmc_sec_title">Machine Ethics</h2>
<p id="Par9">Summarized by machine ethicist Susan Anderson, the “ultimate goal of machine ethics is to create autonomous ethical machines” (<a href="#CR6" class="usa-link" aria-describedby="CR6">2007</a>, p. 15). The term machine ethics was first used in 1987 by Mitchell Waldrop in the AI Magazine article “A Question of Responsibility” (Waldrop <a href="#CR67" class="usa-link" aria-describedby="CR67">1987</a>). In 2005 the AAAI held a symposium on machine ethics which resulted in the edited volume <em>Machine Ethics</em> in 2011 by Susan Leigh and Michael Anderson (Anderson and Anderson <a href="#CR8" class="usa-link" aria-describedby="CR8">2011</a>). The field may be referred to by other names, e.g. machine morality, but for the purposes of this paper, machine ethics is a field of study dedicated to the computational entity as a moral entity.<a href="#Fn4" class="usa-link">4</a></p>
<p id="Par11">There are several phrases and terms for discussing robots with moral reasoning capabilities (e.g. moral machines, implicit vs explicit ethical agents).<a href="#Fn5" class="usa-link">5</a> For the purposes of this article, however, the term artificial moral agent (AMA) will be used for consistency and clarity.<a href="#Fn6" class="usa-link">6</a> This clearly restricts the discussion to robots capable of engaging in autonomous moral reasoning, that is, moral reasoning about a situation without the direct real time input from a human user. This moral reasoning is aimed at going beyond safety and security decisions about a context. How this might be done, and whether or not this can be achieved in practice, are questions that go beyond the scope of this paper (these are the questions underpinning the field of machine ethics itself). Rather, the interest of this paper is in targeting the reasons offered in support of developing such machines.</p>
<p id="Par14">What a robot or machine would act like if it were to think in an ethical way is a central feature in the 1950 works of science fiction writer Isaac Asimov. Asimov, who coined the term ‘robotics’ (the study of robots) is best known for his work articulating and exploring the three laws of robotics (Asimov <a href="#CR11" class="usa-link" aria-describedby="CR11">1963</a>). In short, these three laws were a kind of principled or deontological approach to embedding ethics into a machine. Through a series of short stories Asimov reveals the difficulty and nuances of robots acting in an ethical manner because each ethical principle conflicts with another to such a degree that experience, wisdom, and intuition are required to come to a solution or resolution of the conflict. His stories highlight the struggle to define ethics in a computational form.</p>
<p id="Par15">From the academic domain a variety of scholars in the fields of ethics and technology and/or robot ethics have argued against the development of AMAs. On one hand, scholars insist that the technology ought to be designed in such a way that responsibility distribution remains “tethered to humans” (Johnson and Miller <a href="#CR32" class="usa-link" aria-describedby="CR32">2008</a>). Similarly, computer scientist Joanna Bryson argues that robots ought to remain in the instrumental service of humans, as slaves if you will, meeting the needs of their human users and intentionally designed not to be a moral agent (Bryson <a href="#CR13" class="usa-link" aria-describedby="CR13">2008</a>). This claim is predicated on the assumption that humans will own robots and as such will be responsible for their existence and capacities. On the other hand, philosopher Ryan Tonkens argues that given the impossibility of finding universal agreement concerning the ethical theory used to program a machine, the initiative is moot (Tonkens <a href="#CR58" class="usa-link" aria-describedby="CR58">2009</a>).</p>
<p id="Par16">Outside of these arguments robot ethicist Amanda Sharkey outlines the misappropriation of the use of ‘ethical’ in the quest to make moral machines and insists on the creation of “safe” machines instead. In the same line of thinking, Miller et al. argue that responsible development requires careful use of terminology and representation in the media (Miller et al. <a href="#CR37" class="usa-link" aria-describedby="CR37">2017</a>).</p>
<p id="Par17">The above arguments are still waiting to be adequately answered by the machine ethics community. However, the purpose of this paper is to question the positive reasons offered by the machine ethicists <em>for</em> building AMAs. These reasons have not yet been fully evaluated as yet and a closer inspection of them reveals a lack of sufficient justification. Given the high stakes and of the research and development in question coupled with the current speed of (and funding for) machine ethics initiatives these must be addressed now.</p></section><section id="Sec3"><h2 class="pmc_sec_title">Reasons for Developing Moral Machines</h2>
<p id="Par18">Machine ethicists have offered six reasons (found in the literature) in favor of and/or promoting the development of moral machines. These are not stand alone reasons; rather, they are often intertwined. Part of the reason it sounds so convincing (at first glance) is because of their interdependency rather than the strength of any reason on its own. Disentangling these reasons shows their dubious foundation and allows one to challenge the endeavor of machine ethics.</p>
<section id="Sec4"><h3 class="pmc_sec_title">Inevitability</h3>
<blockquote class="text-italic"><p>Robots with moral decision making abilities will become a technological necessity (Wallach <a href="#CR68" class="usa-link" aria-describedby="CR68">2007</a>).</p></blockquote>
<blockquote class="text-italic"><p>[Artificial Moral Agents] are necessary and, in a weak sense, inevitable (Colin Allen and Wallach <a href="#CR3" class="usa-link" aria-describedby="CR3">2011</a>).</p></blockquote>
<p id="Par19">Machine ethicists claim that robots in morally salient contexts will not and cannot be avoided, i.e. their development is inevitable (Anderson and Anderson <a href="#CR7" class="usa-link" aria-describedby="CR7">2010</a>; Moor <a href="#CR38" class="usa-link" aria-describedby="CR38">2006</a>; Scheutz <a href="#CR47" class="usa-link" aria-describedby="CR47">2016</a>; Wallach <a href="#CR69" class="usa-link" aria-describedby="CR69">2010</a>).</p>
<p id="Par22">First, what exactly is meant by morally salient contexts is unclear. For some researchers this would include contexts such as healthcare, elder care, childcare, sex, and or the military—where life and death decisions are being made on a daily (or hourly) basis (Arkin <a href="#CR10" class="usa-link" aria-describedby="CR10">2009</a>; Lokhorst and van den Hoven <a href="#CR34" class="usa-link" aria-describedby="CR34">2011</a>; Sharkey <a href="#CR49" class="usa-link" aria-describedby="CR49">2016</a>; Sharkey <a href="#CR51" class="usa-link" aria-describedby="CR51">2008</a>; Sharkey and Sharkey <a href="#CR53" class="usa-link" aria-describedby="CR53">2011</a>; Sharkey et al. <a href="#CR54" class="usa-link" aria-describedby="CR54">2017</a>; van Wynsberghe <a href="#CR62" class="usa-link" aria-describedby="CR62">2012</a>). There is no question that robots are entering these service sectors. The International Federation for Robotics Executive Summary of 2016 tells us that “the total number of professional service robots sold in 2015 rose considerably by 25% to 41,060 units up from 32,939 in 2014” and “service robots in defense applications accounted for 27% of the total number of service robots for professional use sold in 2015”. Moreover, sales of medical robots increased by 7% from 2014 to 2015.<a href="#Fn7" class="usa-link">7</a></p>
<p id="Par24">For others, morally salient context is much broader than a pre-defined space or institution:</p>
<blockquote class="text-italic"><p id="Par25">any ordinary decision-making situation from daily life can be turned into a morally charged decision-making situation, where the artificial agent finds itself presented with a moral dilemma where any choice of action (or inaction) can potentially cause harm to other agents. (Scheutz, <a href="#CR47" class="usa-link" aria-describedby="CR47">2016</a>, p. 516)</p></blockquote>
<p>
From the above quote Scheutz is saying that a morally charged situation can arise at any moment in the event that someone could be harmed through (in)action of a robot. This thin description of a morally charged decision making situation adds further ambiguity to the discussion, namely (1) what level of autonomy does the robot have, and (2) what definition of harm is Scheutz talking about? There seems to be an assumption being made in the above quote concerning the robot that the robot must make a choice for action or inaction and thus that the robot must be autonomous. According to Scheutz then any autonomous robot interacting with a human user that has the potential to harm its user should be endowed with moral reasoning capabilities. What would Scheutz have us do with industrial robots that possess divergent levels of autonomy, work with humans in their presence, and for which it has already been shown that the robots can bring serious harm or sometimes death, to humans? Scheutz’s position would imply that industrial robots as well ought to be developed into AMAs.</p>
<p id="Par27">Consider also the definition of ‘harm’ that ought to be adopted. Is it only physical harm to the corporeal body and mind that is the object of discussion here and if so what about the robot’s, or AI algorithm’s, ability to collect, store and share information about its users in a home setting? Considering the real possibility that home robots will be connected to the Internet of Things (IoT) which holds the potential for hackers and/or for companies not related to the robotics company to access personal data from users. The harm that can come from the mis-appropriation of one’s data has proven to be noteworthy of late: people can be refused mortgage loans, stalked, blackmailed, harassed online, or worse. If harm is to be extended to include the risk of one’s digital information, and interaction with a machine that might cause harm demands that it be endowed with ethical reasoning capacities, then one must concede that every device that one interacts with in a day (your tv, phone, fridge, alarm clock, kettle, etc.) ought to have such capabilities. Thus, Scheutz’s position leads to the conclusion that any technology that one interacts with and for which there is a potential for harm (physical or otherwise) must be developed as an AMA and this is simply untenable.</p>
<p id="Par28">Second, a distinction must be made between <em>being in a morally charged situation</em>, on the one hand, and <em>being delegated a moral role</em> on the other. Consider animals used for therapeutic purposes in an elderly care facility; one would never demand that a dog placed in this context would need to reason ethically because of its role in therapy and the potential for harm in this context. Indeed the dog would be trained to ensure a degree of safety and reliability when interacting with it but would the dog be a <em>moral dog</em> in the end?<a href="#Fn8" class="usa-link">8</a></p>
<p id="Par30">With this thought in mind let us day the discussion will be limited to an examination of a <em>morally salient context</em> to contexts such as the military and healthcare, which are often thought of as morally salient, and agree that it is inevitable that robots will be placed within these contexts. In this case there is a different, more nuanced problem that can be put into the form of a dilemma: when placed in a morally salient context either machines will be delegated a moral role or they will not. If one chooses the first horn—that the machine is delegated a moral role—then one must accept that it is inevitable that machines will be delegated a moral role in addition to the inevitability of the machine being in this morally salient context. However, this is simply not the case. There are plenty of machines operating in morally salient contexts which have not been delegated a moral role and are providing a valuable service. Consider for example:</p>
<p id="Par31">Corti is an AI which listens in on emergency phone calls and makes correlations between the breathing and speech patterns of a caller and the risk of heart attack (Peters <a href="#CR43" class="usa-link" aria-describedby="CR43">2018</a>). The information from the AI is presented to the phone operator to assist in their decision making. In short, the AI is a support system for the decision making of the operator. This may be akin to the cancer detection AI or other more traditional technologies in the operating suite (e.g. electrocardiogram, respiratory monitor and so on). Corti is clearly in a morally salient context (life and death), yet the machine is not delegated a moral role. The human being is still in charge and holds all of the responsibility for decision making. If one agrees with machine ethicists then one should accept that it is inevitable that a moral role reserved for the human in this case will be assigned to the machine. While this is probably unnecessary and most likely harmful, the point is that there is simply no reason to believe that this is <em>inevitable</em>.</p>
<p id="Par32">If, however, one takes the other horn of the dilemma then the claim is as follows: robots will inevitably be in morally salient contexts without being delegated a morally salient role. The problem with this is that there is little new here. Microwaves and coffee machines exist in the hospital with no need for moral reasoning capabilities; this horn should be of little interest to machine ethicists. In short, there is not any evidence to suggest that it is inevitable that there will be a need for machines with moral reasoning capabilities regardless of whether or not they function in a morally salient context.</p></section><section id="Sec5"><h3 class="pmc_sec_title">Artificial Moral Machines to Prevent Harm to Humans</h3>
<p id="Par33">For many scholars the development of moral machines is aimed at preventing a robot from hurting human beings. To ensure that humans can overcome the potential for physical harm, a technological solution is presented; namely, to develop AMAs:</p>
<blockquote class="text-italic"><p id="Par34">the only way to minimize human harm is to build morally competent robots that can detect and resolve morally charged situations in human-like ways (Scheutz <a href="#CR47" class="usa-link" aria-describedby="CR47">2016</a>).</p></blockquote>
<p>
The line of reasoning here is pretty straight forward in that: “it is clear that machines…will be capable of causing harm to human beings” (Anderson and Anderson <a href="#CR7" class="usa-link" aria-describedby="CR7">2010</a>) and this can be mitigated and/or reduced by endowing the robot with ethical reasoning capabilities. This also speaks to the interconnection of the reasons in favor of AMAs; robots are inevitable, robots could harm us, therefore robots should be made into AMAs.</p>
<p id="Par36">It is unclear that AMAs are the solution to this problem, however. There are plenty of technologies capable of harming human beings (e.g. lawn mowers, automatic doors, curling irons, blenders); the solution has always been either to design them with safety features or to limit the contexts in which a technology can be used. An elevator door has a sensor so that it does not close on people; lawn mowers have a guard to protect us from their blades; and ovens have lights to warn us when our stovetop is hot. One does not normally use barbeques indoors or chainsaws in daycare centers. Machine ethicists are the first to suggest endowing technology with moral reasoning capabilities as a solution to problems of safety.</p>
<p id="Par37">Furthermore, machine ethicists may also agree with the pursuit of safe robots and then the real concern for ethicists is that ethics is being reduced to safety. Notions such as values, rights, freedoms, good vs bad, right vs wrong, are central to the study of ethics and form the basis for a discussion of competing conceptions of the good life. One may believe that the values of safety and security are fundamental to achieving the good life; however, ethics cannot be reduced to these issues. So if AMAs are simply a solution to possibly harmful machines, then <em>safety—</em>not <em>moral agency—</em>is the object of debate.</p>
<p id="Par38">In this case the world ‘moral’ is a linguistic ‘trojan horse’—a word</p>
<blockquote class="text-italic"><p id="Par39">that smuggles in a rich interconnected web of human concepts that are not part of a computer system or how it operates (Sharkey <a href="#CR52" class="usa-link" aria-describedby="CR52">2012</a>, p. 793)</p></blockquote>
<p>The concept of <em>moral</em> machines or artificial <em>moral</em> agents invites, or more strongly requests that, the user believe the robot may care for him/her, or that the robot can experience feelings. For robot developers this could increase desirability of the robot and therefore profits. However, this is problematic for the public in that it invites a kind of fictive, asymmetric, deceptive relationship between human and robot.</p>
<p id="Par41">Thus, machine ethicists must either distinguish what makes their machines “moral” above and beyond “safe” or they must stop using the world “moral” as the word is not appropriate—only the most reductionist account of morality would equate it with preventing harm.</p></section><section id="Sec6"><h3 class="pmc_sec_title">Complexity</h3>
<blockquote class="text-italic"><p id="Par42">as systems get more sophisticated and their ability to function autonomously in different contexts and environments expands, it will become more important for them to have ‘ethical subroutines’ of their own (Allen et al. <a href="#CR4" class="usa-link" aria-describedby="CR4">2006</a>, p. 14)</p></blockquote>
<p id="Par43">The idea behind using complexity as an argument in favor of AMAs is that robots are, and will increasingly become, so complex in terms of their programming that it is no longer possible know what they will do in novel situations. This uncertainty results in the impossibility of the engineer to predict every scenario and as such it will not be possible for the engineer to predict the robot’s actions. Consequently, one cannot simply foresee a morally problematic situation and pre-program what the robot should do. Instead, authors who use the complexity argument to promote development of AMAs claim the robot needs to have moral competence in order to govern its unpredictable actions in the inevitably unpredictable and unstructured human environments that the robot will be placed.</p>
<p id="Par44">First, using complexity as a reason for developing AMAs expects both that there will be complex robots and that such robots ought to be placed in contexts for which this complexity (i.e. unpredictability) could cause problems for human beings.</p>
<p id="Par45">Next, of importance for this issue is the context within which the robot will be placed. In other words, this problem can be mitigated simply by restricting the context within which these machines are used. For example, designers of Google’s complex machine AlphaGo may not have any idea what their machine will do next (which move it will make in the notoriously difficult game of GO); however, this is not an ethical or moral problem because the context (the game of GO) is restricted. Its complexity will not pose a problem for us.</p>
<p id="Par46">One may argue that human beings are unpredictable and can cause harm to other human beings. The solution has not been to prevent the delegation of moral roles to human beings. One might ask: why treat machines differently? While it is outside of the scope of this paper to engage in a debate on just how predictable humans are, it can be noted that with regard to serious moral values—killing, non-consensual sex, harming innocent people for fun—society places restrictions on unpredictable human beings (i.e. imprisonment). Humans may be unpredictable in terms of what they will do next, but most of us assume that a random person will not intentionally cause us harm.</p></section><section id="Sec7"><h3 class="pmc_sec_title">Public Trust</h3>
<p id="Par47">Other machine ethicists argue that making AMAs will increase public trust: “Constructing artificial moral agents serves at least two purposes: one, better understanding of moral reasoning, and, two, increasing our trust and confidence in creating autonomous agents acting on our behalf” (Wiegel <a href="#CR71" class="usa-link" aria-describedby="CR71">2006</a>). There has been talk in the media expressing concerns surrounding AI and robotics—voiced by the likes of Elon Musk and Steven Hawking (Cellan-Jones <a href="#CR14" class="usa-link" aria-describedby="CR14">2014</a>; Markoff <a href="#CR35" class="usa-link" aria-describedby="CR35">2015</a>). Rather than preventing the development of robots that are the source of these fears “machine ethics may offer a viable, more realistic solution” (Anderson and Anderson <a href="#CR6" class="usa-link" aria-describedby="CR6">2007</a>).</p>
<p id="Par48">This line of thinking assumes that if robots are given moral competence then this will put the public at ease and lead to public acceptance. It should be noted here that acceptance differs from acceptability. As an example, the public may <em>accept</em> geo tagging and tracking algorithms on their smartphone devices but this does not meant that such privacy breaching technologies and/or the lack of transparency about their existence are <em>acceptable</em> practices for upholding societal values.</p>
<p id="Par49">Some important clarifications are needed when discussing trust as a concept. Traditionally speaking, trust is described as an interaction between persons or between a person and an institution and so on. For scholar John Hardwig trust can be placed in people, processes and in knowledge (Hardwig <a href="#CR29" class="usa-link" aria-describedby="CR29">1991</a>). In more recent years scholars are discussing a new form of trust; trust in algorithms (Simon <a href="#CR56" class="usa-link" aria-describedby="CR56">2010</a>). This new form of trust is most commonly referred to as ‘algorithmic authority’ and is described as a practice of placing confidence in the decisions made by an algorithm (Shirky <a href="#CR55" class="usa-link" aria-describedby="CR55">2009</a>). Wikipedia is an example of this form of trust as it requires trust not in persons but in the algorithms regulating the content on the website.<a href="#Fn9" class="usa-link">9</a></p>
<p id="Par51">If trust is broken the result will be feelings of disappointment on the part of the truster. These resulting negative feelings are what relate trust to the concept of reliability: if either one is misplaced the result is oftentimes feelings of disappointment (Simon <a href="#CR56" class="usa-link" aria-describedby="CR56">2010</a>). Trust is distinguished from reliability in the intensity of the emotions experienced afterwards; “trust differs from reliance because if we are let down we feel betrayed and not just disappointed” (Baier <a href="#CR12" class="usa-link" aria-describedby="CR12">1986</a>; Simon <a href="#CR56" class="usa-link" aria-describedby="CR56">2010</a>). Relatedly, Simon claims that one cannot speak of trust for socio-technical systems but rather of reliance: “we usually do not ascribe intentionality to unanimated objects, which is why we do not feel betrayed by them” (p 347). Hence, we do not trust unanimated objects, we rely on them.</p>
<p id="Par52">With the formulation of Hardwig in mind—trust can be placed in people, processes, and in knowledge and when it concerns placing trust in robots one must ask: who, or what, are machine ethicists asking the public to trust: the algorithm directing the robot; the designer; or, the development process?</p>
<p id="Par53">If the public is being asked to trust the algorithm then one must consider that:</p>
<blockquote class="text-italic"><p id="Par54">unfortunately we often trust<a href="#Fn10" class="usa-link">10</a> algorithms blindly. Algorithms are hidden within a system. In most cases we are not aware of how they work and we cannot assess their impact on the information we receive. In other words: algorithms are black-boxed (Simon <a href="#CR56" class="usa-link" aria-describedby="CR56">2010</a>).</p></blockquote>
<p>Consequently, if the public is being asked to trust an algorithm and it is considered a black-box, then, as Simon rightly asserts, it must be <em>opened</em>—the way it works, the decisions made in its development, and alternatives—must be made transparent and subject to scrutiny.</p>
<p id="Par57">If, however, the public is being asked to trust the designer then designers and developers ought to develop a code of conduct (perhaps in the form of soft law) to adhere to. Again, transparency of this is required for the public to have the knowledge required for trust.</p>
<p id="Par58">Last, if the public is being asked to trust the process through which the robot is being developed, a kind of procedural trust, then standards and certifications must be developed to once again provide the user with the knowledge required to place trust in the process through which the robot was developed. Examples of such procedural trust are FairTrade, ISO, GMOs, and so on.</p>
<p id="Par59">In any case it is important to point out the inconsistency between the promotion of AMAs for reasons of complexity and for reasons of trust: it is inconsistent to expect unpredictability in a machine and to expect trust in a machine at the same time. While this may not be the case for people—one might trust persons who are at the same time unpredictable—more clarity is needed in understanding who/what society is being asked to trust and what level of (un)predictability one can assume.</p></section><section id="Sec8"><h3 class="pmc_sec_title">Preventing Immoral Use</h3>
<p id="Par60">In the 2012 American science fiction comedy-drama movie “Robot &amp; Frank” there is a compelling story of how a retired cat burglar convinces his robot to help him enter the business once again. The story raises the question about human–robot interaction not in the sense of safe or reliable interactions but rather should the robot be capable of evaluating a human’s request for action. Thus, another reason put forward for the development of AMAs can be stated as: preventing humans from misusing, or inappropriately using, a robot requires that the robot be developed as a moral machine and can thus prevent misuse of itself, itself.</p>
<p id="Par61">The main problem with this reason has to do with the potential to constrain the autonomy of humans. It’s not always clear what is the good thing to do and oftentimes context is required for this (Miller et al. <a href="#CR37" class="usa-link" aria-describedby="CR37">2017</a>). Consider for example a couple who are at home having a few drinks together, domestic violence ensues after a heated argument, the women tries to get away in her car but the breathalyzer in her car picks up the alcohol in her system and the car won’t start. What would be the right thing to do in such an instance? How should the device be programmed? A deontologist would have us believe that one should never get behind the wheel after consuming alcohol, while a utilitarian might suggest that if the woman’s life is saved in doing so the overall good is maximized (unless of course the women were to get into a car accident and harm two others). These are not easy problems to solve and require particular details of context and individuals.</p>
<p id="Par62">Consider another example where misuse is unclear. If an elderly person at home wants to have a fourth glass of wine and asks his/her robot to fetch it. If the robot fetches the wine, is the robot being misused in so far as it is contributing to poor health choices of the user? Or is the robot ‘good’ in so far as it fulfilled the request of its user. Presenting scenarios like these is meant to show the difficulty in determining the right or the good thing to do. And yet if one is claiming that robots should be involved in the decision making procedure it must be very clear how a ‘good’ robot is distinguished from a ‘bad’ one.</p></section><section id="Sec9"><h3 class="pmc_sec_title">Morality: Better with Moral Machines</h3>
<p id="Par63">Endowing the robot with the capability to override or edit a human’s decisions draws us into the discussion of the robot as a superior moral reasoner to a human. Computer science Professor James Gips suggested back in 1994 that “not many human beings live their lives flawlessly as moral saints. But a robot could (Gips <a href="#CR24" class="usa-link" aria-describedby="CR24">1994</a>, p. 250). Also along the same lines, Professor of Philosophy Eric Dietrich has suggested that:</p>
<blockquote class="text-italic"><p id="Par64">humans are genetically hardwired to be immoral…let us – the humans – exit the stage, leaving behind a planet populated with machines who, although not perfect angels, will nevertheless be a vast improvement over us (Dietrich <a href="#CR18" class="usa-link" aria-describedby="CR18">2001</a>)</p></blockquote>
<p>
The assumption here is that a robot could be better at moral decision making than a human given that it would be impartial, unemotional, consistent, and rational every time it made a decision. Thus, no decisions would be based on bias or emotions, no decision would be the result of an affinity towards one person (or group of people) over another. More importantly, the robot would never tire but would have the energy to be consistent in decision making: to make the same choice time after time.</p>
<p id="Par66">This line of reasoning to promote AMAs is also often invoked when speaking of robots in military contexts. In particular computer scientist/roboticist Ronald Arkin discusses the power of autonomous military robots for overcoming the shortcomings of humans on the battlefield (Arkin <a href="#CR10" class="usa-link" aria-describedby="CR10">2009</a>). These robots would not rape or pillage the villages taken over during wartime and would be programmed as ethical agents according to the Laws of Just war and/or the Rules of Engagement.</p>
<p id="Par67">There are some general concerns with this reason. First, the underlying programming which will enable machines to reason morally implies that one has an understanding of moral epistemology such that one can program machines to “learn” the correct moral truths—or at least know enough to have AMAs learn something that works. This gets complicated as there is no moral epistemology which does not have serious philosophical objections and therefore presents a barrier to being reduced to a programming language.</p>
<p id="Par68">Machines could only be better if there is some standard of moral truth with which to judge. This implies that there are objective moral truths in a moral realist sense and further that it is possible to know what they are. This is opposed to error theory (the idea that there are no moral truths at all—so nothing to know), and moral skepticism (there are moral truths, but it is not possible that we as humans can know them).</p>
<p id="Par69">Furthermore, based on the above quotes it seems that the moral truths that machines would be better at knowing are truths which are independent of human attitudes. Russ Shafer-Laundau calls these stance-independent moral truths (Shafer-Landau <a href="#CR48" class="usa-link" aria-describedby="CR48">1994</a>). If—and that is a big if—there are stance independent moral truths whereby the truths have no dependence upon human desires, beliefs, needs, etc. then there are objections to how one could come to know such truths (Finlay <a href="#CR20" class="usa-link" aria-describedby="CR20">2007</a>). If a machine was built which did somehow discover moral truths that have heretofore yet to be discovered (because morality would be a lot easier if we simply knew the moral truths) then one would have to accept on faith that machines are better than we are.</p>
<p id="Par70">The moral consistency promised by machine ethicists is only a public good if the moral truths are known in advance—the opposite of the situation human beings find themselves in. For, as shown in previous sections, AMAs are argued to be needed because one cannot predict the kind of situations or moral dilemmas they will be faced with. But this is not a chess game where the outcome is a win or a loss. An autonomous car which drives off a cliff—killing its one passenger—in order to save five passengers in another car would not be a clear cut situation that everyone could agree was the correct decision. Indeed, books are written about that very decision and human disagreement about what should be done (i.e. the trolley problem) (see e.g. Greene <a href="#CR25" class="usa-link" aria-describedby="CR25">2013</a>).</p>
<p id="Par71">Lastly this all presumes that human emotions, human desires, and our evolutionary history are all getting in the way of our moral reasoning—causing it to be worse than it could be. There are those who include moral emotions as a necessary part of moral judgment and reasoning (Kristjánsson <a href="#CR33" class="usa-link" aria-describedby="CR33">2006</a>; Pizarro <a href="#CR44" class="usa-link" aria-describedby="CR44">2000</a>; Roeser <a href="#CR45" class="usa-link" aria-describedby="CR45">2010</a>). If this is so, then AMAs would require emotions—something not even on the horizon of AI and robotics.</p>
<p id="Par72">Let us say that there are moral principles and that humans can know what they are. So there is a standard with which to judge AMAs. Furthermore, let us also assume they live up to their promise and are better moral reasoners than humans. It might then make sense to outsource our moral decisions to machines. This would assume that being good at moral reasoning is not a necessary part of a human being’s good life. Aristotle believed leading a moral life and gaining a moral understanding through practice was necessary to leading a good life (Aristotle et al. <a href="#CR9" class="usa-link" aria-describedby="CR9">1998</a>). Many contemporary philosophers agree. Outsourcing our moral reasoning to machines could cause an undesirable moral deskilling in human beings (Vallor <a href="#CR59" class="usa-link" aria-describedby="CR59">2015</a>). The point is that it is not clear at all if machines were better moral reasoners than us that this would be a good reason to use them. Added to this, to make such an assumption is to assume we have an understanding of morality and the good life that we may not.</p></section><section id="Sec10"><h3 class="pmc_sec_title">Better Understanding of Morality</h3>
<p id="Par73">Finally, machine ethicists sometimes argue that developing robots with moral reasoning capabilities will ultimately lead to a better understanding of human morality:</p>
<blockquote class="text-italic"><p id="Par74">the hope is that as we try to implement ethical systems on the computer we will learn much more about the knowledge and assumptions built into the ethical theories themselves. That as we build the artificial ethical reasoning systems we will learn how to behave more ethically ourselves (Gips <a href="#CR24" class="usa-link" aria-describedby="CR24">1994</a>)</p></blockquote>
<p> In short, regardless of the resulting machine the very process of attempting to create such a machine would benefit humans in so far as we would learn about ourselves and our moral attributes (Gips <a href="#CR24" class="usa-link" aria-describedby="CR24">1994</a>; Moor <a href="#CR38" class="usa-link" aria-describedby="CR38">2006</a>; Wiegel <a href="#CR71" class="usa-link" aria-describedby="CR71">2006</a>).</p>
<p id="Par76">The most important consideration in response to this claim is that ethical theories are not (and have little to do with) how people reason morally so the work doesn’t help understand <em>human</em> morality. Experiments in moral psychology show us that human morality is deeply influenced by irrelevant situational factors (Doris <a href="#CR19" class="usa-link" aria-describedby="CR19">1998</a>; Merritt <a href="#CR36" class="usa-link" aria-describedby="CR36">2000</a>), is driven by emotion (Haidt <a href="#CR27" class="usa-link" aria-describedby="CR27">2001</a>; Haidt and Joseph <a href="#CR28" class="usa-link" aria-describedby="CR28">2008</a>), and influenced by our evolutionary past (Street <a href="#CR57" class="usa-link" aria-describedby="CR57">2006</a>). To be sure, there is an intense debate in the literature with regard to each of these studies. The point is that human morality, in the descriptive sense, is dependent upon many complex factors and building a machine that tries to perfectly emulate human morality must use each of these factors combined rather than rely on ethical theory alone.</p></section></section><section id="Sec11"><h2 class="pmc_sec_title">Conclusion</h2>
<p id="Par77">In this paper the reasons offered by machine ethicists promoting the development of moral machines are shown to fall short when one takes a closer look at the assumptions underpinning their claims and/or the claims themselves. While autonomous robots and AI can and should be used in morally salient contexts this need not require that the robot be endowed with ethical reasoning capabilities. Merely placing something in an ethical situation, like a heart monitor in an ICU hospital ward, does not also demand the thing to ethically reflect on its course of action. The power of such robots in said contexts can still be harnessed even without making them into so-called moral machines.</p>
<p id="Par78">This article has shown here that AMAs are promoted for reasons of inevitability, complexity, establishing public trust, preventing immoral use, because they would be better moral reasoners than us, or because there would be a better understanding of human morality with AMAs. None of these reasons—as they have been articulated in the literature—warrant the development of moral machines nor will they work in practice. This is so because of: inherent bias to learn how to be ethical, the impossibility or difficulty of understanding the complexity of the robot’s decision, how to evaluate or trust the superior ethical reasoning of the robot and so on.</p>
<p id="Par79">There are dangers in the language used for these endeavors. One should not refer to moral machines, artificial moral agents, or ethical agents if the goal is really to create safe, reliable machines. Rather, they should be called what they are: safe robots. The best way to avoid this confusion, considering that no critical or unique operational function appears to be gained through the endowment of ethical reasoning capabilities into robots, is to simply not do it. To that end the authors suggest an implication for policy makers and academics: place a moratorium on the commercialization of robots claiming to have ethical reasoning skills. This would allow academics to study the issues while at the same time protecting users—the consumer, the indirect user, and society at large—from exposure to this technology which poses an existential challenge.</p>
<p id="Par80">In closing, our goal for this paper was to pick apart the reasons in favor of moral machines as a way of shifting the burden of proof back to the machine ethicists. It is not up to ethicists anymore to tell you why they think the pursuit of an AMA is flawed; rather, now that it has been shown that the motivations for developing moral machines do not withstand closer inspection machine ethicists need to provide better reasons. So, to the machine ethicists out there: the ball is in your court.</p></section><section id="ack1" class="ack"><h2 class="pmc_sec_title">Acknowledgements</h2>
<p>This research is supported by the Netherlands Organization for Scientific Research (NWO), Project number 275-20-054. Scott Robbins wishes to acknowledge the European Research Council (ERC) Advanced Grant titled Global Terrorism and Collective Moral Responsibility: Redesigning Military, Police and Intelligence Institutions in Liberal Democracies (GTCMR 670172) which in part made research for this paper possible. We would also like to thank Deborah Johnson for graciously providing such incredibly useful feedback and insights.</p></section><section id="fn-group1" class="fn-group"><h2 class="pmc_sec_title">Footnotes</h2>
<div class="fn-group p font-secondary-light font-sm">
<div class="fn p" id="Fn1">
<sup>1</sup><p class="display-inline" id="Par4">For more on this see <a href="https://ifr.org/ifr-press-releases/news/world-robotics-report-2016" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">https://ifr.org/ifr-press-releases/news/world-robotics-report-2016</a>.</p>
</div>
<div class="fn p" id="Fn2">
<sup>2</sup><p class="display-inline" id="Par6">For more on the popular news articles see: (Deng <a href="#CR17" class="usa-link" aria-describedby="CR17">2015</a>; “Morals and the machine” <a href="#CR40" class="usa-link" aria-describedby="CR40">2012</a>; Rutkin <a href="#CR46" class="usa-link" aria-describedby="CR46">2014</a>).</p>
</div>
<div class="fn p" id="Fn3">
<sup>3</sup><p class="display-inline" id="Par7">Robot Sophie of Hanson Robotics, first robot granted citizenship in Saudi Arabia, see (Gershgorn <a href="#CR23" class="usa-link" aria-describedby="CR23">2017</a>; Hatmaker <a href="#CR30" class="usa-link" aria-describedby="CR30">2017</a>).</p>
</div>
<div class="fn p" id="Fn4">
<sup>4</sup><p class="display-inline" id="Par10">For more readings on machine ethics see Wallach and Allen (<a href="#CR70" class="usa-link" aria-describedby="CR70">2010</a>), Anderson and Anderson (<a href="#CR6" class="usa-link" aria-describedby="CR6">2007</a>, <a href="#CR8" class="usa-link" aria-describedby="CR8">2011</a>), Anderson (<a href="#CR5" class="usa-link" aria-describedby="CR5">2011</a>), Moor (<a href="#CR39" class="usa-link" aria-describedby="CR39">2009</a>, <a href="#CR38" class="usa-link" aria-describedby="CR38">2006</a>), Scheutz (<a href="#CR47" class="usa-link" aria-describedby="CR47">2016</a>), and Allan et al. (<a href="#CR4" class="usa-link" aria-describedby="CR4">2006</a>).</p>
</div>
<div class="fn p" id="Fn5">
<sup>5</sup><p class="display-inline" id="Par12">For more on this see Wallach and Allen (<a href="#CR70" class="usa-link" aria-describedby="CR70">2010</a>), Moor (<a href="#CR39" class="usa-link" aria-describedby="CR39">2009</a>, <a href="#CR38" class="usa-link" aria-describedby="CR38">2006</a>).</p>
</div>
<div class="fn p" id="Fn6">
<sup>6</sup><p class="display-inline" id="Par13">The concept and notion of artificial moral agents has built momentum as a thought experiment and/or a possible reality. For a rich and detailed discussion of AMAs the authors recommend the following: (Allen et al. <a href="#CR1" class="usa-link" aria-describedby="CR1">2005</a>, <a href="#CR2" class="usa-link" aria-describedby="CR2">2000</a>; Floridi and Sanders <a href="#CR21" class="usa-link" aria-describedby="CR21">2004</a>; Himma <a href="#CR31" class="usa-link" aria-describedby="CR31">2009</a>; Johnson and Miller <a href="#CR32" class="usa-link" aria-describedby="CR32">2008</a>; Nagenborg <a href="#CR41" class="usa-link" aria-describedby="CR41">2007</a>; Wiegel <a href="#CR72" class="usa-link" aria-describedby="CR72">2010</a>).</p>
</div>
<div class="fn p" id="Fn7">
<sup>7</sup><p class="display-inline" id="Par23">For more on this please refer to: <a href="https://ifr.org/downloads/press/02_2016/Executive_Summary_Service_Robots_2016.pdf" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">https://ifr.org/downloads/press/02_2016/Executive_Summary_Service_Robots_2016.pdf</a>.</p>
</div>
<div class="fn p" id="Fn8">
<sup>8</sup><p class="display-inline" id="Par29">See also the work of van Wynsberghe illustrating how robots in healthcare need not be delegated roles for which ethical reasoning and/or moral responsibility are required. (van Wynsberghe <a href="#CR62" class="usa-link" aria-describedby="CR62">2012</a>, <a href="#CR63" class="usa-link" aria-describedby="CR63">2013</a>, <a href="#CR64" class="usa-link" aria-describedby="CR64">2015</a>, <a href="#CR65" class="usa-link" aria-describedby="CR65">2016</a>). Furthermore there are existing frameworks and applications for realizing ethical values in technological design. See e.g. (Friedman and Nissenbaum <a href="#CR22" class="usa-link" aria-describedby="CR22">1996</a>; Nissenbaum <a href="#CR42" class="usa-link" aria-describedby="CR42">2001</a>; van de Poel <a href="#CR60" class="usa-link" aria-describedby="CR60">2013</a>; van den Hoven <a href="#CR61" class="usa-link" aria-describedby="CR61">2007</a>; van Wynsberghe and Robbins <a href="#CR66" class="usa-link" aria-describedby="CR66">2013</a>).</p>
</div>
<div class="fn p" id="Fn9">
<sup>9</sup><p class="display-inline" id="Par50">This form of trust may also be referred to as procedural trust (Simon) as it concerns trust in the process through which knowledge is created rather than in actions of persons.</p>
</div>
<div class="fn p" id="Fn10">
<sup>10</sup><p class="display-inline" id="Par55">The word trust is used here because it comes from a quotation; however, it should be noted that the authors are inclined to use the work rely instead.</p>
</div>
</div></section><section id="_ci93_" lang="en" class="contrib-info"><h2 class="pmc_sec_title">Contributor Information</h2>
<p>Aimee van Wynsberghe, Email: aimeevanrobot@gmail.com.</p>
<p>Scott Robbins, Email: scott@scottrobbins.org.</p></section><section id="Bib1" class="ref-list"><h2 class="pmc_sec_title">References</h2>
<section id="Bib1_sec2"><ol class="ref-list font-sm">
<li id="CR1">
<cite>Allen C, Smit I, Wallach W. Artificial morality: Top-down, bottom-up, and hybrid approaches. Ethics and Information Technology. 2005;7(3):149–155.</cite> [<a href="https://scholar.google.com/scholar_lookup?journal=Ethics%20and%20Information%20Technology&amp;title=Artificial%20morality:%20Top-down,%20bottom-up,%20and%20hybrid%20approaches&amp;author=C%20Allen&amp;author=I%20Smit&amp;author=W%20Wallach&amp;volume=7&amp;issue=3&amp;publication_year=2005&amp;pages=149-155&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR2">
<cite>Allen C, Varner G, Zinser J. Prolegomena to any future artificial moral agent. Journal of Experimental &amp; Theoretical Artificial Intelligence. 2000;12(3):251–261.</cite> [<a href="https://scholar.google.com/scholar_lookup?journal=Journal%20of%20Experimental%20&amp;%20Theoretical%20Artificial%20Intelligence&amp;title=Prolegomena%20to%20any%20future%20artificial%20moral%20agent&amp;author=C%20Allen&amp;author=G%20Varner&amp;author=J%20Zinser&amp;volume=12&amp;issue=3&amp;publication_year=2000&amp;pages=251-261&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR3">
<cite>Allen C, Wallach W.  Moral machines: Contradition in terms of abdication of human responsibility? In: Lin P, Abney K, Bekey GA, editors. Robot ethics: The ethical and social implications of robotics. Cambridge: MIT Press; 2011. pp. 55–68.</cite> [<a href="https://scholar.google.com/scholar_lookup?title=Robot%20ethics:%20The%20ethical%20and%20social%20implications%20of%20robotics&amp;author=C%20Allen&amp;author=W%20Wallach&amp;publication_year=2011&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR4">
<cite>Allen C, Wallach W, Smit I. Why machine ethics? IEEE Intelligent Systems. 2006;21(4):12–17.</cite> [<a href="https://scholar.google.com/scholar_lookup?journal=IEEE%20Intelligent%20Systems&amp;title=Why%20machine%20ethics?&amp;author=C%20Allen&amp;author=W%20Wallach&amp;author=I%20Smit&amp;volume=21&amp;issue=4&amp;publication_year=2006&amp;pages=12-17&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR5"><cite>Anderson, S. L. (2011). <em>Machine metaethics</em>. In M. Anderson, &amp; S. L. Anderson (Eds.), Machine Ethics. New York: Cambridge University Press.</cite></li>
<li id="CR6">
<cite>Anderson M, Anderson SL. Machine ethics: Creating an ethical intelligent agent. AI Magazine. 2007;28(4):15–26.</cite> [<a href="https://scholar.google.com/scholar_lookup?journal=AI%20Magazine&amp;title=Machine%20ethics:%20Creating%20an%20ethical%20intelligent%20agent&amp;author=M%20Anderson&amp;author=SL%20Anderson&amp;volume=28&amp;issue=4&amp;publication_year=2007&amp;pages=15-26&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR7">
<cite>Anderson M, Anderson SL. Robot be good: A call for ethical autonomous machines. Scientific American. 2010;303(4):15–24. doi: 10.1038/scientificamerican1010-72.</cite> [<a href="https://doi.org/10.1038/scientificamerican1010-72" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/20923132/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Scientific%20American&amp;title=Robot%20be%20good:%20A%20call%20for%20ethical%20autonomous%20machines&amp;author=M%20Anderson&amp;author=SL%20Anderson&amp;volume=303&amp;issue=4&amp;publication_year=2010&amp;pages=15-24&amp;pmid=20923132&amp;doi=10.1038/scientificamerican1010-72&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR8">
<cite>Anderson M, Anderson SL.  Machine ethics. Cambridge: Cambridge University Press; 2011. </cite> [<a href="https://scholar.google.com/scholar_lookup?title=Machine%20ethics&amp;author=M%20Anderson&amp;author=SL%20Anderson&amp;publication_year=2011&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR9"><cite>Aristotle, Ross, W. D., Ackrill, J. L., &amp; Urmson, J. O. (1998). <em>The Nicomachean ethics</em>. Oxford University Press. Retrieved from <a href="http://books.google.nl/books?id=Dk2VFlZyiJQC" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">http://books.google.nl/books?id=Dk2VFlZyiJQC</a>. Accessed 24 Oct 2014.</cite></li>
<li id="CR10">
<cite>Arkin R.  Governing lethal behavior in autonomous robots. Boca Raton: CRC Press; 2009. </cite> [<a href="https://scholar.google.com/scholar_lookup?title=Governing%20lethal%20behavior%20in%20autonomous%20robots&amp;author=R%20Arkin&amp;publication_year=2009&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR11">
<cite>Asimov I.  I, Robot. New York: Spectra; 1963. </cite> [<a href="https://scholar.google.com/scholar_lookup?title=I,%20Robot&amp;author=I%20Asimov&amp;publication_year=1963&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR12">
<cite>Baier A. Trust and antitrust. Ethics. 1986;96(2):231–260.</cite> [<a href="https://scholar.google.com/scholar_lookup?journal=Ethics&amp;title=Trust%20and%20antitrust&amp;author=A%20Baier&amp;volume=96&amp;issue=2&amp;publication_year=1986&amp;pages=231-260&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR13"><cite>Bryson, J. (2008). Robots should be slaves. In Y. Wilks (Ed.), <em>Close Engagements with artificial companions: Key social, psychological, ethical and design issue</em> (pp. 63–74). Amsterdam: John Benjamins Publishing. Retrieved from <a href="https://books.google.nl/books?id=EPznZHeG89cC" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">https://books.google.nl/books?id=EPznZHeG89cC</a>. Accessed 7 Mar 2017.</cite></li>
<li id="CR14"><cite>Cellan-Jones, R. (2014). Stephen Hawking warns artificial intelligence could end mankind. <em>BBC News</em>. Retrieved from <a href="http://www.bbc.com/news/technology-30290540" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">http://www.bbc.com/news/technology-30290540</a>. Accessed 29 Aug 2016.</cite></li>
<li id="CR15">
<cite>Coeckelbergh M. Robot rights? Towards a social-relational justification of moral consideration. Ethics and Information Technology. 2010;12(3):209–221.</cite> [<a href="https://scholar.google.com/scholar_lookup?journal=Ethics%20and%20Information%20Technology&amp;title=Robot%20rights?%20Towards%20a%20social-relational%20justification%20of%20moral%20consideration&amp;author=M%20Coeckelbergh&amp;volume=12&amp;issue=3&amp;publication_year=2010&amp;pages=209-221&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR16"><cite>Darling, K. (2012). <em>Extending Legal protection to social robots: The effects of anthropomorphism, empathy, and violent behavior towards robotic objects</em>. Rochester, NY. Retrieved from <a href="https://papers.ssrn.com/abstract=2044797" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">https://papers.ssrn.com/abstract=2044797</a>.</cite></li>
<li id="CR17">
<cite>Deng B. Machine ethics: The robot’s dilemma. Nature News. 2015;523(7558):24. doi: 10.1038/523024a.</cite> [<a href="https://doi.org/10.1038/523024a" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/26135432/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Nature%20News&amp;title=Machine%20ethics:%20The%20robot%E2%80%99s%20dilemma&amp;author=B%20Deng&amp;volume=523&amp;issue=7558&amp;publication_year=2015&amp;pages=24&amp;pmid=26135432&amp;doi=10.1038/523024a&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR18">
<cite>Dietrich E. Homo sapiens 2.0: Why we should build the better robots of our nature. Journal of Experimental &amp; Theoretical Artificial Intelligence. 2001;13(4):323–328.</cite> [<a href="https://scholar.google.com/scholar_lookup?journal=Journal%20of%20Experimental%20&amp;%20Theoretical%20Artificial%20Intelligence&amp;title=Homo%20sapiens%202.0:%20Why%20we%20should%20build%20the%20better%20robots%20of%20our%20nature&amp;author=E%20Dietrich&amp;volume=13&amp;issue=4&amp;publication_year=2001&amp;pages=323-328&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR19"><cite>Doris, J. M. (1998). Persons, situations, and virtue ethics. <em>Nous</em>, <em>32</em>(4), 504–530. Retrieved from <a href="http://www.jstor.org/stable/pdfplus/2671873.pdf?acceptTC=true" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">http://www.jstor.org/stable/pdfplus/2671873.pdf?acceptTC=true</a>.</cite></li>
<li id="CR20">
<cite>Finlay S. Four faces of moral realism. Philosophy Compass. 2007;2(6):820–849.</cite> [<a href="https://scholar.google.com/scholar_lookup?journal=Philosophy%20Compass&amp;title=Four%20faces%20of%20moral%20realism&amp;author=S%20Finlay&amp;volume=2&amp;issue=6&amp;publication_year=2007&amp;pages=820-849&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR21">
<cite>Floridi L, Sanders JW. On the morality of artificial agents. Minds and Machines. 2004;14(3):349–379.</cite> [<a href="https://scholar.google.com/scholar_lookup?journal=Minds%20and%20Machines&amp;title=On%20the%20morality%20of%20artificial%20agents&amp;author=L%20Floridi&amp;author=JW%20Sanders&amp;volume=14&amp;issue=3&amp;publication_year=2004&amp;pages=349-379&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR22"><cite>Friedman, B., &amp; Nissenbaum, H. (1996). Bias in computer systems. <em>ACM Transactions on Information Systems</em>, <em>14</em>(3), 330–347. 10.1145/230538.230561. Retrieved 10 Feb 2017.</cite></li>
<li id="CR23"><cite>Gershgorn, D. (2017). Inside the mechanical brain of the world’s first robot citizen. <a href="https://qz.com/1121547/how-smart-is-the-first-robot-citizen/" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">https://qz.com/1121547/how-smart-is-the-first-robot-citizen/</a>. Retrieved 29 Dec 2017.</cite></li>
<li id="CR24"><cite>Gips, J. (1994). Toward the ethical robot. In K. M. Ford, C. Glymour, &amp; P. Hayes (Eds.), <em>Android Epistemology</em>. Cambridge: MIT Press.</cite></li>
<li id="CR25">
<cite>Greene J.  Moral tribes: Emotion, reason, and the gap between us and them. 1. New York: Penguin Press; 2013. </cite> [<a href="https://scholar.google.com/scholar_lookup?title=Moral%20tribes:%20Emotion,%20reason,%20and%20the%20gap%20between%20us%20and%20them&amp;author=J%20Greene&amp;publication_year=2013&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR26">
<cite>Gunkel DJ. A vindication of the rights of machines. Philosophy &amp; Technology. 2014;27(1):113–132.</cite> [<a href="https://scholar.google.com/scholar_lookup?journal=Philosophy%20&amp;%20Technology&amp;title=A%20vindication%20of%20the%20rights%20of%20machines&amp;author=DJ%20Gunkel&amp;volume=27&amp;issue=1&amp;publication_year=2014&amp;pages=113-132&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR27">
<cite>Haidt J. The emotional dog and its rational tail: A social intuitionist approach to moral judgment. Psychological Review. 2001;108(4):814–834. doi: 10.1037/0033-295x.108.4.814.</cite> [<a href="https://doi.org/10.1037/0033-295x.108.4.814" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/11699120/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Psychological%20Review&amp;title=The%20emotional%20dog%20and%20its%20rational%20tail:%20A%20social%20intuitionist%20approach%20to%20moral%20judgment&amp;author=J%20Haidt&amp;volume=108&amp;issue=4&amp;publication_year=2001&amp;pages=814-834&amp;pmid=11699120&amp;doi=10.1037/0033-295x.108.4.814&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR28">
<cite>Haidt J, Joseph C.  The Innate Mind: Volume 3: Foundations and the Future (Evolution and Cognition) In: Carruthers P, Laurence S, Stich S, editors. The innate mind. New York: Oxford University Press; 2008. </cite> [<a href="https://scholar.google.com/scholar_lookup?title=The%20innate%20mind&amp;author=J%20Haidt&amp;author=C%20Joseph&amp;publication_year=2008&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR29">
<cite>Hardwig J. The role of trust in knowledge. The Journal of Philosophy. 1991;88(12):693–708.</cite> [<a href="https://scholar.google.com/scholar_lookup?journal=The%20Journal%20of%20Philosophy&amp;title=The%20role%20of%20trust%20in%20knowledge&amp;author=J%20Hardwig&amp;volume=88&amp;issue=12&amp;publication_year=1991&amp;pages=693-708&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR30"><cite>Hatmaker, T. (2017). Saudi Arabia bestows citizenship on a robot named Sophia. <a href="http://social.techcrunch.com/2017/10/26/saudi-arabia-robot-citizen-sophia/" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">http://social.techcrunch.com/2017/10/26/saudi-arabia-robot-citizen-sophia/</a>. Retrieved 12 Feb 2018.</cite></li>
<li id="CR31">
<cite>Himma KE. Artificial agency, consciousness, and the criteria for moral agency: What properties must an artificial agent have to be a moral agent? Ethics and Information Technology. 2009;11(1):19–29.</cite> [<a href="https://scholar.google.com/scholar_lookup?journal=Ethics%20and%20Information%20Technology&amp;title=Artificial%20agency,%20consciousness,%20and%20the%20criteria%20for%20moral%20agency:%20What%20properties%20must%20an%20artificial%20agent%20have%20to%20be%20a%20moral%20agent?&amp;author=KE%20Himma&amp;volume=11&amp;issue=1&amp;publication_year=2009&amp;pages=19-29&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR32">
<cite>Johnson DG, Miller KW. Un-making artificial moral agents. Ethics and Information Technology. 2008;10(2–3):123–133.</cite> [<a href="https://scholar.google.com/scholar_lookup?journal=Ethics%20and%20Information%20Technology&amp;title=Un-making%20artificial%20moral%20agents&amp;author=DG%20Johnson&amp;author=KW%20Miller&amp;volume=10&amp;issue=2%E2%80%933&amp;publication_year=2008&amp;pages=123-133&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR33"><cite>Kristjánsson, K. (2006). Emulation and the use of role models in moral education. <em>Journal of Moral Education</em>, <em>35</em>(1), 37–49. Retrieved from <a href="http://www.tandfonline.com/doi/abs/10.1080/03057240500495278" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">http://www.tandfonline.com/doi/abs/10.1080/03057240500495278</a>. Accessed 25 Oct 2014.</cite></li>
<li id="CR34">
<cite>Lokhorst G-J, van den Hoven J.  Responsibility for Military Robots. In: Lin P, Abney K, Bekey GA, editors. Robot ethics: The ethical and social implications of robotics. Cambridge: MIT Press; 2011. pp. 145–155.</cite> [<a href="https://scholar.google.com/scholar_lookup?title=Robot%20ethics:%20The%20ethical%20and%20social%20implications%20of%20robotics&amp;author=G-J%20Lokhorst&amp;author=J%20van%20den%20Hoven&amp;publication_year=2011&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR35"><cite>Markoff, J. (2015). Relax, the terminator is far away. <em>The New York Times</em>. Retrieved from <a href="http://www.nytimes.com/2015/05/26/science/darpa-robotics-challenge-terminator.html" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">http://www.nytimes.com/2015/05/26/science/darpa-robotics-challenge-terminator.html</a>. Accessed 29 Aug 2016.</cite></li>
<li id="CR36">
<cite>Merritt M. Virtue ethics and situationist personality psychology. Ethical Theory and Moral Practice. 2000;3(4):365–383.</cite> [<a href="https://scholar.google.com/scholar_lookup?journal=Ethical%20Theory%20and%20Moral%20Practice&amp;title=Virtue%20ethics%20and%20situationist%20personality%20psychology&amp;author=M%20Merritt&amp;volume=3&amp;issue=4&amp;publication_year=2000&amp;pages=365-383&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR37">
<cite>Miller KW, Wolf MJ, Grodzinsky F. This “ethical trap” is for roboticists, not robots: on the issue of artificial agent ethical decision-making. Science and Engineering Ethics. 2017;23(2):389–401. doi: 10.1007/s11948-016-9785-y.</cite> [<a href="https://doi.org/10.1007/s11948-016-9785-y" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/27116039/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Science%20and%20Engineering%20Ethics&amp;title=This%20%E2%80%9Cethical%20trap%E2%80%9D%20is%20for%20roboticists,%20not%20robots:%20on%20the%20issue%20of%20artificial%20agent%20ethical%20decision-making&amp;author=KW%20Miller&amp;author=MJ%20Wolf&amp;author=F%20Grodzinsky&amp;volume=23&amp;issue=2&amp;publication_year=2017&amp;pages=389-401&amp;pmid=27116039&amp;doi=10.1007/s11948-016-9785-y&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR38">
<cite>Moor JH. The nature, importance, and difficulty of machine ethics. IEEE Intelligent Systems. 2006;21(4):18–21.</cite> [<a href="https://scholar.google.com/scholar_lookup?journal=IEEE%20Intelligent%20Systems&amp;title=The%20nature,%20importance,%20and%20difficulty%20of%20machine%20ethics&amp;author=JH%20Moor&amp;volume=21&amp;issue=4&amp;publication_year=2006&amp;pages=18-21&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR39"><cite>Moor, J. (2009). Four kinds of ethical robots. <em>Philosophy Now</em>, (72), 12–14. Retrieved from <a href="https://philosophynow.org/issues/72/Four_Kinds_of_Ethical_Robots" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">https://philosophynow.org/issues/72/Four_Kinds_of_Ethical_Robots</a>. Accessed 10 Feb 2017.</cite></li>
<li id="CR40"><cite>Morals and the machine. (2012). <em>The Economist</em>. Retrieved from <a href="http://www.economist.com/node/21556234" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">http://www.economist.com/node/21556234</a>. Accessed 7 Mar 2017.</cite></li>
<li id="CR41"><cite>Nagenborg, M. (2007). Artificial moral agents: An intercultural perspective. <em>International Review of Information Ethics,</em><em>7,</em> 129–133. <a href="http://www.i-r-i-e.net/inhalt/007/13-nagenborg.pdf" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">http://www.i-r-i-e.net/inhalt/007/13-nagenborg.pdf</a>. Retrieved 12 Feb 2018.</cite></li>
<li id="CR42">
<cite>Nissenbaum H. How computer systems embody values. Computer -Los Almalitos- 2001;34:120.</cite> [<a href="https://scholar.google.com/scholar_lookup?journal=Computer%20-Los%20Almalitos-&amp;title=How%20computer%20systems%20embody%20values&amp;author=H%20Nissenbaum&amp;volume=34&amp;publication_year=2001&amp;pages=120&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR43"><cite>Peters, A. (2018). Having a heart attack? This AI helps emergency dispatchers find out. Retrieved January 16, 2018, from <a href="https://www.fastcompany.com/40515740/having-a-heart-attack-this-ai-helps-emergency-dispatchers-find-out" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">https://www.fastcompany.com/40515740/having-a-heart-attack-this-ai-helps-emergency-dispatchers-find-out</a>.</cite></li>
<li id="CR44">
<cite>Pizarro D. Nothing More than Feelings? The Role of Emotions in Moral Judgment. Journal for the Theory of Social Behaviour. 2000;30(4):355–375.</cite> [<a href="https://scholar.google.com/scholar_lookup?journal=Journal%20for%20the%20Theory%20of%20Social%20Behaviour&amp;title=Nothing%20More%20than%20Feelings?%20The%20Role%20of%20Emotions%20in%20Moral%20Judgment&amp;author=D%20Pizarro&amp;volume=30&amp;issue=4&amp;publication_year=2000&amp;pages=355-375&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR45">
<cite>Roeser S.  Moral emotions and intuitions. Berlin: Springer; 2010. </cite> [<a href="https://scholar.google.com/scholar_lookup?title=Moral%20emotions%20and%20intuitions&amp;author=S%20Roeser&amp;publication_year=2010&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR46"><cite>Rutkin, A. (2014). Ethical trap: Robot paralysed by choice of who to save. <a href="https://www.newscientist.com/article/mg22329863-700-ethical-trap-robot-paralysed-by-choice-of-who-to-save/" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">https://www.newscientist.com/article/mg22329863-700-ethical-trap-robot-paralysed-by-choice-of-who-to-save/</a>. Retrieved 12 Feb 2018.</cite></li>
<li id="CR47"><cite>Scheutz, M. (2016). The need for moral competency in autonomous agent architectures. In V. C. Müller (Ed.) (pp. 515–525). Springer International Publishing. Retrieved from <a href="http://link.springer.com/chapter/10.1007/978-3-319-26485-1_30" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">http://link.springer.com/chapter/10.1007/978-3-319-26485-1_30</a>. Accessed 29 Aug 2016.</cite></li>
<li id="CR48">
<cite>Shafer-Landau R. Ethical disagreement, ethical objectivism and moral indeterminacy. Philosophy and Phenomenological Research. 1994;54(2):331–344.</cite> [<a href="https://scholar.google.com/scholar_lookup?journal=Philosophy%20and%20Phenomenological%20Research&amp;title=Ethical%20disagreement,%20ethical%20objectivism%20and%20moral%20indeterminacy&amp;author=R%20Shafer-Landau&amp;volume=54&amp;issue=2&amp;publication_year=1994&amp;pages=331-344&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR49">
<cite>Sharkey A. Should we welcome robot teachers? Ethics and Information Technology. 2016</cite> [<a href="https://scholar.google.com/scholar_lookup?journal=Ethics%20and%20Information%20Technology&amp;title=Should%20we%20welcome%20robot%20teachers?&amp;author=A%20Sharkey&amp;publication_year=2016&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR50">
<cite>Sharkey A. Can robots be responsible moral agents? And why should we care? Connection Science. 2017;29(3):210–216.</cite> [<a href="https://scholar.google.com/scholar_lookup?journal=Connection%20Science&amp;title=Can%20robots%20be%20responsible%20moral%20agents?%20And%20why%20should%20we%20care?&amp;author=A%20Sharkey&amp;volume=29&amp;issue=3&amp;publication_year=2017&amp;pages=210-216&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR51">
<cite>Sharkey N. The ethical frontiers of robotics. Science. 2008;322(5909):1800–1801. doi: 10.1126/science.1164582.</cite> [<a href="https://doi.org/10.1126/science.1164582" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/19095930/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Science&amp;title=The%20ethical%20frontiers%20of%20robotics&amp;author=N%20Sharkey&amp;volume=322&amp;issue=5909&amp;publication_year=2008&amp;pages=1800-1801&amp;pmid=19095930&amp;doi=10.1126/science.1164582&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR52">
<cite>Sharkey N. The evitability of autonomous robot warfare. International Review of the Red Cross. 2012;94(886):787–799.</cite> [<a href="https://scholar.google.com/scholar_lookup?journal=International%20Review%20of%20the%20Red%20Cross&amp;title=The%20evitability%20of%20autonomous%20robot%20warfare&amp;author=N%20Sharkey&amp;volume=94&amp;issue=886&amp;publication_year=2012&amp;pages=787-799&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR53">
<cite>Sharkey N, Sharkey A.  The Rights and Wrongs of Robot Care. In: Lin P, Abney K, Bekey GA, editors. Robot ethics: The ethical and social implications of robotics. Cambridge: MIT Press; 2011. pp. 267–282.</cite> [<a href="https://scholar.google.com/scholar_lookup?title=Robot%20ethics:%20The%20ethical%20and%20social%20implications%20of%20robotics&amp;author=N%20Sharkey&amp;author=A%20Sharkey&amp;publication_year=2011&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR54"><cite>Sharkey, N., van Wynsberghe, A., Robbins, S., &amp; Hancock, E. (2017). <em>Our Sexual Future with Robots</em>. The Hague, Netherlands. Retrieved from <a href="https://responsible-roboticsmyxf6pn3xr.netdna-ssl.com/wp-content/uploads/2017/11/FRR-Consultation-Report-Our-Sexual-Future-with-robots-.pdf" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">https://responsible-roboticsmyxf6pn3xr.netdna-ssl.com/wp-content/uploads/2017/11/FRR-Consultation-Report-Our-Sexual-Future-with-robots-.pdf</a>. Accessed 1 Feb 2018.</cite></li>
<li id="CR55"><cite>Shirky, C. (2009). A speculative post on the idea of algorithmic authority. <a href="http://www.shirky.com/weblog/2009/11/a-speculative-post-on-the-idea-of-algorithmic-authority/" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">http://www.shirky.com/weblog/2009/11/a-speculative-post-on-the-idea-of-algorithmic-authority/</a>. Retrieved 12 Feb 2018.</cite></li>
<li id="CR56">
<cite>Simon J. The entanglement of trust and knowledge on the Web. Ethics and Information Technology. 2010;12(4):343–355.</cite> [<a href="https://scholar.google.com/scholar_lookup?journal=Ethics%20and%20Information%20Technology&amp;title=The%20entanglement%20of%20trust%20and%20knowledge%20on%20the%20Web&amp;author=J%20Simon&amp;volume=12&amp;issue=4&amp;publication_year=2010&amp;pages=343-355&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR57">
<cite>Street S. A darwinian dilemma for realist theories of value. Philosophical Studies. 2006;127(1):109–166.</cite> [<a href="https://scholar.google.com/scholar_lookup?journal=Philosophical%20Studies&amp;title=A%20darwinian%20dilemma%20for%20realist%20theories%20of%20value&amp;author=S%20Street&amp;volume=127&amp;issue=1&amp;publication_year=2006&amp;pages=109-166&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR58">
<cite>Tonkens R. A challenge for machine ethics. Minds and Machines. 2009;19(3):421–438.</cite> [<a href="https://scholar.google.com/scholar_lookup?journal=Minds%20and%20Machines&amp;title=A%20challenge%20for%20machine%20ethics&amp;author=R%20Tonkens&amp;volume=19&amp;issue=3&amp;publication_year=2009&amp;pages=421-438&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR59">
<cite>Vallor S. Moral deskilling and upskilling in a new machine age: Reflections on the ambiguous future of character. Philosophy &amp; Technology. 2015;28(1):107–124.</cite> [<a href="https://scholar.google.com/scholar_lookup?journal=Philosophy%20&amp;%20Technology&amp;title=Moral%20deskilling%20and%20upskilling%20in%20a%20new%20machine%20age:%20Reflections%20on%20the%20ambiguous%20future%20of%20character&amp;author=S%20Vallor&amp;volume=28&amp;issue=1&amp;publication_year=2015&amp;pages=107-124&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR60">
<cite>van de Poel I.  Translating Values into design requirements. In: Mitchfelder D, McCarty N, Goldberg DE, editors. Philosophy and engineering: Reflections on practice, principles, and process. Dordrecht: Springer; 2013. </cite> [<a href="https://scholar.google.com/scholar_lookup?title=Philosophy%20and%20engineering:%20Reflections%20on%20practice,%20principles,%20and%20process&amp;author=I%20van%20de%20Poel&amp;publication_year=2013&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR61">
<cite>van den Hoven J.  ICT and value sensitive design. In: Goujon P, Lavelle S, Duquenoy P, Kimppa K, editors. The information society: Innovation, legitimacy, ethics and democracy in honor of professor Jacques Berleur s.j. Boston: Springer; 2007. pp. 67–72.</cite> [<a href="https://scholar.google.com/scholar_lookup?title=The%20information%20society:%20Innovation,%20legitimacy,%20ethics%20and%20democracy%20in%20honor%20of%20professor%20Jacques%20Berleur%20s.j&amp;author=J%20van%20den%20Hoven&amp;publication_year=2007&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR62">
<cite>van Wynsberghe A. Designing robots for care: Care centered value-sensitive design. Science and Engineering Ethics. 2012;19(2):407–433. doi: 10.1007/s11948-011-9343-6.</cite> [<a href="https://doi.org/10.1007/s11948-011-9343-6" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC3662860/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/22212357/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Science%20and%20Engineering%20Ethics&amp;title=Designing%20robots%20for%20care:%20Care%20centered%20value-sensitive%20design&amp;author=A%20van%20Wynsberghe&amp;volume=19&amp;issue=2&amp;publication_year=2012&amp;pages=407-433&amp;pmid=22212357&amp;doi=10.1007/s11948-011-9343-6&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR63">
<cite>van Wynsberghe A. A method for integrating ethics into the design of robots. Industrial Robot: An International Journal. 2013;40(5):433–440.</cite> [<a href="https://scholar.google.com/scholar_lookup?journal=Industrial%20Robot:%20An%20International%20Journal&amp;title=A%20method%20for%20integrating%20ethics%20into%20the%20design%20of%20robots&amp;author=A%20van%20Wynsberghe&amp;volume=40&amp;issue=5&amp;publication_year=2013&amp;pages=433-440&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR64"><cite>van Wynsberghe, A. (2015). <em>Healthcare robots: Ethics, design and implementation</em>. <em>Healthcare Robots: Ethics, Design and Implementation</em>. Retrieved from <a href="https://www.scopus.com/inward/record.uri%3feid%3d2-s2.0-84946412196%26partnerID%3d40%26md5%3d5c270c5c2c8d9f4983cbe6c4f2369c97" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">https://www.scopus.com/inward/record.uri?eid=2-s2.0-84946412196&amp;partnerID=40&amp;md5=5c270c5c2c8d9f4983cbe6c4f2369c97</a>. Accessed 29 Aug 2016.</cite></li>
<li id="CR65">
<cite>van Wynsberghe A. Service robots, care ethics, and design. Ethics and Information Technology. 2016;18(4):311–321.</cite> [<a href="https://scholar.google.com/scholar_lookup?journal=Ethics%20and%20Information%20Technology&amp;title=Service%20robots,%20care%20ethics,%20and%20design&amp;author=A%20van%20Wynsberghe&amp;volume=18&amp;issue=4&amp;publication_year=2016&amp;pages=311-321&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR66">
<cite>van Wynsberghe, A., &amp; Robbins, S. (2014). Ethicist as designer: A pragmatic approach to ethics in the lab. <em>Science and Engineering Ethics</em>, <em>20</em>(4), 947–961. 10.1007/s11948-013-9498-4.</cite> [<a href="https://doi.org/10.1007/s11948-013-9498-4" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/24254219/" class="usa-link">PubMed</a>]</li>
<li id="CR67">
<cite>Waldrop MM. A question of responsibility. AI Magazine. 1987;8(1):28.</cite> [<a href="https://scholar.google.com/scholar_lookup?journal=AI%20Magazine&amp;title=A%20question%20of%20responsibility&amp;author=MM%20Waldrop&amp;volume=8&amp;issue=1&amp;publication_year=1987&amp;pages=28&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR68">
<cite>Wallach W. Implementing moral decision making faculties in computers and robots. AI &amp; Society. 2007;22(4):463–475.</cite> [<a href="https://scholar.google.com/scholar_lookup?journal=AI%20&amp;%20Society&amp;title=Implementing%20moral%20decision%20making%20faculties%20in%20computers%20and%20robots&amp;author=W%20Wallach&amp;volume=22&amp;issue=4&amp;publication_year=2007&amp;pages=463-475&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR69">
<cite>Wallach W. Robot minds and human ethics: The need for a comprehensive model of moral decision making. Ethics and Information Technology. 2010;12(3):243–250.</cite> [<a href="https://scholar.google.com/scholar_lookup?journal=Ethics%20and%20Information%20Technology&amp;title=Robot%20minds%20and%20human%20ethics:%20The%20need%20for%20a%20comprehensive%20model%20of%20moral%20decision%20making&amp;author=W%20Wallach&amp;volume=12&amp;issue=3&amp;publication_year=2010&amp;pages=243-250&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR70"><cite>Wallach, W., &amp; Allen, C. (2010). <em>Moral machines: Teaching robots right from wrong</em> (1st ed.). New York: Oxford University Press. Retrieved from <a href="https://www.amazon.com/Moral-Machines-Teaching-Robots-Right/dp/0199737975" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">https://www.amazon.com/Moral-Machines-Teaching-Robots-Right/dp/0199737975</a>. Accessed 10 Feb 2017.</cite></li>
<li id="CR71"><cite>Wiegel, V. (2006). <em>Building blocks for artificial moral agents</em>. In <em>Proceedings of EthicalALife06 Workshop</em>. <a href="https://www.researchgate.net/profile/Vincent_Wiegel/publication/228615030_Building_blocks_for_artificial_moral_agents/links/55fabe5708aeafc8ac3fe6f8/Buildingblocks-for-artificial-moral-agents.pdf" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">https://www.researchgate.net/profile/Vincent_Wiegel/publication/228615030_Building_blocks_for_artificial_moral_agents/links/55fabe5708aeafc8ac3fe6f8/Buildingblocks-for-artificial-moral-agents.pdf</a>. Retrieved 12 Feb 2018.</cite></li>
<li id="CR72">
<cite>Wiegel V. Wendell Wallach and Colin Allen: Moral machines: Teaching robots right from wrong. Ethics and Information Technology. 2010;12(4):359–361.</cite> [<a href="https://scholar.google.com/scholar_lookup?journal=Ethics%20and%20Information%20Technology&amp;title=Wendell%20Wallach%20and%20Colin%20Allen:%20Moral%20machines:%20Teaching%20robots%20right%20from%20wrong&amp;author=V%20Wiegel&amp;volume=12&amp;issue=4&amp;publication_year=2010&amp;pages=359-361&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
</ol></section></section></section><footer class="p courtesy-note font-secondary font-sm text-center"><hr class="headless">
<p>Articles from Science and Engineering Ethics are provided here courtesy of <strong>Springer</strong></p></footer></section></article>

                      

                    </main>
                </div>
            </div>
        </div>

        



<!-- Secondary navigation placeholder -->
<div class="pmc-sidenav desktop:grid-col-4 display-flex">
    <section class="pmc-sidenav__container" aria-label="Article resources and navigation">
        <button type="button" class="usa-button pmc-sidenav__container__close usa-button--unstyled">
            <img src="/static/img/usa-icons/close.svg" role="img" alt="Close" />
        </button>
    <div class="display-none desktop:display-block">
       <section class="margin-top-4 desktop:margin-top-0">
              <h2 class="margin-top-0">ACTIONS</h2>
           <ul class="usa-list usa-list--unstyled usa-list--actions">
               
               <li>
                     <a
                             href="https://doi.org/10.1007/s11948-018-0030-8"
                             class="usa-button usa-button--outline width-24 font-xs display-inline-flex flex-align-center flex-justify-start padding-left-1"
                             target="_blank"
                             rel="noreferrer noopener"
                             data-ga-category="actions"
                             data-ga-action="click"
                             data-ga-label="publisher_link_desktop"
                     >
                         <svg class="usa-icon width-3 height-3" aria-hidden="true" focusable="false" role="img" hidden>
                            <use xlink:href="/static/img/sprite.svg#launch"></use>
                         </svg>
                         <span class="display-inline-flex flex-justify-center flex-1 padding-right-2">View on publisher site</span>
                     </a>
               </li>
               
               
               <li>
                    <a
                            href="pdf/11948_2018_Article_30.pdf"
                            class="usa-button usa-button--outline width-24 display-inline-flex flex-align-center flex-justify-start padding-left-1"
                            data-ga-category="actions"
                            data-ga-action="click"
                            data-ga-label="pdf_download_desktop"
                    >
                         <svg class="usa-icon width-3 height-3" aria-hidden="true" focusable="false" role="img" hidden>
                            <use xlink:href="/static/img/sprite.svg#file_download"></use>
                        </svg>
                        <span class="display-inline-flex flex-justify-center flex-1">PDF (676.6 KB)</span>
                    </a>
               </li>
               
                
               <li>
                   <button role="button" class="usa-button width-24 citation-dialog-trigger display-inline-flex flex-align-center flex-justify-start padding-left-1"
                        aria-label="Open dialog with citation text in different styles"
                        data-ga-category="actions"
                        data-ga-action="open"
                        data-ga-label="cite_desktop"
                        data-all-citations-url="/resources/citations/6591188/"
                        data-citation-style="nlm"
                        data-download-format-link="/resources/citations/6591188/export/"
                    >
                        <svg class="usa-icon width-3 height-3" aria-hidden="true" focusable="false" role="img" hidden>
                            <use xlink:href="/static/img/sprite.svg#format_quote"></use>
                        </svg>
                       <span class="display-inline-flex flex-justify-center flex-1 button-label">Cite</span>
                    </button>
               </li>
                
               <li>

                        <button class="usa-button width-24 collections-dialog-trigger collections-button display-inline-flex flex-align-center flex-justify-start padding-left-1 collections-button-empty"
                              aria-label="Save article in MyNCBI collections."
                              data-ga-category="actions"
                              data-ga-action="click"
                              data-ga-label="collections_button_desktop"
                              data-collections-open-dialog-enabled="false"
                              data-collections-open-dialog-url="https://account.ncbi.nlm.nih.gov/?back_url=https%3A%2F%2Fpmc.ncbi.nlm.nih.gov%2Farticles%2FPMC6591188%2F%23open-collections-dialog"
                              data-in-collections="false">
                            <svg class="usa-icon width-3 height-3 usa-icon--bookmark-full" aria-hidden="true" focusable="false" role="img" hidden>
                                <use xlink:href="/static/img/action-bookmark-full.svg#icon"></use>
                            </svg>
                            <svg class="usa-icon width-3 height-3 usa-icon--bookmark-empty" aria-hidden="true" focusable="false" role="img" hidden>
                                <use xlink:href="/static/img/action-bookmark-empty.svg#icon"></use>
                            </svg>
                            <span class="display-inline-flex flex-justify-center flex-1">Collections</span>
                       </button>
               </li>
               <li class="pmc-permalink">
                    <button
                            type="button"
                            class="usa-button width-24 display-inline-flex flex-align-center flex-justify padding-left-1 shadow-none"
                            aria-label="Show article permalink"
                            aria-expanded="false"
                            aria-haspopup="true"
                            data-ga-category="actions"
                            data-ga-action="open"
                            data-ga-label="permalink_desktop"
                    >
                         <svg class="usa-icon width-3 height-3" aria-hidden="true" focusable="false" role="img" hidden>
                            <use xlink:href="/static/img/sprite.svg#share"></use>
                        </svg>
                        <span class="display-inline-flex flex-justify-center flex-1 button-label">Permalink</span>
                    </button>
                   

<div class="pmc-permalink__dropdown" hidden>
    <div class="pmc-permalink__dropdown__container">
          <h2 class="usa-modal__heading margin-top-0 margin-bottom-2 text-uppercase font-sans-xs">PERMALINK</h2>
          <div class="pmc-permalink__dropdown__content">
              <input type="text" class="usa-input" value="https://pmc.ncbi.nlm.nih.gov/articles/PMC6591188/" aria-label="Article permalink">
              <button class="usa-button display-inline-flex pmc-permalink__dropdown__copy__btn margin-right-0" title="Copy article permalink" data-ga-category="save_share" data-ga-action="link" data-ga-label="copy_link">
                  <svg class="usa-icon" aria-hidden="true" focusable="false" role="img">
                    <use xlink:href="/static/img/sprite.svg#content_copy"></use>
                  </svg>
                  <span class="margin-left-1">Copy</span>
              </button>
          </div>
    </div>
</div>
               </li>
           </ul>
       </section>
     </div>

        <section class="pmc-resources margin-top-6 desktop:margin-top-4" data-page-path="/articles/PMC6591188/">
            <h2 class="margin-top-0">RESOURCES</h2>
            
                <div class="usa-accordion usa-accordion--multiselectable" data-allow-multiple>
                    <h3 class="usa-accordion__heading">
                        <button
                        type="button"
                        class="usa-accordion__button"
                        aria-expanded="false"
                        aria-controls="resources-similar-articles"
                        data-ga-category="resources_accordion"
                        data-ga-action="open_similar_articles"
                        data-ga-label="/articles/PMC6591188/"
                        data-action-open="open_similar_articles"
                        data-action-close="close_similar_articles"
                        >
                            Similar articles
                        </button>
                    </h3>
                    <div
                            id="resources-similar-articles"
                            class="usa-accordion__content usa-prose"
                            
                                data-source-url="/resources/similar-article-links/29460081/"
                            
                    >
                        
                    </div>
                    <h3 class="usa-accordion__heading">
                        <button
                        type="button"
                        class="usa-accordion__button"
                        aria-expanded="false"
                        aria-controls="resources-cited-by-other-articles"
                        data-ga-category="resources_accordion"
                        data-ga-action="open_cited_by"
                        data-ga-label="/articles/PMC6591188/"
                        data-action-open="open_cited_by"
                        data-action-close="close_cited_by"
                        >
                             Cited by other articles
                        </button>
                    </h3>
                    <div
                            id="resources-cited-by-other-articles"
                            class="usa-accordion__content usa-prose"
                            
                                data-source-url="/resources/cited-by-links/29460081/"
                            
                    >
                          
                    </div>
                    
                        <h3 class="usa-accordion__heading">
                            <button
                            type="button"
                            class="usa-accordion__button"
                            aria-expanded="false"
                            aria-controls="resources-links-to-ncbi-databases"
                            data-ga-category="resources_accordion"
                            data-ga-action="open_NCBI_links"
                            data-ga-label="/articles/PMC6591188/"
                            data-action-open="open_NCBI_links"
                            data-action-close="close_NCBI_link"
                            >
                                 Links to NCBI Databases
                            </button>
                        </h3>
                        <div
                                id="resources-links-to-ncbi-databases"
                                class="usa-accordion__content usa-prose"
                                data-source-url="/resources/db-links/6591188/"
                        >
                        </div>
                    
                    
                </div>
            
        </section>


        <section
        class="usa-in-page-nav usa-in-page-nav--wide margin-top-6 desktop:margin-top-4"
        data-title-text="On this page"
        data-title-heading-level="h2"
        data-scroll-offset="0"
        data-root-margin="-10% 0px -80% 0px"
        data-main-content-selector="main"
        data-threshold="1"
        hidden
        ></section>
    </section>
</div>


        

<div class="overlay" role="dialog" aria-label="Citation Dialog" hidden>
    <div class="dialog citation-dialog" aria-hidden="true">
        <div class="display-inline-flex flex-align-center flex-justify width-full margin-bottom-2">
            <h2 class="usa-modal__heading margin-0">Cite</h2>
             <button type="button" class="usa-button usa-button--unstyled close-overlay text-black width-auto"  tabindex="1">
                <svg class="usa-icon width-3 height-3" aria-hidden="true" focusable="false" role="img">
                    <use xlink:href="/static/img/sprite.svg#close"></use>
                </svg>
             </button>
        </div>

        

<div class="citation-text-block">
  <div class="citation-text margin-bottom-2"></div>
  <ul class="usa-list usa-list--unstyled display-inline-flex flex-justify width-full flex-align-center">
      <li>
        <button
          class="usa-button usa-button--unstyled text-no-underline display-flex flex-align-center copy-button dialog-focus"
          data-ga-category="save_share"
          data-ga-action="cite"
          data-ga-label="copy"
          tabindex="2">
            <svg class="usa-icon width-3 height-3" aria-hidden="true" focusable="false" role="img">
                <use xlink:href="/static/img/sprite.svg#content_copy"></use>
            </svg>
            <span>Copy</span>
        </button>
      </li>
      <li>
          <a
              href="#"
              role="button"
              class="usa-button usa-button--unstyled text-no-underline display-flex flex-align-center export-button"
              data-ga-category="save_share"
              data-ga-action="cite"
              data-ga-label="download"
              title="Download a file for external citation management software"
              tabindex="3">
                <svg class="usa-icon width-3 height-3" aria-hidden="true" focusable="false" role="img">
                    <use xlink:href="/static/img/sprite.svg#file_download"></use>
                </svg>
                <span class="display-none mobile-lg:display-inline">Download .nbib</span>
                <span class="display-inline mobile-lg:display-none">.nbib</span>
            </a>
      </li>
      <li>
          

<div class="display-inline-flex flex-align-center">
  <label class="usa-label margin-top-0">Format:</label>
  <select aria-label="Format" class="usa-select citation-style-selector padding-1 margin-top-0 border-0 padding-right-4" tabindex="4" >
    
      <option data-style-url-name="ama"
              value="AMA"
              >
        AMA
      </option>
    
      <option data-style-url-name="apa"
              value="APA"
              >
        APA
      </option>
    
      <option data-style-url-name="mla"
              value="MLA"
              >
        MLA
      </option>
    
      <option data-style-url-name="nlm"
              value="NLM"
              selected="selected">
        NLM
      </option>
    
  </select>
</div>
      </li>
  </ul>
</div>
    </div>
</div>

        <div class="overlay" role="dialog" hidden>
  <div id="collections-action-dialog" class="dialog collections-dialog" aria-hidden="true">
   <div class="display-inline-flex flex-align-center flex-justify width-full margin-bottom-2">
        <h2 class="usa-modal__heading margin-0">Add to Collections</h2>
    </div>
    <div class="collections-action-panel action-panel">
      


<form id="collections-action-dialog-form"
      class="usa-form maxw-full collections-action-panel-form action-panel-content action-form action-panel-smaller-selectors"
      data-existing-collections-url="/list-existing-collections/"
      data-add-to-existing-collection-url="/add-to-existing-collection/"
      data-create-and-add-to-new-collection-url="/create-and-add-to-new-collection/"
      data-myncbi-max-collection-name-length="100"
      data-collections-root-url="https://www.ncbi.nlm.nih.gov/myncbi/collections/">

    <input type="hidden" name="csrfmiddlewaretoken" value="fDYnAPMbeqY93gdkcU1pEnuqcfd8bMPxg8ShbLz0hSiOecHcq9NqzVT6a8EDVkT5">

    <fieldset class="usa-fieldset margin-bottom-2">
        <div class="usa-radio">
            <input type="radio"
            id="collections-action-dialog-new"
            class="usa-radio__input usa-radio__input--tile collections-new  margin-top-0"
            name="collections"
            value="new"
            data-ga-category="collections_button"
            data-ga-action="click"
            data-ga-label="collections_radio_new" />
            <label class="usa-radio__label" for="collections-action-dialog-new">Create a new collection</label>
        </div>
        <div class="usa-radio">
            <input type="radio"
            id="collections-action-dialog-existing"
            class="usa-radio__input usa-radio__input--tile collections-existing"
            name="collections"
            value="existing"
            checked="true"
            data-ga-category="collections_button"
            data-ga-action="click"
            data-ga-label="collections_radio_existing" />
            <label class="usa-radio__label" for="collections-action-dialog-existing">Add to an existing collection</label>
        </div>
    </fieldset>

    <fieldset class="usa-fieldset margin-bottom-2">
        <div class="action-panel-control-wrap new-collections-controls">
           <label for="collections-action-dialog-add-to-new" class="usa-label margin-top-0">
                Name your collection
               <abbr title="required" class="usa-hint usa-hint--required text-no-underline">*</abbr>
          </label>
          <input
            type="text"
            name="add-to-new-collection"
            id="collections-action-dialog-add-to-new"
            class="usa-input collections-action-add-to-new"
            pattern="[^&quot;&amp;=&lt;&gt;/]*" title="The following characters are not allowed in the Name field: &quot;&amp;=&lt;&gt;/"
            maxlength=""
            data-ga-category="collections_button"
            data-ga-action="create_collection"
            data-ga-label="non_favorties_collection"
            required
          />
        </div>
        <div class="action-panel-control-wrap existing-collections-controls">
             <label for="collections-action-dialog-add-to-existing" class="usa-label margin-top-0">
                Choose a collection
              </label>
              <select id="collections-action-dialog-add-to-existing"
                      class="usa-select collections-action-add-to-existing"
                      data-ga-category="collections_button"
                      data-ga-action="select_collection"
                      data-ga-label="($('.collections-action-add-to-existing').val() === 'Favorites') ? 'Favorites' : 'non_favorites_collection'">
              </select>
              <div class="collections-retry-load-on-error usa-input-error-message selection-validation-message">
                Unable to load your collection due to an error<br>
                <a href="#">Please try again</a>
              </div>
        </div>
    </fieldset>

    <div class="display-inline-flex">
        <button class="usa-button margin-top-0 action-panel-submit"
            type="submit"
            data-loading-label="Adding..."
            data-pinger-ignore
            data-ga-category="collections_button"
            data-ga-action="click"
            data-ga-label="add">
          Add
        </button>
        <button class="usa-button usa-button--outline margin-top-0 action-panel-cancel"
                aria-label="Close 'Add to Collections' panel"
                ref="linksrc=close_collections_panel"
                data-ga-category="collections_button"
                data-ga-action="click"
                data-ga-label="cancel">
          Cancel
        </button>
    </div>
</form>
    </div>
  </div>
</div>

        

      </div>
    </div>
  </div>



        
    
    

<footer class="ncbi-footer ncbi-dark-background " >
    
        <div class="ncbi-footer__icon-section">
            <div class="ncbi-footer__social-header">
                Follow NCBI
            </div>

            <div class="grid-container ncbi-footer__ncbi-social-icons-container">
                
                    <a href="https://twitter.com/ncbi"
                       class="ncbi-footer__social-icon ncbi-footer__social-icon--gray"
                       target="_blank"
                       rel="noreferrer noopener">
                        <svg width="40"
                             height="40"
                             viewBox="0 0 40 40"
                             fill="none"
                             xmlns="http://www.w3.org/2000/svg"
                             focusable="false"
                             aria-hidden="true">
                            <path d="m6.067 8 10.81 13.9L6 33.2h4.2l8.4-9.1 7.068 9.1H34L22.8 18.5 31.9 8h-3.5l-7.7 8.4L14.401 8H6.067Zm3.6 1.734h3.266l16.8 21.732H26.57L9.668 9.734Z">
                            </path>
                        </svg>
                        <span class="usa-sr-only">NCBI on X (formerly known as Twitter)</span>
                    </a>
                

                
                    <a href="https://www.facebook.com/ncbi.nlm"
                       class="ncbi-footer__social-icon ncbi-footer__social-icon--gray"
                       target="_blank"
                       rel="noreferrer noopener">
                        <svg width="16"
                             height="29"
                             focusable="false"
                             aria-hidden="true"
                             viewBox="0 0 16 29"
                             fill="none"
                             xmlns="http://www.w3.org/2000/svg">
                            <path d="M3.8809 21.4002C3.8809 19.0932 3.8809 16.7876 3.8809 14.478C3.8809 14.2117 3.80103 14.1452 3.54278 14.1492C2.53372 14.1638 1.52334 14.1492 0.514288 14.1598C0.302626 14.1598 0.248047 14.0972 0.248047 13.8936C0.256034 12.4585 0.256034 11.0239 0.248047 9.58978C0.248047 9.37013 0.302626 9.30224 0.528931 9.3049C1.53798 9.31688 2.54837 9.3049 3.55742 9.31555C3.80103 9.31555 3.8809 9.26097 3.87957 9.00272C3.87158 8.00565 3.85428 7.00592 3.90753 6.00884C3.97142 4.83339 4.31487 3.73115 5.04437 2.78467C5.93095 1.63318 7.15699 1.09005 8.56141 0.967577C10.5582 0.79319 12.555 0.982221 14.5518 0.927641C14.7102 0.927641 14.7462 0.99287 14.7449 1.13664C14.7449 2.581 14.7449 4.02668 14.7449 5.47104C14.7449 5.67604 14.6517 5.68669 14.4946 5.68669C13.4523 5.68669 12.4113 5.68669 11.3703 5.68669C10.3506 5.68669 9.92057 6.10868 9.90593 7.13904C9.89661 7.7647 9.91525 8.39303 9.89794 9.01869C9.88995 9.26364 9.96583 9.31822 10.2015 9.31688C11.7204 9.30623 13.2393 9.31688 14.7595 9.3049C15.0257 9.3049 15.0723 9.3728 15.0444 9.62439C14.89 10.9849 14.7515 12.3467 14.6144 13.7085C14.5691 14.1571 14.5785 14.1585 14.1458 14.1585C12.8386 14.1585 11.5313 14.1665 10.2254 14.1518C9.95119 14.1518 9.89794 14.2317 9.89794 14.4899C9.90593 19.0799 9.89794 23.6752 9.91125 28.2612C9.91125 28.5674 9.8407 28.646 9.53186 28.6433C7.77866 28.6273 6.02414 28.6366 4.27094 28.634C3.82499 28.634 3.87158 28.6992 3.87158 28.22C3.87602 25.9472 3.87913 23.6739 3.8809 21.4002Z">
                            </path>
                        </svg>
                        <span class="usa-sr-only">NCBI on Facebook</span>
                    </a>
                

                
                    <a href="https://www.linkedin.com/company/ncbinlm"
                       class="ncbi-footer__social-icon ncbi-footer__social-icon--gray"
                       target="_blank"
                       rel="noreferrer noopener">
                        <svg width="25"
                             height="23"
                             viewBox="0 0 26 24"
                             fill="none"
                             xmlns="http://www.w3.org/2000/svg"
                             focusable="false"
                             aria-hidden="true">
                            <path d="M14.6983 9.98423C15.6302 9.24808 16.5926 8.74754 17.6762 8.51991C19.673 8.09126 21.554 8.30824 23.1262 9.7526C24.2351 10.7723 24.7529 12.1115 25.0165 13.5612C25.1486 14.3363 25.2105 15.1218 25.2015 15.9081C25.2015 18.3043 25.2015 20.6898 25.2082 23.0806C25.2082 23.3468 25.1549 23.444 24.8621 23.4414C23.1297 23.4272 21.3992 23.4272 19.6704 23.4414C19.4041 23.4414 19.3429 23.3588 19.3442 23.1019C19.3535 20.5194 19.3442 17.9368 19.3442 15.3543C19.3442 14.0005 18.3258 12.9448 17.0266 12.9488C15.7273 12.9528 14.6983 14.0071 14.6983 15.361C14.6983 17.9328 14.6917 20.5047 14.6983 23.0753C14.6983 23.3708 14.6198 23.444 14.3296 23.4427C12.6185 23.4294 10.9079 23.4294 9.19779 23.4427C8.93155 23.4427 8.86099 23.3735 8.86232 23.1086C8.8783 19.7619 8.88628 16.4144 8.88628 13.066C8.88628 11.5688 8.87874 10.0708 8.86365 8.57182C8.86365 8.3575 8.90758 8.27896 9.14054 8.28029C10.9048 8.29094 12.6687 8.29094 14.4321 8.28029C14.6464 8.28029 14.6983 8.34818 14.6983 8.54653C14.6903 9.00047 14.6983 9.45441 14.6983 9.98423Z">
                            </path>
                            <path d="M6.55316 15.8443C6.55316 18.2564 6.55316 20.6699 6.55316 23.082C6.55316 23.3629 6.48127 23.4388 6.19906 23.4374C4.47737 23.4241 2.75568 23.4241 1.03399 23.4374C0.767751 23.4374 0.69986 23.3629 0.701191 23.1006C0.709178 18.2648 0.709178 13.4281 0.701191 8.59053C0.701191 8.34026 0.765089 8.27237 1.01669 8.2737C2.74991 8.28435 4.48048 8.28435 6.20838 8.2737C6.47462 8.2737 6.5465 8.33627 6.54517 8.6065C6.54783 11.0186 6.55316 13.4308 6.55316 15.8443Z">
                            </path>
                            <path d="M3.65878 0.243898C5.36804 0.243898 6.58743 1.45529 6.58743 3.1406C6.58743 4.75801 5.32145 5.95742 3.60819 5.96807C3.22177 5.97614 2.83768 5.90639 2.47877 5.76299C2.11985 5.61959 1.79344 5.40546 1.51897 5.13334C1.24449 4.86123 1.02755 4.53668 0.881058 4.17902C0.734563 3.82136 0.661505 3.43788 0.666231 3.05141C0.67555 1.42601 1.9362 0.242566 3.65878 0.243898Z">
                            </path>
                        </svg>
                        <span class="usa-sr-only">NCBI on LinkedIn</span>
                    </a>
                

                
                    <a href="https://github.com/ncbi"
                       class="ncbi-footer__social-icon ncbi-footer__social-icon--gray"
                       target="_blank"
                       rel="noreferrer noopener">
                        <svg width="28"
                             height="27"
                             viewBox="0 0 28 28"
                             fill="none"
                             xmlns="http://www.w3.org/2000/svg"
                             focusable="false"
                             aria-hidden="true">
                            <path d="M16.7228 20.6334C17.5057 20.5527 18.2786 20.3944 19.0301 20.1608C21.3108 19.4193 22.5822 17.8259 22.963 15.4909C23.1228 14.5112 23.1814 13.5287 22.9883 12.5437C22.8106 11.6423 22.4013 10.8028 21.8007 10.1076C21.7526 10.0605 21.7197 10 21.7064 9.934C21.6931 9.86799 21.7 9.79952 21.7262 9.73748C22.0856 8.6206 21.9711 7.51969 21.601 6.42677C21.582 6.3497 21.5345 6.2827 21.468 6.23923C21.4016 6.19577 21.3211 6.17906 21.2429 6.19248C20.7329 6.21649 20.2313 6.33051 19.7611 6.52928C19.1103 6.7908 18.4899 7.12198 17.9104 7.51703C17.84 7.56996 17.7581 7.60551 17.6713 7.62078C17.5846 7.63605 17.4954 7.6306 17.4112 7.60489C15.2596 7.05882 13.0054 7.06203 10.8554 7.61421C10.7806 7.63586 10.7018 7.63967 10.6253 7.62534C10.5487 7.611 10.4766 7.57892 10.4148 7.53167C9.64788 7.03247 8.85171 6.58918 7.96368 6.33359C7.65781 6.24338 7.34123 6.19458 7.02239 6.18849C6.94879 6.17986 6.87462 6.19893 6.81432 6.242C6.75402 6.28507 6.71191 6.34904 6.69621 6.42145C6.32342 7.51437 6.2209 8.61527 6.56307 9.73348C6.59635 9.84264 6.64694 9.93316 6.54177 10.0516C5.47666 11.2604 5.09988 12.6834 5.19574 14.2676C5.2663 15.4244 5.46201 16.5466 6.01454 17.5769C6.84399 19.1171 8.21664 19.9119 9.85158 20.3352C10.3938 20.4706 10.9444 20.5698 11.4998 20.632C11.5384 20.7492 11.4506 20.7798 11.408 20.8291C11.1734 21.1179 10.9894 21.4441 10.8634 21.7942C10.7622 22.0458 10.8315 22.4039 10.6065 22.5516C10.263 22.7766 9.83827 22.8485 9.42421 22.8871C8.17936 23.0056 7.26471 22.4877 6.6283 21.4348C6.25552 20.8184 5.76956 20.3325 5.08523 20.0663C4.76981 19.9325 4.42139 19.8967 4.08537 19.9638C3.7898 20.029 3.73788 20.1901 3.93891 20.4111C4.03639 20.5234 4.14989 20.6207 4.27575 20.6999C4.9796 21.1318 5.51717 21.7884 5.80152 22.5636C6.37002 23.9973 7.48039 24.5697 8.93825 24.6323C9.43741 24.6575 9.93768 24.615 10.4254 24.5058C10.5892 24.4672 10.6531 24.4872 10.6517 24.6762C10.6451 25.4936 10.6637 26.3123 10.6517 27.131C10.6517 27.6635 10.1684 27.9297 9.58663 27.7393C8.17396 27.2671 6.84977 26.5631 5.66838 25.656C2.59555 23.2891 0.720966 20.1861 0.217704 16.3376C-0.357453 11.9127 0.911353 8.00824 3.98551 4.73881C6.11909 2.42656 8.99932 0.939975 12.1203 0.540191C16.5351 -0.0601815 20.4347 1.14323 23.7232 4.16373C26.2449 6.47869 27.724 9.37672 28.1048 12.7726C28.5828 17.0325 27.3686 20.7945 24.4768 23.9827C22.9762 25.6323 21.0956 26.8908 18.9982 27.6488C18.8783 27.6927 18.7585 27.738 18.636 27.7726C18.0356 27.9404 17.6189 27.6395 17.6189 27.0098C17.6189 25.7452 17.6308 24.4806 17.6295 23.2159C17.6329 22.9506 17.6128 22.6856 17.5696 22.4238C17.4325 21.6664 17.3419 21.484 16.7228 20.6334Z">
                            </path>
                        </svg>
                        <span class="usa-sr-only">NCBI on GitHub</span>
                    </a>
                

                
                    <a href="https://ncbiinsights.ncbi.nlm.nih.gov/"
                       class="ncbi-footer__social-icon ncbi-footer__social-icon--gray"
                       target="_blank"
                       rel="noreferrer noopener">
                        <svg width="26"
                             height="26"
                             viewBox="0 0 27 27"
                             fill="none"
                             xmlns="http://www.w3.org/2000/svg"
                             focusable="false"
                             aria-hidden="true">
                            <path d="M23.7778 26.4574C23.1354 26.3913 22.0856 26.8024 21.636 26.3087C21.212 25.8444 21.4359 24.8111 21.324 24.0347C19.9933 14.8323 14.8727 8.80132 6.09057 5.85008C4.37689 5.28406 2.58381 4.99533 0.779072 4.99481C0.202773 4.99481 -0.0229751 4.83146 0.00455514 4.21479C0.0660406 3.08627 0.0660406 1.95525 0.00455514 0.826734C-0.0413285 0.0815827 0.259669 -0.0193618 0.896534 0.00266238C6.96236 0.222904 12.3693 2.24179 16.9889 6.16209C22.9794 11.2478 26.1271 17.7688 26.4372 25.648C26.4629 26.294 26.3179 26.5271 25.6609 26.4684C25.0827 26.417 24.4991 26.4574 23.7778 26.4574Z">
                            </path>
                            <path d="M14.8265 26.441C14.0924 26.441 13.2371 26.6795 12.6626 26.3786C12.0092 26.0372 12.3781 25.0644 12.246 24.378C11.1154 18.5324 6.6849 14.5497 0.74755 14.1001C0.217135 14.0615 -0.0104482 13.9422 0.0134113 13.3659C0.0519536 12.1454 0.0482829 10.9213 0.0134113 9.69524C-0.00127145 9.14464 0.196946 9.03268 0.703502 9.04736C9.21217 9.27128 16.5994 16.2511 17.2804 24.7231C17.418 26.4446 17.418 26.4446 15.6579 26.4446H14.832L14.8265 26.441Z">
                            </path>
                            <path d="M3.58928 26.4555C2.64447 26.4618 1.73584 26.0925 1.06329 25.4289C0.39073 24.7653 0.00933763 23.8617 0.0030097 22.9169C-0.00331824 21.9721 0.365937 21.0635 1.02954 20.3909C1.69315 19.7184 2.59675 19.337 3.54156 19.3306C4.48637 19.3243 5.39499 19.6936 6.06755 20.3572C6.7401 21.0208 7.1215 21.9244 7.12782 22.8692C7.13415 23.814 6.7649 24.7226 6.10129 25.3952C5.43768 26.0677 4.53409 26.4491 3.58928 26.4555Z">
                            </path>
                        </svg>
                        <span class="usa-sr-only">NCBI RSS feed</span>
                    </a>
                
            </div>
        </div>
    

    <div data-testid="gridContainer"
         class="grid-container ncbi-footer__container">
        <div class="grid-row ncbi-footer__main-content-container"
             data-testid="grid">
            
                <div class="ncbi-footer__column">
                    
                        <p class="ncbi-footer__circled-icons-heading">
                            Connect with NLM
                        </p>
                    

                    <div class="ncbi-footer__circled-icons-list">
                        
                            <a href=https://twitter.com/nlm_nih class="ncbi-footer__social-icon ncbi-footer__social-icon--circled" target="_blank" rel="noreferrer noopener">
                                <svg width="32"
                                     height="32"
                                     viewBox="0 0 40 40"
                                     fill="none"
                                     xmlns="http://www.w3.org/2000/svg"
                                     focusable="false"
                                     aria-hidden="true">
                                    <path d="m6.067 8 10.81 13.9L6 33.2h4.2l8.4-9.1 7.068 9.1H34L22.8 18.5 31.9 8h-3.5l-7.7 8.4L14.401 8H6.067Zm3.6 1.734h3.266l16.8 21.732H26.57L9.668 9.734Z">
                                    </path>
                                </svg>
                                <span class="usa-sr-only">NLM on X (formerly known as Twitter)</span>
                            </a>
                        

                        
                            <a href=https://www.facebook.com/nationallibraryofmedicine class="ncbi-footer__social-icon ncbi-footer__social-icon--circled" target="_blank" rel="noreferrer noopener">
                                <svg width="13"
                                     height="24"
                                     viewBox="0 0 13 24"
                                     fill="none"
                                     xmlns="http://www.w3.org/2000/svg"
                                     focusable="false"
                                     aria-hidden="true">
                                    <path d="M4.11371 23.1369C4.11371 23.082 4.11371 23.0294 4.11371 22.9745V12.9411H0.817305C0.6709 12.9411 0.670898 12.9411 0.670898 12.8016C0.670898 11.564 0.670898 10.3287 0.670898 9.09341C0.670898 8.97903 0.705213 8.95158 0.815017 8.95158C1.8673 8.95158 2.91959 8.95158 3.97417 8.95158H4.12057V8.83263C4.12057 7.8055 4.12057 6.7738 4.12057 5.74897C4.1264 4.92595 4.31387 4.11437 4.66959 3.37217C5.12916 2.38246 5.94651 1.60353 6.95717 1.1921C7.64827 0.905008 8.3913 0.764035 9.13953 0.778051C10.0019 0.791777 10.8644 0.830666 11.7268 0.860404C11.8869 0.860404 12.047 0.894717 12.2072 0.90158C12.2964 0.90158 12.3261 0.940469 12.3261 1.02968C12.3261 1.5421 12.3261 2.05452 12.3261 2.56465C12.3261 3.16857 12.3261 3.7725 12.3261 4.37642C12.3261 4.48165 12.2964 4.51367 12.1912 4.51138C11.5369 4.51138 10.8804 4.51138 10.2261 4.51138C9.92772 4.51814 9.63058 4.5526 9.33855 4.61433C9.08125 4.6617 8.84537 4.78881 8.66431 4.97766C8.48326 5.16652 8.3662 5.40755 8.32972 5.66661C8.28476 5.89271 8.26027 6.1224 8.25652 6.35289C8.25652 7.19014 8.25652 8.02969 8.25652 8.86923C8.25652 8.89439 8.25652 8.91955 8.25652 8.95615H12.0219C12.1797 8.95615 12.182 8.95616 12.1614 9.10714C12.0768 9.76596 11.9876 10.4248 11.9029 11.0813C11.8312 11.6319 11.7626 12.1824 11.697 12.733C11.6719 12.9434 11.6787 12.9434 11.4683 12.9434H8.26338V22.899C8.26338 22.979 8.26338 23.0591 8.26338 23.1392L4.11371 23.1369Z">
                                    </path>
                                </svg>
                                <span class="usa-sr-only">NLM on Facebook</span>
                            </a>
                        

                        
                            <a href=https://www.youtube.com/user/NLMNIH class="ncbi-footer__social-icon ncbi-footer__social-icon--circled" target="_blank" rel="noreferrer noopener">
                                <svg width="21"
                                     height="15"
                                     viewBox="0 0 21 15"
                                     fill="none"
                                     xmlns="http://www.w3.org/2000/svg"
                                     focusable="false"
                                     aria-hidden="true">
                                    <path d="M19.2561 1.47914C18.9016 1.15888 18.5699 0.957569 17.2271 0.834039C15.5503 0.678484 13.2787 0.655608 11.563 0.65332H9.43556C7.71987 0.65332 5.4483 0.678484 3.77151 0.834039C2.43098 0.957569 2.097 1.15888 1.74242 1.47914C0.813665 2.32097 0.619221 4.62685 0.598633 6.89384C0.598633 7.31781 0.598633 7.74101 0.598633 8.16345C0.626084 10.4121 0.827391 12.686 1.74242 13.521C2.097 13.8412 2.4287 14.0425 3.77151 14.1661C5.4483 14.3216 7.71987 14.3445 9.43556 14.3468H11.563C13.2787 14.3468 15.5503 14.3216 17.2271 14.1661C18.5676 14.0425 18.9016 13.8412 19.2561 13.521C20.1712 12.6929 20.3725 10.451 20.3999 8.22064C20.3999 7.74025 20.3999 7.25986 20.3999 6.77946C20.3725 4.54907 20.1689 2.30724 19.2561 1.47914ZM8.55942 10.5311V4.65201L13.5601 7.50005L8.55942 10.5311Z"
                                          fill="white" />
                                </svg>
                                <span class="usa-sr-only">NLM on YouTube</span>
                            </a>
                        
                    </div>
                </div>
            

            
                <address class="ncbi-footer__address ncbi-footer__column">
                    
        <p>
            <a class="usa-link usa-link--external"
            href="https://www.google.com/maps/place/8600+Rockville+Pike,+Bethesda,+MD+20894/%4038.9959508,
            -77.101021,17z/data%3D!3m1!4b1!4m5!3m4!1s0x89b7c95e25765ddb%3A0x19156f88b27635b8!8m2!3d38.9959508!
            4d-77.0988323"
            rel="noopener noreferrer" target="_blank">National Library of Medicine
            <br/> 8600 Rockville Pike<br/> Bethesda, MD 20894</a>
        </p>
    
                </address>
            

            
                <ul class="usa-list usa-list--unstyled ncbi-footer__vertical-list ncbi-footer__column">
                    
                        <li class="ncbi-footer__vertical-list-item">
                            









    <a href="https://www.nlm.nih.gov/web_policies.html" class="usa-link  usa-link--alt ncbi-footer__link"  >
        

        
            Web Policies
        

        
    </a>


                        </li>
                    
                        <li class="ncbi-footer__vertical-list-item">
                            









    <a href="https://www.nih.gov/institutes-nih/nih-office-director/office-communications-public-liaison/freedom-information-act-office" class="usa-link  usa-link--alt ncbi-footer__link"  >
        

        
            FOIA
        

        
    </a>


                        </li>
                    
                        <li class="ncbi-footer__vertical-list-item">
                            









    <a href="https://www.hhs.gov/vulnerability-disclosure-policy/index.html" class="usa-link usa-link--external usa-link--alt ncbi-footer__link" rel="noreferrer noopener" target='_blank' >
        

        
            HHS Vulnerability Disclosure
        

        
    </a>


                        </li>
                    
                </ul>
            

            
                <ul class="usa-list usa-list--unstyled ncbi-footer__vertical-list ncbi-footer__column">
                    
                        <li class="ncbi-footer__vertical-list-item">
                            









    <a href="https://support.nlm.nih.gov/" class="usa-link  usa-link--alt ncbi-footer__link"  >
        

        
            Help
        

        
    </a>


                        </li>
                    
                        <li class="ncbi-footer__vertical-list-item">
                            









    <a href="https://www.nlm.nih.gov/accessibility.html" class="usa-link  usa-link--alt ncbi-footer__link"  >
        

        
            Accessibility
        

        
    </a>


                        </li>
                    
                        <li class="ncbi-footer__vertical-list-item">
                            









    <a href="https://www.nlm.nih.gov/careers/careers.html" class="usa-link  usa-link--alt ncbi-footer__link"  >
        

        
            Careers
        

        
    </a>


                        </li>
                    
                </ul>
            
        </div>

        
            <div class="grid-row grid-col-12" data-testid="grid">
                <ul class="ncbi-footer__bottom-links-list">
                    
                        <li class="ncbi-footer__bottom-list-item">
                            









    <a href="https://www.nlm.nih.gov/" class="usa-link  usa-link--alt ncbi-footer__link"  >
        

        
            NLM
        

        
    </a>


                        </li>
                    
                        <li class="ncbi-footer__bottom-list-item">
                            









    <a href="https://www.nih.gov/" class="usa-link  usa-link--alt ncbi-footer__link"  >
        

        
            NIH
        

        
    </a>


                        </li>
                    
                        <li class="ncbi-footer__bottom-list-item">
                            









    <a href="https://www.hhs.gov/" class="usa-link usa-link--external usa-link--alt ncbi-footer__link" rel="noreferrer noopener" target='_blank' >
        

        
            HHS
        

        
    </a>


                        </li>
                    
                        <li class="ncbi-footer__bottom-list-item">
                            









    <a href="https://www.usa.gov/" class="usa-link usa-link--external usa-link--alt ncbi-footer__link" rel="noreferrer noopener" target='_blank' >
        

        
            USA.gov
        

        
    </a>


                        </li>
                    
                </ul>
            </div>
        
    </div>
</footer>

    


        
        
    
  <script  type="text/javascript" src="https://cdn.ncbi.nlm.nih.gov/core/pinger/pinger.js"> </script>


    
        

<button class="back-to-top" data-ga-category="pagination" data-ga-action="back_to_top">
    <label>Back to Top</label>
    <svg class="usa-icon order-0" aria-hidden="true" focusable="false" role="img">
        <use xlink:href="/static/img/sprite.svg#arrow_upward"></use>
    </svg>
</button>
    


        
    
    
    
        
    <script type="application/javascript">
    window.ncbi = window.ncbi || {};
    window.ncbi.pmc = window.ncbi.pmc || {};
    window.ncbi.pmc.options = {
        logLevel: 'INFO',
        
        staticEndpoint: '/static/',
        
        citeCookieName: 'pmc-cf',
    };
</script>
    <script type="module" crossorigin="" src="/static/assets/base-133c6271.js"></script>
    
    <script type="text/javascript" src="https://cdn.ncbi.nlm.nih.gov/core/jquery/jquery-3.6.0.min.js">&#xA0;</script>
    <script type="text/javascript">
        jQuery.getScript("https://cdn.ncbi.nlm.nih.gov/core/alerts/alerts.js", function () {
            galert(['div.nav_and_browser', 'div.header', '#universal_header', '.usa-banner', 'body > *:nth-child(1)'])
        });
    </script>


    <script type="text/javascript">var exports = {};</script>
     <script src="/static/CACHE/js/output.13b077bc3ffd.js"></script>

    <script type="module" crossorigin="" src="/static/assets/article-44d04358.js"></script>
    
    

    </body>
</html>
