equality, justice and equity, non­discrimination, 2 Callegaro, M. (extracted from202.pdf)
is racial bias in the risk scores used in the US crimi­ The situation is further complicated by the absence nal justice system. (extracted from202.pdf)
Skip to main content Skip to "About Canada.ca" Language selection Français fr Search CanadaBuys Search Canada.ca Menu Main Menu Home Jobs and the workplace Immigration and citizenship Travel and tourism Business and industry Benefits Health Taxes Environment and natural resources National security and defence Culture, history and sport Policing, justice and emergencies Transport and infrastructure Canada and the world Money and finances Science and innovation Mid Level Menu Account access CanadaBuys Menu Home Getting started Tender opportunities How procurement works Buyer’s Portal Support Account access You are here Canada.ca CanadaBuys home CanadaBuys CanadaBuys Welcome to the home for doing business with the Government of Canada and the broader Canadian public sector. (extracted from593.html)
Conclusions __________________________________________________________________ 70 VIII AI and digital tools in workplace management and evaluation List of abbreviations AGI Artificial general intelligence AI Artificial intelligence AIaaS AI as a service Regulation of the European Parliament and of the Council laying down harmonised AI act rules on artificial intelligence (Artificial Intelligence act) And amending certain union legislative acts AIDA Special Committee on Artificial Intelligence in a Digital Age BCI Brain-computer interface CDEI Centre for Data Ethics and Innovation Charter EU Charter of Fundamental Rights CIPD Chartered Institute of Personnel and Development CJEU Court of Justice of the European Union CNIL Commission Nationale de l'Informatique et des Libertés DPA Data protection authority DPIA Data Protection Impact Assessment DPO Data protection officer ECHR European Convention on Human Rights ECtHR European Court of Human Rights EDPB European Data Protection Body EDPS European Data Protection Supervisor EESC European Economic and Social Committee ETUC European Trade Union Confederation EU-OSHA European Occupational Safety and Health at Work Authority FLI Future of Life Institute GDPR General Data Protection Regulation HBS Harvard Business School HRM Human resource management ICO Information Commissioner's Office ILO International Labour Organization IP Internet protocol IX STOA | Panel for the Future of Science and Technology ML Machine learning NFC Near-field communication OECD Organisation for Economic Co-operation and Development OSH Occupational safety and health PwC PricewaterhouseCoopers SMS Social media screening TUC Trades Union Congress WEF World Economic Forum X AI and digital tools in workplace management and evaluation 1. (extracted from160.pdf)
To that extent, as the Court of Justice of the European Union (CJEU) ruled, 'an employer that does not allow a worker to exercise his right to paid annual leave must bear the consequences.'72 In no way should an employer be allowed to hide behind an algorithm in this respect. (extracted from160.pdf)
72 European Court of Justice 29 November 2017, Case No. (extracted from160.pdf)
73 European Court of Justice 21 February 2018, Case No. (extracted from160.pdf)
Rudy Matzak; European Court of Justice 9 March 2021, Case No. (extracted from160.pdf)
Radiotelevizija Slovenija; European Court of Justice 9 March 2021, Case No. (extracted from160.pdf)
The purpose of this Act, as explained in Recital 1 of the draft, 'is to improve the functioning of the internal market by laying down a uniform legal framework in particular for the development, marketing and use of 82 'Directive 75/117 on equal pay for men and women must be interpreted as meaning that where an undertaking applies a system of pay which is totally lacking in transparency, it is for the employer to prove that his practice in the matter of wages is not discriminatory, if a female worker establishes, in relation to a relatively large number of employees, that the average pay for women is less than that for men.' European Court of Justice 17 October 1989, Case No. (extracted from160.pdf)
86 'AI is pervasive and will be used in diverse fields – such as consultancy, consumer products and services, mobility, online connectivity, energy production and distribution, police and justice administration –, where EU and MS liability rules are already sector-specific. (extracted from160.pdf)
Van der Mei, Anne Pieter, 'Fixed-Term work: Recent developments in the case law of the Court of Justice of the European Union', European Labour Law Journal, 2020. (extracted from160.pdf)
Vecchione, Briana, Barocas, Solon and Karen Levy, 'Algorithmic Auditing and Social Justice: Lessons from the History of Audit Studies, Equity and Access' in Algorithms, Mechanisms, and Optimization, 2021. (extracted from160.pdf)
The specific human rights implications for AI systems can be viewed through provisions of the European Convention of Human Rights (ECHR) and the European Social Charter (ESC), including its specific guarantees regarding liberty and justice, privacy, freedom of expression, equality and non-discrimination, and social and economic rights. (extracted from606.pdf)
14 Liberty and Justice: AI can adversely affect the liberty A system that supports and justice of individuals, particularly when implemented criminal sentencing decisions in high impact contexts such as criminal justice. (extracted from606.pdf)
AI systems in sensitive public policy areas, including but not limited to law enforcement, -AI systems can also give rise to unjust justice, asylum and migration, health, social categorisation based on new types of security, and employment. (extracted from606.pdf)
This should also include the possibility used in the field of justice and law enforcement of receiving insight into and challenging AI- are in line with the essential requirements of informed decisions in the context of law the right to a fair trial. (extracted from606.pdf)
To this end, they should enforcement or justice, including the right to ensure the quality and security of judicial review of such decision by a human. (extracted from606.pdf)
Such information must especially be provided when AI systems are used in the field of justice and law enforcement, both as concerns the role of AI systems within the process, and the right to challenge the decisions informed or made thereby. (extracted from606.pdf)
We hope that, taken together, this material can function as a kind of launching pad for meaningful reflection on the prospects for a principles-based legal framework for governing AI research and innovation in accordance with the Council of Europe's stewardship of fundamental rights and freedoms, justice, and democratic values. (extracted from606.pdf)
Policy makers, scholars, and activists are tasked with proposing and critiquing strategies and actions aimed at promoting general well-being and social justice. (extracted from606.pdf)
Work in the field of justice European Ethical Charter on the use of AI in judicial systems and their environment (2018) Five key principles are outlined in this charter including respect for fundamental rights, non- discrimination, quality and security, transparency, impartiality and fairness, and “under user control.” Most applications of AI in the judicial field have been found to be in the private sector – “commercial initiatives aimed at insurance companies, legal departments, lawyers, and individuals.” Some potential uses of AI in a judicial setting include case-law enhancement, access to law, and the creation of new strategic tools Other considerations that require considerable methodological precautions include the creation of scales, support for alternative dispute settlement measures in civil matters, pre- litigation resolution of disputes online (when a later appeal to the judge remains possible), or identification of where criminal offences are being committed 4.5. (extracted from606.pdf)
The rel- evant concepts include freedom and self-determination, privacy and intimacy, sovereignty and power, beneficence and non-maleficence, as well as justice, solidarity and responsibility. (extracted from612.pdf)
73) The collection and transmission of large volumes of health-relevant data touches on fundamental questions of justice. (extracted from612.pdf)
As a normalising principle of social relations, justice demands that the arbitrary privi- leging of certain persons or groups be avoided. (extracted from612.pdf)
2244 ExEcutIvE Summary 74) As regards big data applications in the healthcare sector, four sets of problems stand out as especially relevant to questions of justice: first, access to datasets for the research sector; second, the insidious con- solidation of monopolistic structures; third, the inclusion of health apps, as well as various devices that facilitate private self-tracking, in determining health insurance premiums; and fourth, aspects of social justice, understood in terms of the capabilities approach, as they con- cern the responsible handling of health-relevant data. (extracted from612.pdf)
Solidarity is frequent- ly understood as complimentary to – and often subsidiary to – the concept of justice. (extracted from612.pdf)
The shaping of such freedom is responsible when it also ori- ents itself towards the legal and societal demands of solidarity and justice. (extracted from612.pdf)
Ensure justice and solidarity C1. (extracted from612.pdf)
To do justice to the complexity and significance of this issue, for example, com- panies and institutions could expand their efforts to establish internal data science departments. (extracted from612.pdf)
AI tightly developed under human oversight and control), embed the principles of fairness and justice in algorithms, applied in all phases of AI systems’ design, implementation and testing, while following through bias complaints and other undesired effects reporting. (extracted from174.pdf)
During the 71st session of the United Nations General Assembly, on 29 September 2016, UNICRI (United Nations International Crime and Justice research Institute) announced the opening of the first Centre on AI and Robotics 30 PE 626.074 European Artificial Intelligence (AI) leadership, the path for an integrated vision within the United Nations system. (extracted from174.pdf)
AI tightly developed under human oversight and control), embed the principles of fairness and justice in algorithms, applied in all phases of AI systems’ design, implementation and testing, while following through bias complaints and other undesired effects reporting. (extracted from174.pdf)
Accept additional cookies Reject additional cookies View cookies Hide cookie message Skip to main content GOV.UK Navigation menu Menu Menu Services and information Benefits Births, death, marriages and care Business and self-employed Childcare and parenting Citizenship and living in the UK Crime, justice and the law Disabled people Driving and transport Education and learning Employing people Environment and countryside Housing and local services Money and tax Passports, travel and living abroad Visas and immigration Working, jobs and pensions Government activity Departments Departments, agencies and public bodies News News stories, speeches, letters and notices Guidance and regulation Detailed guidance, regulations and rules Research and statistics Reports, analysis and official statistics Policy papers and consultations Consultations and strategy Transparency Data, Freedom of Information releases and corporate reports Search GOV.UK × Search GOV.UK Search Search GOV.UK Search Home Business and industry Science and innovation Artificial intelligence Establishing a pro-innovation approach to regulating AI Department for Science, Innovation & Technology Office for Artificial Intelligence Department for Digital, Culture, Media & Sport Department for Business, Energy & Industrial Strategy Policy paper Establishing a pro-innovation approach to regulating AI Updated 20 July 2022 This was published under the 2019 to 2022 Johnson Conservative government Contents Ministerial foreword by the Secretary of State for Digital, Culture, Media and Sport Ministerial foreword by the Secretary of State for Business, Energy and Industrial Strategy Executive summary Context The scope A new pro-innovation approach Putting our approach into practice Next steps Share your views Print this page © Crown copyright 2022 This publication is licensed under the terms of the Open Government Licence v3.0 except where otherwise stated. (extracted from182.html)
Cancel Services and information Benefits Births, death, marriages and care Business and self-employed Childcare and parenting Citizenship and living in the UK Crime, justice and the law Disabled people Driving and transport Education and learning Employing people Environment and countryside Housing and local services Money and tax Passports, travel and living abroad Visas and immigration Working, jobs and pensions Government activity Departments News Guidance and regulation Research and statistics Policy papers and consultations Transparency How government works Get involved Support links Help Privacy Cookies Accessibility statement Contact Terms and conditions Rhestr o Wasanaethau Cymraeg Government Digital Service All content is available under the Open Government Licence v3.0 , except where otherwise stated © Crown copyright (extracted from182.html)
Pah et al., How to Build a More Open Justice System, 369 Sci. (extracted from362.pdf)
However, “had he been able to examine and contest the logic of the COMPAS system to prove that its score gave a distorted picture of his life, he might have gone home much earlier” (Wexler 2017b) Rodríguez’s case is an example of the discriminatory use of AI in criminal justice, which also includes prominent AI applications for the purposes of predictive policing. (extracted from411.pdf)
In such cases, which include law enforcement and criminal justice applications, the attempt to modify the data to reduce or eliminate underlying biases may inadvertently introduce new challenges. (extracted from411.pdf)
The desirability of the use of AI solutions is also something that should be duly considered– with regard to the purpose, advantages and burden imposed by them on social values, justice and the public interest. (extracted from411.pdf)
It has links to broader societal structures and the justice of our socio-economic systems and thus relates to the problem of surveillance capitalism. (extracted from411.pdf)
(2018) outline how criminal justice risk assessment tools could benefit low-risk individuals through increased pre-trial releases and shorter sentences. (extracted from411.pdf)
verification of the authenticity of travel documents); and in the administration of justice and democratic processes (e.g. (extracted from411.pdf)
Directorate-General for Justice and Consumers, European Commission, Brussels. (extracted from411.pdf)
Expert Group on Liability and New Technologies, Directorate-General for Justice and Consumers, European Commission, Brussels. (extracted from411.pdf)
For instance, the opening of the Universal Declaration of Human Rights states that “recognition of the inherent dignity and of the equal and inalienable rights of all members of the human family is the foundation of freedom, justice and peace in the world” (UN 1948). (extracted from411.pdf)
Federal Ministry of Justice and Federal Office of Justice, Berlin. (extracted from411.pdf)
One could perhaps even argue that AI has been linked directly to international justice and sustainability through the SDGs. (extracted from411.pdf)
include human oversight, explainability or interpretability, legal status of AI systems, and the equitable economic effect of AI.31 A separate analysis of 84 AI ethics documents done in 2019 found that there has been a global convergence around “transparency, justice and fairness, non-maleficence, responsibility and privacy.”32 While much progress has been made aligning on responsible AI, there remain differences—even among FCAI participants. (extracted from74.pdf)
is other values.211 Work to advance data free with trust has continued through the handicapped in World Economic Forum.212 advocating for free flow of data so long The “Schrems II” judgment by the Court of Justice of the European Union (CJEU) in 2020, however, has been a seismic event for international transfers as it remains an of personal information, the aftershocks of which are still reverberating and outlier on privacy magnify the impact of the EU regime. (extracted from74.pdf)
Kerry, ”The oracle at Luxembourg: The EU Court of Justice judges the world on surveillance and privacy,” The Brookings Institution, January 11, 2021, https://www.brookings.edu/ research/the-oracle-at-luxembourg-the-eu-court-of-justice-judges-the-world-on-surveillance-and-privacy/. (extracted from74.pdf)
Kerry, ”The oracle at Luxembourg: The EU Court of Justice judges the world on surveillance and privacy,” The Brookings Institution, January 11, 2021, https://www.brookings.edu/research/the-oracle-at-luxembourg-the-eu-court-of-justice-judges-the-world-on- surveillance-and-privacy/. (extracted from74.pdf)
European Parliament 2014-2019 TEXTS ADOPTED P8_TA(2019)0081 A comprehensive European industrial policy on artificial intelligence and robotics European Parliament resolution of 12 February 2019 on a comprehensive European industrial policy on artificial intelligence and robotics (2018/2088(INI)) The European Parliament, – having regard to its resolution of 16 February 2017 with recommendations to the Commission on Civil Law Rules on Robotics1, – having regard to its resolution of 1 June 2017 on digitising European industry2, – having regard to its resolution of 12 September 2018 on autonomous weapon systems3, – having regard to its resolution of 11 September 2018 on language equality in the digital age4, – having regard to the Commission proposal of 6 June 2018 establishing the Digital Europe programme for the period 2021-2027 (COM(2018)0434), – having regard to Council Regulation (EU) 2018/1488 of 28 September 2018 establishing the European High Performance Computing Joint Undertaking5, – having regard to Rule 52 of its Rules of Procedure, – having regard to the report of the Committee on Industry, Research and Energy and the opinions of the Committee on the Internal Market and Consumer Protection, the Committee on Legal Affairs, the Committee on Civil Liberties, Justice and Home Affairs and the Committee on the Environment, Public Health and Food Safety (A8- 0019/2019), A. (extracted from175.pdf)
Points out that the guiding ethical framework should be based on the principles of beneficence, non-maleficence, autonomy and justice, on the principles and values enshrined in Article 2 of the Treaty on European Union and in the Charter of Fundamental Rights, such as human dignity, equality, justice and equity, non- discrimination, informed consent, private and family life and data protection, as well as on other underlying principles and values of Union law, such as non-stigmatisation, transparency, autonomy, individual responsibility and social responsibility, and on existing ethical practices and codes; 148. (extracted from175.pdf)
• Non-economic services, such as the police, justice and statutory social security schemes, are not subject to specific European legislation or to internal market and competition rules. (extracted from161.pdf)
Responsible innovation, anticipation and responsiveness: case studies of algorithms in decision support in justice and security, and an exploration of potential, unintended, undesirable, higher-order effects. (extracted from161.pdf)
In turn, impact police data, predictive policing systems, and justice’, NYU Law Review, Vol. (extracted from203.pdf)
Beyond that, gender identity is mentioned only in Recital 9 of the Victims’ Rights Directive75 in the context of criminal law.76 According to the Court of Justice of the European Union, gender identity is only partly covered by the principle of equal treatment between men and women.77 Legal protection against discrimination based on religion is currently also limited under EU law.78 Nevertheless, one may argue that many comments referring to people who identify as lesbian, gay, bisexual, transgender and intersex (LGBTI), Jewish or Muslim fall under either the Racial Equality Directive or the Gender Goods and Services Directive, because discrimination based on sexual orientation, gender identity or religion predominantly affects a specific race or gender. (extracted from203.pdf)
Furthermore, there is a lack of representative and high- quality datasets to develop algorithms, and taking into account differences in speech patterns and changing patterns of speech remains difficult.88 The use of algorithms may further increase the opacity of content moderation and further increase challenges linked to fairness and justice.89 Without proper safeguards, such tools can lead to censorship and biased enforcement of laws and platforms’ terms and conditions.90 A potential increase in discrimination is just one of the challenges when using algorithms to support speech detection for content moderation purposes. (extracted from203.pdf)
Article 29 Working Party (2017a), Opinion on some key issues of the Law Enforcement Directive (EU 2016/680), WP 258, Brussels, European Commission Directorate-General Justice and Consumers. (extracted from203.pdf)
Article 29 Working Party (2017b), Guidelines on automated individual decision- making and profiling for the purposes of Regulation 2016/679, WP251rev.01, Brussels, European Commission Directorate-General Justice, p. (extracted from203.pdf)
CJEU (Court of Justice of the European Union) (1991), C-184/89, Helga Nimz v. (extracted from203.pdf)
(2021), Automating Injustice – The use of artificial intelligence and automated decision-making systems in criminal justice in Europe, London, Fair Trials. (extracted from203.pdf)
FRA (2016), Ensuring justice for hate crime victims: Professional perspectives, Luxembourg, Publications Office. (extracted from203.pdf)
(2019), ‘Dirty data, bad predictions: How civil rights violations impact police data, predictive policing systems, and justice’, NYU Law Review, Vol. (extracted from203.pdf)
The Law Society (2019), Algorithms in the criminal justice system, London, The Law Society, p. (extracted from203.pdf)
Venice Commission :: Council of Europe EN FR Login Venice Commission Council of Europe Home The Council in brief Human Rights Democracy Rule of Law Organisation 46 Countries Topics Newsroom Council of Europe Human Rights and Rule of Law Venice Commission EN | FR | DE | IT | RU The Commission About us Types of activities Founding documents News Programme References Members Member states Individual members Map of member States Positions Statements Democratic institutions and fundamental rights Constitutional reforms Fundamental rights Democratic institutions Rule of law Judicial reforms Ombudsman Elections, referendums and political parties Council for Democratic Elections Conferences of Electoral Management Bodies Seminars and assistance Political parties VOTA database Council of Europe and elections Constitutional justice Regional co-operation Constitutional Courts CODICES E-Bulletin World Conference on Constitutional Justice Non-European Partners Southern Mediterranean Overview of activities Highlights by beneficiary Recent events UniDem Med Campus List of UniDem seminars Intercultural Workshops on Democracy Arabic Speaking EMBs Tunisia : support to independent bodies Central Asia Kyrgyzstan – support to electoral reform Central Asia Rule of Law Programme List of events Latin America Overview of activities Recent events Sub-commission on Latin America Main documents Statute Main reference documents Compilations of studies and opinions Annual reports Documents Opinions Ongoing opinions Follow-up Studies Recent documents access by: Countries Topics Series Language Search Web Resources Newsletter Publications Articles on the Commission Links Library Constitutions Contact us For the media Reset password CDL-AD(2020)037-e Study - Principles for a fundamental rights-compliant use of digital technologies in electoral processes, approved by the Council for Democratic Elections at its 70th meeting (online, 10 December 2020) and adopted by the Venice Commission at its 125th Plenary Session (online, 11-12 December 2020) Show related documents (3) Choose a year all 2020 CDL(2020)043 English 24/11/2020 - Public Draft principles for a fundamental rights-compliant use of digital technologies in electoral processes (H. (extracted from157.html)
In particular, in medicine, CNOM refers in its report to the applicability to digital technologies of the four principles of medical ethics: beneficence, non-maleficence, autonomy, and justice. (extracted from201.pdf)
Regulation and soft law (self-compliance and voluntary certification) The ruling on December 7 last by the European Court of Justice notably circumscribed national capacity for regulation on digital innovation in the healthcare sphere.41 This Community framework therefore opens the door to methods of regulation that do not entail enforceable law. (extracted from201.pdf)
41 The Court of Justice of the European Union (CJEU) was called upon to rule on a prejudicial question regarding whether a software program for prescription support meets the definition of a medical device, if that program provides at least one function that can be used to process data specific to the patient for the purpose of helping the doctor to establish a prescription, in particular by detecting contraindications, drug interactions, and overdoses, although it does not of itself act in or upon the human body (CE, June 8, 2016, n° 387156). (extracted from201.pdf)
This issue has particular resonance for policy makers, because algorithmic systems increasingly play a role in determining outcomes in public sector realms like the welfare or criminal justice systems. (extracted from215.pdf)
Canada-France Statement on Artificial Intelligence Skip to main content Skip to "About government" Language selection Français Government of Canada Search Search website Search Menu Main Menu Jobs and the workplace Immigration and citizenship Travel and tourism Business and industry Benefits Health Taxes Environment and natural resources National security and defence Culture, history and sport Policing, justice and emergencies Transport and infrastructure Canada and the world Money and finances Science and innovation You are here: Canada.ca Canada and the world Foreign policy and international relations Canada's partnerships and priorities by world region Canada and Europe Canada-France Statement on Artificial Intelligence Canada-France Statement on Artificial Intelligence Canada and France affirm that artificial intelligence is a revolution whose impact is being felt more and more each day. (extracted from78.html)
For the Government of Canada Navdeep Singh BAINS Minister of Innovation, Science and Economic Development For the Government of the French Republic Frédérique VIDAL Minister of Higher Education, Research and Innovation Date modified: 2018-06-07 About this site Government of Canada All contacts Departments and agencies About government Themes and topics Jobs and the workplace Immigration and citizenship Travel and tourism Business and industry Benefits Health Taxes Environment and natural resources National security and defence Culture, history and sport Policing, justice and emergencies Transport and infrastructure Canada and the world Money and finances Science and innovation Indigenous Peoples Veterans and military Youth About government Social media Mobile applications About Canada.ca Terms and conditions Privacy (extracted from78.html)
"Why Machine Learning May Lead to Unfairness: Evidence from Risk Assessment for Juvenile Justice in Catalonia", Best Paper Award, International Conference on AI and Law, 2019 Certain AI programmes for facial analysis display gender and racial bias, demonstrating low errors for determining the gender of lighter-skinned men but high errors in determining gender for darker-skinned women. (extracted from229.pdf)
Individuals and legal entities may face difficulties with effective access to justice in situations where such decisions may negatively affect them. (extracted from229.pdf)
This is without prejudice to the question whether, for the purpose of liability to end-users or other parties suffering harm and ensuring effective access to justice, which party should be liable for any damage caused. (extracted from229.pdf)
It might also compound further the problems with international transfer of data after that the US Safe Harbour and later the EU-US Privacy Shield were invalidated by the Court of Justice (Schrems I and II case-law, C-362/14 and C-311/18), as illustrated by Kiner (2020). (extracted from177.pdf)
The Schrems II judgment of the Court of Justice and the future of data transfer regulation. (extracted from177.pdf)
European Law Blog: https://europeanlawblog.eu/2020/07/17/the-schrems-ii- judgment-of-the-court-of-justice-and-the-future-of-data-transfer-regulation/. (extracted from177.pdf)
• Utilise AI innovations pro-socially so as to enable bonds of interpersonal solidarity to form and individuals to be socialised and recognised by each other • Use AI technologies to foster this capacity to connect so as to reinforce the edifice of trust, empathy, reciprocal responsibility, and mutual understanding upon which all ethically well- founded social orders rest → CARE for the wellbeing of each and all: • Design and deploy AI systems to foster and to cultivate the welfare of all stakeholders whose interests are affected by their use • Do no harm with these technologies and minimise the risks of their misuse or abuse Understanding Artificial Intelligence Ethics and Safety 10 • Prioritise the safety and the mental and physical integrity of people when scanning horizons of technological possibility and when conceiving of and deploying AI applications → PROTECT the priorities of social values, justice, and the public interest: • Treat all individuals equally and protect social equity • Use digital technologies as an essential support for the protection of fair and equal treatment under the law • Prioritise social welfare, public interest, and the consideration of the social and ethical impacts of innovation in determining the legitimacy and desirability of AI technologies • Use AI to empower and to advance the interests and well-being of as many individuals as possible • Think big-picture about the wider impacts of the AI technologies you are conceiving and developing. (extracted from605.pdf)
Ethical considerations about looking after patient wellbeing and clinical safety are paramount and wider justice concerns about improving healthcare for all and health equity factor in as well. (extracted from605.pdf)
I am also incredibly grateful for the impact that our interactions with the Ministry of Justice (MoJ)’s Data Science Hub has had on developing the framing for this guide. (extracted from605.pdf)
'It's reducing a human being to a percentage': Perceptions of justice in algorithmic decisions. (extracted from605.pdf)
Optimized scoring systems: Toward trust in machine learning for healthcare and criminal justice. (extracted from605.pdf)
Criminal Justice and Behavior, 46(2), 185-209. (extracted from605.pdf)
Data Justice Lab. (extracted from605.pdf)
Dirty data, bad predictions: How civil rights violations impact police data, predictive policing systems, and justice. (extracted from605.pdf)
• The proposal must also grant consumers the means to seek justice and redress in case of harm. (extracted from76.pdf)
Access to justice and right to redress, including collective redress ..................23 7.7. (extracted from76.pdf)
57 See also section 7.6 on access to justice. (extracted from76.pdf)
Access to justice and right to redress, including collective redress Greater protection in terms of transparency, safety, non-discrimination, or fairness are vital before consumers can trust AI-powered products and services. (extracted from76.pdf)
However, it is equally important to ensure that consumers have access to justice if AI-associated risks materialise. (extracted from76.pdf)
Representative actions are their only realistic possibility to seek justice. (extracted from76.pdf)
96 An Advocate-General from the Court of Justice of the European Union has already referred to standardisation as “legislative delegation in favour of a private standardisation body”. (extracted from76.pdf)
Increasing the availability of open data - Continuing the development open MKM Ongoing activity --- data portal - Project to support both the open data demand and publishing 2 Abbreviations: EAS – Enterprise Estonia, HITSA – Information Technology Foundation for Education, HTM – Ministry of Education and Research, JM – Ministry of Justice, MKM – Ministry of Economic Affairs and Communications, STAT – Statistics Estonia 3 ’---’ marks action item which will not need additional or targeted budget, or it is not possible to distinguish such costs from rest of activity’s budget 4 See https://www.etag.ee/en/funding/programmes/rita/ 2 Estonia’s National AI Strategy – Government of the Republic of Estonia – July 2019 Expert group proposals and existing Action item Responsible Deadline Budget3 measures agency2 Additional activities and measures: 1.8. (extracted from349.pdf)
for technologies in support to Justice as defined in NRRP), initiatives for Transitions 4.0, co-funded by MUR and by private companies with NRRP incentives, for Space data analysis, for Environment and ecological transitions (e.g. (extracted from412.pdf)
The jurisprudence of the Court of Justice of the EU on the use of AI 35 2.3.1. (extracted from162.pdf)
Policy recommendations for the best use of AI in the fisheries and its value chain 74 REFERENCES 76 4 Artificial Intelligence and the fisheries sector LIST OF ABBREVIATIONS AFMA Australian Fisheries Management Authority AI Artificial Intelligence AIA proposal Proposal for a Regulation of the European Parliament and of the Council laying down harmonised rules on artificial intelligence (Artificial Intelligence Act) AIS Automatic Identification System ANN Artificial Neural Network BN Bayesian Network CAP Common Agricultural Policy CCTV Closed-Circuit Television CFP Common Fisheries Policy CJEU Court of Justice of the European Union CNN Convolutional neural network COM Common Organisation of the Markets DCF Data Collection Framework DL Deep Learning EGD European Green Deal EM Electronic Monitoring EMFAF European Maritime, Fisheries and Aquaculture Fund EU European Union EUMOFA European Market Observatory for Fisheries and Aquaculture FAO Food and Agriculture Organisation of the United Nations FCR Fisheries Control Regulation 5 IPOL | Policy Department for Structural and Cohesion Policies FPS Frontal protection systems FROODS Fishing Route Optimization Decision Support System GDP Gross Domestic Product GDPR General Data Protection Regulation GES Good Environmental Status GVA Gross Value Added H2020 Horizon 2020 EU research and innovation funding programme IATTC Inter American Tropical Tuna Commission International Commission for the Conservation of Atlantic Tunas ICCAT ICES International Council for the Exploitation of the Sea IMO International Maritime Organization IOTC Indian Ocean Tuna Commission IUU Illegal, Unreported and Unregulated LO Landing obligation Minimum Conservation Reference Size MCRS Machine Learning ML MPI Ministry of Primary Industries of New Zealand MSFD Marine Strategy Framework Directive MSY Maximum Sustainable Yield NMFS National Marine and Fisheries Services NOAA North Oceanic and Atmospheric Administration OJ Official Journal PECH Committee European Parliament’s Committee on Fisheries PET Protected Endangered and Threatened species 6 Artificial Intelligence and the fisheries sector RFMO Regional fisheries management organisation SDG Sustainable Development Goal SME Small and Medium Enterprise STECF Scientific Technical Economic Committee on Fisheries SVM Support Vector Machine Total Allowable Catches TAC Treaty on European Union TEU Treaty on the Functioning of the European Union TFEU United Nations UN US United States of America VMS Vessel Monitoring System WCPFC Western and Central Pacific Fisheries Commission 7 IPOL | Policy Department for Structural and Cohesion Policies LIST OF FIGURES Figure 1: Conceptual diagram showing the role of AI in relation with other digitalisation activities 15 Figure 2: Classification of AI techniques and approaches in the AIA proposal expanded with further subcategories used in this study 42 Figure 3: Simplified diagram of an expert system 47 Figure 4: Simplified schema of the Fish Value Chain 55 Figure 5: Traceability aspects improvable by AI systems 56 Figure 6: Published papers related to fisheries, AI and traceability keywords 57 LIST OF TABLES Table 1: Non-exhaustive summary of the recitals and articles of the most relevant EU fisheries legislation containing elements making the use of AI systems possible 31 Table 2: Similarities, differences and keywords associated to specific ML methods 43 8 Artificial Intelligence and the fisheries sector EXECUTIVE SUMMARY This study reviews the main applications of Artificial Intelligence (AI) systems in fisheries and identifies current challenges for fisheries that have the potential to be dealt with through AI. (extracted from162.pdf)
The third section (2.3) includes a reference to the scarce jurisprudence of the Court of Justice of the European Union (CJEU) that has so far, directly, or indirectly, considered AI- related issues with projection on the fisheries sector. (extracted from162.pdf)
58 European Commission, Directorate-General for Justice and Consumers, Liability for artificial intelligence and other emerging digital technologies, Publications Office, 2019. (extracted from162.pdf)
The jurisprudence of the Court of Justice of the EU on the use of AI The CJEU as the judicial authority of the EU by virtue of Article 19 TEU, in cooperation with the judicial bodies of the EU Member States, ensures the uniform application and interpretation of EU law. (extracted from162.pdf)
An appeal has now been brought against this judgment before the Court of Justice 61. (extracted from162.pdf)
Case on illegal mechanical device for fish classification A Court of Justice case was found where an illegal mechanical fish classification device was used. (extracted from162.pdf)
In its judgment of 11 February 2021 in Case K.M.62, the Court of Justice ruled on the reference for a preliminary ruling received from the Irish Court of Appeal under Article 267 TFEU. (extracted from162.pdf)
62 Judgment of the Court of Justice of 11 February 2021, K.M., C-77/20, ECLI:EU:C:2021:112. (extracted from162.pdf)
The Court of Justice was asked whether Article 89 and Article 90 of FCR, read in the light of the principle of proportionality enshrined in Article 49(3) of the Charter, were to be interpreted as precluding a national provision which, in order to penalise an infringement of Article 32 of Regulation No 850/98, provided for the imposition of a fine and the mandatory confiscation of prohibited or non-compliant catches and fishing gear found on board the vessel concerned (paragraph 25). (extracted from162.pdf)
The Court of Justice found that the mandatory confiscation of prohibited or non-compliant catches and fishing gear may deter the persons concerned from infringing the prohibition on sorting equipment, laid down in Article 32(1) of Regulation No 850/98, by depriving them of the illegally obtained benefits which they could otherwise enjoy, and of the possibility of continuing to use such equipment (paragraph 44). (extracted from162.pdf)
65 Among other judgments, see: judgment of the Court of Justice of 16 July 2015, Chmielewski, C-178/03, ECLI:EU:C:2015:475, paragraph 21. (extracted from162.pdf)
Governing data and artificial intelligence for all Models for sustainable and just data governance STUDY Panel for the Future of Science and Technology EPRS | European Parliamentary Research Service Scientific Foresight Unit (STOA) EN PE 729.533 – July 2022 Governing data and artificial intelligence for all Models for sustainable and just data governance With a particular focus on artificial intelligence (AI), this study identifies and examines policy options for the EU's data governance framework that align with a data justice perspective. (extracted from176.pdf)
A data justice approach is one that centres on equity, recognition and representation of plural interests, and the creation and preservation of public goods as its principal goals. (extracted from176.pdf)
Four benchmarks for good data governance are proposed, in line with the principles of justice: preserving and strengthening public infrastructure and public goods, inclusiveness, contestability and accountability, and global responsibility. (extracted from176.pdf)
STOA | Panel for the Future of Science and Technology AUTHORS This study was written by Joan Lopez Solano, Aaron Martin, Siddharth de Souza and Linnet Taylor of the Global Data Justice project, Tilburg University, at the request of the Panel for the Future of Science and Technology (STOA) and managed by the Scientific Foresight Unit, within the Directorate-General for Parliamentary Research Services (EPRS) of the Secretariat of the European Parliament. (extracted from176.pdf)
The Global Data Justice project would like to acknowledge valuable contributions to the analysis in this report from: Maria Anagnostu, Shweta Degalahal, Paula Ferreira Vidal, Yash Kaushal, Andrew Key, Janne Joosten, Alexis Manus, Franklyn Ohai, Gargi Sharma and Zsuzsanna Véghné Ujj. (extracted from176.pdf)
PE 729.533 ISBN: 978-92-846-9623-9 doi: 10.2861/915401 QA-05-22-170-EN-N http://www.europarl.europa.eu/stoa (STOA website) http://www.eprs.ep.parl.union.eu (intranet) http://www.europarl.europa.eu/thinktank (internet) http://epthinktank.eu (blog) II Governing data and artificial intelligence for all Executive summary With particular regard to artificial intelligence (AI), this study aims to identify and examine policy options for Europe's data governance framework that align with a data justice perspective. (extracted from176.pdf)
A data justice approach is one that centres on equity, the recognition and representation of plural interests, and the creation and preservation of public goods as its principal goals. (extracted from176.pdf)
A data justice perspective is a particularly appropriate tool for this analysis, because AI is not a bottom-up class of technology, in terms of either development or use. (extracted from176.pdf)
Starting from research on data justice, we propose four benchmarks for good governance: preserving and strengthening public infrastructure and public goods, inclusiveness, contestability and accountability, and global responsibility. (extracted from176.pdf)
Policy options and alternatives: a data justice analysis ______________________________ 58 6.1. (extracted from176.pdf)
An important component of that representation can be framed as 'data justice' - the view that data governance should not only seek to do no harm, but should positively contribute to people's autonomy and to their ability to participate in society and make claims about their needs, on a more general level.1 In contrast, our current worldwide model for data governance represents a very specific set of interests: those of the largest players in the technology sphere, and the states in whose economies they are embedded. (extracted from176.pdf)
1 Linnet Taylor, 'What Is Data Justice? (extracted from176.pdf)
Technology, https://www.theguardian.com/technology/2017/dec/20/uber-european-court-of-justice-ruling-barcelona-taxi-drivers- ecj-eu. (extracted from176.pdf)
Lessons from Cybersecurity Vulnerability Disclosure for Algorithmic Harms Discovery, Disclosure, and Redress' (Algorithmic Justice League, January 2022), https://www.ajl.org/bugs. (extracted from176.pdf)
One such proposal from India is the creation of an Interoperable Criminal Justice Database. (extracted from176.pdf)
The stated objective of this initiative is that for the criminal justice system to work more effectively, data needs to be shared across different institutions in the system, and be made accessible and interoperable.59 The Supreme Court of India, in its vision document for a digitised judiciary, has called for the 'seamless exchange of live data' rather than data being shared on a need-to-know basis. (extracted from176.pdf)
59 High Court of Tripura, 'Interoperable Criminal Justice System,' accessed April 26, 2022, https://thc.nic.in/user%20manual/ICJS-manual.pdf. (extracted from176.pdf)
62 It was found for instance that the creation of registers of repeat offenders had an underlying caste bias, and rather than challenging the existence of such registers, they would now be part of a centralised data base where their use would be further cemented.The Indian Express, 'The Dangers of a Centralised Database for Justice System,' May 28, 2021, https://indianexpress.com/article/opinion/columns/the-dangers-of-a-centralised-database-for-justice-system-7333252/. (extracted from176.pdf)
For instance in the Dutch government's misuse of data to predict fraud among 63 Linnet Taylor, 'What Is Data Justice? (extracted from176.pdf)
These values relating to the economic growth and wellbeing of EU Member States potentially stand in tension with the rights- and justice-based orientation of the other core value statements to do with digital strategy, for instance in the statement that the innovation principle is legislatively as important as the precautionary principle, which underlies much thinking about digital rights and data protection. (extracted from176.pdf)
One way to arbitrate between these differing visions is to address them through a justice lens, which asserts that there are tests we can apply to statements about good governance and related models, to see whether they align with the core assumptions that make governance functional for people, and place it at the service of the public rather than in the interests of the most powerful and privileged. (extracted from176.pdf)
In 2017, the Global Data Justice project (the authors of this report, based in the Netherlands) asserted three fundamental pillars of 'good' data governance.159 These pillars offer a tool for understanding how public values can be incorporated in governance frameworks. (extracted from176.pdf)
If it has the effect of channelling power and profit toward the best-resourced and most powerful actors, whether governmental or corporate, a governance model is not in line with principles of social justice and requires reorienting. (extracted from176.pdf)
This justice-based reasoning leads to a set of core benchmarks for good governance of data: 159 Linnet Taylor, 'What Is Data Justice? (extracted from176.pdf)
In terms of the first of these two, these infrastructure-related goods include the social safety net, scientific knowledge,163 public education and healthcare, access to justice, electoral processes and law enforcement. (extracted from176.pdf)
See: Karen Maex: 'Protect independent and public knowledge.' Speech January 8th 2021 https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=&cad=rja&uact=8&ved=2ahUKEwiavsCj5_D4AhV L- aQKHQzEA64QFnoECAUQAQ&url=https%3A%2F%2Fwww.uva.nl%2Fbinaries%2Fcontent%2Fassets%2Fuva%2Fnl%2Fo ver-de-uva%2Fspeech-karen-maex---dies-2021.pdf&usg=AOvVaw2V5_UZK8444_8hFUU2XU9s 164 See the work of the Global Data Justice project on pandemic-related commercialisation of public goods: https://globaldatajustice.org/sphere-trans/ 165 For further explanation, see for example Timo Meynhardt, 'Public Value Inside: What Is Public Value Creation?,' International Journal of Public Administration 32, no. (extracted from176.pdf)
Inclusiveness An inclusive perspective on data governance demands that any model centre justice and take into account the interests of all who will be affected by it, rather than simply a body of citizens in a particular state or region. (extracted from176.pdf)
Data regulation tends to cite both human rights and market efficiency, the former of which aligns well with a social justice-oriented governance model. (extracted from176.pdf)
In contrast, a social justice perspective starts from the assumption that all societies are unequal playing fields where people's circumstances give them different degrees of agency and ability to claim their rights. (extracted from176.pdf)
A social justice-oriented data governance model suggests additions to our way of conceptualising vulnerability in relation to technology. (extracted from176.pdf)
Beyond the notion that people are more easily harmed if they have certain minority or sensitive attributes, a social justice view would add another facet to these considerations: that of unequally distributed power and agency. (extracted from176.pdf)
A data justice perspective demands that we broaden our conceptualisation of contestability beyond claims based on competition and consumer protection regulation. (extracted from176.pdf)
Taking a justice perspective, our analysis explores the extent to which these interests productively interact, where governance becomes skewed toward economic priorities at the expense of people, and how the plural interests of the EU's diverse populations can be recognised and represented through the process of governing technologies. (extracted from176.pdf)
'Abnormal justice.' Critical inquiry, 34(3), 39. (extracted from176.pdf)
In our discussion so far on thinking about data governance from a social justice standpoint, there are few considerations that have emerged that require further discussion. (extracted from176.pdf)
Policy options and alternatives: a data justice analysis In this section we will analyse policy options first in relation to the European Strategy for Data, and then in relation to the principal legislative files currently under development. (extracted from176.pdf)
233 Sohel Sarkar and Amay Korjan, eds., A Digital New Deal: Visions of Justice in a Post-Covid World (IT for Change and Just Net Coalition, 2021). (extracted from176.pdf)
Starting from a definition of governance as arbitration between different interests with regard to public and private goods, we have offered a justice-based analysis of the legislative context with regard to artificial intelligence and contributing data technologies and argue that along with considerations of its economic benefits, a strategy as to how European AI generates public value could be central to the EU's policy aims. (extracted from176.pdf)
We then conducted an analysis of the core set of legislative files relating to AI, and explored how they could be aligned with these justice-based benchmarks for good governance. (extracted from176.pdf)
67 With a particular focus on artificial intelligence (AI), this study identifies and examines policy options for the EU's data governance framework that align with a data justice perspective. (extracted from176.pdf)
A data justice approach is one that centres on equity, recognition and representation of plural interests, and the creation and preservation of public goods as its principal goals. (extracted from176.pdf)
We propose four benchmarks for good data governance according to principles of justice: preserving and strengthening public infrastructures and public goods, inclusiveness, contestability and accountability, and global responsibility. (extracted from176.pdf)
———.“TheEUCodeofConductonCounteringIllegalHateSpeechOnline:TheRobustResponseProvided bytheEuropeanUnion.”AccessedJuly11,2022.https://ec.europa.eu/info/policies/justice-and-funda- THEBRUSSELSEFFECTANDARTIFICIALINTELLIGENCE•86 mental-rights/combatting-discrimination/racism-and-xenophobia/eu-code-conduct-countering-illegal- hate-speech-online_en. (extracted from88.pdf)
The values of equality, tolerance, respect for others, and justice govern this principle. (extracted from54.html)
And it was in this mindset that the CNIL set a collective approach in motion, for several months overseeing a public debate with the help of partners from various sectorial fields (health, justice...). (extracted from200.pdf)
Among them, we could mention the “Ligue de l’Enseignement” (associa- tion that focused on education concerns), French Insu- rance Federation (FFA), French Ministry of Culture (DG- MIC), Open Law (association that reflects on the justice Ethical thinking concerns system) as well as trade unions such as CFE-CFC and FO Cadres (for recruitment and HR), etc. (extracted from200.pdf)
The views of the diverse stakeholders (trade unions, associations, businesses, researchers, citizens, etc.) across a wide range of sectors (from insurance to education, justice and healthcare) thus informed the writing of this report, which provides an overview of the ethical matters raised by algorithms and artificial intel- ligence in their current applications and their potential uses in the relatively short term. (extracted from200.pdf)
THE ETHICAL MATTERS RAISED BY ALGORITHMS AND ARTIFICIAL INTELLIGENCE ALGORITHMS AND ARTIFICIAL INTELLIGENCE TODAY The main functions of algorithms and AI across different sectors Education Justice Health Security Work, HR Culture Other Better identify Reveal the Tap into the Identify Understand Create cultural Fine-tune an learners’ different ways vast amount unsuspected social showpieces insurance abilities judgments are of scientific links for solving phenomena (painting, music) company Generating handed down publications gendarmerie-led in the workplace customer’s risk knowledge between regions investigations profile Allocate higher Allocate patients Match a list of Match education places for participation applicants to a “compatible” to candidates in a clinical trial job vacancy profiles (APB) on dating apps, Matching etc. (extracted from200.pdf)
The next step of what some refer to as “predic- on the sheer range of the spectrum of serious or less serious tive justice” would involve entrusting systems with the task matters in the use of such or such an algorithm. (extracted from200.pdf)
Incidentally, the CNIL-led public debate brought a legal rules and categories no longer on the grounds of controversy to light in this regard, concerning IBM’s Watson our ideal of justice, but so that they are more readily platform. (extracted from200.pdf)
And anyone deploying algorithms ligence be limited to crucial decisions, sectors where the that are likely to be used on a large scale should be urged impact on humans is undeniable, such as medicine, justice, to bear it in mind. (extracted from200.pdf)
Practical examples yet, are we absolutely certain that, within certain limits, of algorithms being used by the authorities as well as the forms of regional disparity do not, in fact, reflect a res- example of predictive justice give a clearer idea of this ponsible exercising of caution on the part of the judge? (extracted from200.pdf)
A complex chain a predictive justice. (extracted from200.pdf)
For it enables juries to take on board new insights over the course of Similarly, at the symposium on predictive justice orga- hearing the different arguments, and to change opinion, nised on 19 May 2017 by the Lille Bar, Law Department as shows more clearly than any demonstration the film of Université catholique de Lille and the Douai Court of by Sidney Lumet, Twelve Angry Men. (extracted from200.pdf)
Predictive justice applications are being subjected to particularly close public scrutiny as regards their Management Profile for Alternative Sanction) tool designed to come up with a recidivism risk score for prisoners or defendants on trial. (extracted from200.pdf)
Medicine our day-to-day lives, as long as answers are and justice are other sectors where this question might forthcoming. (extracted from200.pdf)
It is paramount that they have eness of the ethical dimensions of a decision-making the fullest possible awareness of the ethical and social process that must not exclude human intervention and by implications of their work and of the very fact that these honing critical thinking in some particularly sensitive sec- can even extend to societal choices which they should tors, such as medicine, recruitment, justice and perhaps not by rights be able to judge alone. (extracted from200.pdf)
The partners in the public debate • Allistene’s research committee on ethics (CERNA) • Institut Mines-Télécom (IMT), Research Chair “Values • Bordeaux’s Cognitique Institute (ENSC) and Politics of Personal Information” • Bordeaux University • Laboratory for Collective and Artificial Intelligence (LICA) • Caisse des dépôts et consignations (CDC) • Law Department of Université Catholique de Lille, • Club des Juristes (thinktank) Centre of research on relations between risk and law • Collège des Bernardins • Law Department of Université Catholique de Lyon • Complex Systems Institute of Paris Ile-de-France (ISC-PIF) • Ligue des Droits de l’Homme (Human Rights League/LDH) • Confédération française de l’encadrement – Confédération • Ligue de l’Enseignement (Education League) générale des cadres (CFE-CGC, trade union) • Lille 2 University • Communication Publique • Lille Association of Lawyers • Conseil National des Barreaux (national institution • Lyon’s administrative court of appeal that represents all practising lawyers in France/CNB) • Microsoft • Conseil Supérieur de l’Audiovisuel (independent authority • Ministry of Culture, via the General Directorate of Media to protect audiovisual communication freedom/CSA) and Cultural Industries (DGMIC) • Conservatoire National des Arts et Métiers (leading • Ministry of National Education, via the Directorate higher education and research institution dedicated of Digital Technology for Education (DNE) and its Numéri’lab to adult continuing education/CNAM) • National Academy of Technologies of France • Douai court of appeal • National Institute of Higher Studies on Defence (IHEDN) • ESCP Europe, IoT Chair • National Institute of Higher Studies on Security and Justice • Etalab(body that works in France on data sharing (INHESJ) in the public sector) • National Institute of Applied Sciences (INSA) • “Familles rurales” association • Necker Hospital • Federal University of Toulouse • OpenLaw (association) • French Association for Artificial intelligence (AFIA) • Paris II University • French Association for Employment Law (AFDT) • Randstad • French Development Agency(AFD) • Research Centre of the National Gendarmerie School • French governmental advisory council on bioethics issues of Officers (CREOGN) (CCNE) • Rhône Département-level Council of the Medical • French Insurance Federation (FFA) Association • French National Center for Scientific Research (CNRS)’s • Renaissance Numérique (thinktank) ethics committee (COMETS) • School of Advanced Studies in the Social Sciences (EHESS) • FO-Cadres (trade union) • Sciences Po Lille • Fondation Internet Nouvelle Génération (FING) • Sciences Po Paris • Fotonower • Société informatique de France (association devoted • Génotoul societal (bioscience and ethics platform) to computer science/SIF) • Groupe VYV (MGEN – ISTYA – Harmonie) • The Future Society at Harvard Kennedy School, AI Initiative • Imagine Institute on genetic diseases • Universcience • INNOvation Ouverte par Ordinateur (INNOOO) • Visions d’Europe (association) The other contributors The 37 citizens who took part in the public consultation • A rbre des connaissances (association) organised in Montpellier on 14 October 2017. (extracted from200.pdf)
23/01/2017 LAUNCH EVENT ROUNDTABLE SESSIONS “Algorithms and humans” and “Fairness, transparency and plurality of algorithms” > CNIL 23/03/2017 SYMPOSIUM “Towards new forms of humanity?” 25/03/2017 > Universcience 31/03/2017 CONFERENCE “Algorithms and law” > Lille II University 06/04/2017 CONFERENCE “The choice in the age of Big Data” > Sciences Po Lille and Visions d’Europe 08/04/2017 DEBATE “The governance of emerging technosciences” > German American Conference at Harvard University 18/04/2017 DEBATE “Transatlantic perspectives on: AI in the age of social media; privacy, security and the future of political campaigning” > The Future Society at Harvard Kennedy School 18/04/2017 ROUNDTABLE SESSIONS “Big Data, human resources: algorithms on the agenda” > FO-Cadres 04/05/2017 CONFERENCE “Fairness of algorithmic decision-making” > Toulouse III – Paul Sabatier University 16/05/2017 DEBATE “Will digital technology spell the end of the rule of law?” > Collège des Bernadins 19/05/2017 SYMPOSIUM “Predictive justice” > Douai Court of Appeal, Lille Association of Lawyers and Law Department of Université Catholique de Lille 02/06/2017 WORKSHOPS “Fairness of algorithmic decision-making and processing” > LabEx International Centre for Mathematics and Computer Science in Toulouse 64 HOW CAN HUMANS KEEP THE UPPER HAND? (extracted from200.pdf)
THE ETHICAL MATTERS RAISED BY ALGORITHMS AND ARTIFICIAL INTELLIGENCE EVENTS 08/06/2017 DEBATE “Algorithms in healthcare: what ethics?” > Groupe VYV (MGEN – ISTYA – Harmonie) 14/06/2017 ROUNDTABLE SESSION “Artificial intelligence: ethics, at the intersection of HR and Big Data” > Confédération française de l’encadrement – Confédération générale des cadres (CFE-CGC) 16/06/2017 DEBATE “Algorithms, employment and ethics” > French Association for Employment Law (AFDT) 19/06/2017 DAY “Ethical algorithms, a moral requirement and competitive advantage” > Allistene’s CERNA and Société Informatique de France (SIF) 19/06/2017 SYMPOSIUM “Human, non-human in the age of artificial intelligence” > Paris II University 21/06/2017 SYMPOSIUM “Artificial intelligence: autonomy, delegation and accountability” > Bordeaux’s Cognitique Institute (ENSC) 22/06/2017 WORKSHOP “Ethics of algorithms: implications for healthcare” > Genotoul (bioscience and ethics platform) 22/06/2017 CROWDSOURCING WORKSHOP “Artificial intelligence and law” > OpenLaw 22/06/2017 SYMPOSIUM “ The many dimensions of data ” 23/06/2017 > Institut Mines-Télécom, Values and Politics of Personal Information Research Chair 27/06/2017 SYMPOSIUM “Security and justice, the challenge of the algorithm” > National Institute of Higher Studies of Security and Justice (INHESJ) 28/06/2017 MOCK TRIAL AND ROUNDTABLE SESSION “ Ethique, algorithmes and justice ” > Law Department of Université Catholique de Lyon and Lyon’s Administrative Court of Appeal 28/06/2017 STUDY DAY “Admission Post-bac, textbook case of public algorithms” > Fondation Internet Nouvelle Génération (FING) and Etalab 03/07/2017 DAY “Algorithms and digital sovereignty” > Allistene’s CERNA 05/07/2017 DAY “Ethics and artificial intelligence” > French National Center for Scientific Research (CNRS)’s ethics committee (COMETS) and French Association for AI (AFIA) 22/08/2017 DEBATES on algorithms in education. (extracted from200.pdf)
Other sions need to acknowledge that there is a human rights rights are also affected; for example, an automated framework setting binding legal obligations around system used in the justice system, which is based on AI, which should be seen as a starting point for any poor quality data, can negatively impact on the right evaluation of the opportunities and challenges brought to a fair trial and effective remedy, as well as on the by new technologies. (extracted from204.pdf)
See for 7 Committee on Civil Liberties, Justice and Home Affairs example: Salganik, M. (extracted from204.pdf)
Data protection legislation offers min- tion and the criminal justice system. (extracted from204.pdf)
The use of AI imal guidance on the topic – the principle of data and algorithms in the area of justice needs testing, accuracy in the General Data Protection Regulation as highlighted in the CEPEJ European ethical char- (GDPR)34 is related to data quality, but in a very nar- ter.30 One potential problem might be the use of row sense as it only focuses on the obligation to biased data for automated systems.31 In addition, keep personal data accurate and up to date. (extracted from204.pdf)
15 Data quality and artificial intelligence – mitigating bias and error to protect fundamental rights References Access Now (2018), Human Rights in the age of Committee on Civil Liberties, Justice and Home Artificial Intelligence. (extracted from204.pdf)
Affairs (2018), Opinion of the Committee on Civil Liberties, Justice and Home Affairs for the Alpaydin, E. (extracted from204.pdf)
(forthcoming), ‘Dirty Data, Bad Predictions: How (2017), ‘Men Also Like Shopping: Reducing Gender Bias Civil Rights Violations Impact Police Data, Predictive Amplification using Corpus-level Constraints’, paper Policing Systems, and Justice’, New York University given at the 2017 Conference on Empirical Methods Law Review Online. (extracted from204.pdf)
18 FRA – EUROPEAN UNION AGENCY FOR FUNDAMENTAL RIGHTS Schwarzenbergplatz 11 – 1040 Vienna – Austria Tel: +43 158030-0 – Fax: +43 158030-699 fra.europa.eu © European Union Agency for Fundamental Rights, 2019 facebook.com/fundamentalrights linkedin.com/company/eu-fundamental-rights-agency Print: ISBN 978-92-9474-605-4, doi:10.2811/615718 twitter.com/EURightsAgency PDF: ISBN 978-92-9474-606-1, doi:10.2811/546219 TK-01-19-330-EN-C (print); TK-01-19-330-EN-N (PDF) Further information: The following FRA publications offer further information relevant to the topic of the paper: • #BigData: Discrimination in data-supported decision making (2018) http://fra.europa.eu/en/publication/2018/big-data-discrimination • Under watchful eyes: biometrics, EU IT systems and fundamental rights (2018) http://fra.europa.eu/en/publication/2018/biometrics-rights-protection • Fundamental rights and the interoperability of EU information systems: borders and security (2017) http://fra.europa.eu/en/publication/2017/fundamental-rights-interoperability • Surveillance by intelligence services: fundamental rights safeguards and remedies in the EU - Volume II: field perspectives and legal update (2017) http://fra.europa.eu/en/publication/2017/surveillance-intelligence-socio-lega • Surveillance by intelligence services: fundamental rights safeguards and remedies in the European Union - Mapping Member States’ legal frameworks (2015) http://fra.europa.eu/en/publication/2015/surveillance-intelligence-services • The impact on fundamental rights of the proposed Regulation on the European Travel Information and Authorisation System (ETIAS) (2017) http://fra.europa.eu/en/opinion/2017/etias-impact • Handbook on European data protection law - 2018 edition (2018) https://fra.europa.eu/en/publication/2018/handbook-european-data-protection-law • Handbook on European law relating to access to justice (2016) https://fra.europa.eu/en/publication/2016/handbook-european-law-relating-access-justice • Handbook on European non-discrimination law – 2018 edition (2018) http://fra.europa.eu/en/publication/2018/handbook-european-law-non-discrimination (extracted from204.pdf)
A 'principled' artificial intelligence could improve justice Toggle navigation My ABA Events CLE Marketplace Shop ABA Member Directory Join Log In News Latest News Topics Features Latest Issue Podcasts Asked and Answered Legal Rebels Modern Law Library Columnists Bryan Garner on Words Erwin Chemerinsky Mind Your Business Your Voice How I Won the Case My Path to Law Survival Guide, Esq. (extracted from361.html)
Nicole Black Ari Kaplan Adam Banner Randy Maniloff Marcel Strigberger Members Who Inspire Submit Home Legal Rebels A 'principled' artificial intelligence could improve justice The New Normal A ‘principled’ artificial intelligence could improve justice October 3, 2017, 8:20 am CDT By Nicolas Economou Tweet Email Print Nicolas Economou is the CEO of electronic discovery and information retrieval firm H5, a Senior Advisor to the AI Initiative of the Future Society at Harvard Kennedy School, and is an advocate of the application of scientific methods to electronic discovery. (extracted from361.html)
In rendering the central fact-finding mission of the legal process more effective and efficient, expertly designed and executed hybrid intelligence processes can reduce errors in the determination of guilt or innocence, accelerate the resolution of disputes, and provide access to justice to parties who would otherwise lack the financial wherewithal. (extracted from361.html)
Alarming reports have detailed how discrim- inatory algorithms are already deployed in the justice system, wherein judges use these tools for sentencing that purport to predict the likelihood a criminal defendant will reoffend.21 In Automating Inequality, Virginia Eubanks details how government actors implement automated and surveillance technologies that harm marginalized groups.22 Eubanks studied automated systems in the US that discriminated against the poor’s receipt of DATA & SOCIETY 10 GOVERNING ARTIFICIAL INTELLIGENCE government assistance. (extracted from172.pdf)
Baluarte and Christian De Vos, From Judgment to Justice: Implementing International and Regional Human Rights Decisions, Open Society Justice Initiative (November 2010), https:// www.opensocietyfoundations.org/sites/default/files/from-judgment-to-justice-20101122.pdf. (extracted from172.pdf)
The majority of services underpinned by AI, from education, to strategies recognised that developing high quality, in- healthcare, to justice, as well as AI having a key role country AI expertise is vital for a country to remain at in policy-making itself.15 Other countries are creating the forefront of the AI revolution. (extracted from73.pdf)
Digitalization, Anticorruption, Rule of Ariana British Embassy in Mexico City International Cooperation Law and Competition Policy Carla Vazquez RUCAM Ministry of Economy Government Carlos Gershenson Researcher Metro CDMX/ UNAM Academia CDMX Carlos López Franco Researcher UDG/CUCEI Jal Academia Head of Human Rights, Security and Justice at the Chris Wall British Embassy in Mexico City International Cooperation British Embassy in Mexico City Coordinator of Development of Industrial Ministry of Innovation, Science and Claudia Araujo Gálvez Jal Govt Technological Platforms Technology Claudia Pando Programme Manager, Future Cities British Embassy in Mexico City International Cooperation Cristina Cardenas General Coordinator @prende.mx SEP Government Ministry of Innovation, Science and David Bates Social Innovation Programmes Coordinator Jal Govt Technology Edgar Nelson Sánchez Researcher CINVESTAV Jal Academia Camperos Eduardo Barbosa Emerging Technologies Director Ciudad Creativa Digital Jal Govt Eduardo Clark Public Innovation Deputy Director CEDN Government Eduardo Farina CEO Bluemessaging Start-up Bluemessaging Eduardo Morales AI Phd INAOE Puebla Academia Elsa Ayala General Director of Normatividad Mercantil Ministry of Economy Government Enrique Jaime Herrera Researcher CIATEJ Jal Academia López Enrique Sucar Senior Research Scientist INAOEP Academia Enrique Zapata General Director for Open Data CEDN Government Technical Secretary of the Urban Development Fernando Cota Senate Government Commission Member of the Science and Francisco Búrquez Senator Legislative branch Technology Commission Gabriella Gomez Mont Director Laboratorio de la Ciudad CDMX Govt Director of Technological Platforms Development Ministry of Innovation, Science and Gerardo Rodríguez Barba Jal Govt and Promotion Technology Gustavo Carreon Researcher Metro CDMX/ UNAM Academia CDMX Gustavo Pares Nearshore CEO Nearshore Start-up Isaac Avila Coordinator CANIETI Occidente GDL Industry Ivan Millan General Director Jalisco Talent Land GDL Javier Mata Yalo CEO Yalo Start-up Jessica Paola Avila Open Data Director SEPAF Gobierno Jalisco Jal Govt 39 TOWARDS AN AI STRATEGY IN MEXICO: Harnessing the AI Revolution Name Job Title Organisation Sector Jesus Cepeda CEO One Smart City MTY Civil Society/Consultancy José Cantoral Researcher CIATEQ Jal Academia Foro Consultivo Científico y Jose Franco General Coordinator Academia / Civil Society Tecnológico Juan Pablo Escobar Director Civica Digital MTY Civil Society/Consultancy Katie Allan Associate Oxford Insights Consultancy Laura Caccia Consultant Oxford Insights Consultancy Director - Center for Business Development in IT Lorenzo Valle TEC Academia and Big Data Initiative Lorenzo Valle Garcilazo Big Data Center Coordinator ITESM Academia ITESM Luis Cadena General Administrator of Communications and ICT SAT, SHCP Government Luis Valtierra President IJALTI Cluster Manuel Avalos IBM Storage Cloud and Solutions for IBM WW IBM/ Watson Jal Industry General Director of Social Innovation and Ministry of Innovation, Science and Margarita Solis Jal Govt Entrepreneurship Technology María de Lourdes Martínez Vocal SMIA Academia Villaseñor, SMIA Future Cities Programme and Policy Officer– Marian Urizar British Embassy in Mexico City International Cooperation Programme Team Mario Angel Siller Researcher CINVESTAV Jal Academia Gonzalez Martha González Pérez- Director Cognitive Solutions, IBM Mexico Industry Sandi Matt Pasiensky VP of International Operations Wizeline GDL Industry Miguel Gonzalez President & Researcher SMIA, ITESM Academia Miguel González Mendoza President Mexico's AI Society Academia / Civil Society Miguel Salazar Director Ejecutivo Codeando Civil Society GDL Miriam Díaz Rodríguez Researcher/ Professor Tecnológico Mario Molina Jal Academia General Director of Science and Technological Ministry of Innovation, Science and Morris Schwarzblat y Katz Jal Govt Development Technology Nancy Guadalupe Arana Director, Systems Control & AI Center UDG/CUCEI Jal Academia Daniel Neil Hernández Gress Researcher ITESM Academia ITESM Olivia Barron AI PhD UDEM MTY Academia Oliver Rice Associate Oxford Insights Consultancy Omar Gonzalez KYSE Agritech UNAM CDMX Startup Raymundo Vazquez SW Engineering Manager INTEL Jal Industry Ricardo Reyes Data Wuki & CEO Data Wuki and Quantum Labs Start-up Quantum Labs Head of Anti-corruption, Competition, Digitalisation Rodrigo Félix British Embassy in Mexico City International Cooperation and Rule Of Law Policy Saiph Savage Coordinator BanFakeNews Bot CDMX Sebastian Sposito Public Policy and Government Affairs Advisor Google Industry Sissi de la Peña Regional Markets CEDN Government Sophie Marment Head of Prosperity Fund FCO Government Tania Cruz Digital Government Services CEDN Government Victor Gutierrez Industry CCE Industry Yamin Ruiz Global Proteus CEO Global Proteus Start-up Yolanda Martinez Coordinator National Digital Strategy Government 40 TOWARDS AN AI STRATEGY IN MEXICO: Harnessing the AI Revolution Appendix 2: Innovation in Mexico’s Regions and major economic sectors. (extracted from73.pdf)
If the data is not representative for minority populations then it could be potentially harmful.’ Berk Ustun, Postdoctoral Fellow, Center for Research in Computation and Society, Harvard University Racial bias in criminal justice algorithms The Correctional Offender Management Profiling for Alternative Sanctions (COMPAS) is an algorithmic risk score used to help judges in certain US states decide sentencing, by predicting a defendant’s risk of reoffending. (extracted from9.pdf)
In areas like humanitarian response, where fundamental principles of The third principle, justice, considers who gets chosen to be a research privacy like accuracy are inherently challenged and consent is not subject and what population segments stand to benefit — or be harmed a panacea given that benefits are associated with risks, ethical and — by the research results. (extracted from370.pdf)
IBM’s researcher or non-profit with a social justice mission. (extracted from370.pdf)
With traditional values such as dignity, freedom, democracy, regard to AI, heightened focus should be placed on the equality, autonomy, and justice are part of discussions FIPPs of Security Safeguards and Accountability. (extracted from403.pdf)
This list is to the general public, the bar graph shows the non-exhaustive, and many important ethical issues incidence of identified ethical challenges across -- including justice, economic development, poverty 59 AI Principles documents (Figure 8.1b). (extracted from417.pdf)
The chart below indicates the inclusive societies for sustainable development, number of AI use cases in MGI’s library that could provide access to justice for all and build effective, support each of the UN SDGs (Figure 8.3a). (extracted from417.pdf)
of violence, addressing bias to ensure fair and equal A number of use cases that leverage AI support access to justice, to optimizing the management of medical diagnoses: for example, researchers at the public and social sector institutions. (extracted from417.pdf)
List of Organizational Documents Acronym Document Title Document Categorization Issuer COE European ethical Charter on the use of Artificial Intelligence in Official Government/Regulation EUROPEAN COMMISSION judicial systems and their environment FOR THE EFFICIENCY OF JUSTICE (CEPEJ) EUR European Guidelines for Trustworthy AI Official Government/Regulation AI HLEG AUS Artificial Intelligence - Australia’s Ethics Framework Official Government/Regulation Australian Government - Department of Industry, Innovation & Science DUB SMART DUBAI AI ETHICS PRINCIPLES & GUIDELINES Official Government/Regulation Smart Dubai OEC OECD Principles on AI Think Tanks / Policy Institutes OECD G20 G20 Ministerial Statement on Trade and Digital Economy Official Government/Regulation G20 PDP Singapore Personal Data Protection Commission Official Government/Regulation Singapore PDPC DLT AI Ethics: The Next Big Thing In Government Industry & Consultancy Deloitte MEA Work in the age of artificial intelligence. (extracted from417.pdf)
Secretary of State for Business, Energy and 1/6/2018 Industrial Strategy Global Artificial Intelligence and Robotics for Law United Nations Interregional Crime and 1/1/2019 Enforcement Justice Research Institute Global Unprecedented Technological Risks Future of Humanity Institute, Oxford 9/1/2014 University,Centre for the Study of Existential Risk, University of Cambridge Global Artificial Intelligence: The Race Is On The FTI Consulting 2/1/2018 Global Policy Response To AI Topic Concept Graph of AI Strategy Documents Fig. (extracted from417.pdf)
7"Commerce, Justice, Science And Related Agencies Appropriations Bill, 2021 - Report Together With Minority Views," House Committee on Appropriations, July 2020, https://appropriations.house.gov/sites/democrats.appropriations.house.gov/files/July%209th%20report%20for%20circulation_0.pdf, 23. (extracted from8.pdf)
(2021) define AIAs as “emerging governance practices for delineating accountability, rendering visible the harms caused by algorithmic systems, and ensuring practical steps are taken to ameliorate those harms.”34Typical sources of riskto be identified include the presence of bias in datasets used to train an AI system, as well as the fairness and explainability of the model; identification of potential impacts can include contextual considerations related to equity and justice, as well as the economic interests, health, and well-being of users or populations potentially affected by the proposed system. (extracted from8.pdf)
60Corporation of the Canadian Civil Liberties Association and Lester Brown v.Toronto Waterfront Revitalization Corporation, et al., (Ontario Superior Court of Justice File No. (extracted from8.pdf)
In the Judicial Branch digital certification is also widely available, especially in electronic filing, available at the Federal Supreme Court, the Superior Justice Court and at several other courts nationwide. (extracted from72.pdf)
In other words, it means to apply the transforming potential of ICT to benefit the society, as in: • Goods and services more adequate to citizens’ needs; • Simple access to services; 101 • Offering of public service offering which meet the needs for justice, equality, efficiency and effectiveness; • Distributing public benefits efficiently and proportionally; and • Creating value from open government data. (extracted from72.pdf)
115 MEMBERS OF THE E-DIGITAL WORKGROUP MINISTRY OF SCIENCE, TECHNOLOGY, INNOVATION AND COMMUNICATIONS Maximiliano Salvadori Martinhão Miriam Wimmer Daniel Brandão Cavalcanti Artur Coimbra de Oliveira MINISTRY OF INDUSTRY, FOREIGN TRADE AND SERVICES Marcos Vinicius De Souza José Henrique Videira Menezes MINISTRY OF PLANNING, DEVELOPMENT AND MANAGEMENT Wagner Silva de Araújo Elise Sueli Pereira Gonçalves MINISTRY OF FOREIGN AFFAIRS José Antonio Marcondes de Carvalho Benedicto Fonseca Filho MINISTRY OF JUSTICE AND PUBLIC SAFETY Frederico Fernandes Moesch Dim Michelle Ferreira Rodrigues MINISTRY OF CULTURE Rodolfo T. (extracted from72.pdf)
Indeed, several notions of fairness exist that are not only technically defined but also entangled with concepts of social justice, specifically the concept of privilege, held by virtue of belonging to certain social identity groups34. (extracted from167.pdf)
These values are common to the Member States in a society in which pluralism, non-discrimination, tolerance, justice, solidarity and equality between women and men prevail.' Furthermore, its article 3(3) states that the Union 'shall combat social exclusion and discrimination, and shall promote social justice and protection, equality between women and men, solidarity between generations and protection of the rights of the child.' Similar ideas can be found in the Treaty on the Functioning of the European Union, especially in its Part Two, entitled 'Non-Discrimination and Citizenship of the Union' (see article 19.1.) and the EU Charter of Fundamental Rights (article 21). (extracted from167.pdf)
Even worse, this contextually limited interpretation of the concept of discrimination has been endorsed by the Court of Justice of the EU43. (extracted from167.pdf)
There is a nice argument that supports such additional use of the concept: even though the European Court of Justice has never defined the notion of fairness in data protection law, it has used this notion of fairness in two different contexts: fair balance and transparency.48 If we consider that fairness has to do with the reasonable expectations of data subjects, then it should help us to avoid some of the discriminatory results that are not so easy to uncover: a data subject would hardly allow the type of processing that would cause him/her to suffer a damage that other people do not suffer. (extracted from167.pdf)
'Fairness in Criminal Justice Risk Assessments: The State of the Art.' Sociological Methods & Research, vol. (extracted from167.pdf)
Gerards, J., Xenidis, R., Algorithmic discrimination in Europe: challenges and opportunities for gender equality and non-discrimination law, European Commission, Directorate-General for Justice and Consumers, Publications Office, 2021, https://data.europa.eu/doi/10.2838/77444 Gianclaudio Malgieri and Vincenzo Tiani, How the EU Council is rewriting the AI Act, REPORT - 6 December 2021, December 2021, at: https://brusselsprivacyhub.eu/publications/how-the-eu-council-is- rewriting-the-ai-act, last accessed 24/03/2022. (extracted from167.pdf)
For instance, the Slovenian criminal justice.3 There are also some concerns about the impact Ministry of Finance uses a of AI technologies and robotics on the labour market (e.g. (extracted from173.pdf)
The eight principles with regard to AI are: harmony and friendliness; fairness and justice; inclusivity and sharing; respect for privacy; secure/safe and controllable; shared responsibility; open collaboration; and agile governance. (extracted from173.pdf)
Case 36: Automated credit scoring is not qualifying ADM if a human ultimately decides whether to grant a loan or not In anearly pre-GDPR ruling from 2014, the German Federal Court of Justice (Bundesgerichtsoft) stated that “credit-scoring only amounts to an automated individual decision where the responsible body takes a decision with a legal consequence for the person concerned or a decision that has a significant impact on the person concerned, solely on the basis of a score result without further examination of the content. (extracted from211.pdf)
Article 2 from the initial version of the law stated that “Aucune décision de justice impliquant une appréciation sur un comportement humain ne peut avoir pour fondement un traitement automatisé d’informations donnant une définition du profil ou de la personnalité de l’intéressé. (extracted from211.pdf)
150 CJEU, Order of the President of the Court of Justice “Deletion” in Case C-552/21, January 25, 2022, ECLI:EU:C:2022:105. (extracted from211.pdf)
Our results reveal a global convergence emerging around five ethical principles (transparency, justice and fairness, non-maleficence, responsibility and privacy), with substantive divergence in relation to how these principles are interpreted; why they are deemed important; what issue, domain or actors they pertain to; and how they should be implemented. (extracted from205.pdf)
Protecting individuals’ Intel Corporation USA privacy and data in the artificial intelligence world Introducing Unity’s Guiding Principles for Ethical AI – Unity Unity Technologies USA Blog Digital Decisions Center for Democracy & Technology USA Science, Law and Society (SLS) Initiative The Future Society USA AI Now 2018 Report AI Now Institute USA Responsible bots: 10 guidelines for developers of conversational Microsoft USA AI Preparing for the future of Artificial Intelligence Executive Office of the President; National Science and Technology USA Council; Committee on Technology The National Artificial Intelligence Research and Development National Science and Technology Council; Networking and USA Strategic Plan Information Technology Research and Development Subcommittee 5 AI Now 2017 Report AI Now Institute USA Position on Robotics and Artificial Intelligence The Greens (Green Working Group Robots) EU Report with recommendations to the Commission on Civil Law European Parliament EU Rules on Robotics Ethics Guidelines for Trustworthy AI High-Level Expert Group on Artificial Intelligence EU AI4People—An Ethical Framework for a Good AI Society: AI4People EU Opportunities, Risks, Principles, and Recommendations European ethical Charter on the use of Artificial Intelligence in Concil of Europe: European Commission for the efficiency of EU judicial systems and their environment Justice (CEPEJ) Statement on Artificial Intelligence, Robotics and 'Autonomous' European Commission, European Group on Ethics in Science and EU Systems New Technologies Artificial Intelligence and Machine Learning: Policy Paper Internet Society international Report of COMEST on Robotics Ethics COMEST/UNESCO international Ethical Principles for Artificial Intelligence and Data Analytics Software & Information Industry Association (SIIA), Public Policy international Division ITI AI Policy Principles Information Technology Industry Council (ITI) international Ethically Aligned Design. (extracted from205.pdf)
These are, by frequency of the number of sources in which they were featured: transparency, justice and fairness, non-maleficence, responsibility, privacy, beneficence, freedom and autonomy, trust, dignity, sustainability, and solidarity (cf. (extracted from205.pdf)
7 Table 2 – Ethical principles identified in existing AI guidelines Ethical principle Number of Included codes documents Transparency 73/84 Transparency, explainability, explicability, understandability, interpretability, communication, disclosure, showing Justice & fairness 68/84 Justice, fairness, consistency, inclusion, equality, equity, (non-)bias, (non-)discrimination, diversity, plurality, accessibility, reversibility, remedy, redress, challenge, access and distribution Non-maleficence 60/84 Non-maleficence, security, safety, harm, protection, precaution, prevention, integrity (bodily or mental), non-subversion Responsibility 60/84 Responsibility, accountability, liability, acting with integrity Privacy 47/84 Privacy, personal or private information Beneficence 41/84 Benefits, beneficence, well-being, peace, social good, common good Freedom & 34/84 Freedom, autonomy, consent, choice, self-determination, liberty, autonomy empowerment Trust 28/84 Trust Sustainability 14/84 Sustainability, environment (nature), energy, resources (energy) Dignity 13/84 Dignity Solidarity 6/84 Solidarity, social security, cohesion No single ethical principle appeared to be common to the entire corpus of documents, although there is an emerging convergence around the following principles: transparency, justice and fairness, non-maleficence, responsibility, and privacy. (extracted from205.pdf)
Justice, fairness, and equity Justice is mainly expressed in terms of fairness23,25,27–29,48,50,58,60,66,72–77, and of prevention, monitoring or mitigation of unwanted bias23,28,33,40,47,52,54,58,64,69,73,74,78–80 and discrimination28,33,36,38,44,45,50,55,56,60,68,81–84, the latter being significantly less referenced than the first two by the private sector. (extracted from205.pdf)
Whereas some sources focus on justice as respect for diversity31,38,56,59,65,66,70,72,78,80,85,86, inclusion31,45,47,51,72,80 and equality41,45,51,59,60,72,78, others call for a possibility to appeal or challenge decisions28,35–37,74,79, or the right to redress33,42,45,46,50,68,85 and remedy45,48. (extracted from205.pdf)
9 If specified, the preservation and promotion of justice are proposed to be pursued through: (a) technical solutions such as standards50,68,89 or explicit normative encoding28,37,43,67; (b) transparency54,62, notably by providing information36,38,79 and raising public awareness of existing rights and regulation28,59; (c) testing52,58,67,69, monitoring54,56 and auditing39,46,50,67, the preferred solution of notably data protection offices; (d) developing or strengthening the rule of law and the right to appeal, recourse, redress, or remedy37,38,42,45,46,48,68,74,79; (e) via systemic changes and processes such as governmental action42,45,87,92 and oversight94, a more interdisciplinary47,65,85,93 or otherwise diverse58,59,70,85,87,95 workforce, as well as better inclusion of civil society or other relevant stakeholders in an interactive manner28,33,41,46,55,57,58,65,68,69,79,80,86 and increased attention to the distribution of benefits25,33,38,48,63,76. (extracted from205.pdf)
Our analysis shows the emergence of an apparent cross-stakeholder convergence on promoting the ethical principles of transparency, justice, non-maleficence, responsibility, and privacy. (extracted from205.pdf)
Although no single ethical principle is explicitly endorsed by all existing guidelines, transparency, justice and fairness, non-maleficence, responsibility and privacy are each referenced in more than half of all guidelines. (extracted from205.pdf)
In particular, the prevalence of calls for transparency, justice and fairness points to an emerging moral priority to require transparent processes throughout the entire AI continuum (from transparency in the development and design of algorithms to transparent practices for AI use), and to caution the global community against the risk that AI might increase inequality if justice and fairness considerations are not adequately addressed. (extracted from205.pdf)
Both these themes appear to be intertwined with the theme of responsibility, as the promotion of both transparency and justice seems to postulate increased responsibility and accountability on the side of AI makers and deployers. (extracted from205.pdf)
Justice and Justification: Reflective Equilibrium in Theory and Practice. (extracted from205.pdf)
Associa- 26-Feb- self Manual in- ogy: European and Radiology; European tional tion/Society 2019 clusion North American Mul- Society of Radiology; tisociety Statement Radiology Society of North America; Soci- ety for Imaging Infor- matics in Medicine; European Society of Medical Imaging In- formatics; Canadian Association of Radiol- ogists; American As- sociation of Physicists in Medicine European ethical "The five principles Concil of Europe: Eu- EU IGO/supra-na- xx-Feb- multiple (public Manual in- Charter on the use of of the Ethical Char- ropean Commission tional 2019 and private clusion Artificial Intelligence ter on the Use of for the efficiency of stakeholders) in judicial systems and Artificial Intelli- Justice (CEPEJ) their environment gence in Judicial Systems and their environment" Ethically Aligned De- General Principles Institute of Electrical interna- Prof. (extracted from205.pdf)
Justice, Fairness & Equity VIII. (extracted from205.pdf)
Discrimination (duplicate in Justice&Fairness) VI. (extracted from205.pdf)
In this regard, many countries (13) mentioned to have at least one department, unit or dedicated team working on stimulating the For example, in The Netherlands, the uptake of AI in the public sector, researching the effects of AI or Ministry of Justice and Safety now has a preparing new AI-specific regulations. (extracted from239.pdf)
Of these 13 countries, 9 number of people working in the Justice reported to have more than one of these units. (extracted from239.pdf)
2020 37 As presented at the Data Justice Lab in 2019 on AI Realism and structural alternatives 49 With that purpose in mind, one should consider how existing data governance regimes and national regulatory practices can be transforming and not just intensifying existing power asymmetries. (extracted from239.pdf)
As an example, the strategy highlights the use of hackathons in the justice domain to develop AI solutions for concrete policy issues. (extracted from239.pdf)
A number of policy domains where the Dutch government is exploring the use of AI or will stimulate other actors to use AI in their fields are mentioned and are listed below:  The use of AI in the field of security and justice. (extracted from239.pdf)
A good starting point can be Article 2 of the Treaty on the European Union, which defines the values on which the Union is founded and are common to the Member States as ‘a society in which pluralism, non- discrimination, tolerance, justice, solidarity and equality between women and men prevail’51 An additional set of shared European values (rights and freedoms) is defined by the Charter of Fundamental Rights of the European Union52 and only apply in cases where Member States implement EU regulation directly or transpose it into national legislation. (extracted from239.pdf)
In addition, there are also preventive administrative measures, such as the decision of the Belgian police regulator to forbid piloting the use of face recognition technology at the Zaventem airport57 ; or the negative advice by the French data protection regulator regarding two pilots using facial recognition technology in French schools58; or the cease and desist letter issued by the French data protection regulator to the French Ministry of the Interior regarding the use of Automatic number plate recognition (ANPR) systems 59 54 http://www.sigmaweb.org/publications/principles-public-administration.htm 55 https://www.europarl.europa.eu/charter/pdf/text_en.pdf 56 The French Justice Reform Act, Article 33, https://www.legifrance.gouv.fr/affichTexteArticle.do;jsessionid=98B09D0394DAE57F1618DC21F30405F6.tplgfr34s_1?idArticle=JORFARTI 000038261761&categorieLien=id&cidTexte=JORFTEXT000038261631&dateTexte= 57 https://www.vrt.be/vrtnws/nl/2019/09/20/politie-mag-geen-automatische-gezichtsherkenning-gebruiken-op-de 58 https://www.cnil.fr/fr/experimentation-de-la-reconnaissance-faciale-dans-deux-lycees-la-cnil-precise-sa-position 59 https://www.cnil.fr/fr/radars-troncons-mise-en-demeure-du-ministere-de-linterieur 73 In our initial framework it is thus proposed to consider the multiple elements of AI in public services that can be grouped into macro-areas labelled as: Digital Infrastructure, Organisational Resources, Digital Government Development and Digital Society Development, as described in Figure 16 below. (extracted from239.pdf)
In 1890, future Supreme Court Justice Louis Brandeis took the first step in advocating for privacy protection when he co-authored an article with colleague Samuel Warren in the Harvard Law Review advocating “the right to be let alone.” The two argued that the development of “instantaneous photographs” and their circulation by newspapers for commercial gain had created the need to protect people with a new “right to privacy.” Technology today gives a new meaning to “instantaneous photographs” that Brandeis and Warren probably never imagined. (extracted from434.html)
United States , Chief Justice John Roberts wrote for a majority of the court that an individual has a “legitimate expectation of privacy in the record of his physical movements” that are recorded in these cell site records. (extracted from434.html)
REPORT with recommendations to the Commission on Civil Law Rules on Robotics | A8-0005/2017 | European Parliament Access to page content (press "Enter") Direct access to language menu (press "Enter") EN - English BG - български ES - español CS - čeština DA - dansk DE - Deutsch ET - eesti keel EL - ελληνικά EN - English FR - français GA - Gaeilge HR - hrvatski IT - italiano LV - latviešu valoda LT - lietuvių kalba HU - magyar MT - Malti NL - Nederlands PL - polski PT - português RO - română SK - slovenčina SL - slovenščina FI - suomi SV - svenska News Topics MEPs About Parliament Plenary Committees Delegations EU budget Other websites View other websites News Topics MEPs About Parliament Plenary Committees Delegations Multimedia Centre Presidency Secretariat-general Elections Think tank EP Newshub At your service Visits Legislative Observatory Legislative train Contracts and Grants Register Open Data Portal Liaison offices Report - A8-0005/2017 Report A8-0005/2017 European Parliament Download A-8-2017-0005_EN (PDF - 573 KB) A-8-2017-0005_EN (DOC - 118 KB) European Parliament REPORT with recommendations to the Commission on Civil Law Rules on Robotics 27.1.2017 - (2015/2103(INL)) Committee on Legal Affairs Rapporteur: Mady Delvaux (Initiative – Rule 46 of the Rules of Procedure) Rapporteurs for the opinions (*): Georg Mayer, Committee on Transport and Tourism Michał Boni, Committee on Civil Liberties, Justice and Home Affairs (*) Associated committees – Rule 54 of the Rules of Procedure Amendments 001-001 (PDF - 10 KB) 001-001 (DOC - 48 KB) 002-003/REV1 (PDF - 100 KB) 002-003/REV1 (DOC - 50 KB) 004-008 (PDF - 134 KB) 004-008 (DOC - 59 KB) 009-010/REV1 (PDF - 102 KB) 009-010/REV1 (DOC - 51 KB) 009-013 (PDF - 114 KB) 009-013 (DOC - 14 KB) 012-013/REV1 (PDF - 102 KB) 012-013/REV1 (DOC - 50 KB) Procedure : 2015/2103(INL) Document stages in plenary Document selected : A8-0005/2017 Texts tabled : A8-0005/2017 Debates : PV 15/02/2017 - 14 CRE 15/02/2017 - 14 Votes : PV 16/02/2017 - 6.9 Explanations of votes Texts adopted : P8_TA(2017)0051 MOTION FOR A EUROPEAN PARLIAMENT RESOLUTION ANNEX TO THE MOTION FOR A RESOLUTION:DETAILED RECOMMENDATIONS AS TO THE CONTENT OF THE PROPOSAL REQUESTED EXPLANATORY STATEMENT OPINION of the Committee on Transport and Tourism (*) OPINION of the Committee on Civil Liberties, Justice and Home Affairs (*) OPINION of the Committee on Employment and Social Affairs OPINION of the Committee on the Environment, Public Health and Food Safety OPINION of the Committee on Industry, Research and Energy OPINION of the Committee on the Internal Market and Consumer Protection RESULT OF FINAL VOTE IN COMMITTEE RESPONSIBLE MOTION FOR A EUROPEAN PARLIAMENT RESOLUTION with recommendations to the Commission on Civil Law Rules on Robotics ( 2015/2103(INL) ) The European Parliament , – having regard to Article 225 of the Treaty on the Functioning of the European Union, – having regard to the Product Liability Directive 85/374/EEC, – having regard to Rules 46 and 52 of its Rules of Procedure, – having regard to the report of the Committee on Legal Affairs and the opinions of the Committee on Transport and Tourism, the Committee on Civil Liberties, Justice and Home Affairs, the Committee on Employment and Social Affairs, the Committee on the Environment, Public Health and Food Safety, the Committee on Industry, Research and Energy and the Committee on the Internal Market and Consumer Protection (A8-0005/2017), Introduction A. (extracted from290.html)
Points out that the guiding ethical framework should be based on the principles of beneficence, non-maleficence, autonomy and justice, on the principles and values enshrined in Article 2 of the Treaty on European Union and in the Charter of Fundamental Rights, such as human dignity, equality, justice and equity, non-discrimination, informed consent, private and family life and data protection, as well as on other underlying principles and values of the Union law, such as non-stigmatisation, transparency, autonomy, individual responsibility and social responsibility, and on existing ethical practices and codes; 14. (extracted from290.html)
Researchers in the field of robotics should commit themselves to the highest ethical and professional conduct and abide by the following principles: Beneficence – robots should act in the best interests of humans; Non-maleficence – the doctrine of ‘first, do no harm’, whereby robots should not harm a human; Autonomy – the capacity to make an informed, un-coerced decision about the terms of interaction with robots; Justice – fair distribution of the benefits associated with robotics and affordability of homecare and healthcare robots in particular. (extracted from290.html)
LICENCE FOR DESIGNERS – You should take into account the European values of dignity, autonomy and self-determination, freedom and justice before, during and after the process of design, development and delivery of such technologies including the need not to harm, injure, deceive or exploit (vulnerable) users. (extracted from290.html)
OPINION of the Committee on Civil Liberties, Justice and Home Affairs (*) (23.11.2016) for the Committee on Legal Affairs with recommendations to the Commission on Civil Law Rules on Robotics ( 2015/2103(INL) ) Rapporteur: Michał Boni (Initiative – Rule 46 of the Rules of Procedure) (*) Associated committee – Rule 54 of the Rules of Procedure SUGGESTIONS The Committee on Civil Liberties, Justice and Home Affairs calls on the Committee on Legal Affairs, as the committee responsible: – to incorporate the following suggestions into its motion for a resolution: A. (extracted from290.html)
Whereas a number of third countries have adopted guidelines and legislation on robotics and some Member States have launched specific reflections in this area; whereas a regulatory framework that governs at Union level the development and the use of robotics and artificial intelligence and builds on existing rules such as the Union’s General Data Protection Regulation [1] could prevent a fragmentation of rules in the single market and further safeguard the protection of the fundamental rights of all EU citizens to human dignity, privacy and family life, the protection of personal data and intellectual property, freedom of expression and information, equality and non-discrimination, solidarity, and citizens’ rights and justice, as well as security and safety, while being subject to the principle of proportionality; Ethical principles 1. (extracted from290.html)
Believes that robotics and artificial intelligence, especially those with built-in autonomy, including the capability to independently extract, collect and share sensitive information with various stakeholders, and the possibility of self-learning or even evolving to self-modify, should be subject to robust conceptual laws or principles, such as that a robot may not kill or harm a human being and that it must obey and be controlled by a human being; that the process by which robots and artificial intelligence collect, use and process personal data must be transparent and comprehensible; believes that these principles should be technology neutral and based on empirical research; supports the development of an ethics-by-default framework for researchers, academia and engineers which ensures that these technological solutions will not hinder research and technological developments but will be in compliance with existing Union and national ethical practices and codes as well as with the rights and principles enshrined in the CFR, in particular human dignity, the respect for and protection of private and family life, security and safety, the protection of personal data, protection of intellectual property, the freedom of expression and information, equality and non-discrimination, solidarity, and citizens’ rights and justice, and should be subject to proportionality; 3. (extracted from290.html)
RECs and the Commission are encouraged to start a reflection in order to develop a code of conduct for researchers/designers and users of medical CPS, that should be based on the principles enshrined in the Union’s Charter of Fundamental Rights (such as human dignity and human rights, equality, justice and equity, benefit and harm, dignity, non-discrimination and non-stigmatisation, autonomy and individual responsibility, informed consent, privacy and social responsibility as well as the rights of the elderly, the integration of persons with disabilities, the right to healthcare, and the right to consumer protection) and on existing ethical practices and codes. (extracted from290.html)
Accept additional cookies Reject additional cookies View cookies Hide cookie message Skip to main content GOV.UK Navigation menu Menu Menu Services and information Benefits Births, death, marriages and care Business and self-employed Childcare and parenting Citizenship and living in the UK Crime, justice and the law Disabled people Driving and transport Education and learning Employing people Environment and countryside Housing and local services Money and tax Passports, travel and living abroad Visas and immigration Working, jobs and pensions Government activity Departments Departments, agencies and public bodies News News stories, speeches, letters and notices Guidance and regulation Detailed guidance, regulations and rules Research and statistics Reports, analysis and official statistics Policy papers and consultations Consultations and strategy Transparency Data, Freedom of Information releases and corporate reports Search GOV.UK × Search GOV.UK Search Search GOV.UK Search Home Defence and armed forces Ambitious, safe, responsible: our approach to the delivery of AI-enabled capability in Defence Ministry of Defence Policy paper Ambitious, safe, responsible: our approach to the delivery of AI-enabled capability in Defence Published 15 June 2022 This was published under the 2019 to 2022 Johnson Conservative government Contents Executive Summary Ambitious delivery of capability Our approach and AI-enabled weapons Key challenges to Defence AI Adoption Using AI Safely Using AI Legally Using AI Ethically Partnerships and Consultation Governance Implementation – building justified trust Annex A: Ethical Principles for AI in Defence Annex B: The Ministry of Defence AI Ethics Advisory Panel ANNEX C: Lethal Autonomous Weapon Systems (LAWS) Print this page © Crown copyright 2022 This publication is licensed under the terms of the Open Government Licence v3.0 except where otherwise stated. (extracted from382.html)
Cancel Services and information Benefits Births, death, marriages and care Business and self-employed Childcare and parenting Citizenship and living in the UK Crime, justice and the law Disabled people Driving and transport Education and learning Employing people Environment and countryside Housing and local services Money and tax Passports, travel and living abroad Visas and immigration Working, jobs and pensions Government activity Departments News Guidance and regulation Research and statistics Policy papers and consultations Transparency How government works Get involved Support links Help Privacy Cookies Accessibility statement Contact Terms and conditions Rhestr o Wasanaethau Cymraeg Government Digital Service All content is available under the Open Government Licence v3.0 , except where otherwise stated © Crown copyright (extracted from382.html)
Study on the use of innovative technologies in the justice field - Publications Office of the EU $InputBox.getData() Navigation Skip to Content Publications Office of the EU Study on the use of innovative technologies in the justice field DisplayCustomHeader DisplayLogo Publications Office of the European Union MyPortal Log in English Select your language Close Official EU languages: bg български es español cs čeština da dansk de Deutsch et eesti keel el Ελληνικά en English fr français ga Gaeilge hr hrvatski it italiano lv latviešu valoda lt lietuvių kalba hu magyar mt Malti nl Nederlands pl polski pt português ro română sk slovenčina sl slovenščina fi suomi sv svenska Other languages: tr trtrtrtr Publications Office of the European Union MainSearch Select All collections EU law EU publications EU official directory Web pages Summaries of EU legislation EU tenders Search More Advanced search Browse by subject Expert Search BasketSummary 0 Basket X Basket items X This item has been added. (extracted from228.html)
Publication Detail Portlet Publication detail Home EU publications Download Order Study on the use of innovative technologies in the justice field Final report Publication metadata In the White Paper on Artificial Intelligence (AI), the EU recognises the need to step up actions aiming to build an ecosystem of excellence supporting the development and acceptance of AI across the EU economy and public administration. (extracted from228.html)
The e-Justice Strategy and Action Plan 2019-2023 identify as priority areas the use of AI and blockchain/DLT in the justice field. (extracted from228.html)
In this context, the present study explores the existing policies and strategies at European and national level, as well as the state-of-play of the use of innovative technologies in justice. (extracted from228.html)
View more View less How to cite Citation style EU APA HARVARD VANCOUVER Export format RIS BibTeX EndNote Export Download and languages Close Available languages and formats Download X Available languages and formats English (en) pdf Publication details Related publications Published: 2020 Corporate author(s): Directorate-General for Justice and Consumers ( European Commission ) , Trasys International Themes: Law and justice , Information technology and telecommunications Subject: artificial intelligence , blockchain , data processing , electronic government , innovation , legal system , new technology , public administration , report , sample survey PDF ISSN ISBN 978-92-76-21347-5 DOI 10.2838/585101 Catalogue number DS-02-20-605-EN-N PDF ISBN 978-92-76-21347-5 DOI 10.2838/585101 Catalogue number DS-02-20-605-EN-N Released on EU publications website: 2020-09-14 View more View less Publication Viewer Document viewer Open web version in a separate window Your browser does not support IFrames. (extracted from228.html)
Fairness is discussed through the lens of social justice, highlighting the I STOA | Panel for the Future of Science and Technology potential for algorithmic systems to systematically disadvantage, or even discriminate against, different social groups and demographics. (extracted from159.pdf)
The second step included a review of the types and degrees of impact that algorithmic systems have on social justice, fair decision-making and the associated technological and societal need/limits for algorithmic literacy, transparency, oversight and information symmetry. (extracted from159.pdf)
We discuss fairness through the lens of social justice and highlight the potential for algorithmic systems to systematically disadvantage, or even discriminate against, different social groups and demographics. (extracted from159.pdf)
Therefore, we also understand fairness within the lens of social justice, as opposed to individual cases in which there is a perceived imbalance of goods or penalties ('Why did she get more cookies than me?), an uneven applications of a rule ('You let him throw the ball out of turn'), a case of discrimination based on irrelevant factors that are not subject to rights claims ('You didn’t pick me for the team even though I’m faster than the person you did pick'), etc. (extracted from159.pdf)
Social justice is another complex term with many potential definitions [38, 39, 40, 41, 42]. (extracted from159.pdf)
Discussions of social justice (in academic, policy and public discourses) typically recognise that ensuring a fair distribution is complicated by inherent inequalities in contemporary society; there are various differences of perspective over the extent to which a fair distribution should accommodate for, or attempt to address, such inequalities [44,45]. (extracted from159.pdf)
One high-profile campaigner is Joy Buolamwini, computer scientist at MIT and founder of the Algorithmic Justice League [54]. (extracted from159.pdf)
Algorithm based decision-making in the US criminal justice system In the early 2000s the US criminal justice system began using risk assessments to assist decision- making [91, 92]. (extracted from159.pdf)
In written evidence submitted to the UK government’s inquiry into Algorithms Used in decision-making [101], the Head of Criminal Justice at Durham Constabulary reported that it was too early to make conclusions about the accuracy of HART, but research into it is being conducted in order to support evidence based good practice, and that the results of this research would be made available. (extracted from159.pdf)
● Justice: where citizens feel that algorithms are biased or even discriminatory, this can compromise their feeling that they live in a just society. (extracted from159.pdf)
Each of these values is closely entwined with understandings of fairness and social justice. (extracted from159.pdf)
Various means to achieve fairness and social justice in algorithms have been suggested. (extracted from159.pdf)
[111] considered is the use of algorithmic decision making in the criminal justice system and they note the controversy surrounding the use of COMPAS in the US court system. (extracted from159.pdf)
As Rawls [42] describes, justice encompasses an overall acceptability that existing institutions generate mutual benefit and cooperation in society. (extracted from159.pdf)
An examples of (failed) accountability related co-regulation is the Safe-Harbour Principles for commercial data transfers between the USA and the EU, which was invalidated when the European Court of Justice (ECJ) ruled that the company self-certification practices under Safe- Harbour had failed to provide sufficient privacy safeguards for EU citizens [309, 310]. (extracted from159.pdf)
Accountability Measures for Algorithmic System use by Public Authorities Algorithmic systems are currently being used in government, reshaping how criminal justice systems work via risk assessment algorithms and predictive policing [412, 413], optimizing energy use in critical infrastructure through AI-driven resource allocation [414, 415] and changing government resource allocation and monitoring practices [412, 413]. (extracted from159.pdf)
It also revealed a lack of general knowledge about the systems among the authorities, leading to situations where the students had to explain what ‘criminal justice algorithms’ were to the public servants in charge of providing the records on their use. (extracted from159.pdf)
487], restrictions of due process in criminal justice proceedings [488] and more. (extracted from159.pdf)
The Times' investigation led to broad media coverage and a Department of Justice inquiry into potential criminal behaviour by the company [568]. (extracted from159.pdf)
Justice and care: Essential readings in feminist ethics. (extracted from159.pdf)
Sex and social justice. (extracted from159.pdf)
Unveiling the meaning of social justice in Colombia. (extracted from159.pdf)
Justice as fairness: A restatement. (extracted from159.pdf)
Defining social justice in a socially unjust world. (extracted from159.pdf)
Defining social justice. (extracted from159.pdf)
Value differences underlying public views about social justice. (extracted from159.pdf)
[46] Michael Walzer, Spheres of Justice, (NY: Basic Books, 1983) [47] Foster, A. (extracted from159.pdf)
[54] Algorithmic Justice League https://www.ajlunited.org/ Accessed on: 28 September 2018 [55] Puri, R. (extracted from159.pdf)
Algorithms in the Criminal Justice System: Assessing the Use of Risk Assessments in Sentencing. (extracted from159.pdf)
Criminal Justice and Behavior 31(3), 306–323. (extracted from159.pdf)
'Fairness in criminal justice risk assessments: the state of the art.' arXiv preprint arXiv:1703.09207 (2017). (extracted from159.pdf)
[416] Kade Crockford, 'Risk assessment tools in the criminal justice system: inaccurate, unfair, and unjust?,' ACLU of Massachusetts, March 8, 2018, https://privacysos.org/blog/risk-assessment-tools-criminal-justice- system-inaccurate-unfair-unjust [417] Virginia Eubanks, Automating Inequality: How High-Tech Tools Profile, Police, and Punish the Poor, (New York: St. (extracted from159.pdf)
Martin’s Press, 2018); Nazgol Ghandnoosh, Black Lives Matter: Eliminating Racial Inequity in the Criminal Justice System (Washington DC: The Sentencing Project, 2015), http://sentencingproject.org/wp- content/uploads/2015/11/Black-Lives-Matter.pdf [418] Insha Rahman, 'The State of Bail: A Breakthrough Year for Bail Reform,' Vera Institute of Justice, 2017, https://www.vera.org/state-of-justice-reform/2017/bail-pretrial [419] Dillon Reisman, Jason Schultz, Kate Crawford, Meredith Whittaker, 'Algorithmic Impact Assessments: A Practical Framework for Public Agency Accountability', AI Now, April 2018 https://ainowinstitute.org/aiareport2018.pdf [420] Ali Winston, 'Transparency Advocates Win Release of NYPD ‘Predictive Policing’ Documents,' The Intercept, Jan. (extracted from159.pdf)
29, 2017, https://www.axios.com/lawmakers-are-trying-to-understand-how-tech-giants-algorithms-work- 1513307255-b4109efc-9566-4e69-8922-f37d9e829f1f.html [441] Eric Holder, 'Speech at the National Association of Criminal Defense Lawyers 57th Annual Meeting and 13th State Criminal Justice Network Conference' (Philadelphia, PA, Aug. (extracted from159.pdf)
1, 2014), Department of Justice, https://www.justice.gov/opa/speech/attorney-general-eric-holder-speaks-national-association-criminal- defense-lawyers-57th 100 A governance framework for algorithmic accountability and transparency [442] John Fry, Anne Maxwell, Sarah Apere, Paddy McAweeney, Luke McSharry, and Ainhoa Gonza�lez, 'Non- Technical Summaries-Due Care and Attention,' In 34th IAIA Annual Conference, http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.567.8444&rep=rep1&type=pdf. (extracted from159.pdf)
Times, July 29, 2016, https://www.nytimes.com/roomfordebate/2014/08/06/is-big-data-spreading-inequality/big-data- should-be-regulated-by-technological-due-process [461] Wexler, 'Life, Liberty, and Trade Secrets'; Ram, 'Innovating Criminal Justice'. (extracted from159.pdf)
Department of Justice, Office of the Inspector General (2018), https://oig.justice.gov. (extracted from159.pdf)
[475] Rebecca Wexler, 'Life, Liberty, and Trade Secrets: Intellectual Property in the Criminal Justice System,' 70 Stan. (extracted from159.pdf)
Rev., (forthcoming 2018), https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2920883 [476] Natalie Ram, 'Innovating Criminal Justice,' Northwestern L. (extracted from159.pdf)
3, 2017 [568] Mike Isaac, 'Justice Department Expands Its Inquiry Into Uber’s Greyball Tool,' The New York Times, Mar. (extracted from159.pdf)
17 http://ec.europa.eu/justice/gender-equality/tools/statistics-indicators/platform- action/index_en.htm 15308/18 PL/mk 14 ANNEX II to ANNEX LIFE 1.C EN - Council Conclusions on Enhancing the Skills of Women and Men in the EU Labour Market (6889/17). (extracted from165.pdf)
- Opinion of the Advisory Committee on Equal Opportunities for Women and Men on how to overcome occupational segregation http://ec.europa.eu/justice/gender- equality/files/opinions_advisory_committee/151125_opinion_occ_segregation_en.pdf - Code of Conduct on Countering Illegal Hate Speech Online http://ec.europa.eu/newsroom/just/item-detail.cfm?item_id=54300 - 2018 Report on equality between women and men in the European Union" https://publications.europa.eu/en/publication-detail/-/publication/950dce57-6222-11e8- ab9c-01aa75ed71a1/language-en 15308/18 PL/mk 15 ANNEX II to ANNEX LIFE 1.C EN - Digital Skills and Jobs Coalition under the New Skills Agenda for Europe (COM(2016) 381 final, 10.6.2016) https://ec.europa.eu/digital-single-market/en/news/european-commission-invites- organisations-attract-more-girls-and-women-digital - Expert group set up under the European Union Work Plan for Youth for 2016-2018: "Developing digital youth work. (extracted from165.pdf)
The mission of the Data Science Initiative is to harness the value of data science and artificial intelligence for peace, justice and security. (extracted from617.pdf)
79 Hackathon for Peace, Justice and Security, The Hague, (2018, 2019) www.hackathonforgood.org. (extracted from617.pdf)
The Future of Digital Justice Boston Consulting Group (BCG) is a global management consulting firm and the world’s leading advisor on business strategy. (extracted from64.pdf)
The Future of Digital Justice Dirk Hartung, Florian Brunnader, Christian Veith, Philipp Plog, and Tim Wolters June 2022 AT A GLANCE Justice systems worldwide are under increasing pressure as legal complexity rises and more people rightfully demand access to justice. (extracted from64.pdf)
Digital justice is an immense chance to improve the lives of millions of people, but only if the technology is understood and implemented fast. (extracted from64.pdf)
Drivers of Digital Justice Globally, the digitalization of justice systems is just beginning and generally lagging behind the digital transformation of the rest of society. (extracted from64.pdf)
Status quo of Digital Justice Transformation The current state of digital justice can be understood using an adapted version of the three-layer framework previously developed for private legal technology: It differenti- ates between enabler, process support, and substantial law solutions. (extracted from64.pdf)
International Best Practices Austria, Canada, Singapore, and the United Kingdom are leading in justice digitalization. (extracted from64.pdf)
They demonstrate several key traits that help make for the successful digital transfor- mation of a justice system. (extracted from64.pdf)
Future of Digital Justice Digital justice enables faster, more efficient case management and the effective resolu- tion of legal conflicts, better working conditions within courts, and greater access to the legal system. (extracted from64.pdf)
2 2 TThhee FFuuttuurree ooff DDiiggiittaall JJuussttiiccee Drivers of Digital Justice The administration of justice is a core function of modern societies. (extracted from64.pdf)
Globally, justice systems exist at very different levels of maturity: According to the World Justice Project, nearly a quarter billion people live in conditions of extreme injustice such as slavery, lacking all fundamental rights. (extracted from64.pdf)
Another 1.5 billion people cannot obtain justice to resolve everyday legal issues, and an astonishing 4.5 billion people lack the legal tools to protect their assets or are incapable of accessing the public services to which they have a right. (extracted from64.pdf)
While some of these challenges are linked to poverty and lack of institutions, insuffi- cient access to justice persists in some of the richest, most developed countries. (extracted from64.pdf)
Many factors often simultaneously influence the availability and administration of justice, but one is at the center: legal complexity. (extracted from64.pdf)
In this report, we argue that this is pri- marily due to the lack of digitalization in our justice systems. (extracted from64.pdf)
Justice systems, intended to satisfy both those seeking and administering justice, instead leave each group dissatisfied. (extracted from64.pdf)
Therefore, this report is, first and foremost, a strong call to action regarding digital justice reform. (extracted from64.pdf)
Our research, however, also reveals grounds for optimism, including some coun- tries that are celebrating great improvements with digital justice systems. (extracted from64.pdf)
While not every aspect is universally transferable, the strategies employed by today’s leading nations can provide strategic guidance and specific blueprints to strengthen justice systems in less advanced nations. (extracted from64.pdf)
ABOUT THE STUDY The study, conducted by the Boston During the interviews, we gained in- Consulting Group, Bucerius Law School, sights on questions such as how courts and the German Legal Tech Association, successfully launch digital prototypes comprised nearly 50 in-depth interviews and procedures, which tools both judges with judges, clerks, and court IT manag- and parties find most helpful, what is ers, government officials, general coun- most challenging about establishing sels, partners and managers from large digital justice, how those challenges can law firms, insurance company owners best be overcome, and how these mea- and managers, board members of trade sures are intertwined with legislative associations, and academics research- actions and law reform. (extracted from64.pdf)
44 TThhee FFuuttuurree ooff DDiiggiittaall JJuussttiiccee A successful digital transformation of the justice system does not merely require the right set of technologies; it needs extensive change management and coordinated legislative reform. (extracted from64.pdf)
An enormous task in and of itself and not without demanding challenges and risks, digitalization currently presents our greatest chance to meaningfully and sustainably improve access to justice. (extracted from64.pdf)
Status Quo of Digital Justice Given the vastness of the term justice and the great diversity of justice systems world- wide, it is important to specify what we call digital justice for the purposes of this report. (extracted from64.pdf)
Justice is closely related to courts, which are its most important institutions. (extracted from64.pdf)
Courts were originally named after the location where justice was administered, but today that meaning is shifting. (extracted from64.pdf)
Digital justice can be understood as a process rather than a location or institution. (extracted from64.pdf)
Against the backdrop of an increasingly digital society, some aspects of justice systems seem archaic. (extracted from64.pdf)
As the judiciary is a traditionally monopolistic provider of justice, few of the larger trends of digitalization have taken place within it. (extracted from64.pdf)
As processes at traditional courts and experiences in everyday life diverge more and more, socie- tal friction between the justice system and its users increases. (extracted from64.pdf)
This results in fewer individuals and companies enforcing their rights in court, as evidenced by the dra- matic reduction of case numbers in some parts of the justice system in countries like Germany (-40% from 1995 to 2020 in civil proceedings). (extracted from64.pdf)
These developments in turn further deter those seeking justice from using the legal system. (extracted from64.pdf)
These include easy and permanent EXHIBIT 1 | Case Numbers and Duration of Proceedings in Germany First Instance Civil Cases, Germany Baseline 6 The Future of Digital Justice egnahC evitaleR +40% +20% -20% -40% 1995 2000 2005 2010 2015 2020 Duration Judgments Source: Geschäftsentwicklung der Zivilsachen, Bundesamt für Justiz, buceri.us/verfdauerlg 6 The Future of Digital Justice access to court records and automatization of recurring administrative tasks such as scheduling trials or monitoring deadlines. (extracted from64.pdf)
It has proven both flexible and popular in recent years and can be equally used in the justice technology space. (extracted from64.pdf)
These categories can be ordered by their specificity, from general-purpose to those specifically built for legal and justice use cases. (extracted from64.pdf)
Our research reveals dangerous gaps in connectivity and cybersecu- rity if justice institutions treat enabler technologies as an afterthought. (extracted from64.pdf)
Tools in the justice system are most numerous and advanced at the intermediate level: support process solutions. (extracted from64.pdf)
Within many justice systems, some form of electronic case management system has been implemented. (extracted from64.pdf)
8 The Future of Digital Justice Germany: A Case Study Germany is the largest economy in Europe and consistently ranked among the leading countries in the World Justice Project Rule of Law Index. (extracted from64.pdf)
The country’s civil justice system performs even better, currently bringing it to third place. (extracted from64.pdf)
Its lowest results concern accessibility and affordability of civil justice and freedom from unreasonable delay, which fall behind other categories such as freedom of improper government influence, corruption, or effective enforcement. (extracted from64.pdf)
With many of the traditional, straightforward solutions for access to justice, such as legal aid, already implemented and legal expense insurance widely popular, Germany has to look for new approaches to further improve its access to justice. (extracted from64.pdf)
So where does the German justice system currently stand? (extracted from64.pdf)
The digita- lization of the justice system is lagging 10–15 years behind leading countries, while case overloads, cost pressure, and an impending wave of retirement (over 25 percent of all judges will retire by 2030) increase the pressure to modernize and digitalize the courts. (extracted from64.pdf)
Under a recently formed government and initiatives from the Ministry of Justice, digitalization seems to be a new priority. (extracted from64.pdf)
In summary, digitalization presents an enormous chance for a meaningful and impactful justice reform in Germany. (extracted from64.pdf)
International Best Practices In addition to our case study of Germany laid out above, we have selected four focus countries for this report based on their reputations for digital justice: Austria, Canada, the United Kingdom, and Singapore. (extracted from64.pdf)
10 The Future of Digital Justice Singapore: A Global Leader Uniquely Positioned for Justice Digitalization Singapore is an island city-state with a population of nearly 5.5 million people and a surface area of roughly 730 square kilometers at the southern tip of the Malay Peninsula. (extracted from64.pdf)
Singapore handles around 380,000 court cases per year in a three-tier court system with the Supreme Court, the State Courts, and the Family Justice Courts. (extracted from64.pdf)
In total, the Singaporean justice system consists of 12 civil courts, eight criminal courts, and three appellate courts. (extracted from64.pdf)
Since the early 1980s, the digitalization of the judicial branch has been a key component of broader government strategies initiated by several prime ministers, driven by their ministers of justice and presidents of the Supreme Court, and executed by the Attorney-General’s Chambers, the Singapore judiciary, and the Singapore Academy of Law. (extracted from64.pdf)
As a result, Singapore has the most comprehensively digitalized justice system in the world and has established itself as a clear global leader. (extracted from64.pdf)
While its size, man- ageable surface area, and prosperity encourage digital justice, we find a set of strategic decisions to be at the center of this success. (extracted from64.pdf)
While the full extent of digital justice tools in Singapore would go beyond the scope of this report and can be found in the materials suggested for further read- ing, its most outstanding characteristic is its full integration. (extracted from64.pdf)
The current infrastructure enables and facili- tates the development of the justice technology of tomorrow as well. (extracted from64.pdf)
While Singapore was included in our focus countries due to its sheer breadth of digital justice projects, Canada is featured for one narrow yet impressive digital judicial endeavor. (extracted from64.pdf)
The Civil Resolution Tribunal (CRT) in British Columbia (BC) 12 The Future of Digital Justice might well be the most advanced online dispute resolution solution in the world. (extracted from64.pdf)
Digital innovation in the justice system in Canada is naturally not limited to the Civil Resolution Tribunal, as several Canadian jurisdictions operate video hear- ing, online filing, and case management systems including digital court records and mandatory electronic communications. (extracted from64.pdf)
The justice system is separated into ordinary (civil and criminal) and public (adminis- trative and constitutional) courts. (extracted from64.pdf)
Additionally, on the enabler solution level, Austria operates infrastructure for video and remote hearings and a platform for justice-specific e-learning offerings; it also provides access to vast numbers of court decisions online and free of charge via the legal information system. (extracted from64.pdf)
While not specifi- cally developed for the justice system but rather all forms of digital government, the application is at the core of the new justice portal. (extracted from64.pdf)
This has freed developers of the justice portal from developing their own login and user management infrastructure. (extracted from64.pdf)
It requires, however, some alignment between the justice portal and the existing solution. (extracted from64.pdf)
14 The Future of Digital Justice In its development of modern justice technology, Austria has benefited from a long tradition of relative openness toward digital solutions and successful use of digital infrastructure in the public domain. (extracted from64.pdf)
The Federal Ministry of Justice is seen as a reliable partner and accelerator for digitalization projects, and the relationship between the ministry, the courts, and stakeholder groups is often described as very productive. (extracted from64.pdf)
United Kingdom: The Most Ambitious Digital Justice Reform The United Kingdom is a parliamentary democracy and constitutional monarchy in northwestern Europe and the sixth largest economic power in the world. (extracted from64.pdf)
The public justice system consists of up to five levels of courts including the Supreme Court, the Court of Appeal, the High Court, Crown Court and County Courts, and Magistrate’s Court(s) and Tribunals. (extracted from64.pdf)
The reform is directed at improving access to justice and operational excellence in the entire court system, stretching from consumers and victims of crimes to families and commercial businesses. (extracted from64.pdf)
The reform is intended to transform the United Kingdom’s justice system into a user-centric, future-ready version of itself. (extracted from64.pdf)
The COVID-19 pandemic Boston Consulting Group • Bucerius Law School • Legal Tech Association 15 refocused some of the efforts in 2020, with remote hearings, paperless systems, and digital services becoming an even more important component of a function- ing justice system. (extracted from64.pdf)
HMCTS, an executive agency of the Ministry of Justice, is the single driving force behind this ambitious project. (extracted from64.pdf)
In addition, the reform is independently evaluated on a regular basis to ensure it actually improves access to justice (see our further reading suggestions for more operational details). (extracted from64.pdf)
The current reform is grounded in a general understanding of English law and the United Kingdom’s justice system as location factors and export goods and a geopolitical tool. (extracted from64.pdf)
Firmly rooted in its past as an early global commercial power, 16 The Future of Digital Justice the United Kingdom views a modern and functioning justice system not only as a societal achievement but as an instrument to attract businesses, increase legal services, and spur the economy in general. (extracted from64.pdf)
Lessons Learned Our international survey provides a number of learnings from a group of best-in- class nations in digital justice. (extracted from64.pdf)
First and foremost, the digitalization of justice systems is a monumental but ulti- mately very rewarding task. (extracted from64.pdf)
EXHIBIT 3 | Success Factors for Digital Transformation Five success factors are critical for the acceleration and successful implementation of digital justice Governance Legal framework Strategy Budget Buy-in Digital justice Supporting legal Holistic digitalization Sufficient budget Buy-in from governance with framework for strategy for digitalization relevant clear responsibility justice digitalization at top level initiatives stakeholders Source: Expert interviews BBoossttoonn CCoonnssuullttiinngg GGrroouupp •• BBuucceerriiuuss LLaaww SScchhooooll •• LLeeggaall TTeecchh AAssssoocciiaattiioonn 1177 Finally, most large-scale advances have taken years and sometimes decades to fully materialize. (extracted from64.pdf)
For digital justice reform, inclusion in a larger government-level strategy can be dispropor- tionately helpful, and particularly high impact is often linked to bold ambition from a passionate leader at the cabinet member or supreme court level. (extracted from64.pdf)
The Future of Digital Justice For digital justice systems to improve worldwide, we need to take two important steps. (extracted from64.pdf)
EXHIBIT 4 | Country Comparison A country comparison reveals major potential for improvement in Germany Layer Key solution 1 Online Dispute Resolution Substantive Law 3 2 Self-Service Solution Explorer Solutions 3 Justice Chatbot 4 Digital Case Management Support Process 2 5 Online Filing Solutions 6 Court Analytics 7 Video Hearings 8 Single Cloud Platform Enabler 1 9 Mobile Signature for Digital Justice Services 10 Mobile Infocomm Technology in Court Rooms Covered Partially covered Source: Expert interviews, BCG analysis & project experience 1188 TThhee FFuuttuurree ooff DDiiggiittaall JJuussttiiccee A Vision for a Future Justice System As the international best practices in our focus countries demonstrate, meaning- ful progress toward digital justice systems is possible and desirable, but getting there requires resources and perseverance. (extracted from64.pdf)
We have therefore asked our interview partners to describe their ideal version of the future: The justice system of the future is rooted in the fundamental values and achieve- ments of civilization that are the components of a modern understanding of the rule of law: Equality before the law, laws that are publicly disclosed, transparent, and applied proactively (never retroactively), consistent laws and processes, and an independent judiciary. (extracted from64.pdf)
In these regards, it mirrors current justice systems, but it also improves and repairs current systems that cannot handle increasing legal complexity (see above). (extracted from64.pdf)
On the contrary, EXHIBIT 5 | Fully Digital Online Courts Flow of future court proceedings Hearing Assessment Self-service Online ADR/ of case facts solution Online filing Enforcement Negotiation and legal explorer Evidence decision submission Source: Adapted from "Online Courts and the Future of Justice" by Richard Susskind 2019, Expert interviews BBoossttoonn CCoonnssuullttiinngg GGrroouupp •• BBuucceerriiuuss LLaaww SScchhooooll •• LLeeggaall TTeecchh AAssssoocciiaattiioonn 1199 appearing in court may become more similar to people’s day-to-day interactions, boosting trust as the justice system grows more and more approachable. (extracted from64.pdf)
EXHIBIT 6 | Path toward the Future of Justice for a leading country Path towards the future of justice for a country leading in digitization -10 years Today +5 years +10 years +15 years Current Exclusively position of digital countries courts in not leading the distant in digital future justice Digitalization of Optimization of Integration of single Introduction of new existing court processes through solutions to a digital facilities such processes through focus on user platform with one as virtual reality automation and centricity and data interface and seam- courts standardization analytics less E2E processes Fully offline Hybrid Fully digital Source: Expert interviews 2200 TThhee FFuuttuurree ooff DDiiggiittaall JJuussttiiccee If a party then wanted to initiate formal proceedings, an alternative dispute reso- lution or negotiation pre-stage could help sort out conflicts that could be settled without the court’s full attention. (extracted from64.pdf)
Finally, the exchange of information with those institutions of the justice system tasked with enforcement would be swift, so that neither time nor information would be lost. (extracted from64.pdf)
Based on near-real-time court analytics and by applying continuous, user-centric process improvement, a more flexible justice system could be adapted more easily and faster than today. (extracted from64.pdf)
How to Get There The above vision of a future justice system, shared in full or in part by many of our interview partners, currently does not even exist in the most advanced nations. (extracted from64.pdf)
As laid out above, the process is lengthy and requires a myriad of interconnected decisions specific to the individual justice system and therefore impossible to lay out in detail. (extracted from64.pdf)
Some steps are generalizable but in turn depend on the current status of a justice system. (extracted from64.pdf)
As the pressure to provide better access to justice grows with increasing legal complexity, costs rise and they risk playing catch-up with new technology and corresponding user expectations forever. (extracted from64.pdf)
FURTHER READING This report provides a very condensed • Shannon Salter and Darin Thomp- view of access to justice through digitali- son, Public-Centred Civil Justice Rede- zation and emphasizes its organization- sign: A Case Study of the British Colum- al and economic consequences. (extracted from64.pdf)
For bia Civil Resolution Tribunal, 2017, those interested in more details, we McGill Journal of Dispute Resolution recommend the following materials and publications: • Richard Susskind, Online Courts and the Future of Justice, 2019, OUP • Aedit Abdullah and Tan Ken Hwee, Practice of Law—Courts in Law and • Jason Tashea, Justice-as-a-Platform, Technology in Singapore, edited by 2021, MIT Computational Law Report Simon Chesterman, Goh Yihan, and Andrew Phang Boon Leong, 2021, Our own analysis of and experience in SAL Academy Publishing justice digitalization naturally goes beyond the content of this summary • Natalie Byrom, Digital Justice: report. (extracted from64.pdf)
Please do not hesitate to reach HMCTS Data Strategy and Delivering out to any of the institutions or authors Access to Justice, 2019, The Legal to learn more, discuss specific projects, Education Foundation or request presentations tailored to your particular audience. (extracted from64.pdf)
Pah et al., How to build a more open justice system, 2020, Science Magazine 2222 TThhee FFuuttuurree ooff DDiiggiittaall JJuussttiiccee Instead, these countries could benefit from the experience of global leaders and adapt their strategies. (extracted from64.pdf)
At the outset, this requires setting the ambitious goal of becoming a leader in digital justice followed in turn by leadership buy-in, ideally at the minister level or higher. (extracted from64.pdf)
Most importantly, data on the justice system is required, in as much detail as possible. (extracted from64.pdf)
This includes court statistics and further empirical research on users’ expectations of the justice system toward its institutions, including legal service providers, but it does not stop there: All legal information, such as statutes and regulations, decisions by courts, and administrative acts, need to be as easily available as possible. (extracted from64.pdf)
In the digital age, access to justice necessitates easy, unhin- dered access to information. (extracted from64.pdf)
Finally, the digitalization of the justice system requires comprehensive law reforms and a meaningful redesign of the legal framework. (extracted from64.pdf)
Naturally, the creation of new legislation and the amendment of existing laws is a strong suit of justice departments, so they might spur into action here first. (extracted from64.pdf)
In federal countries, the legislative process involves cooperation between the national and subnational governments, and bold, comprehensive justice reform can require extensive coordination. (extracted from64.pdf)
Justice digitalization, on the other hand, is in everyone’s interest as the increase in efficiency can help lower costs and reorient the discus- sions toward productivity. (extracted from64.pdf)
Building a digital justice system is an ambitious task and requires technical, legal, and management capabilities. (extracted from64.pdf)
CONCLUSION Pressure on justice systems in all stages of development is rising as the complexity of legal relationships, rules, and disputes increases in a world shaped by globalization and digitalization. (extracted from64.pdf)
This risks impairing access to justice for both consumers and companies. (extracted from64.pdf)
The most promising answer to these developments is the digitalization of justice systems. (extracted from64.pdf)
Administrations that accept this challenge can look to three sources for guidance: globally leading countries in justice digitalization, general government technology initiatives, and successful legal technology pro- viders in the private sector. (extracted from64.pdf)
As demonstrated in our international case studies, justice digitalization provides a huge chance for countries, their politicians, and their public administration. (extracted from64.pdf)
24 The Future of Digital Justice Boston Consulting Group • Bucerius Law School • Legal Tech Association 25 About the Authors Dirk Hartung is the founder and executive director of the Center for Legal Technology and Data Science at Bucerius Law School. (extracted from64.pdf)
26 The Future of Digital Justice For information or permission to reprint, please contact BCG at permissions@bcg.com. (extracted from64.pdf)
responsibleai.com/ justice, privacy, knowledge, democracy and responsibility. (extracted from58.pdf)
justice, public housing, welfare and health provision are examples of areas where “black box” systems should not be 2. (extracted from429.pdf)
29 In “Fairness in Criminal Justice Risk Assessments: The State of the to the design of the system based on reported issues? (extracted from429.pdf)
Berk et al, 2017, Fairness in Criminal Justice Risk Assessments: The State of the Art https://arxiv.org/ 30 Ai Now Institute 2017 Report abs/1703.09207 12 How to Prevent Discriminatory Outcomes in Machine Learning Bringing principles of non-discrimination Step 3: Being transparent about efforts to identify, to life: Human rights due diligence for prevent, and mitigate human rights risks machine learning For leadership, this step involves explicitly encouraging transparency. (extracted from429.pdf)
For instance, even if a machine's impact is only ever good, the distribution of that good (concerns for justice) might be in question”. (extracted from367.pdf)
öSÍ4öÂÀËëïxR2£b¡¿MP Ù1ÞcMTaÌKûm¿¼sI4i£õâPõ1O/by¹ÇQNï~<Ï¹t ,²Ëógøª"QZ >LµnSµ®J!Déÿ ¿· +úëMù¬¬¼*7ub%>ÒÒ§³Î~8 çBü4aÌ8=J O%àBpV'ãë3ë4ÿ«t³öÒù]Ò Ë`)Ú6 Z s§½*¢ÝtÂÇÌGµX¹KÇIóH¸<~YÇQ/ß £Y¹:¦Ë"ráør¤p ¨l & Ûüä£'MÈïÐ% G»'ip>'½ÈJ4dÅâùù]ôÙl¬1úoq2"# endstream endobj 235 0 obj <</Subtype/Link/Rect[ 54.45 444.57 365.15 454.92] /BS<</W 0>>/F 4/A<</Type/Action/S/URI/URI(http://civilrightsdocs.info/pdf/criminal-justice/Pretrial-Risk-Assessment-Full.pdf) >>/StructParent 126>> endobj 236 0 obj <</Subtype/Link/Rect[ 54.45 431.23 88.464 444.57] /BS<</W 0>>/F 4/A<</Type/Action/S/URI/URI(http://civilrightsdocs.info/pdf/criminal-justice/Pretrial-Risk-Assessment-Full.pdf) >>/StructParent 127>> endobj 237 0 obj <</Subtype/Link/Rect[ 54.45 400.18 365.15 410.53] /BS<</W 0>>/F 4/A<</Type/Action/S/URI/URI(https://www.theguardian.com/commentisfree/2016/jun/10/three-black-teenagers-google-racist-tweet) >>/StructParent 128>> endobj 238 0 obj <</Subtype/Link/Rect[ 54.45 386.83 178.01 400.18] /BS<</W 0>>/F 4/A<</Type/Action/S/URI/URI(https://www.theguardian.com/commentisfree/2016/jun/10/three-black-teenagers-google-racist-tweet) >>/StructParent 129>> endobj 239 0 obj <</Subtype/Link/Rect[ 54.45 295.04 237.54 308.39] /BS<</W 0>>/F 4/A<</Type/Action/S/URI/URI(https://www.ivir.nl/publicaties/download/1796) >>/StructParent 130>> endobj 240 0 obj <</Subtype/Link/Rect[ 76.972 263.99 365.15 274.34] /BS<</W 0>>/F 4/A<</Type/Action/S/URI/URI(https://www.propublica.org/article/facebook-ads-age-discrimination-targeting) >>/StructParent 131>> endobj 241 0 obj <</Subtype/Link/Rect[ 54.45 250.64 93.974 263.99] /BS<</W 0>>/F 4/A<</Type/Action/S/URI/URI(https://www.propublica.org/article/facebook-ads-age-discrimination-targeting) >>/StructParent 132>> endobj 242 0 obj <</Subtype/Link/Rect[ 54.45 219.59 365.15 229.94] /BS<</W 0>>/F 4/A<</Type/Action/S/URI/URI(https://www.propublica.org/article/facebook-advertising-discrimination-housing-race-sex-national-origin) >>/StructParent 133>> endobj 243 0 obj <</Subtype/Link/Rect[ 54.45 206.24 187.52 219.59] /BS<</W 0>>/F 4/A<</Type/Action/S/URI/URI(https://www.propublica.org/article/facebook-advertising-discrimination-housing-race-sex-national-origin) >>/StructParent 134>> endobj 244 0 obj <</Subtype/Link/Rect[ 76.972 175.2 365.15 185.54] /BS<</W 0>>/F 4/A<</Type/Action/S/URI/URI(https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing) >>/StructParent 135>> endobj 245 0 obj <</Subtype/Link/Rect[ 54.45 161.85 136.48 175.2] /BS<</W 0>>/F 4/A<</Type/Action/S/URI/URI(https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing) >>/StructParent 136>> endobj 246 0 obj <</Subtype/Link/Rect[ 54.45 130.8 365.15 141.15] /BS<</W 0>>/F 4/A<</Type/Action/S/URI/URI(https://www.propublica.org/article/asians-nearly-twice-as-likely-to-get-higher-price-from-princeton-review) >>/StructParent 137>> endobj 247 0 obj <</Subtype/Link/Rect[ 54.45 117.45 196.49 130.8] /BS<</W 0>>/F 4/A<</Type/Action/S/URI/URI(https://www.propublica.org/article/asians-nearly-twice-as-likely-to-get-higher-price-from-princeton-review) >>/StructParent 138>> endobj 248 0 obj <</Type/Page/Parent 2 0 R/Resources<</Font<</F1 5 0 R/F9 127 0 R>>/ExtGState<</GS7 7 0 R/GS8 8 0 R>>/ProcSet[/PDF/Text/ImageB/ImageC/ImageI] >>/Annots[ 250 0 R 251 0 R 252 0 R 253 0 R 254 0 R 255 0 R 256 0 R] /MediaBox[ 0 0 419.64 595.32] /Contents 249 0 R/Group<</Type/Group/S/Transparency/CS/DeviceRGB>>/Tabs/S/StructParents 139>> endobj 249 0 obj <</Filter/FlateDecode/Length 4654>> stream xÕ\KsÜ¸¾»ÊÿÇÔ"^R&%¬Ín²µ×© ì Æ²,©bkñìnöß 4HpèärYr@~|ýuCçããÝí±¹¸8¿< w·wï7çWûãqÿé ç¯û|wþrwÿø´;>îÎúùÝnÝì÷Ç»ÃfÓ\]o«×Ïß°fh;Ù¼þðük:û5ªoû^6\¶rh^²­¾ýI7÷_?ë{w5«o?kÎº¶ o^ß¾Yé~ýæõ÷Ï½°}CÿØe¯Z%ó.í]x ÉÚÿëù³¿Ú/^ü°mó0³ ¶ß]7]1\35V%º÷®ç7«ûõZ ãúL¬ àÇÝÚ¬Ïp÷g¸<|¶×{øôe}Ö»oÝf¿Ö« |ñêîÞ77«ë3¹ÚÁÍã±Õ#\º WÍ:¸dð£ çÐÎ}ÒÅzßbñVj2®cÛ±¿®úÍ´¿íÛ ÏâÒÞãðAl~ÉÔFÃczs&à÷ÿ _)µaÌ~¿§oÊì±9SðYæ[ûÿ ó×\ù7(eÿããvH½ÿ­³^å i¸ z)'_Û=fÚÝ5b­î¶M­­l9/Úþ¸V«[Øå£wvóî0à¿~´g¶çú FÒËôóïúVÅ½ç2¬¸¶Ëºõ«-ÃªÛe6Ùª­÷èEÆ®:W¹(îî¸zzÚ ca/Ãã XöRú^q j»T:®ïAðknK?Û¯É_Ãø02r'<DèÄJ$<Í.39æaD,TjKùU:ºÌ ÙDüRÄÅ üHDãÊé ¢Ó*¨Ë SùUZã¤¾K5A¾eC¬·«5ïW·ö¯^ò5ë¬nXÙ4ðãíúÆJì ï×Á°=yc÷Àqøl9!ØÒ´Ñ÷Ï 6_"ØràíÐÿßJ6¿ô=É¶R-_öÚükª ÂºÐ·$ß(J]W·j¡©ÔØÅIÉâ½hA¶Âû0',ï­`¸«_¬ës ÞÃm'M;çü¬{ÒÕ-¶rþòÝèSEªè[CÅ`uvÒ%¢ðhëyl5ÎÂvëéÝÆ!¶ ~¯ÓâÌ=â¢KÉæ µaÁÞ3íw; d$øá\ª­AÒ^¨ J¡(:¶O¹´ïu£v³P0ÙÞ£lFéf£Oädýï0IÆýKãuxÙ¬,¶v=oYÕØ©ªNö@BÛÎþ`Ñ "àrÖîæÎûux plßÖýªððÃ¬!4]«Ú¬ ì Z=P;¨¯@å÷Ûâ6´s¶Ì@÷6!U(²¸ÑI0­ õ¹áB¿M+®: %÷¶~£2Ãäuä­sA®_`, 2ÕÈçoé»q2è p2Ý´S_¢aYJ bóIC±øö«TC ­D¤öíJxáoþ"þ ¬ð.zþæÕ oïcìã ÞüÅ*kýë)Û:ª{ä.}#A_XÔ¿®ðnh¹¢cU¹HY¸j;NÅKÓ4ÿWÅNvÉJE þÜ&tçn³ÌH"2N*-Ôâg E"ð!NHn~K +I*ýÌQ Õc>©@¡ë(©ü&â?z?ýÌ-À =¦ùn|wjkFm_í¢à91ý£ìs;õVüÏ;°øîÏÃ já ­îãÎÃt ÓOjmí0}ò5 ó!çWûê|ºXé>5|@mzo~ho?üÓ~ß¸XÓS Ç£, rêø¨ ßÜøþ qÍ\àzx\~- qõ<-ñhß'+PÛ$c  ÑU{µÿÙå£ßñ,OlA'[BWn` ýv]3 BNz5 jñµHÆ#·ÓÝºÚè¢¸u½ÂT¢ãúÒT9®ªÀ 8óEÜÌ¸¾àD¢/Îl Ò¤§ñC´,£æö3ÁÁ=á¼q3 +1¶rn¶Ve6¯mÆsnõE ðàpuì ¯ÃtpÛvKHÜL.A¹ ûSJt]®U&®9. (extracted from140.html)
More View All Products Training Axon Academy Virtual learning platform for end-user product training VR Training Training for the reality of today Axon Training Innovative training technologies, content and peer networks Industries Law Enforcement Municipal, county, and state law enforcement agencies Federal Federal civilian and defense departments and agencies Corrections Prisons, probation and parole, and juvenile justice Fire Fire response and investigation EMS First Responders and emergency medical professionals Campus Campus law enforcement and school safety teams Justice Legal teams including DAs and prosecutors Healthcare Hospitals and other healthcare facilities Retail Retail trade, food and beverage, and other shopping establishments Private Security Guard services, venues and other on-site businesses Personal Safety TASER devices for personal protection and self-defense View All Industries Products Solutions Training Careers News Resources Support Products Product Bundles Smart Weapons Cameras Software View All Products Resources Resources Events Partners Products / Resources / Company Overview Leadership News Investor Careers Contact Industries Solutions Law Enforcement Federal Corrections Fire EMS Campus Justice Healthcare Retail Private Security Personal Safety Training Axon Academy VR Training Axon Training News & Resources Technology Axon Committed to Listening and Learning So That We can Fulfill our Mission to Protect Life, Together Jun 05, 2022 Rick Smith Axon CEO + Founder Jun 05, 2022 Axon was founded on our mission to protect life. (extracted from63.html)
Regarding these rights, we focus on issues relating to the right to respect private life, human dignity, ownership, safety and liability, freedom of expression and the prohibition of discrimination as well as access to justice and the right to a fair trial. (extracted from372.pdf)
This report analyses the safety of robots and artificial intelligence artefacts and the respective responsibilities of the designer, the operator and the user, as well as the consequences for human dignity, freedom of expression, ownership, the security of robots and artificial intelligence artefacts, discrimination and access to justice. (extracted from372.pdf)
39 3.8 Access to justice and the right to a fair trial .................................................................... (extracted from372.pdf)
The next three sections are dedicated to how automated decisions may affect freedom of expression, the prohibition of discrimination, access to justice and the right to a fair trial (sections 3.6, 3.7 and 3.8). (extracted from372.pdf)
Intelligent artefacts may challenge different dimensions of human rights, for example in terms of freedoms (privacy and data protection, ownership, autonomy, personality), equality (more specifically, non-discrimination) and justice (fair trial, access to justice). (extracted from372.pdf)
Thus, the impact of intelligent artefacts is first discussed in the context of seven existing human rights: the right to respect for private and family life (3.2), the right to human dignity (3.3), the right to the peaceful enjoyment of possessions (3.4), tort rights and safety (3.5), the right to freedom of expression as well as the freedom of thought, conscience and religion (3.6), the prohibition of discrimination (3.7) and access to justice as well as the right to a fair trial (3.8). (extracted from372.pdf)
18 Court of Justice of the European Union 19 October 2016, C-582/14 (Breyer) and Court of Justice of the European Union 24 November 2011, C-70/10 (Scarlet/SABAM), paragraph 51. (extracted from372.pdf)
For instance, on an international level, both the European Court of Human Rights and the Court of Justice of the European Union have voiced their concerns on numerous occasions regarding state surveillance activities and the impact on privacy rights.25 For 21 The Dutch Data Protection Authority imposed penalty payments on a company that could not demonstrate that Wi-Fi tracking in public spaces was necessary for a legitimate purpose. (extracted from372.pdf)
21 instance when the EU Court of Justice declared the Data Retention Directive to be invalid.26 On a national level, the British Investigatory Powers Tribunal, for instance, ruled that during a time span of more than a decade, the British intelligence agencies illegally collected data about innocent citizens and illegally tracked their phone and web use.27 With regard to surveillance enacted by companies, the European Commission proposed the reform of EU data protection regulations, which ultimately led to the General Data Protection Regulation. (extracted from372.pdf)
Court of Justice of the European Union 8 April 2014, Joined Cases C-293/12 and C-594/12 (Digital Rights Ireland and Seitlinger and Others). (extracted from372.pdf)
See also: Court of Justice of the European Union 21 December 2016, Joined Cases C-203/15 (Tele2 Sverige AB v Post-och telestyrelsen) and C-698/15 (Secretary of State for the Home Department v Tom Watson and Others). (extracted from372.pdf)
28 For an overview of (a selection of) Court of Justice of the European Union and European Court of Human Rights cases, see Annex I of Article 29 Data Protection Working Party, Working Document 01/2016 on the justification of interferences with the fundamental rights to privacy and data protection through surveillance measures when transferring personal data (European Essential Guarantees), 13 April 2016. (extracted from372.pdf)
In addition, with regard to decisions of the Court of Justice of the European Union concerning data protection, see: L. (extracted from372.pdf)
These letters are available at: http://ec.europa.eu/justice/data-protection/article-29/documentation/other-document/index_en.htm. (extracted from372.pdf)
42 3.8 Access to justice and the right to a fair trial Access to justice and the right to a fair trial Article 6(2) ECHR plays an important role with regard to predictive AI. (extracted from372.pdf)
The increased use of risk-assessing algorithms in the American justice system raises accountability and transparency issues.100 It has been reported that software used to set bail, conditions for parole and sentencing decisions is biased against African Americans (Angwin et al. (extracted from372.pdf)
To give another example, in response to worries by consumers about Wi-Fi tracking by shop owners, the former Dutch minister of economic affairs and the state secretary of security and justice stated that people should just turn off their smartphone if they do not want to be tracked.102 On the basis of this response, it seems that tracking and tracing people is a right which is deemed more important than peoples’ (privacy) rights. (extracted from372.pdf)
Besides affecting the right to respect for private life in numerous ways, digitisation, virtualisation and robotisation influence human dignity, the right to the peaceful enjoyment of possessions, safety and tort rights, the right to freedom of expression, the prohibition of discrimination, access to justice and the right to a fair trial. (extracted from372.pdf)
Access to justice and the right to a fair trial (see subsection 3.8) The Council of Europe could establish a framework of minimum norms to be taken into account when a court uses AI. (extracted from372.pdf)
(2007) E-Justice, beginselen van behoorlijke elektronische rechtspraak, The Hague: SDu Uitgevers. (extracted from372.pdf)
HOUSE OF LORDS Justice and Home Affairs Committee 1st Report of Session 2021–22 Technology rules? (extracted from366.pdf)
The advent of new technologies in the justice system Ordered to be printed 21 March 2022 and published 30 March 2022 Published by the Authority of the House of Lords HL Paper 180 Justice and Home Affairs Committee The Justice and Home Affairs Committee was appointed by the House of Lords on 14 April 2021 to consider justice and home affairs, including the domestic criminal justice system, and international cooperation in respect of criminal justice, civil justice, migration and asylum. (extracted from366.pdf)
Membership The Members of the Justice and Home Affairs Committee are: Lord Blunkett Baroness Kennedy of The Shaws Baroness Chakrabarti Baroness Pidding Lord Dholakia Baroness Primarolo Baroness Hallett Lord Ricketts Baroness Hamwee (Chair) Baroness Sanderson of Welton Lord Hunt of Wirral Baroness Shackleton of Belgravia Declaration of interests See Appendix 1. (extracted from366.pdf)
A full list of Members’ interests can be found in the Register of Lords’ Interests: http://www.parliament.uk/hlregister Publications All publications of the Committee are available at: http://www.parliament.uk/ 519/justice-and-home-affairs-committee/ Parliament Live Live coverage of debates and public sessions of the Committee’s meetings are available at: http://www.parliamentlive.tv Further information Further information about the House of Lords and its Committees, including guidance to witnesses, details of current inquiries and forthcoming meetings is available at: http://www.parliament.uk/business/lords Committee staff The staff who worked on this inquiry were Sam Kenny (Clerk), Achille Versaevel (Policy Analyst) and Amanda McGrath (Committee Operations Officer). (extracted from366.pdf)
Contact details General correspondence should be addressed to the Clerk of the Justice and Home Affairs Committee, Committee Office, House of Lords, London SW1A 0PW. (extracted from366.pdf)
THE ADVEnT OF nEW TECHnOLOGIES In THE JUSTICE SYSTEM 3 SUMMARY In recent years, and without many of us realising it, Artificial Intelligence has begun to permeate every aspect of our personal and professional lives. (extracted from366.pdf)
Our Committee has limited its investigation to only one area–how these advanced technologies are used in our justice system. (extracted from366.pdf)
Algorithms are being used to improve crime detection, aid the security categorisation of prisoners, streamline entry clearance processes at our borders and generate new insights that feed into the entire criminal justice pipeline. (extracted from366.pdf)
When deployed within the justice system, AI technologies have serious implications for a person’s human rights and civil liberties. (extracted from366.pdf)
Without transparency, there can not only be no scrutiny, but no 4 THE ADVEnT OF nEW TECHnOLOGIES In THE JUSTICE SYSTEM accountability for when things go wrong. (extracted from366.pdf)
Proper trials methodology is fully embedded into medical science but there are no minimum scientific or ethical standards that an AI tool must meet before it can be used in the criminal justice sphere. (extracted from366.pdf)
THE ADVEnT OF nEW TECHnOLOGIES In THE JUSTICE SYSTEM 5 Yet without sufficient safeguards, supervision, and caution, advanced technologies may have a chilling effect on a range of human rights, undermine the fairness of trials, weaken the rule of law, further exacerbate existing inequalities, and fail to produce the promised effectiveness and efficiency gains. (extracted from366.pdf)
The advent of new technologies in the justice system CHAPTER 1: INTRODUCTION 1. (extracted from366.pdf)
Within the application of the law, we included a broad view of the justice system, examining instances where advanced tools were used to discover, deter, rehabilitate, or punish people who breach the law in England and Wales, as well as border management. (extracted from366.pdf)
8 THE ADVEnT OF nEW TECHnOLOGIES In THE JUSTICE SYSTEM Automated decision making (ADM): ADM is the process of making a decision by automated means without any human involvement. (extracted from366.pdf)
Written evidence from Dr Miri Zilka, Dr Adrian Weller and Detective Sergeant Laurence Cartwright laid out some categories of tools used in the justice system. (extracted from366.pdf)
THE ADVEnT OF nEW TECHnOLOGIES In THE JUSTICE SYSTEM 9 (b) Data analysis: software and tools primarily used to analyse data to create insights. (extracted from366.pdf)
We heard the most about tools used by the Home Office, the Ministry of Justice, HM Prisons and Probation Service, and individual police forces. (extracted from366.pdf)
We were told, for example, about the use of polygraphs to monitor sex offenders on parole and manage their level of compliance with parole conditions.14 8 Written evidence from Dr Miri Zilka, Detective Sergeant Laurence Cartwright and Dr Adrian Weller (nTL0040) 9 Written evidence from Association of Police and Crime Commissioners, national Police Chiefs’ Council and Police Digital Service (nTL0049) 10 Written evidence from Avon and Somerset Police (nTL0052) 11 Written evidence from the Serious Fraud Office (nTL0034) 12 Written evidence from Public Law Project (nTL0046) 13 Written evidence from Liberty (nTL0020) 14 Written evidence from Dr Kyriakos n Kotsoglou (nTL0007) 10 THE ADVEnT OF nEW TECHnOLOGIES In THE JUSTICE SYSTEM 6. (extracted from366.pdf)
THE ADVEnT OF nEW TECHnOLOGIES In THE JUSTICE SYSTEM 11 the end of which it would become costly for the customer to opt out. (extracted from366.pdf)
Avon and Somerset Constabulary thought their use of data analytics placed “better insights into the hands of those delivering the business to help empower and support more effective decision making.”23 The Rt Hon Kit Malthouse MP, the Minister for Crime and Policing at the Home Office and Ministry of Justice, told us that he was “very excited about the use of artificial intelligence and machine learning in policing.”24 We also acknowledge that, as many submissions pointed out, advanced tools can provide substantial assistance towards enacting the crucial duties of the police to protect and prevent harm. (extracted from366.pdf)
Matthew Gill, Senior Fellow at the Institute for Government, facilitated a seminar for us to consider the institutional and regulatory frameworks which 22 Q 45 (Professor Elizabeth Joh) 23 Written evidence from Avon and Somerset Police (nTL0052) 24 Q 99 (Kit Malthouse MP) 25 Q 39 (Professor Elizabeth Joh) 26 Written evidence from SAS UK&I (nTL0041) 27 Written evidence from the Information Commissioner’s Office (nTL0016) 12 THE ADVEnT OF nEW TECHnOLOGIES In THE JUSTICE SYSTEM may be put in place. (extracted from366.pdf)
• In 2020, the Scottish Parliament Justice Sub-Committee on Policing published Facial recognition: how policing in Scotland makes use of this technology.30 • In 2021, the European Parliament Committee on Civil Liberties, Justice and Home Affairs published a report on “artificial intelligence in criminal law and its use by the police and judicial authorities in criminal matters”.31 • In 2021, nATO adopted its first Artificial Intelligence Strategy, including principles of the responsible use of AI in Defence and announcing further work to set international AI standards.32 • In 2021, UnESCO adopted a Recommendation on the Ethics of Artificial Intelligence and is working towards establishing the first-ever global normative instrument on the ethics of AI.33 15. (extracted from366.pdf)
We decided to examine the use of these tools throughout the “criminal justice pipeline”34 and in border management, identifying where change was needed, and identifying some principles for the safe and ethical use of such tools. (extracted from366.pdf)
(Report of Session 2017–19, HL Paper 100) 29 Council of Europe, CAHAI Ad Hoc Committee on Artificial Intelligence, ‘Terms of Reference’: https://www.coe.int/en/web/artificial-intelligence/cahai [accessed 6 February 2022] 30 The Scottish Parliament, Justice Sub-Committee on Policing, Facial Recognition: How Policing in Scotland Makes Use of This Technology (1st Report, Session 5, SP Paper 678) 31 Committee on Civil Liberties, Justice and Home Affairs, Report on artificial intelligence in criminal law and its use by the police and judicial authorities in criminal matters (13 July 2021): https://www.europarl. (extracted from366.pdf)
org/ark:/48223/pf0000380455 [accessed 6 February 2022] 34 Written evidence from Dr Miri Zilka, Detective Sergeant Laurence Cartwright and Dr Adrian Weller (nTL0040) THE ADVEnT OF nEW TECHnOLOGIES In THE JUSTICE SYSTEM 13 Chapter 3, we look at transparency: its necessity and proposals to increase it. (extracted from366.pdf)
The key issues we have identified, however, hold true for a much wider context: their application to all functions of the justice system and to border management. (extracted from366.pdf)
14 THE ADVEnT OF nEW TECHnOLOGIES In THE JUSTICE SYSTEM CHAPTER 2: LEGAL AND INSTITUTIONAL FRAMEWORKS 18. (extracted from366.pdf)
It was also indicated that “public failures” could “lead to not just operational defects or inefficiencies, but miscarriages of justice”,42 and that where weaknesses were exposed, they “exacerbate the low level and negative trend in public trust for relevant technology”.43 Professor nigel Harvey and Tobias Harvey referred to accountability for errors and misuse, saying that the use of algorithms “may leave people open to dangers for which no person can be identified as responsible”.44 “A chilling effect”45 22. (extracted from366.pdf)
Various contributors told us that the use of some technologies, notably the use of live facial recognition, created fear or disquiet, and that this risked 35 Written evidence from the Home Office (nTL0055) 36 QQ 103–104 (Kit Malthouse MP) 37 Royal Court of Justice, R v The Chief Constable of South Wales Police, [2020] EWCA Civ 1058. (extracted from366.pdf)
(nTL0022) 43 Written evidence from Archie Drake and Perry Keller (nTL0011) 44 Written evidence from Professor nigel Harvey and Tobias Harvey (nTL0025) 45 Written evidence from Dr Joe Purshouse, Dr nessa Lynch, Dr Marcin Betkier and Professor Liz Campbell (nTL0021) THE ADVEnT OF nEW TECHnOLOGIES In THE JUSTICE SYSTEM 15 damaging the democratic process. (extracted from366.pdf)
16 THE ADVEnT OF nEW TECHnOLOGIES In THE JUSTICE SYSTEM The right to a fair trial 23. (extracted from366.pdf)
We were concerned that, in some instances, the use of advanced tools at certain points of the criminal justice pipeline may impede an individual’s right to a fair trial: whether by a lack of awareness that they were being used, unreliable evidence, or an inability to understand and therefore challenge proceedings. (extracted from366.pdf)
Kotsoglou (nTL0006) 55 Written evidence from Big Brother Watch (nTL0037) THE ADVEnT OF nEW TECHnOLOGIES In THE JUSTICE SYSTEM 17 discrimination”,56 and Professor nigel Harvey and Tobias Harvey wrote that “learning algorithms based on historical data would preserve bias”.57 28. (extracted from366.pdf)
56 Written evidence from Liberty (nTL0020) 57 Written evidence from Professor nigel Harvey and Tobias Harvey (nTL0025) 58 Q 60 (Professor Karen Yeung) 59 Written evidence from Liberty (nTL0020) 18 THE ADVEnT OF nEW TECHnOLOGIES In THE JUSTICE SYSTEM Figure 1: Predictive policing—a vicious circle? (extracted from366.pdf)
We have noted over 30 public bodies, initiatives, and programmes playing a role in the governance of new technologies for the application of the law 60 Q 99 (Kit Malthouse MP) THE ADVEnT OF nEW TECHnOLOGIES In THE JUSTICE SYSTEM 19 (see Box 4). (extracted from366.pdf)
Its three key areas of work are “to pilot new forms of data stewardship and governance”; to increase assurance practices around AI; and, to assist public sector bodies looking to procure technologies— ”facilitating the delivery of transformative data and AI projects in the public sector”.63 • The AI Council is an independent advisory committee which “works to support the growth of AI in the UK” and aims to increase skills, “work on public perception”, and “[explore] how to develop and deploy safe, fair, legal and ethical data sharing frameworks”.64 61 Office for Artificial Intelligence, ‘About us’: https://www.gov.uk/government/organisations/office-for- artificial-intelligence/about [accessed 6 February 2022] 62 Department for Digital, Culture, Media and Sport, National AI Strategy (September 2021): https:// assets.publishing.service.gov.uk/government/uploads/system/uploads/attachment_data/file/1020402 [accessed 24 February 2022] 63 Centre for Data Ethics and Innovation, ‘About us’: https://www.gov.uk/government/organisations/ centre-for-data-ethics-and-innovation/about [accessed 6 February 2022] 64 HM Government, AI Council: https://www.gov.uk/government/groups/ai-council [accessed 6 February 2022] 20 THE ADVEnT OF nEW TECHnOLOGIES In THE JUSTICE SYSTEM Box 4: List of entities and programmes • Her Majesty’s Inspectorate of Constabulary and Fire and Rescue Services • The AI Council • The Association of Police and Crime Commissioners (APCC), and its various working groups and initiatives, including the APCC Biometrics and Data Ethics Working Group • The Biometrics and Forensics Ethics Group • The Biometrics and Surveillance Camera Commissioner • The Centre for Data Ethics and Innovation • The College of Policing • The Data Analytics Community of Practice • The Equalities and Human Rights Commission • The Forensic Science Regulator • The Home Office Digital, Data and Technology function • The Independent Office for Police Conduct • The Information Commissioner’s Office • The national Crime Agency, and its TRACER programme • The national Data Analytics Solution • The national Digital and Data Ethics Guidance Group • The national Digital Exploitation Centre • The national Police Chiefs’ Council, and its eleven co-ordination committees, each responsible for a specific aspect related to new technologies • The national Police Ethics Group • The national Policing Chief Scientific Adviser • The Office for AI • The Police Digital Service, its Data Office and Chief Data Officer • The Police Rewired initiative • The Police Science, Technology, Analysis and Research (STAR) fund • The Police, Science, and Technology Investment Board • The Royal Statistical Society • The Science Advisory Council to the national Policing Chief Scientific Adviser • The Senior Data Governance Panel within the Ministry of Justice • The specialist and generalist ethics committees of some police forces • The Tackling Organised Exploitation programme THE ADVEnT OF nEW TECHnOLOGIES In THE JUSTICE SYSTEM 21 33. (extracted from366.pdf)
65 Q 98 (Professor Paul Taylor) 66 Written evidence from Association of Police and Crime Commissioners, national Police Chiefs’ Council and Police Digital Service (nTL0049) 67 Department for Digital, Culture, Media & Sport, Data: A new direction (10 September 2021), para 409: https://assets.publishing.service.gov.uk/government/uploads/system/uploads/attachment_data/ file/1022315/Data_Reform_Consultation_Document__Accessible_.pdf [accessed 28 January 2022] 68 Q 99 (Kit Malthouse MP) 69 Q 108 (Kit Malthouse MP) 22 THE ADVEnT OF nEW TECHnOLOGIES In THE JUSTICE SYSTEM Figure 2: “Family tree” of relevant governance arrangements Decision Local accountability ensures contextualised and timely decisions making Public SLT, ethics Police and Crime Public consultation committee, Chief Constable Commissioner technic e a t l c e . (extracted from366.pdf)
Dr Christopher Lawless referred to a series of bodies that all play “key roles in oversight” but which “vary in their remit and the extent of their powers”.70 Robin Allen QC and Dee Masters believed that “there has been too much thinking in ‘silos’”.71 There may also be confusion over responsibilities—Dr Lawless gave the example of facial recognition technology 70 Written evidence from Dr Christopher Lawless (nTL0029) 71 Written evidence from Robin Allen QC and Dee Masters (nTL0019) THE ADVEnT OF nEW TECHnOLOGIES In THE JUSTICE SYSTEM 23 to argue that there is a “potentially significant lacuna [in governance] where it is unclear who is statutorily responsible for regulation and oversight”.72 36. (extracted from366.pdf)
79 Q 99 (Kit Malthouse MP), see also Home Office, New Biometrics and Surveillance Camera Commissioner appointed (15 March 2021): https://www.gov.uk/government/news/new-biometrics-and-surveillance- camera-commissioner-appointed [accessed 28 January 2022] 80 Q 84 (Professor Paul Taylor) 24 THE ADVEnT OF nEW TECHnOLOGIES In THE JUSTICE SYSTEM • In September 2021, the Government published its National Artificial Intelligence Strategy. (extracted from366.pdf)
While the Minister said that, as a Minister in both the Home Office and the Ministry of Justice, he was the “living embodiment” of cross- departmental working, this does not appear to have impacted strategic 81 Department for Business, Energy & Industrial Strategy, ‘Guidance national AI Strategy’ (22 September 2021): https://www.gov.uk/government/publications/national-ai-strategy/national-ai- strategy-html-version [accessed 1 February 2022] 82 Department for Digital, Culture, Media & Sport, Data: A new direction, para 409. (extracted from366.pdf)
83 DCMS Consultation: ‘Data: A new direction’ Response by the Biometrics and Surveillance Camera Commissioner: https://assets.publishing.service.gov.uk/government/uploads/system/uploads/ attachment_data/file/1030248/BSCC_DCMS_Consultation_Response.pdf [accessed 1 February 2022] 84 Written evidence from Association of Police and Crime Commissioners, national Police Chiefs’ Council and Police Digital Service (nTL0049) 85 HL Deb, 3 november 2021, cols 1301–1305 THE ADVEnT OF nEW TECHnOLOGIES In THE JUSTICE SYSTEM 25 thinking on the use of new technologies in applying the law, and there is no indication of any collective governmental effort towards a single strategy.86 On the contrary, there are many indications of siloed thinking. (extracted from366.pdf)
Similarly, BEIS and DCMS are collectively responsible for the implementation of the National AI Strategy, which focuses on businesses and the benefits of innovation and does not appear to have considered the needs of the Ministry of Justice or the Home Office at any length, or AI’s potential in their sectors. (extracted from366.pdf)
David Tucker, Faculty Lead on Crime and Criminal Justice at the College of Policing, told us: “We have seen that where decisions are challenged or doubted cases go to court and affect the way policing operates. (extracted from366.pdf)
The Appeal Court said that there was an absence of policy, so we are filling that gap and moving to apply these principles to this piece of technology”.87 86 Q 100 (Kit Malthouse MP) 87 Q 89 (David Tucker) 26 THE ADVEnT OF nEW TECHnOLOGIES In THE JUSTICE SYSTEM Box 5: The Bridges case and the Public Sector Equality Duty 1. (extracted from366.pdf)
88 Ministry of Justice, Public sector equality duty (6 July 2021): https://www.gov.uk/government/ publications/public-sector-equality-duty [accessed 4 February 2022] 89 Royal Court of Justice, R v The Chief Constable of South Wales Police, [2020] EWCA Civ 1058 90 Q 110 (Kit Malthouse MP) 91 Q 92 (Alun Michael) THE ADVEnT OF nEW TECHnOLOGIES In THE JUSTICE SYSTEM 27 We have also been told that both domestic courts and the European Court of Human Rights had been relied upon in the past.92 49. (extracted from366.pdf)
(nTL0022) 93 Written evidence from nCC Group (nTL0005) 94 Written evidence from the Bar Council (nTL0048) 95 Written evidence from nCC Group (nTL0005) 96 Written evidence from Archie Drake and Perry Keller (nTL0011) 97 Written evidence from the Home Office (nTL0055) 98 Q 73 (David Lewis) 99 Written evidence from nCC Group (nTL0005) 100 Q 36 (Dr David Leslie) 28 THE ADVEnT OF nEW TECHnOLOGIES In THE JUSTICE SYSTEM of the EU Commission’s proposed AI regulation” (see Box 6). (extracted from366.pdf)
Robin Allen QC and Dee Masters argued that “public actors and private companies need clear, pragmatic and effective regulatory frameworks because it provides a safety net within which ‘good’ AI can be developed whilst also protecting the fundamental rights of the public.”104 Archie Drake and Perry Keller had a similar view, stating that “legal uncertainty tends to harm business and innovation as well as public trust in the criminal justice system (and technology).”105 In a joint submission, three police bodies wrote that “Government should seek to clarify public appetite for new technologies and legislate so that policing has a clearer basis on which to make policies and decisions about deployment.”106 55. (extracted from366.pdf)
Professor Sandra Wachter, Associate Professor at the University of Oxford, thought that “soft regulation would be irresponsible” because the criminal justice system is “one of the most high- risk areas [she] can think of”.107 Dr Joe Purshouse and his co-contributors reflected that while guidance documents may be cited in court, they “do not provide actionable grounds for an individual to make a complaint”, adding that “non-compliance would not impact on the admissibility of any material gleaned.”108 101 Written evidence from Public Law Project (nTL0046) 102 Written evidence from Professor Pete Fussey, Dr Daragh Murray and Dr Amy Stevens (nTL0017) 103 Ibid. (extracted from366.pdf)
104 Written evidence from Robin Allen QC and Dee Masters (nTL0019) 105 Written evidence from Archie Drake and Perry Keller (nTL0011) 106 Written evidence from the Association of Police and Crime Commissioners (APCC), national Police Chiefs’ Council (nPCC), and Police Digital Service (PDS) (nTL0049) 107 Q 73 (Dr Liam Owens and Professor Sandra Wachter) 108 Written evidence from Dr Joe Purshouse, Dr nessa Lynch, Dr Marcin Betkier and Professor Liz Campbell (nTL0021) THE ADVEnT OF nEW TECHnOLOGIES In THE JUSTICE SYSTEM 29 Box 6: The EU Artificial Intelligence Regulation Proposal 1. (extracted from366.pdf)
30 THE ADVEnT OF nEW TECHnOLOGIES In THE JUSTICE SYSTEM of the national Police Chiefs’ Council, who, among others, thought that any new legal framework should be adopted at national level, but that “it would be helpful if it was not too divergent from international regulation”.115 The practicalities 58. (extracted from366.pdf)
Others favoured a value- and principles-based approach, with Alun Michael saying that “the values and principles need to be established in law”.118 As our witnesses pointed out, “certain things with AI will always be the same … we will always have a data issue, a bias issue and an explainability issue.”119 Professor Raab similarly told us that among the “plethora” of guidance, research and reviews, a consensus had emerged on some principles: “privacy protection, accountability, fairness, non-discrimination, justice, transparency, safety and cybersecurity, serving the common good, explainability, and human oversight”.120 As we highlighted in paragraph 56, the Minister himself said that the Government’s preferred approach was to produce a set of principles—our view is that these should be translated into statute. (extracted from366.pdf)
We 115 Q 73 (David Lewis) 116 Written evidence from nCC Group (nTL0005) 117 Written evidence from Professor Pete Fussey, Dr Daragh Murray and Dr Amy Stevens (nTL0017) 118 Q 98 (Alun Michael) 119 Q 69 (Professor Sandra Wachter) 120 Written evidence from Professor Charles Raab (nTL0014) THE ADVEnT OF nEW TECHnOLOGIES In THE JUSTICE SYSTEM 31 acknowledge the demands that the development of new legislation would place on parliamentary time and Government capacity, and that legislation is not a ‘quick fix’. (extracted from366.pdf)
32 THE ADVEnT OF nEW TECHnOLOGIES In THE JUSTICE SYSTEM 64. (extracted from366.pdf)
The Home Office told us about guidance on the CAID programme (the Child Abuse Image Database, a facial recognition tool which helps identify victims and offenders)126, while guidance on facial recognition is currently being developed by the College of Policing, and the Ministry of Justice is working with the Alan Turing Institute to extend existing guidance on the use of data-driven technologies within the justice system.127 There is also various guidance available from the Surveillance Camera and Information Commissioners128, and a host of guidance from non-governmental sources such as the ALGO-care framework (a practical decision-making framework for the policing context).129 The Metropolitan Police Service noted that the application of the “variety of guidance, opinion, codes, directions and proposals for ethical frameworks … risks confusion and inconsistency”.130 126 Written evidence from the Home Office (nTL0055) 127 College of Policing, ‘Police use of live facial recognition technology—have your say’ (17 May 2021): https://www.college.police.uk/article/police-use-live-facial-recognition-technology-have-your-say [accessed 26 January 2022] and written evidence from the Ministry of Justice (nTL0053) 128 Information Commissioner’s Office, ‘Guidance index’: https://ico.org.uk/for-organisations/guidance- index/ and Biometrics and Surveillance Camera Commissioner, ‘Surveillance camera guidance, tools and templates’ (22 October 2018): https://www.gov.uk/government/collections/surveillance-camera- guidance-tools-and-templates [accessed 7 February 2022] 129 Marion Oswald, ‘Algorithmic risk assessment policing models: lessons from the Durham HART model and ‘Experimental’ proportionality’, Information & Communications Technology Law, vol.27, (3 April 2018): https://www.tandfonline.com/doi/full/10.1080/13600834.2018.1458455 [accessed 7 February 2022] 130 Written evidence from the Metropolitan Police Service (nTL0031) THE ADVEnT OF nEW TECHnOLOGIES In THE JUSTICE SYSTEM 33 Box 9: Application of the Equality Act 2010 • To illustrate the confusion in the application of law, several submissions referred to the Equality Act 2010. (extracted from366.pdf)
As Professor Raab pointed out, comprehensive and practical guidance on 131 Written evidence from Archie Drake and Perry Keller (nTL0011) 132 Cloisters, In the matter of automated data processing in Government decision making (7 September 2019): https://www.cloisters.com/wp-content/uploads/2019/10/Open-opinion-pdf-version-1.pdf [accessed 25 January 2022] 133 Centre for Data Ethics and Innovation, Review into bias in algorithmic decision-making (november 2020), p 12: https://assets.publishing.service.gov.uk/government/uploads/system/uploads/attachment_data/ file/957259/Review_into_bias_in_algorithmic_decision-making.pdf [accessed 2 February 2022] 134 Written evidence from Professor Pete Fussey, Dr Daragh Murray and Dr Amy Stevens (nTL0017) 135 Written evidence from Professor Charles Raab (nTL0014) 136 Written evidence from Robin Allen QC and Dee Masters (nTL0019) 137 College of Policing, ‘APP content’ (4 november 2015): https://www.app.college.police.uk/app- content/ [accessed 4 February 2022] 138 Written evidence from the Serious Fraud Office (nTL0034) 34 THE ADVEnT OF nEW TECHnOLOGIES In THE JUSTICE SYSTEM the use of types of technologies will require consistent review and ongoing updates as tools are used in operational settings, and practical operational issues identified.139 It could not therefore be expected that such guidance will ever tackle all of the specificities of particular tools. (extracted from366.pdf)
The Government should require that national guidance for the use of advanced technological tools in policing and criminal justice is drawn up and, as part of their response to this report, should outline concrete plans for this. (extracted from366.pdf)
142 Written evidence from the Association of Police and Crime Commissioners, national Police Chiefs’ Council, and Police Digital Service (nTL0049) 143 Written evidence from the Law Society of England and Wales (nTL0023) 144 Written evidence from Professor Pete Fussey, Dr Daragh Murray and Dr Amy Stevens (nTL0017) 145 Written evidence from BAE Systems (nTL0056) 146 Written evidence from the Information Commissioner’s Office (nTL0016) THE ADVEnT OF nEW TECHnOLOGIES In THE JUSTICE SYSTEM 35 potential influence on individuals, groups and society”147, this trust is critical. (extracted from366.pdf)
Kit Malthouse MP said that forces must be allowed to fail “before we jump on everything.” It is important to note that the Minister was speaking in this context about technology which proves “not to be terribly useful”152, rather than about a failure to comply with minimum standards or where a miscarriage of justice had occurred. (extracted from366.pdf)
In particular, they thought that the Home Secretary, the Lord Chancellor and Secretary of State for Justice, and the Minister for Crime and Policing should be answerable for “how the Government’s vision of technological change in the system safeguards its effectiveness and legitimacy.”156 The Minister for Crime and Policing agreed that he, and Government as a whole, are “broadly—whether [they] like it or not—responsible for most things.”157 147 Written evidence from Dr Matthias Wienroth et al. (extracted from366.pdf)
(nTL0022) 148 Committee on Standards in Public Life, ‘The Seven Principles of Public Life’ (31 May 1995): https:// www.gov.uk/government/publications/the-7-principles-of-public-life/the-7-principles-of-public- life--2 [accessed 27 January 2022] 149 Written evidence from Public Law Project (nTL0046) 150 Q 72 (Dr Liam Owens) 151 Q 76 (Dr Liam Owens) 152 Q 106 (Kit Malthouse MP) 153 Written evidence from Privacy International (nTL0051) 154 Written evidence from Dr Miri Zilka, Detective Sergeant Laurence Cartwright and Dr Adrian Weller (nTL0040) and Q 31 (Professor Michael Wooldridge) 155 Written evidence from the Bar Council (nTL0048) 156 Written evidence from Archie Drake and Perry Keller (nTL0011) 157 Q 107 (Kit Malthouse MP) 36 THE ADVEnT OF nEW TECHnOLOGIES In THE JUSTICE SYSTEM • Chief Constables. (extracted from366.pdf)
(nTL0012) 167 Written evidence from nCC Group (nTL0005) 168 Written evidence from BAE Systems (nTL0056) 169 Written evidence from Dr Christopher Lawless (nTL0029) THE ADVEnT OF nEW TECHnOLOGIES In THE JUSTICE SYSTEM 37 Lack of recourse 82. (extracted from366.pdf)
174 Written evidence from Robin Allen QC and Dee Masters (nTL0019) 175 Written evidence from Liberty (nTL0020) and Big Brother Watch (nTL0037) 38 THE ADVEnT OF nEW TECHnOLOGIES In THE JUSTICE SYSTEM to ban “clearly harmful or high-risk applications of technology” which lack robust accountability arrangements.176 87. (extracted from366.pdf)
176 Written evidence from Archie Drake and Perry Keller (nTL0011) 177 Written evidence from the Metropolitan Police Service (nTL0031) 178 Committee on Civil Liberties, Justice and Home Affairs, Report on artificial intelligence in criminal law and its use by the police and judicial authorities in criminal matters (13 July 2021): https://www.europarl. (extracted from366.pdf)
europa.eu/doceo/document/A-9-2021–0232_En.html [accessed 3 February 2022] 179 United nations Human Rights Office of the High Commissioner, Artificial intelligence risks to privacy demand urgent action — Bachelet (15 September 2021): https://www.ohchr.org/en/2021/09/artificial- intelligence-risks-privacy-demand-urgent-action-bachelet [accessed 3 February 2022] THE ADVEnT OF nEW TECHnOLOGIES In THE JUSTICE SYSTEM 39 CHAPTER 3: TRANSPARENCY 90. (extracted from366.pdf)
(nTL0022) 185 Written evidence from Public Law Project (nTL0046), see also written evidence from Dr Joe Purshouse, Dr nessa Lynch, Dr Marcin Betkier and Professor Liz Campbell (nTL0021) 186 Written evidence from Robin Allen QC and Dee Masters (nTL0019), see also Q 65 (Peter Dawson) 187 Q 78 (Professor Sandra Wachter) 188 Written evidence from The Bar Council (nTL0048) 189 Written evidence from Public Law Project (nTL0046) 190 Q 57 (Professor Karen Yeung) 40 THE ADVEnT OF nEW TECHnOLOGIES In THE JUSTICE SYSTEM of particular technologies but also for ensuring that the decision-making process for the use of technology is open to public scrutiny.”191 94. (extracted from366.pdf)
The Home Office told us that they were “supporting law enforcement organisations to address … the need for transparency”,192 and that “policing is committed to being transparent.”193 The Ministry of Justice also informed us about an annual review of “analytical algorithms—only a small subset of [which] involve data and decisions about individuals”. (extracted from366.pdf)
196 Written evidence from Robin Allen QC and Dee Masters (nTL0019) 197 Written evidence from the Metropolitan Police Service (nTL0031) THE ADVEnT OF nEW TECHnOLOGIES In THE JUSTICE SYSTEM 41 the use of technological solutions as they cannot know who is using what, for how long, for what purpose, or with what safeguards. (extracted from366.pdf)
This risks undermining trust in the police, the justice system, and the rule of law. (extracted from366.pdf)
42 THE ADVEnT OF nEW TECHnOLOGIES In THE JUSTICE SYSTEM to constantly refine and refresh the model to comply with appropriate ethical and legal oversight and governance.”203 While the full study is expected to be published (at a date yet to be confirmed), there are no commitments as to what information it will contain. (extracted from366.pdf)
Box 10: Previous support for a register • In 2018, the House of Commons Science and Technology Committee recommended that “the Government should produce, publish, and maintain a list of where algorithms with significant impacts are being used within Central Government, along with projects underway or planned for public service algorithms”.207 • A 2019 report by the Law Society of England and Wales concluded that “a national register of algorithmic systems in the criminal justice system should be created”.208 203 Police Professional, ‘Artificial intelligence ‘marginally better’ at predicting re-offending’ (25 January 2022): https://www.policeprofessional.com/news/artificial-intelligence-marginally-better-at-predict ing-reoffending/ [accessed 24 February 2022] 204 Q 65 (Silkie Carlo, Peter Dawson, Professor Karen Yeung) and Q 78 (David Lewis, Dr Liam Owens, Professor Sandra Wachter) 205 Written evidence from Robin Allen QC and Dee Masters (nTL0019) 206 Written evidence from Professor Colin Gavaghan (nTL0047), see also written evidence from Archie Drake and Perry Keller (nTL0011) and Professor Lilian Edwards, Professor Derek McAuley, Dr Lachlan Urquhart and Dr Jiahong Chen (nTL0035). (extracted from366.pdf)
207 Science and Technology Committee, Algorithms in decision-making (Fourth Report, Session 2017– 2019, HC 351) 208 Q 11 (Professor Sylvie Delacroix) see also the Law Society of England and Wales, Algorithm use in the criminal justice system report, p 66: https://www.lawsociety.org.uk/en/topics/research/algorithm-use-in- the-criminal-justice-system-report [accessed 10 January 2022]. (extracted from366.pdf)
THE ADVEnT OF nEW TECHnOLOGIES In THE JUSTICE SYSTEM 43 • A 2020 report by the Royal United Services Institute (RUSI) commissioned by the Centre for Data Ethics and Innovation (CDEI) found that “the nPCC and APCC should … maintain a high-level catalogue for all algorithms used by police forces nationwide”.209 David Lewis told us that the nPCC had recently accepted this recommendation, although caveating that “there is a matter of degree to be debated”.210 There is no indication that such a catalogue would be published. (extracted from366.pdf)
One contributor thought that “it is not always feasible or even desirable to make algorithms in criminal justice fully transparent.”214 In the following paragraphs we examine those arguments. (extracted from366.pdf)
For instance, the information published on a register “could be used to infer how a [Machine Learning] model would make a specific legal decision, and thus what inputs could be crafted to manipulate a desired legal 209 RUSI, Data Analytics and Algorithms in Policing in England and Wales (February 2020), p xi: https:// static.rusi.org/rusi_pub_165_2020_01_algorithmic_policing_babuta_final_web_copy.pdf [accessed 21 January 2022] 210 Q 78 (David Lewis) 211 Centre for Data Ethics and Innovation, Review into bias in algorithmic decision making (november 2020): https://assets.publishing.service.gov.uk/government/uploads/system/uploads/attachment_data/ file/957259/Review_into_bias_in_algorithmic_decision-making.pdf [accessed 21 January 2022] 212 Commission on Race and Ethnic Disparities, Independent report, Forward, introduction and full recommendations, (28 April 2021): https://www.gov.uk/government/publications/the-report-of-the- commission-on-race-and-ethnic-disparities/foreword-introduction-and-full-recommendations#full- recommendations [accessed 10 January 2022] 213 Written evidence from the Independent Office for Police Conduct (nTL0054) 214 Written evidence from Dr Miri Zilka, Detective Sergeant Laurence Cartwright and Dr Adrian Weller (nTL0040) 215 Q 78 (Dr Liam Owens) 216 Written evidence from BAE Systems (nTL0056) 217 Q 78 (Professor Sandra Wachter) and Q 73 (Professor Sandra Wachter) 44 THE ADVEnT OF nEW TECHnOLOGIES In THE JUSTICE SYSTEM outcome.”218 Indeed, several witnesses were worried that a technological solution “could be ‘gamed’ by criminals” if algorithms were published.219 This is what BAE Systems calls “data poisoning”220 and the nCC Group calls “adversarial Machine Learning”.221 107. (extracted from366.pdf)
When some circumscribed their recommendation to “the application of the law”228 or “the criminal justice system”229 only, others advised that it should cover “the public sector”230 or “Government”231 in general. (extracted from366.pdf)
226 Written evidence from the Information Commissioner’s Office (nTL0016) 227 Written evidence from Professor Colin Gavaghan (nTL0047) 228 Written evidence from Big Brother Watch (nTL0037) 229 Q 11 (Professor Sylvie Delacroix) 230 Q 65 (Professor Karen Yeung) 231 Written evidence from the Public Law Project (nTL0046) 232 Written evidence from Professor Colin Gavaghan (nTL0047) 233 Written evidence from Big Brother Watch (nTL0037) THE ADVEnT OF nEW TECHnOLOGIES In THE JUSTICE SYSTEM 45 • The Public Law Project considered that each entry in the register should be accompanied with “executable versions of listed algorithms” and an explanation of how the technology works.234 • Citing the EU’s proposed AI Regulation currently being discussed within the European Union as a reference (see Box 6), Professor Sandra Wachter suggested that the register could include algorithms themselves, the data on which they are trained, as well as information on tests carried out and on oversight mechanisms.235 • Several witnesses asked for “detailed impact assessments”236 to be included, such as Equality Impact Assessments237 or Human Rights Impact Assessments.238 The Algorithmic Transparency Standard 109. (extracted from366.pdf)
240 Cabinet Office, UK government publishes pioneering standard for algorithmic transparency 46 THE ADVEnT OF nEW TECHnOLOGIES In THE JUSTICE SYSTEM allow public bodies the time to submit entries without diverting effort away from operational activities. (extracted from366.pdf)
THE ADVEnT OF nEW TECHnOLOGIES In THE JUSTICE SYSTEM 47 CHAPTER 4: HUMAN-TECHNOLOGY INTERACTIONS 114. (extracted from366.pdf)
(nTL0012) 248 Article 22 of Regulation (EU) 2016/679 of 23 April 2016 on the protection of natural persons with regard to the processing of personal data and on the free movement of such data, and repealing Directive 95/46/EC (General Data Protection Regulation) (Article 22) 4 May 2016 (OJ L 119/1), see written evidence from Professor nigel Harvey and Tobias Harvey (nTL0025) see also written evidence from Professor Lilian Edwards, Professor Derek McAuley, Dr Lachlan Urquhart and Dr Jiahong Chen (nTL0035) 48 THE ADVEnT OF nEW TECHnOLOGIES In THE JUSTICE SYSTEM enforcement body may not take a qualifying significant decision based solely on automated processing unless that decision is required or authorised by law.249 These provisions aim to guarantee that there is a “human in the loop” but only for narrowly specified decisions. (extracted from366.pdf)
251 Oral evidence taken on 27 October 2021 (Session 2021–22) Q 13 (The Rt Hon Priti Patel MP, Home Secretary) 252 Q 86 (Professor Paul Taylor) 253 See, for example, written evidence from the Law Society of England and Wales (nTL0023), Big Brother Watch (nTL0037) and the Public Law Project (nTL0046) 254 Q 33 (Professor Michael Wooldridge) 255 European Commission, Guidelines on Automated individual decision making and Profiling for the purposes of regulation 2016/679 (wp251rev.01), 22 August 2018: https://ec.europa.eu/newsroom/article29/ redirection/document/49826 [accessed 24 January 2022] THE ADVEnT OF nEW TECHnOLOGIES In THE JUSTICE SYSTEM 49 • reviewers must ‘weigh-up’ and ‘interpret’ the recommendation, consider all available input data, and also take into account other additional factors.”256 122. (extracted from366.pdf)
Evidence from the Ministry of Justice stated that “operational decisions are informed by analytical tools rather than being automatic consequences of tool outputs.”257 Similarly, the Home Office stated that they “strongly disagree … that we are allowing sensitive decisions to be delegated to machines in a way that is either contrary to the law or the core principles of the [criminal justice system]”.258 Interactions to date 123. (extracted from366.pdf)
We were told, for instance, that attendees to an event about facial recognition (many of whom had access to and used facial recognition technology) “had a limited understanding of both face recognition technology and human face recognition.”264 A supplier of some tools had found that “criminal justice professionals are typically also lacking an expert, or even good, understanding.”265 Dee Masters and Robin Allen QC had, similarly, “become increasingly aware of the lack of understanding by regulators and the general public of the way in which AI systems are being used in their field of activity.”266 256 Information Commissioner’s Office, ‘Guidance on AI and data protection’: https://ico.org.uk/for- organisations/guide-to-data-protection/key-dp-themes/guidance-on-artificial-intelligence-and-data- protection/ [accessed 17 January 2022] 257 Written evidence from the Ministry of Justice (nTL0053) 258 Written evidence from the Home Office (nTL0055) 259 Written evidence from BAE systems (nTL0056) 260 Q 8 (Professor Charles Raab) 261 Written evidence from Professor nigel Harvey and Tobias Harvey (nTL0025) 262 Written evidence from nCC Group (nTL0005) 263 Written evidence from Archie Drake and Perry Keller (nTL0011) 264 Written evidence from Dr Eilidh noyes and Dr Reuben Moreton (nTL0026) 265 Written evidence from SAS UK&I (nTL0041) 266 Written evidence from Robin Allen QC and Dee Masters (nTL0019) 50 THE ADVEnT OF nEW TECHnOLOGIES In THE JUSTICE SYSTEM 125. (extracted from366.pdf)
Professor Carole McCartney, Professor of Law and Criminal Justice at the University of northumbria, explained this further: “If the humans do not understand the technology and how it is working, how will they spot if it has failed or if they have made a mistake? (extracted from366.pdf)
The Prison Reform Trust were similarly concerned about confidence to challenge outcomes which might appear discriminatory: “managers and policy makers are likely to be less inclined to ‘look under the bonnet’ when the technology they find there is unfamiliar.”268 The lack of understanding does not only apply to the people who are using the tool, but to those who commission it—and those who interact with its outputs later ‘down the justice pipeline’, including judiciary reviewing the conduct and findings of an investigation. (extracted from366.pdf)
Amy Stevens (nTL0017) 270 Police Professional, Artificial intelligence ‘marginally better’ at predicting re-offending (25 January 2022): https://www.policeprofessional.com/news/artificial-intelligence-marginally-better-at-predicting- reoffending/ [accessed 24 February 2022] THE ADVEnT OF nEW TECHnOLOGIES In THE JUSTICE SYSTEM 51 • An automated triage system used by the Home Office, known as the sham marriage algorithm, is used to help “determine whether a proposed marriage should be investigated as a ‘sham’”. (extracted from366.pdf)
280 The Human Rights, Big Data and Technology Project, Independent Report on the London Metropolitan Police Service’s Trial of Live Facial Recognition Technology (July 2019), p 10: http://repository.essex.ac.uk /24946/1/ London-Met-Police-Trial-of-Facial-Recognition-Tech-Report-2.pdf [accessed 7 February 2022] 52 THE ADVEnT OF nEW TECHnOLOGIES In THE JUSTICE SYSTEM 128. (extracted from366.pdf)
The Home Office should, in conjunction with the Ministry of Justice and the College of Policing, undertake or commission appropriate research to determine how the use of predictive algorithms affects decision making, and under what circumstances meaningful human interaction is most likely. (extracted from366.pdf)
283 Q 87 (David Tucker) 284 Written evidence from the Public Law Project (nTL0046) 285 Written evidence from Dr Eilidh noyes and Dr Reuben Moreton (nTL0026) 286 Q 70 (David Lewis) 287 Q 58 (Peter Dawson) THE ADVEnT OF nEW TECHnOLOGIES In THE JUSTICE SYSTEM 53 ongoing288, and regularly reviewed.289 It could also address a skills shortage in the workforce: references were made in the evidence to the difficulty in attracting employees highly skilled in technological tools who can command extremely high salaries in the private sector.290 134. (extracted from366.pdf)
David Tucker, Head of Criminal Justice at the College of Policing, told the Committee that “we have to wait for a moment of maturity, because if we do not we run the risk of trying to give guidance on something that has not settled down and is developing.”291 We are unconvinced by this argument. (extracted from366.pdf)
There is also ensuring that we have systems and processes in place so that does not occur.”293 The Ministry of Justice also outlined some of its safeguarding systems, which included “clear guidance” for “when the data should and should not be used, and support (and sometimes training) … made available to staff”.294 136. (extracted from366.pdf)
It would not be reasonable to expect every police officer, or every Ministry of Justice official (to take but two examples), to be trained in data analytics and in the specificities of sophisticated technological solutions. (extracted from366.pdf)
(nTL0012) 290 Written evidence from the Serious Fraud Office (nTL0034) 291 Q 91 (David Tucker) 292 RUSI, ‘Data analytics and algorithms in policing in England and Wales: Towards a new policy framework’ (2020): https://rusi.org/explore-our-research/publications/occasional-papers/data-analyti cs-and-algorithms-policing-england-and-wales-towards-new-policy-framework [accessed 9 March 2022] 293 Q 89 (Professor Paul Taylor) 294 Written evidence from the Ministry of Justice (nTL0053) 54 THE ADVEnT OF nEW TECHnOLOGIES In THE JUSTICE SYSTEM a part,295 and thus also need a thorough understanding of how algorithmic tools work. (extracted from366.pdf)
As part of continuing professional development, training should also be made available to lawyers, members of the Judiciary, and other professionals involved in the justice system. (extracted from366.pdf)
The Ministry of Justice referred to their sexual offender predictive tool, which is supported by an “overarching” policy framework to “support consistency of use”.298 Evidence from the Prison Reform Trust acknowledged the need for structures and policies enabling a clear route for challenge, but emphasised that these must work well in practice. (extracted from366.pdf)
295 See paras 23–26 296 Independent Chief Inspector of Borders and Immigration, An inspection of entry clearance processing operations in Croydon and Istanbul 297 Written evidence from Professor Colin Gavaghan (nTL0047) 298 Written evidence from the Ministry of Justice (nTL0053) THE ADVEnT OF nEW TECHnOLOGIES In THE JUSTICE SYSTEM 55 144. (extracted from366.pdf)
Professor Karen Yeung, for instance, warned against tools that produce “a nice colour” such as “a little risk assessment, red, green or yellow” because they are so “easy to interpret” that they do not encourage challenge or critical thinking.304 In a similar vein, the Ministry of Justice told us that when designing “tools which 299 Q 57 (Peter Dawson) 300 Written evidence from the Prison Reform Trust (nTL0004) 301 Independent Chief Inspector of Borders and Immigration, An inspection of entry clearance processing operations in Croydon and Istanbul 302 Q 8 (Professor Delacroix) 303 Written evidence from Gary Pugh (nTL0036) 304 Q 57 (Professor Karen Yeung) 56 THE ADVEnT OF nEW TECHnOLOGIES In THE JUSTICE SYSTEM are used to support decisions about individuals”, they “strive to ensure that the tool is designed to be intuitive.”305 150. (extracted from366.pdf)
If achieved, algorithmic explainability would therefore provide more compelling explanations of decisions than humans currently provide.314 Certainly, procurement managers need to understand how the tools they are commissioning or purchasing function.315 Importantly, Professor Wachter added that introducing an explainability requirement on technologies used for the application of the law could help with this: “When people do not want to tell you how [a technological solution] is working, it is either because they do not want to or because they do 305 Written evidence from the Ministry of Justice (nTL0053) 306 Written evidence from the Bar Council (nTL0048) 307 Written evidence from BAE Systems (nTL0056) 308 See Marion Oswald, ‘Algorithm-assisted decision-making in the public sector: framing the issues using administrative law rules governing discretionary power’, University of Winchester, (6 August 2018), p 8–9: https://royalsocietypublishing.org/doi/10.1098/rsta.2017.0359 [accessed 2 February 2022] 309 Written evidence from Big Brother Watch (nTL0037) 310 Written evidence from Dr Miri Zilka, Detective Sergeant Laurence Cartwright and Dr Adrian Weller (nTL0040) 311 Written evidence from the Serious Fraud Office (nTL0034) and Archie Drake and Perry Keller (nTL0011) 312 Written evidence from Professor Lilian Edwards, Professor Derek McAuley, Dr Lachlan Urquhart and Dr Jiahong Chen (nTL0035) and Q 11 (Professor Sylvie Delacroix) 313 Written evidence from the Royal Statistical Society (nTL0033) 314 Written evidence from Professor Colin Gavaghan (nTL0047) 315 Chapter 5 addresses this from the angle of evaluations. (extracted from366.pdf)
THE ADVEnT OF nEW TECHnOLOGIES In THE JUSTICE SYSTEM 57 not know. (extracted from366.pdf)
I do not think either is acceptable, especially in the criminal justice sector. (extracted from366.pdf)
58 THE ADVEnT OF nEW TECHnOLOGIES In THE JUSTICE SYSTEM CHAPTER 5: EVALUATION AND OVERSIGHT 156. (extracted from366.pdf)
The Law Society of England and Wales told us that: “Bias, both conscious and unconscious, can be baked into algorithms and undermine consistently reliable results, and that using algorithms without questioning them or explaining them to the public could lead to decisions which threaten human rights and undermine trust in the justice system”.323 158. (extracted from366.pdf)
ethnicity-facts-figures.service.gov.uk/crime-justice-and-the-law/policing/stop-and-search/ latest#byethnicity [accessed 7 February 2022] 327 Q 71 (Professor Sandra Wachter) THE ADVEnT OF nEW TECHnOLOGIES In THE JUSTICE SYSTEM 59 the application of the categorisation algorithm and on which it depends. (extracted from366.pdf)
60 THE ADVEnT OF nEW TECHnOLOGIES In THE JUSTICE SYSTEM Assessments are a common approach to complying with the Public Sector Equality Duty,335 while Data Protection Impact Assessments are required for data processing operations which are likely to result in a high risk to individuals.336 163. (extracted from366.pdf)
THE ADVEnT OF nEW TECHnOLOGIES In THE JUSTICE SYSTEM 61 and in demonstrating trustworthiness in ways that are more convincing than slogans and pledges, or compliance with legal requirements.344 Community consultation 166. (extracted from366.pdf)
(nTL0022) 350 Written evidence from the Metropolitan Police Service (nTL0031) 351 Written evidence from Big Brother Watch (nTL0037) 62 THE ADVEnT OF nEW TECHnOLOGIES In THE JUSTICE SYSTEM Technical considerations “The system will fail” 170. (extracted from366.pdf)
THE ADVEnT OF nEW TECHnOLOGIES In THE JUSTICE SYSTEM 63 Box 12: Scientific standards In this report, a reference to ‘scientific standards’ is intended to mean a regime of quality standards and processes consistently applied to a person or body developing, maintaining or manufacturing a particular scientific product or technology, providing a scientific service, or incorporating a scientific method into their public service, combined with independent regulatory enforcement. (extracted from366.pdf)
64 THE ADVEnT OF nEW TECHnOLOGIES In THE JUSTICE SYSTEM issues”.362 For instance, we heard that “the accuracy of face recognition technology depends on … the environment in which the technology is deployed” because “in real settings, images may be of suboptimal quality or environmental conditions may be inhibitory to realising the full accuracy of face recognition technology.”363 176. (extracted from366.pdf)
371 Q 40 (Professor Colin Gavaghan) 372 Q 66 (Silkie Carlo), Q 46 (Dr Rosamunde van Brakel), and Q 4 (Professor Charles Raab) 373 Written evidence from Archie Drake and Perry Keller, Kings College London (nTL0011) THE ADVEnT OF nEW TECHnOLOGIES In THE JUSTICE SYSTEM 65 confirmed. (extracted from366.pdf)
66 THE ADVEnT OF nEW TECHnOLOGIES In THE JUSTICE SYSTEM was also critical of the Government’s justification for the increased use of polygraph testing—a technological solution with controversial scientific grounds. (extracted from366.pdf)
David Spreadborough, a forensic analyst, told us that “a technology introduced in the judiciary system should be validated and approved by people technically competent on the matter.”389 BAE Systems agreed that a designated body could “develop a certificate of conformance (or ‘Kitemark’/CE label) for approved AI applications”.390 nCC Group concurred that it is “essential that clear processes are established to vet technologies before they are deployed”391, whereas Dr Liam Owens of technology provider Semantics21 told us about a “review” by “an intermediary”.392 388 Written evidence from the Information Commissioner’s Office (nTL0016) 389 Written evidence from David Spreadborough (nTL0015) 390 Written evidence from BAE Systems (nTL0056) 391 Written evidence from nCC Group (nTL0005) 392 Q 78 (Dr Liam Owens) THE ADVEnT OF nEW TECHnOLOGIES In THE JUSTICE SYSTEM 67 Privacy International agreed with them and detailed mechanisms by which a technological solution should be “approved for use”.393 187. (extracted from366.pdf)
The Ministry of Justice referred us to a peer-reviewed research study that evaluated the OASys Sexual reoffending Predictor (OSP) before this technological solution was approved by the Sexual Offending Management Board of Her Majesty’s Prison and Probation Service.394 Similarly, the Public Law Project drew our attention to the proposed AI Regulation in the European Union, which foresees central “certification indicating conformity to regulatory standards.”395 188. (extracted from366.pdf)
Former Deputy Chief Constable David Lewis told us that “there probably should be more centralised procurement”, alluding to the success of “regional procurement hubs” bringing police forces together.398 BAE Systems agreed 393 Written evidence from Privacy International (nTL0051) 394 Written evidence from the Ministry of Justice (nTL0053) 395 Written evidence from Public Law Project (nTL0046) 396 College of Policing, Fundamental review of the College of Policing: https://assets.college.police.uk/s3fs- public/2022–02/Fundamental-review-of-the-College-of-Policing.pdf [accessed 24 February 2022] 397 HMICFRS, Inspectorate with College standards letter (10 February 2022): https://www.justicein spectorates.gov.uk/hmicfrs/publication-html/inspectorate-relationship-with-college-standards-letter/ [accessed 24 February 2022] 398 Q 77 (David Lewis) 68 THE ADVEnT OF nEW TECHnOLOGIES In THE JUSTICE SYSTEM that they “would support some form of centralised AI procurement within policing and justice.”399 192. (extracted from366.pdf)
The Ministry of Justice and HM Prison & Probation Service have developed several of them, such as: • the Offender Group Reconviction Score (OGRS), a “predictor of proven reoffending within one and two years of noncustodial sentence or discharge from custody”404 • the Offender Assessment System (OASys), which “aims to assess the risk of harm offenders pose to others and how likely an offender is to reoffend”405 • the Digital Categorisation Service (DCS), an algorithm used to support decisions on security categorisations in prisons. (extracted from366.pdf)
405 Written evidence from Big Brother Watch (nTL0037) 406 Written evidence from the Prison Reform Trust (nTL0004) THE ADVEnT OF nEW TECHnOLOGIES In THE JUSTICE SYSTEM 69 Police and the University College London.407 The national Data Analytics Solution, a nationwide project sponsored by the Home Office and led by West Midlands Police in partnership with the national Crime Agency and Accenture, also falls in this category.408 We were told that, in new Zealand, such partnerships were “by far the most common practice” when the government procures technological solutions.409 196. (extracted from366.pdf)
415 Written evidence from the nCC Group (nTL0005) 416 Written evidence from Dr Eilidh noyes and Dr Reuben Moreton (nTL0026) 417 Q 24 (Professor Charles Raab) 70 THE ADVEnT OF nEW TECHnOLOGIES In THE JUSTICE SYSTEM has been overtaken by marketing” because of salespeople who “will take something they do not understand and shout a number that they do not understand” to make accuracy claims.418 200. (extracted from366.pdf)
publishing.service.gov.uk/government/uploads/system/uploads/attachment_data/file/990469/ Guidelines_for_AI_procurement.pdf [accessed 26 January 2022] 424 World Economic Forum, AI Procurement Guidelines (11 June 2020): https://www.weforum.org/reports/ ai-procurement-in-a-box/ai-government-procurement-guidelines [accessed 26 January 2022] 425 Q 73 (David Lewis) THE ADVEnT OF nEW TECHnOLOGIES In THE JUSTICE SYSTEM 71 Chiefs’ Council, and the Police Digital Service confirmed that they “inform contract implementation and management”.426 203. (extracted from366.pdf)
BAE Systems argued that the guidelines should be “more technical”, more “supplier-focused”, and more specific to “policing and the justice context”.427 Professor Wachter told us that these “vague” guidelines were “not good enough” because they were too soft to induce change in procurement practices.428 Dr Liam Owens agreed that the guidelines are “very broad” and “non-specific” and would need to be better tailored to address the needs of technology providers in the context of the application of the law.429 Furthermore, this document refers to ‘guidance’ as well as ‘guidelines’. (extracted from366.pdf)
432 World Economic Forum, AI Procurement Guidelines 72 THE ADVEnT OF nEW TECHnOLOGIES In THE JUSTICE SYSTEM used in the application of the law. (extracted from366.pdf)
THE ADVEnT OF nEW TECHnOLOGIES In THE JUSTICE SYSTEM 73 The West Midlands Ethics Committee model 211. (extracted from366.pdf)
These included: • A recruitment based on merit, 443; with independent membership444 with a range of expertise; 445 • A commitment to publish meetings papers, minutes and conclusions;446 • The Committee’s independence from the police force whose use of technology it is scrutinising; and447 • The Committee’s remit to consider technological solutions throughout their lifecycle.448 441 See, for instance, Q 2 (Professor Carole McCartney), Q 110 (Kit Malthouse MP), also written evidence from Archie Drake and Perry Keller (nTL0011), and defenddigitalme (nTL0044) 442 Written evidence from Association of Police and Crime Commissioners, national Police Chiefs’ Council and Police Digital Service (nTL0049), see also Q 11 (Professor McCartney), Q 48 (Dr Rosamunde van Brakel), and written evidence from BAE Systems (nTL0056) 443 Written evidence from Association of Police and Crime Commissioners, national Police Chiefs’ Council and Police Digital Service (nTL0049) 444 Written evidence from Association of Police and Crime Commissioners, national Police Chiefs’ Council and Police Digital Service (nTL0049) and BAE Systems (nTL0056) 445 Q 11 (Professor McCartney), Q 48 (Dr Rosamunde van Brakel) and written evidence from BAE Systems (nTL0056) 446 Q 11 (Professor McCartney), written evidence from Association of Police and Crime Commissioners, national Police Chiefs’ Council and Police Digital Service (nTL0049), and BAE Systems (nTL0056) 447 Written evidence from Association of Police and Crime Commissioners, national Police Chiefs’ Council and Police Digital Service (nTL0049), Archie Drake and Perry Keller (nTL0011) and BAE Systems (nTL0056) 448 Written evidence from Association of Police and Crime Commissioners, national Police Chiefs’ Council and Police Digital Service (nTL0049) and BAE Systems (nTL0056) 74 THE ADVEnT OF nEW TECHnOLOGIES In THE JUSTICE SYSTEM Box 14: The West Midlands Police Ethics Committee • The West Midlands Police and Crime Commissioner (PCC) and West Midlands Police (WMP) jointly established a specialist Ethics Committee in early 2019. (extracted from366.pdf)
pdf?x41638 [accessed 1 February 2022] 450 HL Deb, 3 november 2021, cols 1301–1305 451 Written evidence from the Home Office (nTL0055) THE ADVEnT OF nEW TECHnOLOGIES In THE JUSTICE SYSTEM 75 which everything in society operates”. (extracted from366.pdf)
76 THE ADVEnT OF nEW TECHnOLOGIES In THE JUSTICE SYSTEM SUMMARY OF CONCLUSIONS AND RECOMMENDATIONS Legal and institutional frameworks 1. (extracted from366.pdf)
(Paragraph 66) THE ADVEnT OF nEW TECHnOLOGIES In THE JUSTICE SYSTEM 77 11. (extracted from366.pdf)
The Government should require that national guidance for the use of advanced technological tools in policing and criminal justice is drawn up and, as part of their response to this report, should outline concrete plans for this. (extracted from366.pdf)
This risks undermining trust in the police, the justice system, and the rule of law. (extracted from366.pdf)
(Paragraph 113) 78 THE ADVEnT OF nEW TECHnOLOGIES In THE JUSTICE SYSTEM Human-technology interactions 21. (extracted from366.pdf)
The Home Office should, in conjunction with the Ministry of Justice and the College of Policing, undertake or commission appropriate research to determine how the use of predictive algorithms affects decision making, and under what circumstances meaningful human interaction is most likely. (extracted from366.pdf)
As part of continuing professional development, training should also be made available to lawyers, members of the Judiciary, and other professionals involved in the justice system. (extracted from366.pdf)
(Paragraph 183) THE ADVEnT OF nEW TECHnOLOGIES In THE JUSTICE SYSTEM 79 30. (extracted from366.pdf)
With the assurance brought by the certification process and the register of algorithms, police forces and other 80 THE ADVEnT OF nEW TECHnOLOGIES In THE JUSTICE SYSTEM public bodies would remain free to procure the technological solutions of their choice, as long as the products have been certified. (extracted from366.pdf)
(Paragraph 219) THE ADVEnT OF nEW TECHnOLOGIES In THE JUSTICE SYSTEM 81 APPENDIx 1: LIST OF MEMBERS AND DECLARATIONS OF INTEREST Members Lord Blunkett Baroness Chakrabarti Lord Dholakia Baroness Hallett Baroness Hamwee (Chair) Lord Hunt of Wirral Baroness Kennedy of The Shaws Baroness Pidding Baroness Primarolo Lord Ricketts Baroness Sanderson of Welton Baroness of Shackleton of Belgravia Declarations of Interest Lord Blunkett Non- Financial: Non-executive Chairman, Cyber Essentials Direct Limited Directorship: Director and Chairman of the Board, University of Law Limited (subsidiary and affiliated institution of Global University Systems and Interactive Pro Limited) Baroness Chakrabarti No relevant interests to declare Lord Dholakia Trustee of the Police Foundation which produced a report on the Strategic Review of Policing in England and Wales on 8 March 2022 Baroness Hallett Retired judge Baroness Hamwee No relevant interests to declare Lord Hunt of Wirral Partner, DAC Beachcroft LLP (International commercial law firm) Honorary Bencher, Inner Temple Baroness Kennedy of The Shaws Member of Microsoft Technology and Human Rights Advisory Council Baroness Pidding No relevant interests to declare Baroness Primarolo Non-Executive Director on the Board of Thompson’s Solicitors LLP. (extracted from366.pdf)
Lord Ricketts No relevant interests to declare Baroness Sanderson of Welton No relevant interests to declare Baroness Shackleton of Belgravia No relevant interests to declare other than those on the Register 82 THE ADVEnT OF nEW TECHnOLOGIES In THE JUSTICE SYSTEM Specialist Adviser Dr Marion Oswald Associate Professor, Northumbria Law School Advisory Board member, Centre for Data Ethics and Innovation Senior Research Associate, The Alan Turing Institute Associate Fellow, Royal United Services Institute for Defence and Security Studies; Independent Chair, West Midlands Police & Crime Commissioner and West Midlands Police Data Ethics Committee; Member, New Zealand Police Expert Panel on Emergent Technologies; Advisory board member of the UKRI Trustworthy Autonomous Systems Hub; Member of the Royal Society Working Group on Privacy Enhancing Technologies (2018–19 and reconstituted 2021); Member of National Statistician’s Data Ethics Advisory Committee since its foundation (2016-date); Executive member, British & Irish Law, Education & Technology Association; Member, Arts and Humanities Research Council Peer Review College. (extracted from366.pdf)
THE ADVEnT OF nEW TECHnOLOGIES In THE JUSTICE SYSTEM 83 APPENDIx 2: LIST OF WITNESSES Evidence is published online at https://committees.parliament.uk/committee/519/ justice-and-home-affairs-committee/publications/ and available for inspection at the Parliamentary Archives (020 7219 3074). (extracted from366.pdf)
Oral evidence in chronological order * Professor Sylvie Delacroix, Professor in Law and QQ 1–24 Ethics at University of Birmingham * Professor Carole McCartney, Professor of Law and QQ 1–24 Criminal Justice at northumbria University ** Professor Charles Raab, Professorial Fellow, Politics QQ 1–24 and International Relations, School of Social and Political Science at The University of Edinburgh) * Dr David Leslie, Ethics Theme Lead at Alan Turing QQ 25–38 Institute * Professor Michael Wooldridge, Head of Department QQ 25–38 of Computer Science, Professor of Computer Science at University of Oxford ** Professor Colin Gavaghan, Director new Zealand QQ 39–51 Law Foundation Centre for Law and Policy in Emerging Technologies at University of Otago * Professor Elizabeth E Joh, Martin Luther King Jr. (extracted from366.pdf)
QQ 39–51 Professor of Law at University of California, Davis * Dr Rosamunde Elise van Brakel, Co-Director, QQ 39–51 Surveillance Studies network, Associate Professor Tilburg University/Vrije Universiteit Brussel (VUB) ** Silkie Carlo, Director, Big Brother Watch QQ 52–67 ** Peter Dawson, Director, Prison Reform Trust QQ 52–67 * Professor Karen Yeung, Interdisciplinary Professorial QQ 52–67 Fellow in Law, Ethics and Informatics, Birmingham Law School at The University of Birmingham * David Lewis, Former Deputy Chief Constable and QQ 68–82 former ethics lead nPCC at Dorset Police * Dr Liam Owens, Founder and Chief Executive QQ 68–82 Officer, Semantics 21 ** Professor Sandra Wachter, Associate Professor and QQ 68–82 Senior Research Fellow at University of Oxford ** Professor Paul Taylor, Chief Scientific Adviser, QQ 83–98 national Police Chiefs’ Council 84 THE ADVEnT OF nEW TECHnOLOGIES In THE JUSTICE SYSTEM ** Alun Michael, Police and Crime Commissioner for QQ 83–98 South Wales and Joint Lead for Data and Bioethics, Association of Police and Crime Commissioners ** Darryl Preston, Police and Crime Commissioner for QQ 83–98 Cambridgeshire and Peterborough and Joint Lead for Data and Bioethics, Association of Police and Crime Commissioners * David Tucker, Faculty Lead on Crime and Criminal QQ 83–98 Justice, College of Policing. (extracted from366.pdf)
* The Rt Hon Kit Malthouse MP, Minister of State for QQ 99–111 Crime and Policing at the Home Office and Ministry of Justice * Dr Christophe Prince, Director for Data and QQ 99–111 Identity, Home Office Alphabetical list of all witnesses Robin Allen QC, Barristers at A1 Law Consultancy/ nTL0019 Cloisters Chambers Dr Arianna Andreangeli, Senior Lecturer in nTL0038 Competition Law, Edinburgh Law School, nTL0039 University of Edinburgh Dr Philip Avenell, Managing Director and Forensic nTL0024 Biologist at Forensic Access Avon and Somerset Police nTL0052 BAE Systems nTL0056 Professor Melanie Bailey, Professor at University of nTL0024 Surrey The Bar Council nTL0048 Dr Marcin Betkier, Lecturer in Law at Victoria nTL0021 University of Wellington Dr Stephen Bleay, Senior Lecturer in Forensic nTL0024 Science at London South Bank University Katy Bourne OBE, Sussex Police and Crime nTL0045 Commissioner Dr Rebecca Brown, Research Fellow at University of nTL0030 Oxford Professor Dame Vicki Bruce DBE, Professor Emerita nTL0012 at newcastle University Professor A Mike Burton, Professor of Psychology at nTL0012 University of York Professor Liz Campbell, Professor and Francine V nTL0021 Mcniff Chair in Criminal Jurisprudence at Monash University THE ADVEnT OF nEW TECHnOLOGIES In THE JUSTICE SYSTEM 85 ** Silkie Carlo, Director, Big Brother Watch nTL0037 (QQ 52–67) Detective Sergeant Laurence Cartwright, Data nTL0040 Analytics lead at Sussex Police Dr Jiahong Chen, Lecturer in Law at Sheffield Law nTL0035 School, University of Sheffield The Crown Prosecution Service nTL0018 Dr Benjamin Davies, Wellcome Trust Society & nTL0030 Ethics Research Fellow at University of Oxford ** Peter Dawson, Director, The Prison Reform Trust nTL0004 (QQ 52–67) defenddigitalme nTL0044 Dr Delphine Defossez (Lecturer in Law), nTL0022 northumbria University * Professor Sylvie Delacroix, Professor in Law and Ethics at University of Birmingham (QQ 1–24) Professor Thomas Douglas, Professor of Applied nTL0013 Philosophy University of Oxford nTL0030 Archie Drake, Research Associate at Kings College, nTL0011 London Professor Gary Edmond, Professor of Law at UnSW nTL0012 Sydney Professor Lilian Edwards, Professor of Law, nTL0035 Innovation and Society at newcastle Law School, newcastle University Professor Seena Fazel, Professor of Forensic nTL0030 Psychiatry & Wellcome Trust Senior Research Fellow in Clinical Science at University of Oxford Dr Lisa Forsberg, British Academy Postdoctoral nTL0013 Fellow at University of Oxford nTL0030 Professor Simona Francese, Professor of Forensic nTL0024 and Bioanalytical Mass Spectrometry at Sheffield Hallam University Professor Pete Fussey, Professor of Sociology, nTL0017 University of Essex Professor Angela Gallop, Professor of Practice/ nTL0024 Director of Forensic Science at University of Strathclyde/Forensic Access ** Professor Colin Gavaghan, Director new Zealand nTL0047 Law Foundation Centre for Law and Policy in Emerging Technologies at University of Otago (QQ 39–51) 86 THE ADVEnT OF nEW TECHnOLOGIES In THE JUSTICE SYSTEM Dr Jamie Grace, Senior Lecturer in Law at Sheffield nTL0001 Hallam University Professor nigel Harvey, Professor of Judgment and nTL0025 Decision Research at UCL London Tobias Harvey, Student of Law at Kings College nTL0025 London Dr Binesh Hass, Research Fellow at University of nTL0030 Oxford The Home Office nTL0055 Independent Office for Police Conduct nTL0054 The Information Commissioner’s Office (ICO) nTL0016 Istanbul Bar Association nTL0028 * Professor Elizabeth E Joh, Martin Luther King Jr. (extracted from366.pdf)
Professor of Law at University of California, Davis (QQ 39–51) Perry Keller, Reader in Media and Information nTL0011 Law, Director of Doctoral Studies at King’s College London Professor Paul Kelly, Professor of Inorganic nTL0024 Chemistry at University of Loughborough Professor Richard I.Kemp, Professor of Psychology nTL0012 at UnSW Sydney Dr Kyriakos n Kotsoglou, Senior Lecturer in Law, nTL0006 northumbria University/Research Fellow, University nTL0007 of Lausanne The Law Society of England and Wales nTL0023 Dr Christopher Lawless, Associate Professor at nTL0029 Durham University * Dr David Leslie, Ethics Theme Lead at Alan Turing Institute (QQ 25–38) * David Lewis, Former Deputy Chief Constable and former ethics lead nPCC at Dorset Police (QQ 68– 82) Liberty nTL0020 Sjors Ligthart LLM PhD candidate at Tilburg nTL0013 University Dr nessa Lynch, Associate Professor of Law at nTL0021 Victoria University of Wellington * The Rt Hon Kit Malthouse MP, Minister of State for Crime and Policing at the Home Office and Ministry of Justice (QQ 99–111) Stephen Mason, Associate Research Fellow, Institute nTL0002 of Advanced Legal Studies THE ADVEnT OF nEW TECHnOLOGIES In THE JUSTICE SYSTEM 87 Dee Masters, Barristers at A1 Law Consultancy/ nTL0019 Cloisters Chambers Professor Derek McAuley, Director of Horizon nTL0035 Digital Economy Research Institute at University of nottingham ** Professor Carole McCartney, Professor of Law nTL0022 and Criminal Justice at northumbria University (QQ 1–24) medConfidential nTL0050 The Metropolitan Police Service nTL0031 Professor Gerben Meynen, Professor of Forensic nTL0013 Psychiatry and Bioethics at Utrecht University and VU University Amsterdam ** Alun Michael, Police and Crime Commissioner for nTL0049 South Wales and Joint Lead for Data and Bioethics, nTL0057 Association of Police and Crime Commissioners (QQ 83–98) Migrants’ Rights network nTL0042 The Ministry of Justice nTL0053 Abhishek Mishra, Doctoral Student at University of nTL0030 Oxford Dr Brent Mittelstadt of the Oxford Internet Institute nTL0058 (OII) Dr Reuben Moreton, Reli Ltd nTL0026 Professor Ruth Morgan, Professor of Crime and nTL0024 Forensic Sciences at University College London Dr Daragh Murray, Senior Lecturer in Human nTL0017 Rights at University of Essex nCC Group nTL0005 Dr Eilidh noyes, University of Huddersfield nTL0026 * Dr Liam Owens, Founder and Chief Executive Officer, Semantics 21 (QQ 68–82) Ms Angela Paul (PhD candidate in Law), nTL0022 northumbria University Police Scotland nTL0043 Dr Susan Pope, DnA expert—chair of the Forensic nTL0024 Science Regulator DnA Specialist Group & is an assessor for the netherlands Register of Court Experts at Principal Forensic Services ** Darryl Preston, Police and Crime Commissioner for nTL0049 Cambridgeshire and Peterborough and Joint Lead for nTL0057 Data and Bioethics, Association of Police and Crime Commissioners (QQ 83–98) 88 THE ADVEnT OF nEW TECHnOLOGIES In THE JUSTICE SYSTEM Dr Christophe Prince, Director for Data and Identity, Home Office (QQ 99–111) Privacy International nTL0051 Public Law Project nTL0046 nTL0059 Gary Pugh, Forensic Science Regulator nTL0036 Dr Jonathan Pugh, Parfit Radcliffe Senior Research nTL0030 Fellow at University of Oxford Dr Joe Purshouse, Senior Lecturer in Criminal Law nTL0021 and Justice at University of Sheffield ** Professor Charles Raab, Professorial Fellow, nTL0014 School of Social and Political Science, University of Edinburgh Fellow, The Alan Turing Institute (QQ 1–24) Dr Liam Ralph (Lecturer in Criminology and nTL0022 Policing) at the Centre for Crime and Policing & the Science and Justice Research, northumbria University Dr Kay L. (extracted from366.pdf)
Mehera San Roque, UnSW nTL0012 Sydney SAS UK&I nTL0041 Professor Julian Savulescu, Professor of Practical nTL0030 Ethics at University of Oxford Serious Fraud Office nTL0034 Professor Ilina Singh, Professor of neuroscience and nTL0030 Society at University of Oxford David Spreadborough, CFVA, Forensic Analyst at nTL0015 Amped Software Dr Amy Stevens, Senior Research Officer, Human nTL0017 Rights, Big Data and Technology Project at University of Essex Dr Clare Sutherland, Senior Lecturer at University nTL0012 of Aberdeen ** Professor Paul Taylor, Chief Scientific Adviser, nTL0049 national Police Chiefs’ Council (QQ 83–98) nTL0057 Dr Alice Towler, Research Fellow at UnSW Sydney nTL0012 ** David Tucker, Faculty Lead on Crime and Criminal nTL0057 Justice, College of Policing (QQ 83–98) THE ADVEnT OF nEW TECHnOLOGIES In THE JUSTICE SYSTEM 89 Professor Gillian Tully, Professor of Practice for nTL0024 Forensic Science Policy and Regulation at King’s College London UCL Centre for the Forensic Sciences nTL0010 Dr Lachlan Urquhart, Lecturer in Technology nTL0035 Law, and Co-Investigator of the UKRI Trustworthy Autonomous Systems node in Governance and Regulation at School of Law University of Edinburgh * Dr Rosamunde Elise van Brakel, Co-Director, Surveillance Studies network, Associate Professor Tilburg University/Vrije Universiteit Brussel (VUB) (QQ 39–51) ** Professor Sandra Wachter, Associate Professor and nTL0058 Senior Research Fellow at Oxford Internet Institute (OII), University of Oxford (QQ 68–82) Dr Adrian Weller, Principal Research Fellow in nTL0040 Machine Learning at the University of Cambridge Dr David White, Senior Lecturer at UnSW Sydney nTL0012 Dr Matthias Wienroth (Vice-Chancellor’s Senior nTL0022 Fellow in Criminology/Sociology), northumbria University Professor Kim Wolff, Director King’s Forensics at nTL0024 King’s College London * Professor Michael Woodridge, Head of Department of Computer Science, Professor of Computer Science at University of Oxford (QQ 25–38) * Professor Karen Yeung, Interdisciplinary Professorial Fellow in Law, Ethics and Informatics, Birmingham Law School at The University of Birmingham (QQ 52–67) Dr Miri Zilka, Research Associate in Machine nTL0040 Learning at the University of Cambridge 90 THE ADVEnT OF nEW TECHnOLOGIES In THE JUSTICE SYSTEM APPENDIx 3: CALL FOR EVIDENCE Scope of the inquiry The Committee seeks to explore the use of new technologies in the application of the law and the experience of people currently or previously engaged with them. (extracted from366.pdf)
Are safeguards needed to ensure that THE ADVEnT OF nEW TECHnOLOGIES In THE JUSTICE SYSTEM 91 technologies cannot be used to serve purposes incompatible with a democratic society? (extracted from366.pdf)
92 THE ADVEnT OF nEW TECHnOLOGIES In THE JUSTICE SYSTEM APPENDIx 4: ABBREVIATIONS, ACRONYMS AND TECHNICAL TERMS ADM Automated Decision Making AFR Automated Facial Recognition AI Artificial Intelligence Algorithm A series of instructions for performing a calculation or solving a problem, especially with a computer. (extracted from366.pdf)
THE ADVEnT OF nEW TECHnOLOGIES In THE JUSTICE SYSTEM 93 ML Machine Learning nDAS national Data Analytics Solution (a national analytics capability being developed by West Midlands Police in conjunction with the Home Office). (extracted from366.pdf)
25 The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems General Principles as clear and appealing as GDP, but more • The International Panel on Social Progress, inclusive of environmental and social aspects Social Justice, Well-Being and Economic of progress.” Organization. (extracted from400.pdf)
It will be essential that educational institutions inform engineering students about ethics, justice, and human rights, address ethical research and business practices surrounding the development of A/IS, and attend to the responsibility of the technology sector vis-à-vis public interest issues. (extracted from400.pdf)
justice, and the environment. (extracted from400.pdf)
Well-being is only one value in the and influencing emotion, etc.) and society (large mix for adoption, where other values to consider data sets representing aggregate individual would be human rights, respect, privacy, justice, subjective and objective data) is widely available freedom, culture, etc. (extracted from400.pdf)
Issue: Well-being as a value is also distinct from A/IS represents opportunities justice, responsibility, and freedom. (extracted from400.pdf)
Encyclical Letter Laudato Si, and ensure equitable environmental justice. (extracted from400.pdf)
Candidate Recommendation Convene a committee to issue findings on the modalities and potentials already identified in which A/IS makes progress toward stewardship and restoration of natural systems; trends in the A/IS field that represent threats to and opportunities for ecological sustainability and environmental justice; and areas for suggested future innovation and implementation. (extracted from400.pdf)
Algorithmic Impact Assessment tool - Canada.ca Skip to main content Skip to "About government" Language selection Français fr / Gouvernement du Canada Search Search Canada.ca Search Menu Main Menu Jobs and the workplace Immigration and citizenship Travel and tourism Business and industry Benefits Health Taxes Environment and natural resources National security and defence Culture, history and sport Policing, justice and emergencies Transport and infrastructure Canada and the world Money and finances Science and innovation Manage life events You are here: Canada.ca About government Government in a digital age Digital government innovation Responsible use of artificial intelligence in government Algorithmic Impact Assessment tool On this page 1. (extracted from642.html)
Page details Date modified: 2025-06-24 About this site Government of Canada All contacts Departments and agencies About government Themes and topics Jobs Immigration and citizenship Travel and tourism Business Benefits Health Taxes Environment and natural resources National security and defence Culture, history and sport Policing, justice and emergencies Transport and infrastructure Canada and the world Money and finances Science and innovation Indigenous Peoples Veterans and military Youth Manage life events Government of Canada Corporate Social media Mobile applications About Canada.ca Terms and conditions Privacy (extracted from642.html)
Accept additional cookies Reject additional cookies View cookies Hide cookie message Skip to main content GOV.UK Navigation menu Menu Menu Services and information Benefits Births, death, marriages and care Business and self-employed Childcare and parenting Citizenship and living in the UK Crime, justice and the law Disabled people Driving and transport Education and learning Employing people Environment and countryside Housing and local services Money and tax Passports, travel and living abroad Visas and immigration Working, jobs and pensions Government activity Departments Departments, agencies and public bodies News News stories, speeches, letters and notices Guidance and regulation Detailed guidance, regulations and rules Research and statistics Reports, analysis and official statistics Policy papers and consultations Consultations and strategy Transparency Data, Freedom of Information releases and corporate reports Search GOV.UK × Search GOV.UK Search Search GOV.UK Search Home Business and industry Press release New national innovation centre to put UK at forefront of big data A new £30m National Innovation Centre for Data (NICD) aims to see the next Google or Facebook started in the UK and help the country capitalise on a potential £40bn a year boost to the economy. (extracted from183.html)
Cancel Services and information Benefits Births, death, marriages and care Business and self-employed Childcare and parenting Citizenship and living in the UK Crime, justice and the law Disabled people Driving and transport Education and learning Employing people Environment and countryside Housing and local services Money and tax Passports, travel and living abroad Visas and immigration Working, jobs and pensions Government activity Departments News Guidance and regulation Research and statistics Policy papers and consultations Transparency How government works Get involved Support links Help Privacy Cookies Accessibility statement Contact Terms and conditions Rhestr o Wasanaethau Cymraeg Government Digital Service All content is available under the Open Government Licence v3.0 , except where otherwise stated © Crown copyright (extracted from183.html)
The process of focusing on quantitative data buried deep in case law brought about the introduction of a new class of tools called “analytical justice tools”, where the search is no longer focused on retrieving relevant textual information, but on amounts of money (in claims or damages calculations), lengths of prison sentence and the extent of other penalties. (extracted from65.pdf)
The Council of the European Union adopted in 2019 the 2019-2023 Action Plan on European e-Justice, which sets out a list of projects and initiatives (‘actions’) to be implemented as part of the 2019-2023 European e-Justice Strategy. (extracted from65.pdf)
The drafting of a guide on the use of AI by lawyers in the EU was mentioned in the Action Plan under the possible actions to be implemented under ‘Artificial Intelligence for Justice’. (extracted from65.pdf)
Predicting Judicial Decisions from Legal Briefs, and What That Means for Access to Justice’ [2021] Texas Law Review. (extracted from65.pdf)
36 European Commission for the Efficiency of Justice (CEPEJ), ‘Guidelines on Electronic Court Filing (e-Filing) and Digitalisation of Courts’ (9 December 2021) 13 accessed 27 December 2021. (extracted from65.pdf)
A report of the Conseil National des Barreaux calls this the level of “informative justice”.50 But it is not only the name of the court and the date of the decision that can be extracted from a legal text. (extracted from65.pdf)
Through the use of such analytical tools, case law becomes more transparent in terms of quantifiable information, and this is what the Conseil National des Barreaux calls “analytical justice”. (extracted from65.pdf)
Analytical justice focuses on making past cases visible to users through queries based on figures. (extracted from65.pdf)
In English speaking countries, these tools are often called “predictive justice” tools. (extracted from65.pdf)
In that sense, even ranking of legal texts retrieved from a database in terms of relevancy is a prediction in itself (prediction of relevancy), in contrast to which predictive justice in this context usually means an output on the expected terms of the judgement or the outcome of a court process based on historical data. (extracted from65.pdf)
We do not label these three levels as levels just because level “two” or “three” (analytical or predictive justice) would be more advanced in many ways than level “one” (informative justice). (extracted from65.pdf)
52 We avoid using the term „simulative justice” as the third level, as suggested in the report from the Conseil National des Barreaux already mentioned (ibid 63.), because that term is based on how the working of a specific tool was explained to drafters of that report, and it would be misleading to use the same term for other tools. (extracted from65.pdf)
29 Guide on the use of AI-based tools by lawyers and law firms in the EU because it highlights the historical process of how solutions in informative justice are a prerequisite for analytical justice, and access to quantifiable figures in case law is in turn a prerequisite for any machine learning based divination or dispute resolution algorithms of a predictive nature. (extracted from65.pdf)
However, one has to be mindful that a predictive justice AI tool comes with its own considerable risks and special dangers compared to the other two levels. (extracted from65.pdf)
In informative and analytical justice, interpretation is undertaken by the user (the lawyer), and there is no risk of introducing further bias into the service provided (other than the bias already included in past cases, but that is rarely the responsibility of the lawyer). (extracted from65.pdf)
A predictive justice AI tool comes with its own considerable risks and special dangers compared to the other two levels [informative and analytic justice]. (extracted from65.pdf)
Advanced searching techniques beyond the text: semantic search and argument mining Even what is called “informative justice” in the previous section has a great deal of potential for improvement, and advances in this area may fundamentally change how lawyers work in the future. (extracted from65.pdf)
If we are able to create a reliable representation of argumentation in case law, that could enhance not only informative justice, but also other levels of legal analytics.59 5.3.4. (extracted from65.pdf)
60 ‘ Article 33 - LOI N° 2019-222 Du 23 Mars 2019 de Programmation 2018-2022 et de Réforme Pour La Justice (1) - Légifrance’ accessed 28 December 2021. (extracted from65.pdf)
66 See Fashion ID judgement of the European Court of Justice (ECLI EU:C:2019:629) on a platform provider and the group administrator both considered as a joint controllers. (extracted from65.pdf)
Bibliography ‘Article 33 - LOI N° 2019-222 Du 23 Mars 2019 de Programmation 2018-2022 et de Réforme Pour La Justice - Légifrance’ accessed 28 December 2021 Ashley KD, Artificial Intelligence and Legal Analytics (First, Cambridge University Press 2017) Bommasani R and others, ‘On the Opportunities and Risks of Foundation Models’ [2021] arXiv:2108.07258 [cs] accessed 19 December 2021 Bourne CP and Hahn TB, A History of Online Information Services, 1963-1976 (Cambridge, Mass : MIT Press 2003) accessed 27 December 2021 Chalkidis I and others, ‘Neural Contract Element Extraction Revisited’ (2021) abs/2101.04355 CoRR Choi CQ, ‘7 Revealing Ways AIs Fail’ (IEEE Spectrum, 21 September 2021) accessed 14 December 2021 Debra Cassens Weiss, ‘“Treated like a Robot,” Contract Lawyers Chafe under Fickle Facial Recognition Surveillance’ (ABA Journal, 15 November 2021) accessed 19 December 2021. (extracted from65.pdf)
European Banking Authority, ‘Guidelines on Outsourcing Arrangements’ (5 June 2019) accessed 12 December 2021 European Commission, ‘Communication from the Commission: Artificial Intelligence for Europe’ (2018) accessed 19 November 2021 European Data Protection Board, ‘Guidelines 02/2021 on Virtual Voice Assistants’ (7 July 2021) accessed 14 January 2022 High-Level Expert Group on Artificial Intelligence, ‘Ethics Guidelines for Trustworthy AI’ accessed 12 December 2021 European Commission for the Efficiency of Justice (CEPEJ), ‘Guidelines on Electronic Court Filing (e-Filing) and Digitalisation of Courts’ (9 December 2021) accessed 27 December 2021 Ferrer X and others, ‘Bias and Discrimination in AI: A Cross-Disciplinary Perspective’ (2021) 40 IEEE Technology and Society Magazine 72 accessed 25 February 2022 Gohel P, Singh P and Mohanty M, ‘Explainable AI: Current Status and Future Directions’ [2021] arXiv:2107.07045 [cs] accessed 12 December 2021 Heaven D, ‘Why Deep-Learning AIs Are so Easy to Fool’ (2019) 574 Nature 163 accessed 25 February 2022 Homoki P, ‘Overview on Average State of the Art IT Capabilities and Comparison with Best Practices United Kingdom, USA and Canada’ (Council of European Bars and Law Societies (CCBE), European Lawyers Foundation) Lippi M and others, ‘CLAUDETTE: An Automated Detector of Potentially Unfair Clauses in Online Terms of Service’ (2019) 27 Artificial Intelligence and Law 117 accessed 25 February 2022 Lippi M and Torroni P, ‘Argumentation Mining: State of the Art and Emerging Trends’ (2016) 16 ACM Transactions on Internet Technology 1 accessed 25 February 2022 Lohn AJ, ‘Estimating the Brittleness of AI: Safety Integrity Levels and the Need for Testing Out-Of- Distribution Performance’ [2020] arXiv:2009.00802 [cs, stat] accessed 14 December 2021 ‘Managed by Bots: Surveillance of Gig Economy Workers’ (Privacy International) accessed 19 December 2021 Medvedeva M and others, ‘Automatic Judgement Forecasting for Pending Applications of the European Court of Human Rights’ (2021) accessed 25 February 2022 Mell P and Grance T, ‘The NIST Definition of Cloud Computing’ (National Institute of Standards and Technology 2011) NIST Special Publication (SP) 800-145 accessed 5 December 2021 Pasquale F, New Laws of Robotics: Defending Human Expertise in the Age of AI (The Belknap Press of Harvard University Press 2020) Poudyal P and others, ‘ECHR: Legal Corpus for Argument Mining’, ARGMINING (2020) accessed 25 February 2022 55 Guide on the use of AI-based tools by lawyers and law firms in the EU ‘Proposal for a Regulation of the European Parliament and of the Council Amending Regulation (EU) No 910/2014 as Regards Establishing a Framework for a European Digital Identity’ accessed 21 November 2021 ‘Proposal for a Regulation of the European Parliament and of the Council Laying Down Harmonised Rules on Artificial Intelligence (Artificial Intelligence Act) and Amending Certain Union Legislative Acts’ accessed 12 December 2021 ‘Proposal for a Regulation of the European Parliament and of the Council on Digital Operational Resilience for the Financial Sector and Amending Regulations (EC) No 1060/2009, (EU) No 648/2012, (EU) No 600/2014 and (EU) No 909/2014’ accessed 12 December 2021 ‘Proposal for a Regulation of the European Parliament and of the Council on European Production and Preservation Orders for Electronic Evidence in Criminal Matters’ accessed 5 December 2021 ‘Survivorship Bias’, Wikipedia (2021) accessed 18 December 2021 Themis Solutions Inc., ‘2021 Legal Trends Report Published by Clio’ (August 2021) accessed 30 December 2021 Themis Solutions Inc., ‘Legal Trends Report 2017 Powered By Clio’ (2017) accessed 30 December 2021 Themis Solutions Inc., ‘Legal Trends Report 2018 Powered By Clio’ (2018) accessed 30 December 2021 Tippett EC and others, ‘Does Lawyering Matter? (extracted from65.pdf)
Predicting Judicial Decisions from Legal Briefs, and What That Means for Access to Justice’ [2021] Texas Law Review Tuggener D and others, ‘LEDGAR: A Large-Scale Multi-Label Corpus for Text Classification of Legal Provisions in Contracts’, Proceedings of the 12th Language Resources and Evaluation Conference (European Language Resources Association 2020) accessed 10 October 2021 Vadász P and others, ‘A Report on the Barriers and Opportunities in the Use of Natural Language Processing Tools in Small Legal Offices’ (Council of European Bars and Law Societies, European Lawyers Foundation) accessed 25 February 2022 Vilone G and Longo L, ‘Explainable Artificial Intelligence: A Systematic Review’ [2020] arXiv:2006.00093 [cs] accessed 12 December 2021 Xudong Pan and others, ‘Privacy Risks of General-Purpose Language Models’, 2020 IEEE Symposium on Security and Privacy (SP) (2020) accessed 18 December 2021. (extracted from65.pdf)
In 1890, future Supreme Court Justice Louis Brandeis took the first step in advocating for privacy protection when he co-authored an article with colleague Samuel Warren in the Harvard Law Review advocating “the right to be let alone.” The two argued that the development of “instantaneous photographs” and their circulation by newspapers for commercial gain had created the need to protect people with a new “right to privacy.” Technology today gives a new meaning to “instantaneous photographs” that Brandeis and Warren probably never imagined. (extracted from253.html)
United States , Chief Justice John Roberts wrote for a majority of the court that an individual has a “legitimate expectation of privacy in the record of his physical movements” that are recorded in these cell site records. (extracted from253.html)
Here, proved our everyday lives in the past decades a POT could be the systematic poisoning of in many ways, we have also become increas- the ML algorithm through manipulation of ingly aware of the severe risks it poses to our the training data by a critical mass of ”Robin privacy and social justice. (extracted from560.pdf)
They are less aware that other rights – such as human dignity, access to justice and consumer protection, among others – can also be at risk. (extracted from206.pdf)
ACCESS TO JUSTICE . (extracted from206.pdf)
6 SAFEGUARDING FUNDAMENTAL RIGHTS – SCOPE, IMPACT ASSESSMENTS AND ACCOUNTABILITY Considering the full scope of fundamental rights with respect to AI FRA OPINION 1 Using AI systems engages a wide range of fundamental rights, regardless of the field of application� These When introducing new policies and adopting new legislation on AI, the include – but also go beyond – privacy, data protection, EU legislator and the Member States, non-discrimination and access to justice� acting within the scope of EU law, must ensure that respect for the full The EU Charter of Fundamental Rights (the Charter) spectrum of fundamental rights, as became legally binding in December 2009 and has the enshrined in the Charter and the EU same legal value as the EU treaties. (extracted from206.pdf)
These include, but also go beyond, legal certainty to both AI developers privacy and data protection, non-discrimination and and users� Voluntary schemes access to justice. (extracted from206.pdf)
In – as a basic principle of the rule of addition to rights concerning privacy and data protection, law and a prerequisite for securing equality and non-discrimination, and access to justice, fundamental rights – the legislator has other rights could be considered. (extracted from206.pdf)
However, other rights, such as non-discrimination or access to justice-related rights, are less well known among business representatives who work with AI. (extracted from206.pdf)
NON-DISCRIMINATION, DATA PROTECTION AND ACCESS TO JUSTICE: THREE HORIZONTAL THEMES The research shows that the use of AI affects various fundamental rights. (extracted from206.pdf)
12 Effective access to justice in cases involving AI-based decisions FRA OPINION 6 To effectively contest decisions based on the use of AI, people need to know that AI is used, and how and where The EU legislator and Member States to complain� Organisations using AI need to be able to should ensure effective access to explain their AI system and decisions based on AI� justice for individuals in cases involving AI-based decisions� Access to justice is both a process and a goal, and is crucial To ensure that available remedies are for individuals seeking to benefit from other procedural accessible in practice, the EU legislator and substantive rights. (extracted from206.pdf)
using AI systems to provide those Accordingly, the notion of access to justice obliges states seeking redress information about to guarantee each individual’s right to go to court – or, the operation of their AI systems� in some circumstances, an alternative dispute resolution This includes information on how body – to obtain a remedy if it is found that the individual’s these AI systems arrive at automated rights have been violated. (extracted from206.pdf)
decisions� This obligation would help achieve equality of arms in cases of In accordance with these standards, a victim of a human individuals seeking justice� It would rights violation arising from the development or use of an also support the effectiveness of AI system by a public or private entity has to be provided external monitoring and human with access to remedy before a national authority. (extracted from206.pdf)
In October 2020, the European Parliament adopted resolutions with recommendations to the European Commission on a framework of ethical aspects of AI, robotics and related technologies,28 and a civil liability regime for AI.29 It also adopted a resolution on intellectual property rights for the development of artificial intelligence technologies,30 and continues to work on resolutions on AI in criminal law and its use by the police and judicial authorities in criminal matters,31 and AI in education, culture and the audio-visual sector.32 It also established a special committee on artificial intelligence in the digital age.33 Following their meeting on 1-2 October 2020, the heads of state and government of the EU Member States declared that the “EU needs to be a global leader in the development of secure, trustworthy and ethical Artificial Intelligence” and invited the Commission to “provide a clear, objective definition of high-risk Artificial Intelligence systems.34 In addition, the Council of the EU adopted Conclusions on shaping Europe’s digital future35 and on seizing the opportunities of digitalisation for access to justice, which included a dedicated section on deploying AI systems in the justice sector.36 The German Presidency of the Council of the EU published conclusions on the Charter of Fundamental Rights in the context of artificial intelligence and digital change; the text was supported, or not objected to, by 26 Member States.37 22 The growing reference to fundamental rights in these discussions indicates that a fundamental rights framework alongside other legal frameworks38 is necessary for an effective and human rights compliant evaluation of the many opportunities and challenges brought by new technologies. (extracted from206.pdf)
36 Council of the European Union, Council Conclusions “Access to Justice – Seizing the Opportunities of Digitalisation”, 13 October 2020. (extracted from206.pdf)
This is then combined with other criminal and environmental data.19 fundamental rights The EU and its Member States have shared competence in the area of freedom, considerations in security, and justice (Article 4 (2) (j) of the TFEU). (extracted from206.pdf)
18 The Law Society Commission on the Use of Algorithm in the Justice System (2019), Algorithms in the criminal justice system, p. (extracted from206.pdf)
Interferences with such fundamental rights can only be justified if they respect the requirements of the Charter and of the ECHR, in case of Charter rights corresponding to rights guaranteed in the ECHR (Article 52 (3) of the Charter).36 Pursuant to Article 52 (1) of the Charter, any limitation on fundamental rights must: ― be provided for by law, ― genuinely meet objectives of general interest recognised by the Union or the need to protect the rights and freedoms of others, ― respect the essence of the right, ― be necessary, and ― be proportionate.37 The Court of Justice of the EU (CJEU) has also emphasised that any limitation on the exercise of the rights and freedoms recognised by in the Charter must respect “the essence” of those rights and freedoms.38 This means that fundamental rights can be limited to a certain extent, but not completely disregarded. (extracted from206.pdf)
22 Estonia, National Audit Office and Chancellor of Justice (2018), Everyone’s Rights in e-State: The e-State Charter. (extracted from206.pdf)
(2017), Data Protection Directive (EU) 2016/680 for Police and Criminal Justice Authorities, available at SSRN. (extracted from206.pdf)
Orla Lynskey (2019), Criminal justice profiling and EU data protection law: Precarious protection from predictive policing, p. (extracted from206.pdf)
datasets.***** 74 4�6� ACCESS TO JUSTICE The right to an effective remedy before a tribunal and to a fair trial (Article 47 of the Charter) is one of the most often used Charter right in legal proceedings. (extracted from206.pdf)
Opportunities to successfully complain about the use of AI and challenge decisions based on AI are essential for providing access to justice. (extracted from206.pdf)
Other rights linked to access to justice set out in the Charter are also impacted, most notably by the use of AI in law enforcement. (extracted from206.pdf)
As a result, the requirements for good administration are directly linked to the discussion and analysis above with respect to the legal processing of data (under data protection), fair decisions (linked to the discussion about non-discrimination), alongside transparency and ways to challenge and explain decisions (with respect to access to justice). (extracted from206.pdf)
33 Aleš Završnik (2019), ‘Algorithmic justice: Algorithms and big data in criminal justice settings’, European Journal of Criminology, p. (extracted from206.pdf)
and Castillo C (2019), ‘Why Machine Learning May Lead to Unfairness: Evidence from Risk Assessment for Juvenile Justice in Catalonia’, Best Paper Award, International Conference on AI and Law, 2019; Richardson R., Schultz J. (extracted from206.pdf)
(2019), Dirty Data, Bad Predictions: How Civil Rights Violations Impact Police Data, Predictive Policing Systems, and Justice, 94 N.Y.U. (extracted from206.pdf)
47 Aleš Završnik (2019), ‘Algorithmic justice: Algorithms and big data in criminal justice settings’, European Journal of Criminology, pp. (extracted from206.pdf)
See also: FRA and CoE (2016), Handbook on European law relating to access to justice, Luxembourg, Publications Office, June 2016, p. (extracted from206.pdf)
(eds.) (2016), The Right to Social Security in the Constitutions of the World: Broadening the moral and legal space for social justice, Geneva, ILO Global Study, Vol. (extracted from206.pdf)
Minister for Justice, Equality and Law Reform, Ireland, Attorney General, 8 May 2014, para. (extracted from206.pdf)
Based on a risk management approach, it supports fair automated decisions and minimising unintentional harm to individuals in the field of the criminal justice, higher-education, social media and other areas. (extracted from206.pdf)
For example, in the field of criminal justice, the ALGO CARE framework36 introduced a step-by-step assessment to evaluate the key legal and practical concerns that should be considered in relation to police using algorithmic risk assessment tools. (extracted from206.pdf)
Some examples include: ― the right to social protection, when working with social benefits; ― the right to freedom of expression and information, when using AI to support online content moderation; ― the right of assembly and of association, when considering the use of facial recognition technology in public spaces; ― the right to education, when using AI in the education sector; ― the right to asylum, when using AI to support migration management; ― the right of collective bargaining and action, when using AI in the ‘gig- economy’; ― the right to fair and just working conditions, when using AI at the workplace; ― the right to access preventive health care, when using AI in health services; ― and the right to the presumption of innocence and the right to defence, when using AI in the justice sector or for law-enforcement purposes. (extracted from206.pdf)
 PROMOTING AND PROTECTING YOUR FUNDAMENTAL RIGHTS ACROSS THE EU ― Artificial intelligence (AI) already plays a role in deciding what unemployment benefits someone gets, where a burglary is likely to take place, whether someone is at risk of cancer, or who sees that catchy advertisement for low mortgage rates� Its use keeps growing, presenting seemingly endless possibilities� But we need to make sure to fully uphold fundamental rights standards when using AI� This report presents concrete examples of how companies and public administrations in the EU are using, or trying to use, AI� It focuses on four core areas – social benefits, predictive policing, health services and targeted advertising� The report discusses the potential implications for fundamental rights and analyses how such rights are taken into account when using or developing AI applications� In so doing, it aims to help ensure that the future EU regulatory framework for AI is firmly grounded in respect for human and fundamental rights� Fun E d U a m Ch e a n r t t a e l r R o ig f hts Access to justice Non-discrimination Information society FRA – EUROPEAN UNION AGENCY FOR FUNDAMENTAL RIGHTS Schwarzenbergplatz 11 – 1040 Vienna – Austria TEL. (extracted from206.pdf)
Fairness is discussed through the lens of social justice, highlighting the I STOA | Panel for the Future of Science and Technology potential for algorithmic systems to systematically disadvantage, or even discriminate against, different social groups and demographics. (extracted from261.pdf)
The second step included a review of the types and degrees of impact that algorithmic systems have on social justice, fair decision-making and the associated technological and societal need/limits for algorithmic literacy, transparency, oversight and information symmetry. (extracted from261.pdf)
We discuss fairness through the lens of social justice and highlight the potential for algorithmic systems to systematically disadvantage, or even discriminate against, different social groups and demographics. (extracted from261.pdf)
Therefore, we also understand fairness within the lens of social justice, as opposed to individual cases in which there is a perceived imbalance of goods or penalties ('Why did she get more cookies than me?), an uneven applications of a rule ('You let him throw the ball out of turn'), a case of discrimination based on irrelevant factors that are not subject to rights claims ('You didn’t pick me for the team even though I’m faster than the person you did pick'), etc. (extracted from261.pdf)
Social justice is another complex term with many potential definitions [38, 39, 40, 41, 42]. (extracted from261.pdf)
Discussions of social justice (in academic, policy and public discourses) typically recognise that ensuring a fair distribution is complicated by inherent inequalities in contemporary society; there are various differences of perspective over the extent to which a fair distribution should accommodate for, or attempt to address, such inequalities [44,45]. (extracted from261.pdf)
One high-profile campaigner is Joy Buolamwini, computer scientist at MIT and founder of the Algorithmic Justice League [54]. (extracted from261.pdf)
Algorithm based decision-making in the US criminal justice system In the early 2000s the US criminal justice system began using risk assessments to assist decision- making [91, 92]. (extracted from261.pdf)
In written evidence submitted to the UK government’s inquiry into Algorithms Used in decision-making [101], the Head of Criminal Justice at Durham Constabulary reported that it was too early to make conclusions about the accuracy of HART, but research into it is being conducted in order to support evidence based good practice, and that the results of this research would be made available. (extracted from261.pdf)
● Justice: where citizens feel that algorithms are biased or even discriminatory, this can compromise their feeling that they live in a just society. (extracted from261.pdf)
Each of these values is closely entwined with understandings of fairness and social justice. (extracted from261.pdf)
Various means to achieve fairness and social justice in algorithms have been suggested. (extracted from261.pdf)
[111] considered is the use of algorithmic decision making in the criminal justice system and they note the controversy surrounding the use of COMPAS in the US court system. (extracted from261.pdf)
As Rawls [42] describes, justice encompasses an overall acceptability that existing institutions generate mutual benefit and cooperation in society. (extracted from261.pdf)
An examples of (failed) accountability related co-regulation is the Safe-Harbour Principles for commercial data transfers between the USA and the EU, which was invalidated when the European Court of Justice (ECJ) ruled that the company self-certification practices under Safe- Harbour had failed to provide sufficient privacy safeguards for EU citizens [309, 310]. (extracted from261.pdf)
Accountability Measures for Algorithmic System use by Public Authorities Algorithmic systems are currently being used in government, reshaping how criminal justice systems work via risk assessment algorithms and predictive policing [412, 413], optimizing energy use in critical infrastructure through AI-driven resource allocation [414, 415] and changing government resource allocation and monitoring practices [412, 413]. (extracted from261.pdf)
It also revealed a lack of general knowledge about the systems among the authorities, leading to situations where the students had to explain what ‘criminal justice algorithms’ were to the public servants in charge of providing the records on their use. (extracted from261.pdf)
487], restrictions of due process in criminal justice proceedings [488] and more. (extracted from261.pdf)
The Times' investigation led to broad media coverage and a Department of Justice inquiry into potential criminal behaviour by the company [568]. (extracted from261.pdf)
Justice and care: Essential readings in feminist ethics. (extracted from261.pdf)
Sex and social justice. (extracted from261.pdf)
Unveiling the meaning of social justice in Colombia. (extracted from261.pdf)
Justice as fairness: A restatement. (extracted from261.pdf)
Defining social justice in a socially unjust world. (extracted from261.pdf)
Defining social justice. (extracted from261.pdf)
Value differences underlying public views about social justice. (extracted from261.pdf)
[46] Michael Walzer, Spheres of Justice, (NY: Basic Books, 1983) [47] Foster, A. (extracted from261.pdf)
[54] Algorithmic Justice League https://www.ajlunited.org/ Accessed on: 28 September 2018 [55] Puri, R. (extracted from261.pdf)
Algorithms in the Criminal Justice System: Assessing the Use of Risk Assessments in Sentencing. (extracted from261.pdf)
Criminal Justice and Behavior 31(3), 306–323. (extracted from261.pdf)
'Fairness in criminal justice risk assessments: the state of the art.' arXiv preprint arXiv:1703.09207 (2017). (extracted from261.pdf)
[416] Kade Crockford, 'Risk assessment tools in the criminal justice system: inaccurate, unfair, and unjust?,' ACLU of Massachusetts, March 8, 2018, https://privacysos.org/blog/risk-assessment-tools-criminal-justice- system-inaccurate-unfair-unjust [417] Virginia Eubanks, Automating Inequality: How High-Tech Tools Profile, Police, and Punish the Poor, (New York: St. (extracted from261.pdf)
Martin’s Press, 2018); Nazgol Ghandnoosh, Black Lives Matter: Eliminating Racial Inequity in the Criminal Justice System (Washington DC: The Sentencing Project, 2015), http://sentencingproject.org/wp- content/uploads/2015/11/Black-Lives-Matter.pdf [418] Insha Rahman, 'The State of Bail: A Breakthrough Year for Bail Reform,' Vera Institute of Justice, 2017, https://www.vera.org/state-of-justice-reform/2017/bail-pretrial [419] Dillon Reisman, Jason Schultz, Kate Crawford, Meredith Whittaker, 'Algorithmic Impact Assessments: A Practical Framework for Public Agency Accountability', AI Now, April 2018 https://ainowinstitute.org/aiareport2018.pdf [420] Ali Winston, 'Transparency Advocates Win Release of NYPD ‘Predictive Policing’ Documents,' The Intercept, Jan. (extracted from261.pdf)
29, 2017, https://www.axios.com/lawmakers-are-trying-to-understand-how-tech-giants-algorithms-work- 1513307255-b4109efc-9566-4e69-8922-f37d9e829f1f.html [441] Eric Holder, 'Speech at the National Association of Criminal Defense Lawyers 57th Annual Meeting and 13th State Criminal Justice Network Conference' (Philadelphia, PA, Aug. (extracted from261.pdf)
1, 2014), Department of Justice, https://www.justice.gov/opa/speech/attorney-general-eric-holder-speaks-national-association-criminal- defense-lawyers-57th 100 A governance framework for algorithmic accountability and transparency [442] John Fry, Anne Maxwell, Sarah Apere, Paddy McAweeney, Luke McSharry, and Ainhoa Gonza�lez, 'Non- Technical Summaries-Due Care and Attention,' In 34th IAIA Annual Conference, http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.567.8444&rep=rep1&type=pdf. (extracted from261.pdf)
Times, July 29, 2016, https://www.nytimes.com/roomfordebate/2014/08/06/is-big-data-spreading-inequality/big-data- should-be-regulated-by-technological-due-process [461] Wexler, 'Life, Liberty, and Trade Secrets'; Ram, 'Innovating Criminal Justice'. (extracted from261.pdf)
Department of Justice, Office of the Inspector General (2018), https://oig.justice.gov. (extracted from261.pdf)
[475] Rebecca Wexler, 'Life, Liberty, and Trade Secrets: Intellectual Property in the Criminal Justice System,' 70 Stan. (extracted from261.pdf)
Rev., (forthcoming 2018), https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2920883 [476] Natalie Ram, 'Innovating Criminal Justice,' Northwestern L. (extracted from261.pdf)
3, 2017 [568] Mike Isaac, 'Justice Department Expands Its Inquiry Into Uber’s Greyball Tool,' The New York Times, Mar. (extracted from261.pdf)
WFEO-CEIT-Big Data and AI Principles in Engineering - 大数据与人工智能工程原则 - WFEO Home Search this website Who we are About us Members President Executive Board Executive Council Board Committees Strategic Documents Partners and Sponsors Becoming a member Membership payment What we do WFEO Academy With the United Nations & other International Organisations WFEO and Small Island Developing States (SIDS) WFEO in Africa Declarations and Statements Events Publications Biennial Reports Awards The Code of Ethics The Code of Practice on Principles of Climate Change Adaptation for Engineers The Code of Practice for Sustainable Development and Environmental Stewardship WFEO in Action World Engineering Day World Engineers Convention 2023 – WEC 2023 50th anniversary celebrations News & Reports Flash-Infos & Newsletters Sustainable Development Goals WFEO Engineering 2030 Plan (PDF) Young Engineers Advancing the UN SDGs (PDF) Goal 1 – No poverty Goal 2 – Zero hunger Goal 3 – Good health and well-being Goal 4 – Quality education Goal 5 – Gender equality Goal 6 – Clean water and sanitation Goal 7 – Affordable and clean energy Goal 8 – Decent work and economic growth Goal 9 – Industry, innovation and infrastructure Goal 10 – Reduced inequalities Goal 11 – Sustainable cities and communities Goal 12 – Responsible consumption and production Goal 13 – Climate action Goal 14 – Life Below Water Goal 15 – Life on land Goal 16 – Peace, justice and strong institutions Goal 17 – Partnerships for the goals Committees & WGs Disaster Risk Management Engineering for Innovative Technologies Engineering and the Environment Energy Young Engineers / Future Leaders Education in Engineering Anti Corruption Women in Engineering Engineering Capacity Building Information and Communication Water Working Group on Engineering and Climate Change WFEO-UN Relations Committee WFEO-CEIT - Big Data and AI Principles in Engineering Overview Themes Chair Sustainable Development Goals Big Data and AI Principles Online Open Courses Members section WFEO-CEIT-Big Data and AI Principles in Engineering – 大数据与人工智能工程原则 Promoting responsible conduct of Big Data and AI innovation and application in Engineering In order to promote responsible conduct of Big Data and Artificial Intelligence (AI) application and innovation in engineering, World Federation of Engineering Organizations (WFEO) has formulated the following 7 Principles and releases it now on the first World Engineering Day Celebration. (extracted from433.html)
Accept additional cookies Reject additional cookies View cookies Hide cookie message Skip to main content GOV.UK Navigation menu Menu Menu Services and information Benefits Births, death, marriages and care Business and self-employed Childcare and parenting Citizenship and living in the UK Crime, justice and the law Disabled people Driving and transport Education and learning Employing people Environment and countryside Housing and local services Money and tax Passports, travel and living abroad Visas and immigration Working, jobs and pensions Government activity Departments Departments, agencies and public bodies News News stories, speeches, letters and notices Guidance and regulation Detailed guidance, regulations and rules Research and statistics Reports, analysis and official statistics Policy papers and consultations Consultations and strategy Transparency Data, Freedom of Information releases and corporate reports Search GOV.UK × Search GOV.UK Search Search GOV.UK Search Home Business and industry Industrial strategy Policy paper Artificial Intelligence Sector Deal A Sector Deal between government and the Artificial Intelligence (AI) sector. (extracted from188.html)
Cancel Services and information Benefits Births, death, marriages and care Business and self-employed Childcare and parenting Citizenship and living in the UK Crime, justice and the law Disabled people Driving and transport Education and learning Employing people Environment and countryside Housing and local services Money and tax Passports, travel and living abroad Visas and immigration Working, jobs and pensions Government activity Departments News Guidance and regulation Research and statistics Policy papers and consultations Transparency How government works Get involved Support links Help Privacy Cookies Accessibility statement Contact Terms and conditions Rhestr o Wasanaethau Cymraeg Government Digital Service All content is available under the Open Government Licence v3.0 , except where otherwise stated © Crown copyright (extracted from188.html)
WHAT IS NEEDED NEXT 32 3.1 From Fairness to Justice 32 3.2 Infrastructural Thinking 33 3.3 Accounting for Hidden Labor in AI Systems 34 3.4 Deeper Interdisciplinarity 36 3.5 Race, Gender and Power in AI 37 3.6 Strategic Litigation and Policy Interventions 39 3.7 Research and Organizing: An Emergent Coalition 40 CONCLUSION 42 ENDNOTES 44 This work is licensed under a C reative Commons Attribution-NoDerivatives 4.0 International License 2 ABOUT THE AI NOW INSTITUTE The AI Now Institute at New York University is an interdisciplinary research institute dedicated to understanding the social implications of AI technologies. (extracted from16.pdf)
AI Now produces interdisciplinary research to help ensure that AI systems are accountable to the communities and contexts they are meant to serve, and that they are applied in ways that promote justice and equity. (extracted from16.pdf)
Our workshop on Immigration, Data, and Automation in the Trump Era, co-hosted with the Brennan Center for Justice and the Center for Privacy and Technology at Georgetown Law, focused on the Trump Administration’s use of data harvesting, predictive analytics, and machine learning to target immigrant communities. (extracted from16.pdf)
Domains like health, education, criminal justice, and welfare all have their own histories, regulatory frameworks, and hazards. (extracted from16.pdf)
Facial recognition technology poses its own dangers, reinforcing skewed and potentially discriminatory practices, from criminal justice to education to employment, and presents risks to human rights and civil liberties in multiple countries. (extracted from16.pdf)
Yet, without a framework that accounts for social and political contexts and histories, these mathematical formulas for fairness will almost inevitably miss key factors, and can serve to paper over deeper problems in ways that ultimately increase harm or ignore justice. (extracted from16.pdf)
Broadening perspectives and expanding research into AI fairness and bias beyond the merely mathematical is critical to ensuring we are capable of addressing the core issues and moving the focus from parity to justice. (extracted from16.pdf)
Expanding AI fairness research beyond a focus on mathematical parity and statistical fairness toward issues of justice 2. (extracted from16.pdf)
9 INTRODUCTION The Social Challenges of AI in 2018 The past year has seen accelerated integration of powerful artificial intelligence systems into core social institutions, against a backdrop of rising inequality, political populism, and industry scandals.1 There have been major movements from both inside and outside technology companies pushing for greater accountability and justice. (extracted from16.pdf)
Facial recognition amplifies civil rights concerns Concerns are intensifying that facial recognition increases racial discrimination and other biases in the criminal justice system. (extracted from16.pdf)
Moreover, the false positives disproportionately occurred among non-white members of Congress, with an error rate of nearly 40% compared to only 5% for white members.5 2 Such results echo a string of findings that have demonstrated that facial recognition technology is, on average, better at detecting light-skinned people than dark-skinned people, and better at detecting men than women.5 3 In its response to the ACLU, Amazon acknowledged that “the Rekognition results can be significantly skewed by using a facial database that is not appropriately representative.”5 4 Given the deep and historical racial biases in the criminal justice system, most law enforcement databases are unlikely to be “appropriately representative.”5 5 Despite these serious flaws, ongoing pressure from civil rights groups, and protests from Amazon employees over the potential for misuse of these technologies, Amazon Web Services CEO Andrew Jassy recently told employees that “we feel really great and really strongly about the value that Amazon Rekognition is providing our customers of all sizes and all types of industries in law enforcement and out of law enforcement.”5 6 Nor is Amazon alone in implementing facial recognition technologies in unaccountable ways. (extracted from16.pdf)
17 1.2 The Risks of Automated Decision Systems in Government Over the past year, we have seen a substantial increase in the adoption of Automated Decision Systems (ADS) across government domains, including criminal justice, child welfare, education, and immigration. (extracted from16.pdf)
For years, criminal justice advocates and researchers have pushed for the elimination of cash bail, which has been shown to disproportionately harm individuals based on race and socioeconomic status while at the same time failing to enhance public safety.8 9 In response, New Jersey and California recently passed legislation aimed at addressing this concern. (extracted from16.pdf)
However, instead of simply ending cash bail, they replaced it with a pretrial assessment system designed to algorithmically generate “risk” scores that claim to predict whether a person should go free or be detained in jail while awaiting trial.9 0 The shift from policies such as cash bail to automated systems and risk assessment scoring is still relatively new, and is proceeding even without substantial research examining the potential to amplify discrimination within the criminal justice system. (extracted from16.pdf)
New Jersey’s law went into effect in 2017, and while the state has experienced a decline in its pretrial population, advocates have expressed worry that racial disparities in the risk 20 assessment system persist.9 1 Similarly, when California’s legislation passed earlier this year, many of the criminal justice advocates who pushed for the end of cash bail, and supported an earlier version of the bill, opposed its final version due to the risk assessment requirement.9 2 Education policy is also feeling the impact of automated decision systems. (extracted from16.pdf)
The National Association for the Advancement of Colored People (NAACP) and the Lawyers’ Committee for Civil Rights and 21 Economic Justice opposed the plan because of the school district’s failure to appreciate that parents of color and lower-income parents often rely on jobs that lack work schedule flexibility and may not be able to afford additional child care.9 9 These failed efforts demonstrate two important issues that policymakers must consider when evaluating the use of these systems. (extracted from16.pdf)
3.1 From Fairness to Justice Any debate about bias and fairness should approach issues of power and hierarchy, looking at who is in a position to produce and profit from these systems, whose values are embedded in these systems, who sets their “objective functions,” and which contexts they are intended to work within.1 65 Echoing the Association for Computing Machinery (ACM) researcher’s call for an acknowledgement of “negative implications” as a requirement for peer review, much more attention must be paid to the ways that AI can be used as a tool for exploitation and control.1 66 We must also be cautious not to reframe political questions as technical concerns.1 67 When framed as technical “fixes,” debiasing solutions rarely allow for questions about the appropriateness or efficacy of an AI system altogether, or for an interrogation of the institutional context into which the “fixed” AI system will ultimately be applied. (extracted from16.pdf)
For example, making a facial recognition system perform equally on people with light and dark skin may be a type of technical progress in terms of parity, but if that technology is disproportionately used on people of color and low-income communities, is it really “fair?” This is why definitions of fairness face a hard limit if they remain purely contained within the technical domain: in short, “parity is not justice.”1 69 32 3.2 Infrastructural Thinking In order to better understand and track the complexities of AI systems, we need to look beyond the technology and the hype to account for the broader context of how AI is shaping and shaped by social and material forces. (extracted from16.pdf)
Rafael Reif described it.1 92 Such initiatives are critical: as AI becomes more deeply embedded in areas like healthcare, criminal justice, hiring, housing, and educational systems, experts from these domains are essential if we are to ensure AI works as envisioned. (extracted from16.pdf)
For these important connections to grow, more protections are needed, including a commitment from technology companies to provide protections for conscientious objectors who do not want to work on military or policing contracts, along with protections for employees involved in labor organizing and ethical whistleblowers.2 34 The last year revealed many of the hardest challenges for accountability and justice as AI systems moved deeper into the social world. (extracted from16.pdf)
Natalie Ram, “Innovating Criminal Justice,” N orthwestern University Law Review 112, no. (extracted from16.pdf)
For a more general description of justice as fairness, see: John Rawls, J ustice as Fairness: A Restatement, ed. (extracted from16.pdf)
Ben Green, “‘Fair’ Risk Assessments: A Precarious Approach for Criminal Justice Reform” (5th Workshop on Fairness, Accountability, and Transparency in Machine Learning, Stockholm, 2018), https://scholar.harvard.edu/files/bgreen/files/18-fatml.pdf. (extracted from16.pdf)
162 5.2 Impact on law enforcement and criminal justice ......................................... (extracted from301.pdf)
Christian Archambeau Executive Director EUIPO 7 Acronyms AAAI American Association for the Advancement of Artificial Intelligence ACR Automatic content recognition AGI Artificial general intelligence AI Artificial intelligence A.L.I.C.E Artificial Linguistic Internet Computer Entity ANI Artificial narrow intelligence ASI Artificial superintelligence ASIMO Advanced Step in Innovative Mobility APT Advanced Persistent Threat BEC Business email compromise CaaS Cybercrime-as-a-Service CAPTCHA Completely Automated Public Turing test to tell Computers and Humans Apart CoE Council of Europe CJEU Court of Justice of the European Union DARPA Defense Advanced Research Projects Agency DDoS Distributed denial of service EC European Commission EG Impact of Technology Expert Group EIPPN European Intellectual Property Prosecutors Network ETL ENISA Threat Landscape ENISA European Union Agency for Cybersecurity EU European Union FGCS Fifth-generation computer systems FORTRAN Formula Translator HNN Hopfield neural network IoE Internet of Everything IOCTA Internet Organised Crime Threat Assessment IoT Internet of Things IP Intellectual property IPR Intellectual property right IPTV Internet protocol television 8 ISP Internet service provider LEA Law enforcement agency LSTM Long Short-Term Memory MANIAC I Mathematical Analyzer, Numerical Integrator, and Computer MASP General Multi-Annual Strategic Plan MITI Ministry of International Trade and Industry of Japan MS Member State(s) (of the European Union) NLP Natural language processing OCR Optical character recognition OAP Operational Action Plan PSP Payment service provider SIENA Secure Information Exchange Network Application SP2025 EUIPO Strategic Plan 2025 TLD Top-level domain UBA User behaviour analytics UNICRI United Nations Interregional Crime and Justice Research Institute WIPO World Intellectual Property Organization 9 Definitions The following terminology and definitions will be used in this study. (extracted from301.pdf)
16 STUDY ON THE IMPACT OF ARTIFICIAL INTELLIGENCE ON THE INFRINGEMENT AND ENFORCEMENT OF COPYRIGHT AND DESIGN The ‘Intellectual Property Tech Chain’ In 2021, the EUIPO commissioned the United Nations Interregional Crime and Justice Research Institute (UNICRI) to carry out the first deep-dive research project applying this methodology in cooperation with the Impact of Technology Expert Group. (extracted from301.pdf)
In 2021, the EUIPO commissioned the United Nations Interregional Crime and Justice Research Institute (UNICRI) (15) to carry out the first deep-dive research project applying this methodology in cooperation with the Impact of Technology Expert Group. (extracted from301.pdf)
(23) Trend Micro, the United Nations Interregional Crime and Justice Research Institute (UNICRI), and Europol. (extracted from301.pdf)
(24) Trend Micro, the United Nations Interregional Crime and Justice Research Institute (UNICRI), and Europol. (extracted from301.pdf)
(25) Trend Micro, the United Nations Interregional Crime and Justice Research Institute (UNICRI), and Europol. (extracted from301.pdf)
The present study will focus on AI technologies’ impact on crime, law enforcement and criminal justice, taking into account the relevant ethical and fundamental rights-related issues (including data protection and privacy concerns). (extracted from301.pdf)
Nevertheless, the experts highlighted that use of AI in the justice system also presents many opportunities to figure out how to effectively use it without violating individual privacy and affecting fundamental rights. (extracted from301.pdf)
https://eur-lex.europa.eu/legal-content/EN/TXT/?uri=COM%3A2018%3A237%3AFIN European Commission for the Efficiency of Justice (CEPEJ) (2019). (extracted from301.pdf)
Artificial Intelligence in the Context of Crime and Criminal Justice. (extracted from301.pdf)
Dirty Data, Bad Predictions: How Civil Rights Violations Impact Police Data, Predictive Policing Systems, and Justice. (extracted from301.pdf)
New York University Law 137 STUDY ON THE IMPACT OF ARTIFICIAL INTELLIGENCE ON THE INFRINGEMENT AND ENFORCEMENT OF COPYRIGHT AND DESIGN Review Online, https://www.nyulawreview.org/online-features/dirty-data-bad-predictions-how-civil- rights-violations-impact-police-data-predictive-policing-systems-and-justice/ Russell S., Norvig P. (extracted from301.pdf)
https://www.usenix.org/conference/usenixsecurity16/technical-sessions/presentation/tramer Trend Micro, the United Nations Interregional Crime and Justice Research Institute (UNICRI), and Europol. (extracted from301.pdf)
5 Overview of the main AI-affected areas relevant for the study For the purpose of this study, three areas in which AI technologies have a significant impact are particularly relevant: crime, law enforcement and criminal justice. (extracted from301.pdf)
Artificial Intelligence in the Context of Crime and Criminal Justice A Report For The Korean Institute Of Criminology. (extracted from301.pdf)
163 STUDY ON THE IMPACT OF ARTIFICIAL INTELLIGENCE ON THE INFRINGEMENT AND ENFORCEMENT OF COPYRIGHT AND DESIGN 5.2 Impact on law enforcement and criminal justice The wide range of existing legitimate AI applications includes systems for crime prevention and detection. (extracted from301.pdf)
As AI and related technologies are used to make determinations and predictions in high-stakes domains such as criminal justice and law enforcement, they have the potential to impact basic fundamental rights and liberties in profound ways. (extracted from301.pdf)
Moreover, as is widely discussed at international level, law enforcement authorities and the criminal justice system should ensure fairness, accountability, transparency and that the use of AI is effectively communicated to the public. (extracted from301.pdf)
Experts highlight that the use of AI in the justice system also presents many opportunities to figure out how to effectively use it without violating an individual’s privacy (293). (extracted from301.pdf)
With regards to criminal justice, numerous examples can be found of judicial systems making use of AI tools in criminal proceedings. (extracted from301.pdf)
Some countries also use automated risk assessment tools in the criminal justice system, though their use may be questioned. (extracted from301.pdf)
In this context, in December 2018, the European Commission for the Efficiency of Justice (CEPEJ) of the Council of Europe adopted the ‘Ethical Charter on the Use of Artificial Intelligence in Judicial Systems and their environment’ (296), which encompasses the following five principles: 1. (extracted from301.pdf)
Dirty Data, Bad Predictions: How Civil Rights Violations Impact Police Data, Predictive Policing Systems, and Justice. (extracted from301.pdf)
New York University Law Review Online, https://www.nyulawreview.org/online-features/dirty-data-bad-predictions-how-civil-rights- violations-impact-police-data-predictive-policing-systems-and-justice/ (296) European Commission for the Efficiency of Justice (CEPEJ) (2019). (extracted from301.pdf)
(297) European Commission for the Efficiency of Justice (CEPEJ) (2019). (extracted from301.pdf)
(2017) “Social justice, epidemiology and health inequalities”, European Journal of Epidemiology, available at https://doi.org/10.1007/ s10654-017-0286-3 134. (extracted from329.pdf)
49 Dr Luke Oakden-Rayner, Radiologist and PhD candidate with the School of Public Health at the University of Adelaide Dr Claudia Pagliari, Senior Lecturer in Primary Care and Informatics and Director of Global eHealth at the University of Edinburgh Imogen Parker, Head of Justice, Citizens and Digital Society Programmes at The Nuffield Foundation Dr Ali Parsa, Founder and CEO of Babylon Health Bakul Patel, Associate Center Director for Digital Health at the Food and Drug Administration (FDA) Nicola Perrin, Head of Understanding Patient Data Carol Platt, Innovation Associate at Alder Hey Children’s Hospital Professor Nasir Rajpoot, Professor in Computational Pathology at the Department of Computer Science, University of Warwick Professor Daniel Ray, Director of Data at NHS Digital Professor Geraint Rees, Dean of the UCL Faculty of Life Sciences and Professor of Cognitive Neurology at University College London Dr Travis Rieder, Assistant Director for Education Initiatives, Director of the Master of Bioethics degree program and Research Scholar at the Berman Institute of Bioethics Professor Renato Rocha Souza, Professor at the Applied Mathematics School, Fundação Getulio Vargas Professor Ferdinando Rodriguez y Baena, Professor of Medical Robotics in the Department of Mechanical Engineering at Imperial College London Dr Caroline Rubin, Vice-President for Clinical Radiology at the Royal College of Radiologists and Consultant Radiologist at the University Hospital Southampton NHS Foundation Trust Dr Benedict Rumbold, Research Fellow in the Department of Philosophy at University College London Professor Burkhard Schafer, Professor of Computational Legal Theory at the University of Edinburgh’s School of Law Professor Stefan Schulz, Professor of Medical Informatics at Medical University Graz, Austria Allan Tucker, Senior Lecturer of Computer Science at Brunel University Professor Rhema Vaithianathan, Co-Director of the Centre for Social Data Analytics at the University of Auckland Jenny Westaway, Head of the Office of the National Data Guardian Hugh Whittall, Director of the Nuffield Council on Bioethics John Wilkinson, Director of Devices at the Medicines and Healthcare products Regulatory Agency (MHRA) Professor Stephen Wilkinson, Professor of Bioethics 50 ETHICAL, SOCIAL, AND POLITICAL CHALLENGES OF ARTIFICIAL INTELLIGENCE IN HEALTH D: Patients and members of the public who contributed to this report Alex Brownrigg Mariana Campos Ann Cawley Annabel Dawson Ruth Day Eric Deeson Fran Husson Elaine Manna John Marsh Richard Melville Ballerand Dave McCormick Kath Pollock Bob Ruane Edward Sherley-Price Chris Warner Marney Williams ETHICAL, SOCIAL, AND POLITICAL CHALLENGES OF ARTIFICIAL INTELLIGENCE IN HEALTH 51 E: List of attendees at expert roundtable Professor Richard Ashcroft, Professor of Bioethics at Queen Mary University of London Shirley Cramer CBE, Chief Executive of the Royal Society for Public Health Professor Bobbie Farsides, Professor of Professor of Clinical and Biomedical Ethics at the University of Sussex Professor John Fox, Professor at the Department of Engineering Science at the University of Oxford Professor Nina Hallowell, Associate Professor at the Nuffield Department of Public Health, University of Oxford Dr Hugh Harvey, Clinical Lead for Kheiron Medical and Royal College of Radiologists Informatics Committee Member Eleonora Harwich, Head of Digital and Technological Innovation at Reform Dr Geraint Lewis, Chief Data Officer at NHS England and an Honorary Clinical Senior Lecturer at University College London Maxine Mackintosh, PhD candidate at University College London’s Farr Institute of Health Informatics and co-founder of One HealthTech Dr Benedict Rumbold, Research Fellow in the Department of Philosophy at University College London Professor Ilina Singh, Professor of Neuroscience & Society at the Department of Psychiatry at the University of Oxford and Co-Director of the Wellcome Trust Centre for Ethics Dr Nicola Strickland, President of the Royal College of Radiologists and Consultant Radiologist at the Imperial College Healthcare NHS Trust Professor Stephen Wilkinson, Professor of Bioethics 52 ETHICAL, SOCIAL, AND POLITICAL CHALLENGES OF ARTIFICIAL INTELLIGENCE IN HEALTH F: Methodology by which patient/public contributors were recruited Patients and members of the public that were interviewed or that participated in our roundtable on the 22nd February 2018 were recruited via one of two methods. (extracted from329.pdf)
Accept additional cookies Reject additional cookies View cookies Hide cookie message Skip to main content GOV.UK Navigation menu Menu Menu Services and information Benefits Births, death, marriages and care Business and self-employed Childcare and parenting Citizenship and living in the UK Crime, justice and the law Disabled people Driving and transport Education and learning Employing people Environment and countryside Housing and local services Money and tax Passports, travel and living abroad Visas and immigration Working, jobs and pensions Government activity Departments Departments, agencies and public bodies News News stories, speeches, letters and notices Guidance and regulation Detailed guidance, regulations and rules Research and statistics Reports, analysis and official statistics Policy papers and consultations Consultations and strategy Transparency Data, Freedom of Information releases and corporate reports Search GOV.UK × Search GOV.UK Search Search GOV.UK Search Home Health and social care National Health Service Digital and data-driven health and care technology Department of Health & Social Care Guidance A guide to good practice for digital and data-driven health technologies Updated 19 January 2021 Contents Introduction 1. (extracted from649.html)
Cancel Services and information Benefits Births, death, marriages and care Business and self-employed Childcare and parenting Citizenship and living in the UK Crime, justice and the law Disabled people Driving and transport Education and learning Employing people Environment and countryside Housing and local services Money and tax Passports, travel and living abroad Visas and immigration Working, jobs and pensions Government activity Departments News Guidance and regulation Research and statistics Policy papers and consultations Transparency How government works Get involved Support links Help Privacy Cookies Accessibility statement Contact Terms and conditions Rhestr o Wasanaethau Cymraeg Government Digital Service All content is available under the Open Government Licence v3.0 , except where otherwise stated © Crown copyright (extracted from649.html)
Machine learning systems are increasingly being deployed or implemented by public authorities in areas that are fundamental to the exercise and enjoyment of human rights, rule of law, due process, freedom of expression, criminal justice, healthcare, access to social welfare benefits, and housing. (extracted from13.html)
The right to justice is a vital element of international human rights law. (extracted from13.html)
The challenges are particularly acute when machine learning systems that recommend, make or enforce decisions are used within the justice system, the very institutions which are responsible for guaranteeing rights, including the right to access to effective remedy. (extracted from13.html)
b) Act cautiously on the use of machine learning systems in justice sector given the risks to fair trial and litigants’ rights. (extracted from13.html)
They are less aware that other rights – such as human dignity, access to justice and consumer protection, among others – can also be at risk. (extracted from328.pdf)
ACCESS TO JUSTICE . (extracted from328.pdf)
6 SAFEGUARDING FUNDAMENTAL RIGHTS – SCOPE, IMPACT ASSESSMENTS AND ACCOUNTABILITY Considering the full scope of fundamental rights with respect to AI FRA OPINION 1 Using AI systems engages a wide range of fundamental rights, regardless of the field of application� These When introducing new policies and adopting new legislation on AI, the include – but also go beyond – privacy, data protection, EU legislator and the Member States, non-discrimination and access to justice� acting within the scope of EU law, must ensure that respect for the full The EU Charter of Fundamental Rights (the Charter) spectrum of fundamental rights, as became legally binding in December 2009 and has the enshrined in the Charter and the EU same legal value as the EU treaties. (extracted from328.pdf)
These include, but also go beyond, legal certainty to both AI developers privacy and data protection, non-discrimination and and users� Voluntary schemes access to justice. (extracted from328.pdf)
In – as a basic principle of the rule of addition to rights concerning privacy and data protection, law and a prerequisite for securing equality and non-discrimination, and access to justice, fundamental rights – the legislator has other rights could be considered. (extracted from328.pdf)
However, other rights, such as non-discrimination or access to justice-related rights, are less well known among business representatives who work with AI. (extracted from328.pdf)
NON-DISCRIMINATION, DATA PROTECTION AND ACCESS TO JUSTICE: THREE HORIZONTAL THEMES The research shows that the use of AI affects various fundamental rights. (extracted from328.pdf)
12 Effective access to justice in cases involving AI-based decisions FRA OPINION 6 To effectively contest decisions based on the use of AI, people need to know that AI is used, and how and where The EU legislator and Member States to complain� Organisations using AI need to be able to should ensure effective access to explain their AI system and decisions based on AI� justice for individuals in cases involving AI-based decisions� Access to justice is both a process and a goal, and is crucial To ensure that available remedies are for individuals seeking to benefit from other procedural accessible in practice, the EU legislator and substantive rights. (extracted from328.pdf)
using AI systems to provide those Accordingly, the notion of access to justice obliges states seeking redress information about to guarantee each individual’s right to go to court – or, the operation of their AI systems� in some circumstances, an alternative dispute resolution This includes information on how body – to obtain a remedy if it is found that the individual’s these AI systems arrive at automated rights have been violated. (extracted from328.pdf)
decisions� This obligation would help achieve equality of arms in cases of In accordance with these standards, a victim of a human individuals seeking justice� It would rights violation arising from the development or use of an also support the effectiveness of AI system by a public or private entity has to be provided external monitoring and human with access to remedy before a national authority. (extracted from328.pdf)
In October 2020, the European Parliament adopted resolutions with recommendations to the European Commission on a framework of ethical aspects of AI, robotics and related technologies,28 and a civil liability regime for AI.29 It also adopted a resolution on intellectual property rights for the development of artificial intelligence technologies,30 and continues to work on resolutions on AI in criminal law and its use by the police and judicial authorities in criminal matters,31 and AI in education, culture and the audio-visual sector.32 It also established a special committee on artificial intelligence in the digital age.33 Following their meeting on 1-2 October 2020, the heads of state and government of the EU Member States declared that the “EU needs to be a global leader in the development of secure, trustworthy and ethical Artificial Intelligence” and invited the Commission to “provide a clear, objective definition of high-risk Artificial Intelligence systems.34 In addition, the Council of the EU adopted Conclusions on shaping Europe’s digital future35 and on seizing the opportunities of digitalisation for access to justice, which included a dedicated section on deploying AI systems in the justice sector.36 The German Presidency of the Council of the EU published conclusions on the Charter of Fundamental Rights in the context of artificial intelligence and digital change; the text was supported, or not objected to, by 26 Member States.37 22 The growing reference to fundamental rights in these discussions indicates that a fundamental rights framework alongside other legal frameworks38 is necessary for an effective and human rights compliant evaluation of the many opportunities and challenges brought by new technologies. (extracted from328.pdf)
36 Council of the European Union, Council Conclusions “Access to Justice – Seizing the Opportunities of Digitalisation”, 13 October 2020. (extracted from328.pdf)
This is then combined with other criminal and environmental data.19 fundamental rights The EU and its Member States have shared competence in the area of freedom, considerations in security, and justice (Article 4 (2) (j) of the TFEU). (extracted from328.pdf)
18 The Law Society Commission on the Use of Algorithm in the Justice System (2019), Algorithms in the criminal justice system, p. (extracted from328.pdf)
Interferences with such fundamental rights can only be justified if they respect the requirements of the Charter and of the ECHR, in case of Charter rights corresponding to rights guaranteed in the ECHR (Article 52 (3) of the Charter).36 Pursuant to Article 52 (1) of the Charter, any limitation on fundamental rights must: ― be provided for by law, ― genuinely meet objectives of general interest recognised by the Union or the need to protect the rights and freedoms of others, ― respect the essence of the right, ― be necessary, and ― be proportionate.37 The Court of Justice of the EU (CJEU) has also emphasised that any limitation on the exercise of the rights and freedoms recognised by in the Charter must respect “the essence” of those rights and freedoms.38 This means that fundamental rights can be limited to a certain extent, but not completely disregarded. (extracted from328.pdf)
22 Estonia, National Audit Office and Chancellor of Justice (2018), Everyone’s Rights in e-State: The e-State Charter. (extracted from328.pdf)
(2017), Data Protection Directive (EU) 2016/680 for Police and Criminal Justice Authorities, available at SSRN. (extracted from328.pdf)
Orla Lynskey (2019), Criminal justice profiling and EU data protection law: Precarious protection from predictive policing, p. (extracted from328.pdf)
datasets.***** 74 4�6� ACCESS TO JUSTICE The right to an effective remedy before a tribunal and to a fair trial (Article 47 of the Charter) is one of the most often used Charter right in legal proceedings. (extracted from328.pdf)
Opportunities to successfully complain about the use of AI and challenge decisions based on AI are essential for providing access to justice. (extracted from328.pdf)
Other rights linked to access to justice set out in the Charter are also impacted, most notably by the use of AI in law enforcement. (extracted from328.pdf)
As a result, the requirements for good administration are directly linked to the discussion and analysis above with respect to the legal processing of data (under data protection), fair decisions (linked to the discussion about non-discrimination), alongside transparency and ways to challenge and explain decisions (with respect to access to justice). (extracted from328.pdf)
33 Aleš Završnik (2019), ‘Algorithmic justice: Algorithms and big data in criminal justice settings’, European Journal of Criminology, p. (extracted from328.pdf)
and Castillo C (2019), ‘Why Machine Learning May Lead to Unfairness: Evidence from Risk Assessment for Juvenile Justice in Catalonia’, Best Paper Award, International Conference on AI and Law, 2019; Richardson R., Schultz J. (extracted from328.pdf)
(2019), Dirty Data, Bad Predictions: How Civil Rights Violations Impact Police Data, Predictive Policing Systems, and Justice, 94 N.Y.U. (extracted from328.pdf)
47 Aleš Završnik (2019), ‘Algorithmic justice: Algorithms and big data in criminal justice settings’, European Journal of Criminology, pp. (extracted from328.pdf)
See also: FRA and CoE (2016), Handbook on European law relating to access to justice, Luxembourg, Publications Office, June 2016, p. (extracted from328.pdf)
(eds.) (2016), The Right to Social Security in the Constitutions of the World: Broadening the moral and legal space for social justice, Geneva, ILO Global Study, Vol. (extracted from328.pdf)
Minister for Justice, Equality and Law Reform, Ireland, Attorney General, 8 May 2014, para. (extracted from328.pdf)
Based on a risk management approach, it supports fair automated decisions and minimising unintentional harm to individuals in the field of the criminal justice, higher-education, social media and other areas. (extracted from328.pdf)
For example, in the field of criminal justice, the ALGO CARE framework36 introduced a step-by-step assessment to evaluate the key legal and practical concerns that should be considered in relation to police using algorithmic risk assessment tools. (extracted from328.pdf)
Some examples include: ― the right to social protection, when working with social benefits; ― the right to freedom of expression and information, when using AI to support online content moderation; ― the right of assembly and of association, when considering the use of facial recognition technology in public spaces; ― the right to education, when using AI in the education sector; ― the right to asylum, when using AI to support migration management; ― the right of collective bargaining and action, when using AI in the ‘gig- economy’; ― the right to fair and just working conditions, when using AI at the workplace; ― the right to access preventive health care, when using AI in health services; ― and the right to the presumption of innocence and the right to defence, when using AI in the justice sector or for law-enforcement purposes. (extracted from328.pdf)
 PROMOTING AND PROTECTING YOUR FUNDAMENTAL RIGHTS ACROSS THE EU ― Artificial intelligence (AI) already plays a role in deciding what unemployment benefits someone gets, where a burglary is likely to take place, whether someone is at risk of cancer, or who sees that catchy advertisement for low mortgage rates� Its use keeps growing, presenting seemingly endless possibilities� But we need to make sure to fully uphold fundamental rights standards when using AI� This report presents concrete examples of how companies and public administrations in the EU are using, or trying to use, AI� It focuses on four core areas – social benefits, predictive policing, health services and targeted advertising� The report discusses the potential implications for fundamental rights and analyses how such rights are taken into account when using or developing AI applications� In so doing, it aims to help ensure that the future EU regulatory framework for AI is firmly grounded in respect for human and fundamental rights� Fun E d U a m Ch e a n r t t a e l r R o ig f hts Access to justice Non-discrimination Information society FRA – EUROPEAN UNION AGENCY FOR FUNDAMENTAL RIGHTS Schwarzenbergplatz 11 – 1040 Vienna – Austria TEL. (extracted from328.pdf)
The resources in the programme will be used in areas where they are expected to be most effective, such as health, justice, consumer protection and public administration. (extracted from466.pdf)
Norway, represented by the Ministry of Justice and Public Security, participates in this work. (extracted from466.pdf)
The Ministry of Justice and Public Security and the Ministry of Defence have overarching responsibility for following up the National Cyber Security Strategy for Norway. (extracted from466.pdf)
46 Ministries (2019): National Cyber Security Strategy 47 Ministry of Justice and Public Security (2019): National Strategy for Cyber Security Competence 64 Artificial intelligence in law enforcement The Norwegian Police University College and NTNU in Gjøvik are cooperating on a project that examines the use of different forms of artificial intelligence for analysing big data, aimed at detecting, preventing and investigating economic crime. (extracted from466.pdf)
The Foundation funds research that informs social policy, primarily in education, welfare and justice. (extracted from17.pdf)
In addition to the Ada Lovelace Institute, the Foundation is also the founder and co-funder of the Nuffield Council on Bioethics and the Nuffield Family Justice Observatory. (extracted from17.pdf)
View more View less How to cite Citation style EU APA HARVARD VANCOUVER Export format RIS BibTeX EndNote Export Download and languages Close Available languages and formats Download X Available languages and formats English (en) pdf pdfx Publication details Related publications Published: 2021 Corporate author(s): Directorate-General for Justice and Consumers ( European Commission ) , European network of legal experts in gender equality and non-discrimination Personal author(s): Gerards, Janneke ; Xenidis, Raphaële Themes: Fundamental rights Subject: anti-discriminatory measure , equal treatment , fundamental rights , gender equality , job vacancy , labour market , machine learning PDF ISSN ISBN 978-92-76-20746-7 DOI 10.2838/544956 Catalogue number DS-02-20-549-EN-N PDF ISBN 978-92-76-20746-7 DOI 10.2838/544956 Catalogue number DS-02-20-549-EN-N Paper ISSN ISBN 978-92-76-20745-0 DOI 10.2838/77444 Catalogue number DS-02-20-549-EN-C Paper ISBN 978-92-76-20745-0 DOI 10.2838/77444 Catalogue number DS-02-20-549-EN-C Released on EU publications website: 2021-03-10 View more View less This publication is available for download in web format (PDF) and in print-quality format (PDF/X). (extracted from235.html)
Comparing European and Canadian AI Regulation November 2021 COMPARING EUROPEAN AND CANADIAN AI REGULATION CONTENTS INTRODUCTION.............................................................................................................................................................................................1 ABOUT THE AUTHORS................................................................................................................................................................................2 The Chair on Accountable AI in a Global Context......................................................................................................................2 The Law Commission of Ontario .....................................................................................................................................................2 BACKGROUND................................................................................................................................................................................................3 The Growth of AI....................................................................................................................................................................................3 Building Trust and Protecting Human Rights .............................................................................................................................4 Innovation and Safety..........................................................................................................................................................................4 Range of Regulatory Options ...........................................................................................................................................................4 WHAT IS THE STATE OF THE LAW IN CANADA?............................................................................................................................5 THE CANADA ADM DIRECTIVE ............................................................................................................................................................6 Purpose and Objectives.......................................................................................................................................................................6 Scope..........................................................................................................................................................................................................6 Form of Regulation................................................................................................................................................................................8 Risk Assessment ....................................................................................................................................................................................8 Disclosure.................................................................................................................................................................................................9 Bias............................................................................................................................................................................................................10 Due Process and Procedural Fairness ..........................................................................................................................................10 Oversight and Enforcement............................................................................................................................................................11 THE EUROPEAN COMMISSION AI REGULATION PROPOSAL.............................................................................................12 History ....................................................................................................................................................................................................12 Status.......................................................................................................................................................................................................12 Legal Basis..............................................................................................................................................................................................13 Purpose and Objectives....................................................................................................................................................................13 Material Scope......................................................................................................................................................................................14 Definitions..............................................................................................................................................................................................14 Annex I.....................................................................................................................................................................................................15 Technological Neutrality...................................................................................................................................................................15 Territorial Scope...................................................................................................................................................................................15 What Activities and Agencies Does It Apply To?......................................................................................................................16 Limitations and Exclusions...............................................................................................................................................................17 Form of Regulation.............................................................................................................................................................................17 Risk Assessment ..................................................................................................................................................................................18 Disclosure...............................................................................................................................................................................................20 Due Process ..........................................................................................................................................................................................21 Oversight ...............................................................................................................................................................................................21 Amendment: Adaptability and Updating ..................................................................................................................................21 A DEEPER LOOK: COMPARING AND CONTRASTING THE CANADA ADM DIRECTIVE AND EC PROPOSAL .................................................................................................................................................................................22 Regulation in the Canadian Federal State vs the EU and its Member States ................................................................22 Bias............................................................................................................................................................................................................22 Risk Management and Impact Assessment ..............................................................................................................................23 Data Governance.................................................................................................................................................................................23 Policing and Criminal Justice .........................................................................................................................................................24 Enforcement..........................................................................................................................................................................................26 Due Process and Procedural Fairness ..........................................................................................................................................27 Consumer Protection .......................................................................................................................................................................28 COMPARING MODELS: STRENGTHS AND WEAKNESSES ...................................................................................................29 The Canada ADM Directive ............................................................................................................................................................29 The EC Proposal....................................................................................................................................................................................31 FINAL THOUGHTS AND QUESTIONS .............................................................................................................................................34 MORE INFORMATION AND HOW TO GET INVOLVED.............................................................................................................37 ENDNOTES....................................................................................................................................................................................................38 COMPARING EUROPEAN AND CANADIAN AI REGULATION INTRODUCTION Regulation of AI and ADM systems has become a pressing issue in Canada and across the world. (extracted from102.pdf)
Through this work, the LCO promotes access to justice, evidence-based law reform and public debate. (extracted from102.pdf)
This report is part of the LCO’s ongoing AI, ADM and the Justice System project. (extracted from102.pdf)
The first phase of this project brings together policymakers, legal professionals, technologists, NGOs and community members to discuss the development, deployment, regulation and impact of AI and algorithms on access to justice, human rights, and due process. (extracted from102.pdf)
The LCO’s project considers this technology in both the criminal and civil/administrative law justice systems. (extracted from102.pdf)
2 COMPARING EUROPEAN AND CANADIAN AI REGULATION • The Rise and Fall of Algorithms in the American Justice System: Lessons for Canada. (extracted from102.pdf)
• LCO Forum on AI and ADM in the Civil and Administrative Justice System. (extracted from102.pdf)
• LCO Forum on AI in Ontario’s Criminal Justice System (with The Citizen Lab, Criminal Lawyers Association and the International Human Rights Program, Faculty of Law, University of Toronto). (extracted from102.pdf)
• AI, Automated Decision-Making: Impact on Access to Justice and Legal Aid. (extracted from102.pdf)
• AI for Lawyers: A Primer on Artificial Intelligence in Ontario’s Justice System with Element AI and Osgoode Hall Law School. (extracted from102.pdf)
For example, the Canada ADM Directive does not govern: • Systems that support government non-administrative decisions and/or decisions that are not “about a client.” • Systems could be deployed in the criminal justice system or criminal proceedings. (extracted from102.pdf)
verification of authenticity of travel documents); • Administration of justice and democratic processes (e.g. (extracted from102.pdf)
Policing and Criminal Justice AI and automated decision-making systems are used extensively by governments and police services in criminal justice systems around the world. (extracted from102.pdf)
24 A DEEPER LOOK: COMPARING AND CONTRASTING THE CANADA ADM DIRECTIVE AND EC PROPOSAL The LCO’s The Rise and Fall of Algorithms in the American Justice System: Lessons for Canada report discusses the risks of AI and automated decision-making systems in the criminal justice system at length. (extracted from102.pdf)
These risks include, but are not limited to: Charter violations, biased data, the “metrics of fairness”, data transparency and opacity, “data scoring”, algorithmic bias, lack of due process, and a lack of access to justice.67 In the United States, there has been an extraordinary backlash to the use of AI and related tools in American criminal justice. (extracted from102.pdf)
Importantly, American systems were invariably introduced before comprehensive regulation.68 Articles 6 and 7 and Annex III of the European Commission’s proposed AI rules crystalize an emerging international standard of prohibited and high-risk AI systems, including facial recognition systems and systems used by law enforcement and systems used in the administration of justice and democratic processes. (extracted from102.pdf)
Administration of justice and democratic processes: a) AI systems intended to assist a judicial authority in researching and interpreting facts and the law and in applying the law to a concrete set of facts. (extracted from102.pdf)
25 A DEEPER LOOK: COMPARING AND CONTRASTING THE CANADA ADM DIRECTIVE AND EC PROPOSAL In contrast to the EC proposed rules, there is a major gap in regulation of AI and automated decision- making technologies in policing and the criminal justice system: • The Directive’s focus on “administrative decisions” suggests that the Directive may not apply to ADM systems that could be deployed in the criminal justice system or criminal proceedings. (extracted from102.pdf)
As a result, it appears the federal government could adopt predictive policing algorithms, facial recognition technology, and/or automated risk assessments in bail and sentencing proceedings without having to comply with the Directive.69 • The Canada ADM Directive does not pre-emptively identify or prohibit the use of facial recognition technologies, nor it include detailed provisions identifying AI systems in “law enforcement” and the “administration of justice” as being pre-emptively high-risk, and thus subject to more detailed and expansive regulatory requirements. (extracted from102.pdf)
• Most importantly, the Canada ADM Directive does not govern the use of AI or automated decision-making technologies in policing or the administration of justice that may be deployed by governments and public institutions far beyond the reach of the Canada ADM Directive, including provincial governments and police services. (extracted from102.pdf)
• Criminal justice. (extracted from102.pdf)
The Canada ADM Directive does not include AI or automated decision- making systems in the federal criminal justice system. (extracted from102.pdf)
In contrast, the European Commission Proposal include detailed provisions identifying AI systems in “law enforcement” and the “administration of justice” as being pre-emptively high-risk, and thus subject to more detailed and expansive regulatory requirements. (extracted from102.pdf)
The Court of Justice of the EU (CJEU) may rely on the recitals and may wish to give greater prominence to the Charter, as it has often done in digital matters and in the protection of personal data. (extracted from102.pdf)
“High-risk” systems include many systems used in law enforcement and the administration of justice, among others. (extracted from102.pdf)
Nor does the Canada ADM Directive regulate law enforcement or criminal justice AI applications. (extracted from102.pdf)
The LCO can be contacted at: Law Commission of Ontario Osgoode Hall Law School, York University 2032 Ignat Kaneff Building 4700 Keele Street Toronto, Ontario, Canada M3J 1P3 Telephone: (416) 650-8406 Toll-free: (866) 950-8406 Email: LawCommission@lco-cdo.org Web: www.lco-cdo.org Twitter: @LCO_CDO 37 ENDNOTES ENDNOTES 1 Law Commission of Ontario, Regulating AI: Critical for excellence and trust in Artificial Intelligence, Issues and Choices, (2021) [Regulating AI], online: online: https://www.lco-cdo.org/en/our-current- https://ec.europa.eu/commission/press- projects/ai-adm-and-the-justice-system/regula corner/detail/en/ip_21_1682. (extracted from102.pdf)
personal data in the field of police and justice. (extracted from102.pdf)
Civil Rights Principles for the Era of Big Data - The Leadership Conference on Civil and Human Rights Mobile Menu Overlay Home About Our Work The Coalition Take Action Media & Resources Podcast Blog Library Contact Us Switch to: Education Fund Careers Donate Right Arrow An arrow pointing to the right Switch to: Education Fund Careers Donate About Us History Task Forces Board of Directors Staff Careers Internships Voting Record 75th Anniversary Ways to Give Our Work Democracy Census and Data Equity Courts Voting Rights Justice Immigrant Rights Justice Reform Policing Inclusion & Opportunity Center for Civil Rights and Technology Economic Justice Education Equity Fighting Hate & Bias Protect DEIA The Coalition Media & Resources Advocacy Letters Amicus Briefs Beyond 100 Days Biden Timeline Comments Fact Sheets Press Releases Project 2025 Reports Resources Testimony Trump Timeline Take Action Podcast Blog Civil Rights Principles for the Era of Big Data Center for Civil Rights and Technology News 02.27.14 Share Technological progress should bring greater safety, economic opportunity, and convenience to everyone. (extracted from624.html)
At the same time, as new technologies allow companies and government to gain greater insight into our lives, it is vitally important that these technologies be designed and used in ways that respect the values of equal opportunity and equal justice. (extracted from624.html)
This requires disclosure of the underlying data, and the right to correct it when inaccurate.Signatories: American Civil Liberties Union Asian Americans Advancing Justice — AAJC Center for Media Justice ColorOfChange Common Cause Free Press The Leadership Conference on Civil and Human Rights NAACP National Council of La Raza National Hispanic Media Coalition National Urban League NOW Foundation New America Foundation’s Open Technology Institute Public Knowledge Back to Press & Media Civil and Human Rights Coalition Applauds Priorities of Presidential Budget Join the fight for justice, inclusion, and fairness for all. (extracted from624.html)
23In considering the applications of AI in areas such as criminal justice and health care, organizations should design, build and deploy AI systems that leverage human judgment and responsibility where they are needed. (extracted from274.pdf)
Suggested lead: Department of Commerce, Department of State, Department of Justice. (extracted from274.pdf)
Suggested lead: Department of Commerce, Department of State, Department of Justice. (extracted from274.pdf)
Conclusions __________________________________________________________________ 70 VIII AI and digital tools in workplace management and evaluation List of abbreviations AGI Artificial general intelligence AI Artificial intelligence AIaaS AI as a service Regulation of the European Parliament and of the Council laying down harmonised AI act rules on artificial intelligence (Artificial Intelligence act) And amending certain union legislative acts AIDA Special Committee on Artificial Intelligence in a Digital Age BCI Brain-computer interface CDEI Centre for Data Ethics and Innovation Charter EU Charter of Fundamental Rights CIPD Chartered Institute of Personnel and Development CJEU Court of Justice of the European Union CNIL Commission Nationale de l'Informatique et des Libertés DPA Data protection authority DPIA Data Protection Impact Assessment DPO Data protection officer ECHR European Convention on Human Rights ECtHR European Court of Human Rights EDPB European Data Protection Body EDPS European Data Protection Supervisor EESC European Economic and Social Committee ETUC European Trade Union Confederation EU-OSHA European Occupational Safety and Health at Work Authority FLI Future of Life Institute GDPR General Data Protection Regulation HBS Harvard Business School HRM Human resource management ICO Information Commissioner's Office ILO International Labour Organization IP Internet protocol IX STOA | Panel for the Future of Science and Technology ML Machine learning NFC Near-field communication OECD Organisation for Economic Co-operation and Development OSH Occupational safety and health PwC PricewaterhouseCoopers SMS Social media screening TUC Trades Union Congress WEF World Economic Forum X AI and digital tools in workplace management and evaluation 1. (extracted from262.pdf)
To that extent, as the Court of Justice of the European Union (CJEU) ruled, 'an employer that does not allow a worker to exercise his right to paid annual leave must bear the consequences.'72 In no way should an employer be allowed to hide behind an algorithm in this respect. (extracted from262.pdf)
72 European Court of Justice 29 November 2017, Case No. (extracted from262.pdf)
73 European Court of Justice 21 February 2018, Case No. (extracted from262.pdf)
Rudy Matzak; European Court of Justice 9 March 2021, Case No. (extracted from262.pdf)
Radiotelevizija Slovenija; European Court of Justice 9 March 2021, Case No. (extracted from262.pdf)
The purpose of this Act, as explained in Recital 1 of the draft, 'is to improve the functioning of the internal market by laying down a uniform legal framework in particular for the development, marketing and use of 82 'Directive 75/117 on equal pay for men and women must be interpreted as meaning that where an undertaking applies a system of pay which is totally lacking in transparency, it is for the employer to prove that his practice in the matter of wages is not discriminatory, if a female worker establishes, in relation to a relatively large number of employees, that the average pay for women is less than that for men.' European Court of Justice 17 October 1989, Case No. (extracted from262.pdf)
86 'AI is pervasive and will be used in diverse fields – such as consultancy, consumer products and services, mobility, online connectivity, energy production and distribution, police and justice administration –, where EU and MS liability rules are already sector-specific. (extracted from262.pdf)
Van der Mei, Anne Pieter, 'Fixed-Term work: Recent developments in the case law of the Court of Justice of the European Union', European Labour Law Journal, 2020. (extracted from262.pdf)
Vecchione, Briana, Barocas, Solon and Karen Levy, 'Algorithmic Auditing and Social Justice: Lessons from the History of Audit Studies, Equity and Access' in Algorithms, Mechanisms, and Optimization, 2021. (extracted from262.pdf)
Concludes that it is the EU’s responsibility to quickly set up a favourable regulatory environment for AI that provides for swift digital law-making, effective governance and balanced ethical standards, while at the same time preventing overregulation and giving enough leeway for innovation; urges that the adequate development and training of AI will require better access to high-quality data, common standards and incentives for voluntary data sharing; calls on its Committees on Legal Affairs (JURI), Internal PR\1224166EN.docx 33/38 PE680.928v01-00 EN Market and Consumer Protection (IMCO), Industry, Research and Energy (ITRE), Civil Liberties, Justice and Home Affairs (LIBE), and Constitutional Affairs (AFCO) to ensure that these goals are met; 160. (extracted from276.pdf)
European Parliament 2019-2024 Plenary sitting A9-0186/2020 8.10.2020 REPORT with recommendations to the Commission on a framework of ethical aspects of artificial intelligence, robotics and related technologies (2020/2012(INL)) Committee on Legal Affairs Rapporteur: Ibán García del Blanco Rapporteurs for the opinion (*): Urmas Paet, Committee on Foreign Affairs Alexandra Geese, Committee on Internal Market and Consumer Protection Valter Flego, Committee on Transport and Tourism Assita Kanko, Committee on Civil Liberties, Justice and Home Affairs (*) Associated committees – Rule 57 of the Rules of Procedure (Initiative – Rule 47 of the Rules of Procedure) RR\1215422EN.docx PE650.508v02-00 EN EN United in diversity PR_INL CONTENTS Page MOTION FOR A EUROPEAN PARLIAMENT RESOLUTION.............................................3 ANNEX TO THE MOTION FOR A RESOLUTION: DETAILED RECOMMENDATIONS AS TO THE CONTENT OF THE PROPOSAL REQUESTED..............................................33 A. (extracted from289.pdf)
TEXT OF THE LEGISLATIVE PROPOSAL REQUESTED............................................37 EXPLANATORY STATEMENT............................................................................................62 OPINION OF THE COMMITTEE ON FOREIGN AFFAIRS................................................65 OPINION OF THE COMMITTEE ON THE INTERNAL MARKET AND CONSUMER PROTECTION..........................................................................................................................76 OPINION OF THE COMMITTEE ON TRANSPORT AND TOURISM...............................84 OPINION OF THE COMMITTEE ON CIVIL LIBERTIES, JUSTICE AND HOME AFFAIRS..................................................................................................................................91 OPINION OF THE COMMITTEE ON EMPLOYMENT AND SOCIAL AFFAIRS..........100 OPINION OF THE COMMITTEE ON THE ENVIRONMENT, PUBLIC HEALTH AND FOOD SAFETY.....................................................................................................................108 OPINION OF THE COMMITTEE ON CULTURE AND EDUCATION............................123 INFORMATION ON ADOPTION IN COMMITTEE RESPONSIBLE...............................129 FINAL VOTE BY ROLL CALL IN COMMITTEE RESPONSIBLE..................................130 PE650.508v02-00 2/130 RR\1215422EN.docx EN MOTION FOR A EUROPEAN PARLIAMENT RESOLUTION with recommendations to the Commission on a framework of ethical aspects of artificial intelligence, robotics and related technologies (2020/2012(INL)) The European Parliament, – having regard to Article 225 of the Treaty on the Functioning of the European Union, – having regard to Article 114 of the Treaty on the Functioning of the European Union, – having regard to the Charter of Fundamental Rights of the European Union, – having regard to Council Regulation (EU) 2018/1488 of 28 September 2018 establishing the European High Performance Computing Joint Undertaking1, – having regard to Council Directive 2000/43/EC of 29 June 2000 implementing the principle of equal treatment between persons irrespective of racial or ethnic origin2 (Racial Equality Directive), – having regard to Council Directive 2000/78/EC of 27 November 2000 establishing a general framework for equal treatment in employment and occupation3 (Equal Treatment in Employment Directive), – having regard to Regulation (EU) 2016/679 of the European Parliament and of the Council of 27 April 2016 on the protection of natural persons with regard to the processing of personal data and on the free movement of such data, and repealing Directive 95/46/EC (General Data Protection Regulation)4 (GDPR), and to Directive (EU) 2016/680 of the European Parliament and of the Council of 27 April 2016 on the protection of natural persons with regard to the processing of personal data by competent authorities for the purposes of the prevention, investigation, detection or prosecution of criminal offences or the execution of criminal penalties, and on the free movement of such data, and repealing Council Framework Decision 2008/977/JHA5, – having regard to the Interinstitutional Agreement of 13 April 2016 on Better Law- Making6, – having regard to the proposal for a regulation of the European Parliament and of the Council of 6 June 2018 establishing the Digital Europe Programme for the period 2021- 2027 (COM(2018)0434), – having regard to the Communication from the Commission to the European Parliament, 1 OJ L 252, 8.10.2018, p. (extracted from289.pdf)
PE650.508v02-00 4/130 RR\1215422EN.docx EN Rights and Fundamental Freedoms, and the European Charter for Regional or Minority Languages, – having regard to the OECD Council Recommendation on Artificial Intelligence adopted on 22 May 2019, – having regard to Rules 47 and 54 of its Rules of Procedure, – having regard to the opinions of the Committee on Foreign Affairs, the Committee on the Internal Market and Consumer Protection, the Committee on Transport and Tourism, the Committee on Civil Liberties, Justice and Home Affairs, the Committee on Employment and Social Affairs, the Committee on the Environment, Public Health and Food Safety and the Committee on Culture and Education, – having regard to the report of the Committee on Legal Affairs (A9-0186/2020), Introduction A. (extracted from289.pdf)
Considers that technologies which can produce automated decisions, thus replacing decisions taken by public authorities, should be treated with the utmost precaution , notably in the area of justice and law enforcement; 68. (extracted from289.pdf)
https://www.sae.org/standards/content/j3016_201806/ PE650.508v02-00 38/130 RR\1215422EN.docx EN Rights of the European Union (the ‘Charter’), settled case-law of the Court of Justice of the European Union, and other European and international instruments which apply in the Union. (extracted from289.pdf)
Dalunde, Karima Delli, Anna Deparnay-Grunenberg, Tilly Metz 0 - 0 0 Key to symbols: + : in favour - : against 0 : abstention PE650.508v02-00 90/130 RR\1215422EN.docx EN 22.9.2020 OPINION OF THE COMMITTEE ON CIVIL LIBERTIES, JUSTICE AND HOME AFFAIRS for the Committee on Legal Affairs with recommendations to the Commission on the framework of ethical aspects of artificial intelligence, robotics and related technologies (2020/2012(INL)) Rapporteur for opinion (*): Assita Kanko (*) Associated committee – Rule 57 of the Rules of Procedure (Initiative – Rule 47 of the Rules of Procedure) SUGGESTIONS The Committee on Civil Liberties, Justice and Home Affairs calls on the Committee on Legal Affairs, as the committee responsible, to incorporate the following suggestions into its motion for a resolution: – having regard to Articles 2 and 3 of the Treaty on European Union (TEU), – having regard to Articles 10, 19, 21 and 167 of the Treaty on the Functioning of the European Union (TFEU), – having regard to the right to petition enshrined in Articles 20 and 227 of the TFEU and Article 44 of the Charter of Fundamental Rights of the European Union (EUCFR), – having regard to Articles 21 and 22 of the EUCFR, – having regard to the preamble to the TEU, – having regard to the Council of Europe’s Framework Convention for the Protection of National Minorities, Protocol No 12 to the Convention for the Protection of Human Rights and Fundamental Freedoms, and the European Charter for Regional or Minority Languages, – having regard to Council Directive 2000/43/EC of 29 June 2000 implementing the principle of equal treatment between persons irrespective of racial or ethnic origin1 1 OJ L 180, 19.7.2000, p. (extracted from289.pdf)
Accept additional cookies Reject additional cookies View cookies Hide cookie message Skip to main content GOV.UK Navigation menu Menu Menu Services and information Benefits Births, death, marriages and care Business and self-employed Childcare and parenting Citizenship and living in the UK Crime, justice and the law Disabled people Driving and transport Education and learning Employing people Environment and countryside Housing and local services Money and tax Passports, travel and living abroad Visas and immigration Working, jobs and pensions Government activity Departments Departments, agencies and public bodies News News stories, speeches, letters and notices Guidance and regulation Detailed guidance, regulations and rules Research and statistics Reports, analysis and official statistics Policy papers and consultations Consultations and strategy Transparency Data, Freedom of Information releases and corporate reports Search GOV.UK × Search GOV.UK Search Search GOV.UK Search Home Health and social care National Health Service Digital and data-driven health and care technology Department of Health & Social Care Guidance A guide to good practice for digital and data-driven health technologies Updated 19 January 2021 Contents Introduction 1. (extracted from192.html)
Cancel Services and information Benefits Births, death, marriages and care Business and self-employed Childcare and parenting Citizenship and living in the UK Crime, justice and the law Disabled people Driving and transport Education and learning Employing people Environment and countryside Housing and local services Money and tax Passports, travel and living abroad Visas and immigration Working, jobs and pensions Government activity Departments News Guidance and regulation Research and statistics Policy papers and consultations Transparency How government works Get involved Support links Help Privacy Cookies Accessibility statement Contact Terms and conditions Rhestr o Wasanaethau Cymraeg Government Digital Service All content is available under the Open Government Licence v3.0 , except where otherwise stated © Crown copyright (extracted from192.html)
The process of focusing on quantitative data buried deep in case law brought about the introduction of a new class of tools called “analytical justice tools”, where the search is no longer focused on retrieving relevant textual information, but on amounts of money (in claims or damages calculations), lengths of prison sentence and the extent of other penalties. (extracted from114.pdf)
The Council of the European Union adopted in 2019 the 2019-2023 Action Plan on European e-Justice, which sets out a list of projects and initiatives (‘actions’) to be implemented as part of the 2019-2023 European e-Justice Strategy. (extracted from114.pdf)
The drafting of a guide on the use of AI by lawyers in the EU was mentioned in the Action Plan under the possible actions to be implemented under ‘Artificial Intelligence for Justice’. (extracted from114.pdf)
Predicting Judicial Decisions from Legal Briefs, and What That Means for Access to Justice’ [2021] Texas Law Review. (extracted from114.pdf)
36 European Commission for the Efficiency of Justice (CEPEJ), ‘Guidelines on Electronic Court Filing (e-Filing) and Digitalisation of Courts’ (9 December 2021) 13 accessed 27 December 2021. (extracted from114.pdf)
A report of the Conseil National des Barreaux calls this the level of “informative justice”.50 But it is not only the name of the court and the date of the decision that can be extracted from a legal text. (extracted from114.pdf)
Through the use of such analytical tools, case law becomes more transparent in terms of quantifiable information, and this is what the Conseil National des Barreaux calls “analytical justice”. (extracted from114.pdf)
Analytical justice focuses on making past cases visible to users through queries based on figures. (extracted from114.pdf)
In English speaking countries, these tools are often called “predictive justice” tools. (extracted from114.pdf)
In that sense, even ranking of legal texts retrieved from a database in terms of relevancy is a prediction in itself (prediction of relevancy), in contrast to which predictive justice in this context usually means an output on the expected terms of the judgement or the outcome of a court process based on historical data. (extracted from114.pdf)
We do not label these three levels as levels just because level “two” or “three” (analytical or predictive justice) would be more advanced in many ways than level “one” (informative justice). (extracted from114.pdf)
52 We avoid using the term „simulative justice” as the third level, as suggested in the report from the Conseil National des Barreaux already mentioned (ibid 63.), because that term is based on how the working of a specific tool was explained to drafters of that report, and it would be misleading to use the same term for other tools. (extracted from114.pdf)
29 Guide on the use of AI-based tools by lawyers and law firms in the EU because it highlights the historical process of how solutions in informative justice are a prerequisite for analytical justice, and access to quantifiable figures in case law is in turn a prerequisite for any machine learning based divination or dispute resolution algorithms of a predictive nature. (extracted from114.pdf)
However, one has to be mindful that a predictive justice AI tool comes with its own considerable risks and special dangers compared to the other two levels. (extracted from114.pdf)
In informative and analytical justice, interpretation is undertaken by the user (the lawyer), and there is no risk of introducing further bias into the service provided (other than the bias already included in past cases, but that is rarely the responsibility of the lawyer). (extracted from114.pdf)
A predictive justice AI tool comes with its own considerable risks and special dangers compared to the other two levels [informative and analytic justice]. (extracted from114.pdf)
Advanced searching techniques beyond the text: semantic search and argument mining Even what is called “informative justice” in the previous section has a great deal of potential for improvement, and advances in this area may fundamentally change how lawyers work in the future. (extracted from114.pdf)
If we are able to create a reliable representation of argumentation in case law, that could enhance not only informative justice, but also other levels of legal analytics.59 5.3.4. (extracted from114.pdf)
60 ‘ Article 33 - LOI N° 2019-222 Du 23 Mars 2019 de Programmation 2018-2022 et de Réforme Pour La Justice (1) - Légifrance’ accessed 28 December 2021. (extracted from114.pdf)
66 See Fashion ID judgement of the European Court of Justice (ECLI EU:C:2019:629) on a platform provider and the group administrator both considered as a joint controllers. (extracted from114.pdf)
Bibliography ‘Article 33 - LOI N° 2019-222 Du 23 Mars 2019 de Programmation 2018-2022 et de Réforme Pour La Justice - Légifrance’ accessed 28 December 2021 Ashley KD, Artificial Intelligence and Legal Analytics (First, Cambridge University Press 2017) Bommasani R and others, ‘On the Opportunities and Risks of Foundation Models’ [2021] arXiv:2108.07258 [cs] accessed 19 December 2021 Bourne CP and Hahn TB, A History of Online Information Services, 1963-1976 (Cambridge, Mass : MIT Press 2003) accessed 27 December 2021 Chalkidis I and others, ‘Neural Contract Element Extraction Revisited’ (2021) abs/2101.04355 CoRR Choi CQ, ‘7 Revealing Ways AIs Fail’ (IEEE Spectrum, 21 September 2021) accessed 14 December 2021 Debra Cassens Weiss, ‘“Treated like a Robot,” Contract Lawyers Chafe under Fickle Facial Recognition Surveillance’ (ABA Journal, 15 November 2021) accessed 19 December 2021. (extracted from114.pdf)
European Banking Authority, ‘Guidelines on Outsourcing Arrangements’ (5 June 2019) accessed 12 December 2021 European Commission, ‘Communication from the Commission: Artificial Intelligence for Europe’ (2018) accessed 19 November 2021 European Data Protection Board, ‘Guidelines 02/2021 on Virtual Voice Assistants’ (7 July 2021) accessed 14 January 2022 High-Level Expert Group on Artificial Intelligence, ‘Ethics Guidelines for Trustworthy AI’ accessed 12 December 2021 European Commission for the Efficiency of Justice (CEPEJ), ‘Guidelines on Electronic Court Filing (e-Filing) and Digitalisation of Courts’ (9 December 2021) accessed 27 December 2021 Ferrer X and others, ‘Bias and Discrimination in AI: A Cross-Disciplinary Perspective’ (2021) 40 IEEE Technology and Society Magazine 72 accessed 25 February 2022 Gohel P, Singh P and Mohanty M, ‘Explainable AI: Current Status and Future Directions’ [2021] arXiv:2107.07045 [cs] accessed 12 December 2021 Heaven D, ‘Why Deep-Learning AIs Are so Easy to Fool’ (2019) 574 Nature 163 accessed 25 February 2022 Homoki P, ‘Overview on Average State of the Art IT Capabilities and Comparison with Best Practices United Kingdom, USA and Canada’ (Council of European Bars and Law Societies (CCBE), European Lawyers Foundation) Lippi M and others, ‘CLAUDETTE: An Automated Detector of Potentially Unfair Clauses in Online Terms of Service’ (2019) 27 Artificial Intelligence and Law 117 accessed 25 February 2022 Lippi M and Torroni P, ‘Argumentation Mining: State of the Art and Emerging Trends’ (2016) 16 ACM Transactions on Internet Technology 1 accessed 25 February 2022 Lohn AJ, ‘Estimating the Brittleness of AI: Safety Integrity Levels and the Need for Testing Out-Of- Distribution Performance’ [2020] arXiv:2009.00802 [cs, stat] accessed 14 December 2021 ‘Managed by Bots: Surveillance of Gig Economy Workers’ (Privacy International) accessed 19 December 2021 Medvedeva M and others, ‘Automatic Judgement Forecasting for Pending Applications of the European Court of Human Rights’ (2021) accessed 25 February 2022 Mell P and Grance T, ‘The NIST Definition of Cloud Computing’ (National Institute of Standards and Technology 2011) NIST Special Publication (SP) 800-145 accessed 5 December 2021 Pasquale F, New Laws of Robotics: Defending Human Expertise in the Age of AI (The Belknap Press of Harvard University Press 2020) Poudyal P and others, ‘ECHR: Legal Corpus for Argument Mining’, ARGMINING (2020) accessed 25 February 2022 55 Guide on the use of AI-based tools by lawyers and law firms in the EU ‘Proposal for a Regulation of the European Parliament and of the Council Amending Regulation (EU) No 910/2014 as Regards Establishing a Framework for a European Digital Identity’ accessed 21 November 2021 ‘Proposal for a Regulation of the European Parliament and of the Council Laying Down Harmonised Rules on Artificial Intelligence (Artificial Intelligence Act) and Amending Certain Union Legislative Acts’ accessed 12 December 2021 ‘Proposal for a Regulation of the European Parliament and of the Council on Digital Operational Resilience for the Financial Sector and Amending Regulations (EC) No 1060/2009, (EU) No 648/2012, (EU) No 600/2014 and (EU) No 909/2014’ accessed 12 December 2021 ‘Proposal for a Regulation of the European Parliament and of the Council on European Production and Preservation Orders for Electronic Evidence in Criminal Matters’ accessed 5 December 2021 ‘Survivorship Bias’, Wikipedia (2021) accessed 18 December 2021 Themis Solutions Inc., ‘2021 Legal Trends Report Published by Clio’ (August 2021) accessed 30 December 2021 Themis Solutions Inc., ‘Legal Trends Report 2017 Powered By Clio’ (2017) accessed 30 December 2021 Themis Solutions Inc., ‘Legal Trends Report 2018 Powered By Clio’ (2018) accessed 30 December 2021 Tippett EC and others, ‘Does Lawyering Matter? (extracted from114.pdf)
Predicting Judicial Decisions from Legal Briefs, and What That Means for Access to Justice’ [2021] Texas Law Review Tuggener D and others, ‘LEDGAR: A Large-Scale Multi-Label Corpus for Text Classification of Legal Provisions in Contracts’, Proceedings of the 12th Language Resources and Evaluation Conference (European Language Resources Association 2020) accessed 10 October 2021 Vadász P and others, ‘A Report on the Barriers and Opportunities in the Use of Natural Language Processing Tools in Small Legal Offices’ (Council of European Bars and Law Societies, European Lawyers Foundation) accessed 25 February 2022 Vilone G and Longo L, ‘Explainable Artificial Intelligence: A Systematic Review’ [2020] arXiv:2006.00093 [cs] accessed 12 December 2021 Xudong Pan and others, ‘Privacy Risks of General-Purpose Language Models’, 2020 IEEE Symposium on Security and Privacy (SP) (2020) accessed 18 December 2021. (extracted from114.pdf)
7"Commerce, Justice, Science And Related Agencies Appropriations Bill, 2021 - Report Together With Minority Views," House Committee on Appropriations, July 2020, https://appropriations.house.gov/sites/democrats.appropriations.house.gov/files/July%209th%20report%20for%20circulation_0.pdf, 23. (extracted from15.pdf)
(2021) define AIAs as “emerging governance practices for delineating accountability, rendering visible the harms caused by algorithmic systems, and ensuring practical steps are taken to ameliorate those harms.”34Typical sources of riskto be identified include the presence of bias in datasets used to train an AI system, as well as the fairness and explainability of the model; identification of potential impacts can include contextual considerations related to equity and justice, as well as the economic interests, health, and well-being of users or populations potentially affected by the proposed system. (extracted from15.pdf)
60Corporation of the Canadian Civil Liberties Association and Lester Brown v.Toronto Waterfront Revitalization Corporation, et al., (Ontario Superior Court of Justice File No. (extracted from15.pdf)
Accept additional cookies Reject additional cookies View cookies Hide cookie message Skip to main content GOV.UK Navigation menu Menu Menu Services and information Benefits Births, death, marriages and care Business and self-employed Childcare and parenting Citizenship and living in the UK Crime, justice and the law Disabled people Driving and transport Education and learning Employing people Environment and countryside Housing and local services Money and tax Passports, travel and living abroad Visas and immigration Working, jobs and pensions Government activity Departments Departments, agencies and public bodies News News stories, speeches, letters and notices Guidance and regulation Detailed guidance, regulations and rules Research and statistics Reports, analysis and official statistics Policy papers and consultations Consultations and strategy Transparency Data, Freedom of Information releases and corporate reports Search GOV.UK × Search GOV.UK Search Search GOV.UK Search Home Government Public services Guidelines for AI procurement Department for Science, Innovation & Technology Office for Artificial Intelligence Department for Digital, Culture, Media & Sport Department for Business, Energy & Industrial Strategy Guidance Guidelines for AI procurement Published 8 June 2020 Contents Introduction Top 10 Considerations AI-specific considerations within the procurement process 1. (extracted from184.html)
Cancel Services and information Benefits Births, death, marriages and care Business and self-employed Childcare and parenting Citizenship and living in the UK Crime, justice and the law Disabled people Driving and transport Education and learning Employing people Environment and countryside Housing and local services Money and tax Passports, travel and living abroad Visas and immigration Working, jobs and pensions Government activity Departments News Guidance and regulation Research and statistics Policy papers and consultations Transparency How government works Get involved Support links Help Privacy Cookies Accessibility statement Contact Terms and conditions Rhestr o Wasanaethau Cymraeg Government Digital Service All content is available under the Open Government Licence v3.0 , except where otherwise stated © Crown copyright (extracted from184.html)
This list is to the general public, the bar graph shows the non-exhaustive, and many important ethical issues incidence of identified ethical challenges across -- including justice, economic development, poverty 59 AI Principles documents (Figure 8.1b). (extracted from699.pdf)
The chart below indicates the inclusive societies for sustainable development, number of AI use cases in MGI’s library that could provide access to justice for all and build effective, support each of the UN SDGs (Figure 8.3a). (extracted from699.pdf)
of violence, addressing bias to ensure fair and equal A number of use cases that leverage AI support access to justice, to optimizing the management of medical diagnoses: for example, researchers at the public and social sector institutions. (extracted from699.pdf)
List of Organizational Documents Acronym Document Title Document Categorization Issuer COE European ethical Charter on the use of Artificial Intelligence in Official Government/Regulation EUROPEAN COMMISSION judicial systems and their environment FOR THE EFFICIENCY OF JUSTICE (CEPEJ) EUR European Guidelines for Trustworthy AI Official Government/Regulation AI HLEG AUS Artificial Intelligence - Australia’s Ethics Framework Official Government/Regulation Australian Government - Department of Industry, Innovation & Science DUB SMART DUBAI AI ETHICS PRINCIPLES & GUIDELINES Official Government/Regulation Smart Dubai OEC OECD Principles on AI Think Tanks / Policy Institutes OECD G20 G20 Ministerial Statement on Trade and Digital Economy Official Government/Regulation G20 PDP Singapore Personal Data Protection Commission Official Government/Regulation Singapore PDPC DLT AI Ethics: The Next Big Thing In Government Industry & Consultancy Deloitte MEA Work in the age of artificial intelligence. (extracted from699.pdf)
Secretary of State for Business, Energy and 1/6/2018 Industrial Strategy Global Artificial Intelligence and Robotics for Law United Nations Interregional Crime and 1/1/2019 Enforcement Justice Research Institute Global Unprecedented Technological Risks Future of Humanity Institute, Oxford 9/1/2014 University,Centre for the Study of Existential Risk, University of Cambridge Global Artificial Intelligence: The Race Is On The FTI Consulting 2/1/2018 Global Policy Response To AI Topic Concept Graph of AI Strategy Documents Fig. (extracted from699.pdf)
Classification and prediction algorithmshavealready proven to cause real-world harm to those least advantaged, whether it’s in the context of criminal justice or child abuse prevention.AIAsareagreatimmediateintervention,butwithout robust accountability measures, they might fall short ofwhatmostofustrulywant:technology that amplifies—not erodes, human dignity. (extracted from470.pdf)
Other scenarios include healthcare diagnosing and treatment, bail and sentencing in the criminal justice system, policing and arrest-making, grading student exams and assignments,and the list goes on. (extracted from470.pdf)
As for topics, I cover the following: AI bias (especially in recognition and prediction systems), AI governance, policy, regulation, ethical frameworks, power, harms/burdens, inclusive design, exploitation/extraction, surveillance, manipulation, disinformation, social justice, techno-solutionism,diversity, data colonialism. (extracted from470.pdf)
They often deal with questions pertaining to autonomy, beneficence, non-maleficence and justice in an attempt to makethe “right” choice or decision. (extracted from470.pdf)
The #BlackLivesMatter, #MeToo, and modern social justice movements are the contemporary banners under which age-old calls for a less exploitative and more equitable society have historically been cast. (extracted from470.pdf)
Given the recent proliferation of literature at the intersections of critical race theory and Science and Technology Studies (STS), The State of AI Ethics Report, Volume 5 (July 2021) 64 or what Ruha Benjamin (2019) calls race critical code studies, as well as increasing calls for critically conscious approaches to the current use of technology that include concepts like design justice, coded equity, and tools from the tech industry that contribute to, rather than detract from, the liberation of all people, we offer the concept of the CRQC in support of this emerging body of work (Benjamin 2019, Buolamwini et. (extracted from470.pdf)
Some have called this digital justice or coded equity (Benjamin, 2019; Noble,2018).Regardlessofthename,aclearneedexistsfortoolsdesignedto support the development of technologies that contribute to a more equitable,liberatedplanet for all people. (extracted from470.pdf)
This tool helps fill that need by supporting the development of the mindsets necessary to code for equity and justice. (extracted from470.pdf)
While we as authors each hold nuanced perspectives on the specifics of the shared, collective end-goal we have stated, we hopethisconceptgetstakenupasaformoftechnology,equitably coded, as one contribution to the movement towards digital justice. (extracted from470.pdf)
We now turn to our own positionality and its possible influence upon this tool to better understand how our own approach to justice and liberation might influencethis contribution. (extracted from470.pdf)
We contribute to this conversation as people concerned, first and foremost, with the learning anddevelopmentofananti-racist,intersectionalracecriticalframeworkforall.Benjamin(2019) identifies the need for our contribution as educators in the technology space when she says, “Justice…isnotastaticvaluebutanongoingmethodologythatcanandshouldbeincorporated into tech design. (extracted from470.pdf)
For this reason, too, it is vital that people engaged in tech development partner with those who do important sociocultural work honing narrative tools through the arts, humanities, and social justice organizing” (pp. (extracted from470.pdf)
We employ an intersectional feminist definition of anti-racism as that which should be understood when working with theCRQCbecauseweespousesuchworkourselves.Webelieve in a definition of justice that recognizes the baseline work of liberation for all people as being The State of AI Ethics Report, Volume 5 (July 2021) 66 most well-represented by the liberation of trans Black women. (extracted from470.pdf)
For example--at the time of its founding, the United States espoused the underlying claim of “liberty and justice for all” if you were an able-bodied, cisgender, heterosexual man racialized as white. (extracted from470.pdf)
This was seen as radical by many because it was a more open andlessoppressiveideologythanthecolonialpowers--wheresimplybeingan able-bodied, cisgender, heterosexual man of anyethnicitywasnotenoughtomerit“libertyand justice”--one also needed titles, for example. (extracted from470.pdf)
When we as anti-racist human qubits work in alignment with each other, organized toward a shared goal, we set up the conditionsforaprobabilitystring of social justice to unfold. (extracted from470.pdf)
The notion of fractals (maree brown, 2017), which suggests the work of liberation must begin with ourselves and our immediate communities, spreading out through our proximal socialnetworksandthroughoutsocietyfrom there, could be considered the social justice analog to a quantum probability string. (extracted from470.pdf)
(2017), in his “Letter from a Birmingham Jail” named the moderate white middle as perhaps the most dangerous contingent in the US for its surreptitious ability to appear progressive while 5We recognize the importance of local initiativesto establish sustainable consumption and buildingtowards justice. (extracted from470.pdf)
(2020).Design justice: Community-ledpractices to build the worlds we need. (extracted from470.pdf)
Navigating community institutions:Black transgender women’s experiences in schools, the criminal justice system,and churches.Sexuality Research and Social Policy,11(4), 274-287. (extracted from470.pdf)
(2020).# HashtagActivism: Networks of race and gender justice. (extracted from470.pdf)
Kaba, Mariame, and Naomi Murakawa.We Do This 'TilWe Free Us: Abolitionist Organizing and Transforming Justice. (extracted from470.pdf)
“Pods and Pod Mapping Worksheet.” Weblog.BAY AREA TRANSFORMATIVE JUSTICE COLLECTIVE Building Transformative JusticeResponses to Child Sexual Abuse (blog). (extracted from470.pdf)
Bay Area Transformative Justice Collective,June 2016. (extracted from470.pdf)
Intersectionality and kyriarchy:A framework for approaching power and social justice in planning and climate change adaptation.Planning Theory, 14(2), 130-151. (extracted from470.pdf)
2018.Care Work: Dreaming Disability Justice. (extracted from470.pdf)
His work sits at the intersections of critical arts pedagogies, liberatory UX design, education change, pushout re-engagement and social justice. (extracted from470.pdf)
Turner is passionate about issues of diversity, justice, inclusion, and accessibility within society-- particularly in higher education and within STEM employment sectors. (extracted from470.pdf)
As the author points out: having more enabling technical environments that normalize carbonconsiderations in the use of this technology is agreatfirststep.Thiscanhelptointegratewithothereffortson climate justice leveraging the expertise and progressfrom those initiatives. (extracted from470.pdf)
All of these questions were tackled with emphasis on the implicationsfor human rights and social justice. (extracted from470.pdf)
Such red-lined zones include theusesofAIincreditscoring, criminal justice, and the provisionofsocialservices.ItalsoprohibitstheuseofAItomanipulate decisions, behaviours, and opinions of people which would put commercial and political manipulation at risk. (extracted from470.pdf)
Accept additional cookies Reject additional cookies View cookies Hide cookie message Skip to main content GOV.UK Navigation menu Menu Menu Services and information Benefits Births, death, marriages and care Business and self-employed Childcare and parenting Citizenship and living in the UK Crime, justice and the law Disabled people Driving and transport Education and learning Employing people Environment and countryside Housing and local services Money and tax Passports, travel and living abroad Visas and immigration Working, jobs and pensions Government activity Departments Departments, agencies and public bodies News News stories, speeches, letters and notices Guidance and regulation Detailed guidance, regulations and rules Research and statistics Reports, analysis and official statistics Policy papers and consultations Consultations and strategy Transparency Data, Freedom of Information releases and corporate reports Search GOV.UK × Search GOV.UK Search Search GOV.UK Search Home Business and industry Science and innovation Artificial intelligence Collection A guide to using artificial intelligence in the public sector Guidance on building and using artificial intelligence in the public sector. (extracted from350.html)
How DFID used satellite images to estimate populations 10 June 2019 Case study How the Department for Transport used AI to improve MOT testing 10 June 2019 Case study How GDS used machine learning to make GOV.UK more accessible 10 June 2019 Case study How a signalling company used AI to help trains run on time 10 June 2019 Case study Natural language processing for Land Registry documentation in Sweden 10 June 2019 Case study Using data from electricity meters to predict energy consumption 10 June 2019 Case study Using natural language processing to structure market research 10 June 2019 Case study How the Ministry of Justice used AI to compare prison reports 26 June 2019 Case study How a UK-based bank used AI to increase operational efficiency 18 October 2019 Case study Updates to this page Published 10 June 2019 Last updated 18 October 2019 + show all updates 18 October 2019 We have added a new case study to the "Examples of artificial intelligence use" section. (extracted from350.html)
Cancel Services and information Benefits Births, death, marriages and care Business and self-employed Childcare and parenting Citizenship and living in the UK Crime, justice and the law Disabled people Driving and transport Education and learning Employing people Environment and countryside Housing and local services Money and tax Passports, travel and living abroad Visas and immigration Working, jobs and pensions Government activity Departments News Guidance and regulation Research and statistics Policy papers and consultations Transparency How government works Get involved Support links Help Privacy Cookies Accessibility statement Contact Terms and conditions Rhestr o Wasanaethau Cymraeg Government Digital Service All content is available under the Open Government Licence v3.0 , except where otherwise stated © Crown copyright (extracted from350.html)
Responsible use of artificial intelligence in government - Canada.ca Skip to main content Skip to "About government" Language selection Français fr / Gouvernement du Canada Search Search Canada.ca Search Menu Main Menu Jobs and the workplace Immigration and citizenship Travel and tourism Business and industry Benefits Health Taxes Environment and natural resources National security and defence Culture, history and sport Policing, justice and emergencies Transport and infrastructure Canada and the world Money and finances Science and innovation Manage life events You are here: Canada.ca About government Government in a digital age Digital government innovation Responsible use of artificial intelligence in government Artificial intelligence (AI) technologies offer promise for improving how the Government of Canada provides digital services. (extracted from645.html)
Features (Article) Driving forward: navigating the AI landscape in the Government of Canada LinkedIn article by Stephen Burt, Chief Data Officer, Government of Canada (Video) AI procurement for a digital world Overview of the AI procurement process (Video) Algorithmic Impact Assessment Introduction to the Algorithmic Impact Assessment From: Treasury Board of Canada Secretariat Page details Date modified: 2025-03-04 About this site Government of Canada All contacts Departments and agencies About government Themes and topics Jobs Immigration and citizenship Travel and tourism Business Benefits Health Taxes Environment and natural resources National security and defence Culture, history and sport Policing, justice and emergencies Transport and infrastructure Canada and the world Money and finances Science and innovation Indigenous Peoples Veterans and military Youth Manage life events Government of Canada Corporate Social media Mobile applications About Canada.ca Terms and conditions Privacy (extracted from645.html)
View more View less How to cite Citation style EU APA HARVARD VANCOUVER Export format RIS BibTeX EndNote Export Download and languages Close Available languages and formats Download X Available languages and formats English (en) pdf Publication details Related publications Published: 2021 Corporate author(s): Directorate-General for Justice and Consumers ( European Commission ) , EY Themes: Law and justice Subject: artificial intelligence , company law , corporate governance , EU law , EU Member State , European undertaking , report , technological change PDF ISSN ISBN 978-92-76-43364-4 DOI 10.2838/790784 Catalogue number DS-01-21-408-EN-N PDF ISBN 978-92-76-43364-4 DOI 10.2838/790784 Catalogue number DS-01-21-408-EN-N Released on EU publications website: 2021-12-15 View more View less Publication Viewer Document viewer Open web version in a separate window Your browser does not support IFrames. (extracted from215.html)
AI Design and Governance 150 Introduction by Michael Klenk, Assistant Professor, Philosophy, Delft University of Technology 150 Go Deep: Research Summaries 153 Experts Doubt Ethical AI Design Will Be Broadly Adopted as the Norm Within the Next Decade 153 The Logic of Strategic Assets: From Oil to AI 156 Corporate Governance of Artificial Intelligence in the Public Interest 158 AI Certification: Advancing Ethical Practice by Reducing Information Asymmetries 160 Collective Action on Artificial Intelligence: A Primer and Review 161 AI Ethics Maturity Model 164 Mapping value sensitive design onto AI for social good principles 166 Embedding Values in Artificial Intelligence (AI) Systems 169 Moral consideration of nonhumans in the ethics of artificial intelligence 171 Governance of artificial intelligence 174 Avoiding an Oppressive Future of Machine Learning: A Design Theory for Emancipatory Assistants 177 Against Interpretability: a Critical Examination 181 Transparency as design publicity: explaining and justifying inscrutable algorithms 183 Ethics-based auditing of automated decision-making systems: intervention points and policy implications 185 Trustworthiness of Artificial Intelligence 188 Getting from Commitment to Content in AI and Data Ethics: Justice and Explainability 190 Foundations for the future: institution building for the purpose of artificial intelligence governance 194 Go Wide: Article Summaries (summarized by Abhishek Gupta) 198 Five Recommendations For Creating More Ethical AI 198 Police Are Telling ShotSpotter to Alter Evidence From Gunshot-Detecting AI 198 Optimizing People You May Know (PYMK) for equity in network creation 199 AI datasets are prone to mismanagement, study finds 200 Apple says collision in child-abuse hashing system is not a concern 201 Deleting unethical data sets isn’t good enough 201 Six Essential Elements Of A Responsible AI Model 202 Why you should hire a chief AI ethics officer 203 Thinking Through the Ethics of New Tech…Before There’s a Problem 204 7. (extracted from471.pdf)
I teach general concepts and trends in Digital Ethics, which covers a wide range of ways technology impacts individuals (such as privacy, accessibility, financial health and opportunity, mental well-being, personal dignity, and legal status), society (such as health care, education, the economy, criminal justice, and law enforcement), and the environment (such as energy use, material use, waste, pollution, and impact on biodiversity). (extracted from471.pdf)
The team used manual coding to identify unifying themes and came up with 11 of them: transparency (appeared in 87% of the documents), justice and Fairness (81%), non-maleficence (71%), responsibility (71%), Privacy (56%), beneficence (49%), freedom and autonomy (40%), Trust (33%), sustainability (17%), dignity (15%), and solidarity (7%). (extracted from471.pdf)
In searching for unifying themes in AI ethics principles, the authors draw from the four ethical principles commonly used in bioethics: beneficence, non-maleficence, autonomy, and justice. (extracted from471.pdf)
Hagendorff identified eight themes: privacy protection (appeared in 82% of documents), fairness, non-discrimination, justice (82%), accountability (77%), transparency/openness (73%), safety, cyber-security (73%), common good, sustainability, well-being (73%), human oversight, control, auditing (54%), and solidarity, inclusion, social cohesion (50%). (extracted from471.pdf)
Other moral values like autonomy, justice, and respect for people were also noticeably absent. (extracted from471.pdf)
Such values are likely to include democracy, justice, privacy, the protection of human rights, and a commitment to environmental protection.” Thus, while they want to avoid adopting ethical absolutism, the authors also voice the need to avoid the trap of ethical relativism. (extracted from471.pdf)
These are by frequency of the number of sources in which they appeared: transparency, justice, and fairness, non-maleficence, responsibility, privacy, beneficence, freedom and autonomy, trust, dignity, sustainability, and solidarity. (extracted from471.pdf)
● The researchers found that no single ethical principle was found common to the entire corpus of document, however, an emerging convergence was found around the following principles: transparency, justice and fairness, non-maleficence, responsibility, and privacy. (extracted from471.pdf)
It seems that MEDC is shaping this debate, which raises concerns about “neglecting local knowledge, cultural pluralism and global fairness.” ● There is an emergence of a cross-stakeholder convergence on promoting the ethical principles of transparency, justice, non-maleficence, responsibility, and privacy. (extracted from471.pdf)
Some of the other values emphasized in the document include improving human well-being, promoting fairness and justice, protecting privacy and safety, and raising ethical literacy. (extracted from471.pdf)
Madaio, Jennifer Wortman Vaughan, Luke Stark and Hanna Wallach] [Research Summary by Anne Boily] Overview : Among the burgeoning literature on AI ethics and the values that would be important to respect in the development and use of artificial intelligence systems (AIS), fairness comes up a few times, perhaps as an echo of the very current notion of social justice. (extracted from471.pdf)
In this sense, there should be a lack of cheap and subversive techniques to avoid complicated issues like justice, with the social good and social infrastructure over innovation and the good of the governments. (extracted from471.pdf)
In Informania, machine learning systems “optimize on outcomes for millions (or billions) of users, with little regard for individual rights within the collective.” Such a future is becoming more likely, the authors state, pointing to China’s social credit system and the US Justice Department’s COMPAS algorithm. (extracted from471.pdf)
Getting from Commitment to Content in AI and Data Ethics: Justice and Explainability [ Original paper by John Basl, Ronald Sandler and Steven Tiell] [Research Summary by Angshuman Kaushik] Overview : AI or data ethics principles or frameworks meant to demonstrate a commitment to addressing the challenges posed by AI are ubiquitous and are an ‘easy first step’. (extracted from471.pdf)
According to it, much of this complexity arises from three key factors: ● ethical concepts such as justice and transparency that often have many senses and meaning; ● which senses of ethical concepts are operative or appropriate is often contextual; and ● ethical concepts are multidimensional e.g., in terms of what needs to be transparent, to whom, and in what form. (extracted from471.pdf)
The State of AI Ethics Report, Volume 6 (January 2022) 190 Further, the objectives of the report are to: ● demonstrate the importance and complexity of moving from general ethical concepts and principles to action-guiding substantive content, i.e., normative content; ● provide detailed analysis of two widely discussed and interconnected ethical concepts, justice and transparency; and ● indicate strategies for moving from general ethical concepts and principles to more specific normative content and ultimately to operationalizing that content. (extracted from471.pdf)
The researchers then narrow down their focus on the complexities involved in moving from core concepts and principles to operationalization of the normative content for two prominently discussed and interconnected AI and data ethics concepts: justice and transparency. (extracted from471.pdf)
Meaning of justice in AI The report mentions that the concept of justice is a complex one, and can mean different things in different contexts. (extracted from471.pdf)
To determine what justice in AI and data use requires in a particular context, it is imperative to clarify the normative content and underlying values. (extracted from471.pdf)
Only then it is possible to specify what is required in specific cases, and in turn how or to what extent justice can be operationalized in technical systems. (extracted from471.pdf)
According to the report, the general principle of justice is that all people should be equally respected and valued in social, economic and political systems and processes. (extracted from471.pdf)
However, there are many ways this very general principle of justice intersects with social structures and systems. (extracted from471.pdf)
As a result, there is a diverse set of more specific justice-oriented principles such as procedural, distributive and recognition justice. (extracted from471.pdf)
What does committing to justice mean? (extracted from471.pdf)
The researchers consider context to be critically important in determining which justice-oriented principles take precedence. (extracted from471.pdf)
Therefore, the first step in specifying the normative content is to identify the justice-oriented principles that are crucial to the work that the AI system does. (extracted from471.pdf)
Only then can a commitment to justice be effectively put into practice. (extracted from471.pdf)
Articulating the relevant justice-oriented principles will also require considering organizational missions, the types of products and services involved, how those products and services could impact communities and individuals etc. (extracted from471.pdf)
Further, the report states that the diversity of the justice-oriented principles and the need to make context-specific determinations about which are relevant and which to prioritize expose the limits of a strictly algorithmic manner in incorporating justice in AI systems. (extracted from471.pdf)
The reason being, firstly, there is no singular, general justice-oriented constraint, optimization or utility function and secondly, there will not be a strictly algorithmic way to fully incorporate justice into decision-making, even once the relevant justice considerations have been identified. (extracted from471.pdf)
The report then goes on to ask the question as to how and to what extent can the salient aspects of justice be achieved algorithmically. (extracted from471.pdf)
According to the researchers, accomplishing justice in AI will require developing justice-informed, techno-social or human-algorithm systems. (extracted from471.pdf)
According to the researchers, a commitment to justice in AI involves remaining open to the possibility that sometimes an AI-oriented approach might not be a just one. (extracted from471.pdf)
They stress on the fact that, organizations that are committed to justice in AI will require significant organizational capacity and processes to operationalize and implement their commitment, in addition to technical capacity and expertise. (extracted from471.pdf)
Transparency in AI In the view of the researchers, in spite of the role that transparency plays in helping to achieve justice, it can also play an important role in realizing other concepts and values. (extracted from471.pdf)
The State of AI Ethics Report, Volume 6 (January 2022) 207 Risk and unacceptable uses The central operating mechanism of the Act is to look at high-risk AI uses-cases which include biometric identification, critical infrastructure that can significantly impact human lives, determining access to education and employment, worker management, access to private and public services (e.g., finance), law enforcement, migration and immigration, and administration of justice and democratic processes. (extracted from471.pdf)
Each of the approaches come with their own pros and cons, in the case of civil offenses, the burden of proof remains lighter making it perhaps easier to obtain justice but criminal offenses carry a higher punitive burden offering a stronger deterrent. (extracted from471.pdf)
● Promote fairness and justice – It talks about adherence to inclusiveness and inclusiveness, which again does not convey any meaning, whatsoever. (extracted from471.pdf)
The other ethical norms clubbed under this broad heading effectively protect the legitimate rights and interests of all relevant subjects, promote social fairness and justice and equal opportunities. (extracted from471.pdf)
Figures such as Archbishop Desmond Tutu, Professor Gessler Muxe Nkondo and Justice Yvonne Mokgoro have commented on Ubuntu. (extracted from471.pdf)
Also from Council of Europe, on 22 October, 2020, the Parliamentary Assembly of the Council of Europe (PACE) adopted seven reports concerning the impact of AI: the need for democratic governance of AI78 ​; the role of AI in policing and criminal justice systems79 ​; preventing ​ ​ discrimination caused by AI80 ​; ethical and legal frameworks for the research and development ​ 78 See Need for democratic governance of artificial intelligence, available at ​ ​ https://pace.coe.int/en/files/28742/html. (extracted from14.pdf)
​ 79 See Justice by algorithm - the role of artificial intelligence in policing and criminal justice systems, available at ​ ​ https://pace.coe.int/en/files/28723/html. (extracted from14.pdf)
She further noted that: The deaths of George Floyd and countless others have prompted a transnational uprising against systemic racism in law enforcement [...] Part of the human rights response must include greater scrutiny of how the design and use of digital technologies is further entrenching this systemic racism.113 ​ Achiume echoes much recent work on AI governance from a human rights perspective when she proclaims that ensuring racial justice and the protection of human rights will require prohibiting certain applications of AI. (extracted from14.pdf)
It was indicated that the question of red lines is under serious discussion on multiple sides, and that the possibility of a ban remains on the table, especially regarding so-called remote biometric identification systems, and the use of AI in sensitive domains such as criminal justice. (extracted from14.pdf)
​ 44 Europe’s Approach to Artificial Intelligence: How AI Strategy is Evolving We have seen how even simple algorithms, such as that used in the UK’s A-Level grading fiasco, can amplify unfair and discriminatory outcomes and mobilize people to demand justice amid chants of “fuck the algorithm”155 ​. (extracted from14.pdf)
Richard Benjamins, Ana Berenguer, Director General for the Observatorio del impacto social y ético de la President of the Valencian Region inteligencia artificial (ODISEIA) Carlos Castillo, Professor of Computer Science, Nuria Oliver, Commissioner for the President of Universitat Pompeu Fabra, Barcelona the Valencian Region on AI Strategy and Data Lorena Jaume-Palasí, founder and CEO of The Science to fight COVID-19, Spain Ethical Tech Society Amparo Alonso Betanzos, computer scientist and president of the Spanish Association for Brussels roundtable: Speakers Nuria Oliver, Commissioner for the President of Friederike Reinhold, senior policy advisor for the Valencian Region on AI Strategy and Data AlgorithmWatch, Germany Science to fight COVID-19, Spain Veronika Žolnerčíková, CyberSecurity & Krzysztof Izdebski, Policy Director of ePaństwo CyberCrime Center of Excellence at Masaryk Foundation, Poland University (C4E), Co-creator of Czech National Meeri Haataja, CEO & co-founder of Saidot, strategy on AI, Czech Republic Finland 47 Europe’s Approach to Artificial Intelligence: How AI Strategy is Evolving Karma Peiro, Data Journalist & Co-director of CommissionMarcel Kolaja, European the Visualization and Transparency Parliament Vice President and member of the Foundation, Spain Czech Pirate Party Sarah Chander, Senior Policy Advisor at Andreas Hartl, Head of Division, Strategy European Digital Rights (EDRi), Belgium Artificial Intelligence, Data Economy, Hanna Zinner, Artificial Intelligence and Digital Blockchain, Federal Ministry for Economic Industry, DG CNECT, European Affairs and Energy, Germany Audience Jim Dratwa, Team Leader, European Group on Despoina Riga, assistant to MEP Anna-Michelle Ethics, European Commission Assimakopoulou Killian McDonagh Dit, Directorate-General for Natalia Joanna Boniecka, assistant to MEP Justice and Consumers, European Commission Andrzej Halicki Anna Moscibroda, Directorate General for Georgios Theodotou, assistant to MEP Elena Justice and Consumers, European Commission Kountoura Zoi Kardasiadou, Directorate General for Matt Mahmoudi, Researcher/Advisor on Justice and Consumers, European Commission Artificial Intelligence & Human Rights at Aimilia Givropoulou, assistant to MEP Patrick Amnesty International Breyer Ella Jakubowska, Policy & Campaigns on Anne van Heijst, assistant to MEP Liesje van biometrics at European Digital Rights Schreinemacher 48 Europe’s Approach to Artificial Intelligence: How AI Strategy is Evolving Sample Agenda . (extracted from14.pdf)
AI Now produces interdisciplinary research to help ensure that AI systems are accountable to the communities and contexts they are meant to serve, and that they are applied in ways that promote justice and equity. (extracted from28.pdf)
Our most recent publications include: ● Dirty Data, Bad Predictions: How Civil Rights Violations Impact Police Data, Predictive Policing Systems, and Justice,​ an article on how “dirty-policing” practices and policies shape the environment and the methodology by which data is created, raising the risk of creating inaccurate, skewed, or systematically biased “dirty data.” ● Anatomy of an AI System,​ a large-scale map and longform essay produced in partnership with S​ HARE Lab,​ which investigates the human labor, data, and planetary resources required to operate an Amazon Echo. (extracted from28.pdf)
​Advances in understanding of bias, fairness, and justice in machine learning research make it clear that assessments of risks and harms are imperative. (extracted from28.pdf)
This includes examples such as criminal justice advocates working to halt the use of discriminatory predictive policing tools, tenants-rights groups opposing facial recognition in housing, and a coalition of Latinx activists, tech workers, and students exposing and protesting lucrative tech company contracts with military and border agencies. (extracted from28.pdf)
Not only are James’ views counter to Google’s stated values, but they are directly counter to the project of ensuring that the development and application of AI prioritizes justice over profit.”​73​ Following the backlash, Google dissolved ATEAC after a little over a week.7​ 4 Yet even if one believes that corporate AI ethics might help guide better tech practices on some level, it is clear that change in the design, development, and implementation of AI systems largely occurs when there is pressure on companies from workers, the press, and policymakers. (extracted from28.pdf)
Barbara Grosz, a professor of natural sciences, imagines a world in which “every time a computer scientist logs on to write an algorithm or build a system, a message will flash across the screen that asks, ‘Have you thought about the ethical implications of what you’re doing?’”8​ 0​ The Design Justice Network takes this further, centering justice, not ethics, and calling on developers and designers to center affected communities in the process of creating technology together.​81 AI developers and researchers make important determinations that can affect billions of people, and helping them consider whom the technology benefits and harms is important. (extracted from28.pdf)
As during the dot-com boom and foreclosure crisis, numerous organizations and collectives formed to organize for housing justice. (extracted from28.pdf)
Accordingly, housing justice groups such as Social Housing Now (Căsi Sociala Acum) are in the midst of organizing against evictions and for the development of social housing.​97 Back in the North, there have been new forms of international solidarity in the works against AI displacement. (extracted from28.pdf)
Thus they have been organizing marches, Google bus blockades, and City Council demonstrations.​98 ​Much of this has taken place in solidarity with organizers and groups in Berlin such as Google Is Not a Good Neighbor (​Google ist kein guter Nachbar)​, which in 2018 collectively blocked Google from launching a new tech campus in the neighborhood of Kreuzberg.9​ 9​ Solidarity has also been found among New York City organizers who successfully fought the development of a new Amazon campus in 2019, and with activists in Toronto committed to thwarting gentrification induced by Sidewalk Labs.​100 During demonstrations, banners, light projections, video clips, and statements of support have expressed international solidarity, revealing a new trend toward urban justice.1​ 01 ​Much work remains to link struggles against forms of tech-sector displacement worldwide. (extracted from28.pdf)
Because AI technologies are often applied in ways that amplify and exacerbate historical patterns of inequality and discrimination, it is these historical practices—not AI systems alone—to which organizers and communities seeking justice are reacting. (extracted from28.pdf)
In July, Mijente joined Media Justice (an organization at the helm of San Francisco’s facial-recognition ban)​114 ​and Tech Workers Coalition1​ 15 ​to host Take Back Tech. (extracted from28.pdf)
This work helped shed light on lucrative tech company contracts with military and border agencies, and mobilized tech workers and students, while also emphasizing the human cost of a deportation campaign rife with human rights abuses.1​ 18​ Protesters catalyzed by the campaign have held regular demonstrations at Palantir’s headquarters in Palo Alto and at its New York City offices.​119 Organizations such as Never Again Action,​120 ​and Jews for Racial and Economic Justice (JFREJ)​121 ​have also led highly visible actions against Amazon, organizing street protests and sit-ins in Amazon bookstores to protest against the company’s ongoing work providing cloud computing services to ICE.​122​ And Immigrant rights groups such as Make the Road New York,1​ 23 along with Mijente, JFREJ, and other advocates, have reached out to academics and computer science and technology professionals through petitions, demanding that prominent conferences drop Palantir as a sponsor, given the company’s role in empowering ICE.1​ 24 ​Community-organized opposition to Palantir’s role in ICE’s detention of immigrants resulted in UC Berkeley’s Privacy Law Scholars Conference,​125 ​Lesbians Who Tech,1​ 26​ and the Grace Hopper Celebration all pulling Palantir as a sponsor.​127 Athena, a recently launched coalition, takes this further. (extracted from28.pdf)
But they also organized around issues like Amazon’s treatment of warehouse workers and its sale of surveillance tech.1​ 29 ​Athena expands on this multi-issue approach, recognizing that Amazon is at the heart of a set of interlocking issues, including worker rights at warehouses, climate justice, and mass surveillance. (extracted from28.pdf)
The pushback against AI thus builds upon the social justice work that organizers have engaged in for a much longer time. (extracted from28.pdf)
Worker organizing around AI is also part of a broader tech-worker movement focused on a broad range of social justice issues, including displacement,1​ 45​ two-tiered workforces and the exploitation of contract workers,1​ 46​ and climate change. (extracted from28.pdf)
Building on the emergence of globally oriented data protection approaches such as the European Union’s General Data Protection Regulation (GDPR), policymakers are moving quickly, driven both by the current sense of urgency to regulate the mass deployment of AI technologies lacking discernible safeguards and by the failure of ethical frameworks to adequately answer the call for accountability and justice. (extracted from28.pdf)
Addressing the specific case of forensic algorithms like automated software used to analyze DNA and predict potential suspects, the Justice in Forensic Algorithms Act of 2019​246 ​prohibits companies from withholding information about their system, such as its source code, from a defendant in a criminal proceeding on trade-secrecy grounds. (extracted from28.pdf)
The documents showed that the Federal Bureau of Investigation (FBI) and ICE were using state driver’s license databases as “the bedrock of an unprecedented surveillance infrastructure” that relied on facial-recognition technology.2​ 89​ The US Justice Department also recently announced plans to collect DNA data from migrants crossing the border, which could create more invasive monitoring of immigrants without any real limits.​290 Outside the US, governments are equally eager to pilot AI systems at border checkpoints. (extracted from28.pdf)
Though it declined to provide any details on how it is being used by customers, it indicated retail as a potential use case, illustrating how stores can feed live images of shoppers to detect emotional and demographic trends.4​ 01 Employment has also experienced a surge in the use of affect recognition, with companies like HireVue and VCV offering to screen job candidates for qualities like “grit” and to track how often they smile.4​ 02​ Call center programs Cogito and Empath use voice-analysis algorithms to monitor the reactions of customers and signal to call agents when they sound distressed.4​ 03 ​Similar programs have been proposed as an assistive technology for people with autism,4​ 04 ​while Boston-based company BrainCo is creating headbands that purport to detect and quantify students’ attention levels through brain-activity detection,4​ 05 ​despite studies that outline significant risks associated with the deployment of emotional AI in the classroom.​406 Affect-recognition software has also joined risk assessment as a tool in criminal justice. (extracted from28.pdf)
This is particularly concerning in contexts such as employment, education, and criminal justice. (extracted from28.pdf)
Despite the fact that social sciences and humanities approaches have a long history in information security and risk management,​491 ​research that addresses both social and technical dimensions in security is necessary, but still relatively nascent.4​ 92 ​Central in this challenge is redrawing the boundaries of analysis and design to expand beyond the algorithm,4​ 93 ​and securing channels for all affected stakeholders to democratically steer system development and to dissent when concerns arise.​494 CONCLUSION Despite the growth of ethical frameworks, AI systems continue to be deployed rapidly across domains of considerable social significance—in healthcare, education, employment, criminal justice, and many others—without appropriate safeguards or accountability structures in place. (extracted from28.pdf)
See Vidushi Marda, “Introduction” in APC, Article 19, and SIDA, “Artificial Intelligence: Human Rights, Social Justice and Development,” Global Information Watch 2019, November 2019, https://giswatch.org/sites/default/files/gisw2019_artificial_intelligence.pdf.​ 65. (extracted from28.pdf)
Design Justice Network Principles, accessed November 24, 2019, https://designjustice.org/read-the-principles​. (extracted from28.pdf)
“The Anti-Eviction Mapping Project: Counter Mapping and Oral History Toward Bay Area Housing Justice.” ​Annals of the American Association of Geographers ​108, no. (extracted from28.pdf)
Media Justice, accessed November 24, 2019, h​ ttps://mediajustice.org/ 115. (extracted from28.pdf)
Jews for Racial and Economic Justice, accessed November 24, 2019, h​ ttps://jfrej.org/​. (extracted from28.pdf)
Sasha Costanza Chock, “Design Justice: Towards an Intersectional Feminist Framework for Design Theory and Practice,” ​Proceedings of the Design Research Society 2018,​ June 3, 2018, https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3189696​. (extracted from28.pdf)
Amazon Employees for Climate Justice, “Open letter to Jeff Bezos and the Amazon Board of Directors,” Medium, April 10, 2019, https://medium.com/@amazonemployeesclimatejustice/public-letter-to-jeff-bezos-and-the-amazon-board- of-directors-82a8405f5e38​. (extracted from28.pdf)
Takano Introduces the Justice in Forensic Algorithms Act to Protect Defendants’ Due Process Rights in the Criminal Justice System,” Takano, September 17, 2019, https://takano.house.gov/newsroom/press-releases/rep-takano-introduces-the-justice-in-forensic-algorith ms-act-to-protect-defendants-due-process-rights-in-the-criminal-justice-system​. (extracted from28.pdf)
Bobby Allyn and Joel Rose, “Justice Department Announces Plan to Collect DNA from Migrants Crossing the Border,” NPR, October 21, 2019, https://www.npr.org/2019/10/21/772035602/justice-department-announces-plan-to-collect-dna-from-migr ants-crossing-the-bord.​ 291. (extracted from28.pdf)
Gray and Siddharth Surl, G​ host Work: How to Stop Silicon Valley from Building a New Global Underclass ​(Boston: Houghton Mifflin Harcourt, 2019); Kate Crawford and Vladan Joler, Anatomy of an AI System, 2018, ​https://anatomyof.ai​ ; Muqing Zhang, “Colonialism Is Alive in the Exploited Tech Work Force”, Outline​, June 6, 2019 https://theoutline.com/post/7533/colonialism-is-alive-in-the-exploited-tech-work-force?zd=2&zi=exrbzkaf​; APC, Article 19, and SIDA, “GISWatch 2019 - Artificial Intelligence: Human rights, social justice and development,” November 2019, h​ ttps://giswatch.org/sites/default/files/gisw2019_artificial_intelligence.pdf​; AI Now 2019 Report | 87 Noopur Raval, “Developing a Framework for Postcolonial Digital Labor,” unpublished manuscript, 2017, https://www.academia.edu/35413303/Developing_a_framework_for_postcolonial_digital_labor​. (extracted from28.pdf)
Alarming reports have detailed how discrim- inatory algorithms are already deployed in the justice system, wherein judges use these tools for sentencing that purport to predict the likelihood a criminal defendant will reoffend.21 In Automating Inequality, Virginia Eubanks details how government actors implement automated and surveillance technologies that harm marginalized groups.22 Eubanks studied automated systems in the US that discriminated against the poor’s receipt of DATA & SOCIETY 10 GOVERNING ARTIFICIAL INTELLIGENCE government assistance. (extracted from101.pdf)
Baluarte and Christian De Vos, From Judgment to Justice: Implementing International and Regional Human Rights Decisions, Open Society Justice Initiative (November 2010), https:// www.opensocietyfoundations.org/sites/default/files/from-judgment-to-justice-20101122.pdf. (extracted from101.pdf)
justice, public housing, welfare and health provision are examples of areas where “black box” systems should not be 2. (extracted from713.pdf)
29 In “Fairness in Criminal Justice Risk Assessments: The State of the to the design of the system based on reported issues? (extracted from713.pdf)
Berk et al, 2017, Fairness in Criminal Justice Risk Assessments: The State of the Art https://arxiv.org/ 30 Ai Now Institute 2017 Report abs/1703.09207 12 How to Prevent Discriminatory Outcomes in Machine Learning Bringing principles of non-discrimination Step 3: Being transparent about efforts to identify, to life: Human rights due diligence for prevent, and mitigate human rights risks machine learning For leadership, this step involves explicitly encouraging transparency. (extracted from713.pdf)
Advisory statement on human ethics in artificial intelligence and big data research (2017) - National Research Council Canada Skip to main content Skip to "About government" Language selection Language selection Français fr / Gouvernement du Canada Search Search Menu Main Menu Home Jobs and the workplace Immigration and citizenship Travel and tourism Business and industry Benefits Health Taxes Environment and natural resources National security and defence Culture, history and sport Policing, justice and emergencies Transport and infrastructure Canada and the world Money and finances Science and innovation You are here Canada.ca National Research Council Canada Corporate Values and ethics Research involving human participants Advisory statement on human ethics in artificial intelligence and big data research (2017) 1. (extracted from278.html)
Related links Research involving human participants From: National Research Council Canada Date modified: 2019-03-26 About this site National Research Council Canada Contact the NRC Order products Directory of science professionals Government of Canada All Contacts Departments and agencies About government Themes and topics Jobs Immigration and citizenship Travel and tourism Business Benefits Health Taxes Environment and natural resources National security and defence Culture, history and sport Policing, justice and emergencies Transport and infrastructure Canada and the world Money and finance Science and innovation Indigenous peoples Veterans and military Youth Government of Canada Corporate Social media Mobile applications About Canada.ca Terms and conditions Privacy Top of Page (extracted from278.html)
For instance, the Slovenian criminal justice.3 There are also some concerns about the impact Ministry of Finance uses a of AI technologies and robotics on the labour market (e.g. (extracted from277.pdf)
The eight principles with regard to AI are: harmony and friendliness; fairness and justice; inclusivity and sharing; respect for privacy; secure/safe and controllable; shared responsibility; open collaboration; and agile governance. (extracted from277.pdf)
• Non-economic services, such as the police, justice and statutory social security schemes, are not subject to specific European legislation or to internal market and competition rules. (extracted from263.pdf)
Responsible innovation, anticipation and responsiveness: case studies of algorithms in decision support in justice and security, and an exploration of potential, unintended, undesirable, higher-order effects. (extracted from263.pdf)
Présentation Les missions de l'institution Le fonctionnement Les membres du Bureau Les comptes Le budget Index EGAPRO Éditos & discours de la présidente Membres Assemblée générale Commissions permanentes & groupes de travail Commissions permanentes Accès au droit et à la Justice Admission des avocats étrangers Affaires européennes et internationales Collaboration Communication institutionnelle Droit et entreprise Egalité Exercice du droit Formation professionnelle Libertés et droits de l’Homme Numérique Prospective et innovation Règles et usages Statut professionnel de l'avocat Textes Groupes de travail Avocats/Experts judiciaires Droit de l'environnement Lutte contre le blanchiment de capitaux et le financement du terrorisme MARD Protection sociale Le CNB dans les médias Observatoire de la profession d'avocat Présentation Les chiffres-clés de la profession Les études de l'observatoire Concours Projets Innovants Ressources Affaires Publiques Vie professionnelle Vie professionnelle Toutes les informations et les outils indispensables dans le cadre de votre exercice professionnel. (extracted from111.html)
Connectez-vous pour accéder à des contenus exclusifs et à l'ensemble des services en ligne S'identifier avec e-dentitas Voir les Conditions générales d'utilisation (nouvelle fenêtre) Vous êtes dans Accueil Actualités Préconisations d’actions pour les legaltechs du domaine de la jurimétrie 13 octobre 2020 Préconisations d’actions pour les legaltechs du domaine de la jurimétrie Numérique Partager par email.' ' .(ouvre votre boîte de messagerie) Le Conseil national des barreaux a toujours manifesté son intention de prendre un rôle actif sur le sujet de la réutilisation de la donnée judiciaire, en demandant notamment la création d’une instance publique chargée de la régulation et du contrôle des algorithmes utilisés pour l’exploitation de la base de données des décisions de justice ainsi que de leur réutilisation, dont doivent, notamment, être membres la Cour de cassation, le Conseil d’Etat et le Conseil national des barreaux. (extracted from111.html)
Après avoir constaté la nécessité de déterminer le fonctionnement technique et éthique de chacune des technologies portées par les Legaltechs du domaine de la « Justice prédictive » ainsi que leur utilité pratique pour les professionnels du droit, l’assemblée générale du Conseil national des barreaux (CNB), réunie les 5 et 6 juillet 2019, a donné mandat au groupe de travail Legaltech de piloter une étude comparative. (extracted from111.html)
Elaborer une stratégie de large intégration du numérique dans les activités des professionnels du droit et de la justice ; 13. (extracted from111.html)
Les travaux réalisés ont ainsi permis de rappeler l’importance de réguler les nouveaux outils de jurimétrie et d’assurer le respect de principes éthiques dans le cadre de la réutilisation de la donnée de justice. (extracted from111.html)
En conséquence, une Charte sur la transparence et l’éthique de l’utilisation des données judiciaires annexée au rapport a été adoptée pour garantir l’autorégulation des acteurs tant s’agissant des algorithmes utilisés pour l’exploitation de la base de données des décisions de justice que de la réutilisation des informations qu’elle contient. (extracted from111.html)
Partager sur les réseaux Partager par email.' ' .(ouvre votre boîte de messagerie) Haut de page Services RPVA Encyclopédie des avocats Plateforme de consultation Compte événement CNB Petites annonces CNMA Voir tous les services POUR ALLER PLUS LOIN Lien Consulter le rapport (accès avocat) En savoir plus S'inscrire à la newsletter Gestion des cookies Justice.fr Contacter le Médiateur Mentions légales Politique de confidentialité CGU Aide et accessibilité Nous contacter ©2025 Conseil National des Barreaux Facebook Twitter Linkedin Youtube Fermer (extracted from111.html)
In this report, we will focus on systems that affect justice, equality, participation and public welfare, either directly or indirectly. (extracted from38.pdf)
because we see value in In case automation as a term was not used, we still scanned for key concepts like knowing about borderline discrimination, justice, equity/equality to take a closer look, since automation processes cases at a stage when all tend to be hiding behind these keywords. (extracted from38.pdf)
Greece page 18 automating society european Union EUROpEAN UNION By Kristina Penner The EU is a lot of things: It is a union of 28 Member States that has a commission, a parlia- ment, a council of national ministers, the Court of Justice, and a couple of other institutions. (extracted from38.pdf)
[…] Whilst AI clearly generates new opportunities, it also poses challenges and risks, for example in the areas of safety and liability, security (criminal use or attacks), bias and discrimination.” external [eU 2] The Commission announced that it will support (basic and industrial) research and innova- tion in fields built on the guiding principle of “responsible AI”, including investment and encouragement of research and testing in sectors such as health, security, public adminis- tration and justice, with the goal to enable policy makers to gain experience and to devise suitable legal frameworks. (extracted from38.pdf)
In addition, there was a mention that fundamental rights “would be at risk if unethical practice is facilitated by virtue of algorithms focused on com- mercial gain, for example because humans ‘allow’ or ‘rely’ on robot sorting techniques that are discriminatory and may be unfair and undermine dignity and justice”. (extracted from38.pdf)
external [eU 43] / Police directive The EU data protection reform package of 2016, which included the GDpR, also involved a directive on data protection in the area of police and justice external [eU 44] as a lex specialis, adopted on May 5, 2016, applicable as of May 6, 2018. (extracted from38.pdf)
eu-LISA, the “European Agency for the Operational Management of large-scale IT Sys- tems in the Area of Freedom, Security and Justice”, is now managing the “strengthened” databases and applications VIS, SISII and EURODAC together. (extracted from38.pdf)
This is a cooperation between the Ministry of Home Affairs, the Digital Agenda and the Ministry of Justice. (extracted from38.pdf)
The committee consists of 16 experts from politics, academia and industry and is led by the Federal Ministry for Justice and Consumer Protection and the Federal Ministry of the Interior, Building and Community. (extracted from38.pdf)
external LINKS: You can find a list Lastly, the Digital Government Agenda mentions that the Wetenschappelijk onderzoeks- of all URLs in the report en documentatiecentrum (Research and Documentation Centre of the Ministry of Justice compiled online at: and Security or WODC) will carry out research on the regulation and legality of algorithms www.algorithmwatch.org/ taking autonomous decisions. (extracted from38.pdf)
On March 29, 2018, the Ministry of Justice and Security organised a round table discussion on the use of Artificial Intelli- gence in the legal field. (extracted from38.pdf)
It analysed and then intervened in a case concerning In January 2018, the Polish the use of algorithms for profil- Ministry of Justice introduced ing in unemployment. (extracted from38.pdf)
Adm in AcTion / ministry of Justice – system of random Allocation of cases on 1 January 2018, the Polish Ministry of Justice introduced the “System of Random Allocation of Cases” (System Losowego Przydziału Spraw), a digital system that, on a once- per-day basis, assigns cases to judges across the country. (extracted from38.pdf)
The Minister of Justice has de- clared that "the selection will be made solely by a machine, a computer system that is blind like Themis, and chooses without emotions, without views or biases, and in a manner fully free from possible accusations of corruption“. (extracted from38.pdf)
SAVRY is in general fair, while the machine learning models tend to discriminate against male defendants, foreigners, or people of specific national groups, says Castillo: “Machine learning could be incorporated into SAVRY, but if aspects of algorithmic justice are not taken into account, it could generate an unfair prediction.” The evaluation external [SP 19] showed that humans were much better than ADM, but that ADM can be more precise with the results. (extracted from38.pdf)
The Department of Justice of the Catalan government launched the tool in 2010 and applied it to all inmates in all prisons, and not just for cases involving violent crime. (extracted from38.pdf)
Anne serves as a chair of the Com- munication and Democracy section of ECREA and is vice-chair of the Activism, Communication and Social Justice Interest Group within ICA. (extracted from38.pdf)
givtraH annA :otoF Taking Stock of Automated Decision-Making in the EU page 131 page 132 Automating society sweden automating Society – united kingdom Taking Stock of Automated Decision- Making in the EU Private company Imosphere provides systems to help town halls and the National Health Service to automatically calcu- The Data Justice Lab at late so-called personal budgets the University of Cardiff for people who receive social examines the relationship welfare payments. (extracted from38.pdf)
between what it calls ‘data- fication’ and social justice. (extracted from38.pdf)
external [uK 6] While recognising the potential benefits to society of ADM, it men- tions discrimination against job applicants and inequities within the criminal justice system as examples of issues that may arise as a result of ADM. (extracted from38.pdf)
external [uK 15] / Data Justice Lab The Data Justice Lab is a research lab at Cardiff University’s School of Journalism, Media and Culture. (extracted from38.pdf)
It seeks to examine the relationship between what it calls ‘datafication’—the collection and processing of massive amounts of data for decision-making and governance page 138 Automating Society United Kingdom across more and more areas of social life—and social justice. (extracted from38.pdf)
Equality and justice 6. (extracted from461.pdf)
Equality and justice Artificial intelligence should not reproduce prejudices that marginalise specific population groups. (extracted from461.pdf)
AI can find broad applications in such fields as education, medical treatment, eldercare, environmental protection, city operation, and justice services, and it is able to greatly improve the precision level of public services and increase the quality of people’s lives across the board. (extracted from582.html)
Human decisions based on automated and predictive 240 technology are often made in settings such as hiring or criminal justice, and can create harmful 241 impacts and amplify and accelerate existing social inequities or, at minimum, perceptions of 242 inequities. (extracted from474.pdf)
Certainly, there is 249 no shortage of examples where bias in some aspect of AI technology and its use has caused harm 250 and negatively impacted people's lives, such as in hiring [5,12,16,17,36,62,118], health care 251 [46,52,55,59,83,88,103,122,123], and criminal justice [7,20,29,41,44,56,66,74,75,78,87, 252 140,142]. (extracted from474.pdf)
Such biases may produce unjust outcomes for racial and 3 316 ethnic minorities in areas such as criminal justice [7,41,56,74,75,78,87,140,142], hiring 317 [4,5,12,16,17,36,118,119], and financial decisions [13,65]. (extracted from474.pdf)
The broad consensus 326 of the literature is that systems meant for decision making or predictive scenarios should 327 demonstrate validity and reliability under the very specific setting in which it is intended to be 328 deployed (hiring purposes, risk assessments in the criminal justice system, etc.). (extracted from474.pdf)
Costanza-Chock, Design Justice, A.I., and Escape from the Matrix of Domination, 826 Journal of Design and Science. (extracted from474.pdf)
19 852 [44] EPIC, Algorithms in the Criminal Justice System: Risk Assessment Tools, Electronic 853 Privacy Information Center, 2020. (extracted from474.pdf)
Government of Canada Strategic Plan for Information Management and Information Technology 2017 to 2021 - Canada.ca Skip to main content Skip to "About government" Language selection Français fr / Gouvernement du Canada Search Search Canada.ca Search Menu Main Menu Jobs and the workplace Immigration and citizenship Travel and tourism Business and industry Benefits Health Taxes Environment and natural resources National security and defence Culture, history and sport Policing, justice and emergencies Transport and infrastructure Canada and the world Money and finances Science and innovation Manage life events You are here: Canada.ca About government Government oversight Information, technology and project management Government of Canada Strategic Plan for Information Management and Information Technology 2017 to 2021 From Treasury Board of Canada Secretariat On this page About this document / what’s new Message from the Government of Canada Chief Information Officer Introduction Citizens’ expectations Workplace and workforce evolution Privacy and security The enterprise approach IM - IT sustainability and aging IT The vision Digital services Open and accessible Security Enterprise management Community Workplace Mission statement Guiding principles Principle 1: client and service-centred design Principle 2: open Principle 3: enterprise first Principle 4: secure Principle 5: cloud first approach Principle 6: enable a modern workplace: anywhere, anytime with anyone Strategic goals Strategic goal 1: service Strategic goal 2: value Strategic goal 3: security Strategic goal 4: agility Strategic actions Service Service management Cloud first Technology modernization Information and data sharing Manage Governance Enterprise architecture alignment and practices Agility and innovation Sustainability Secure Defence in depth Trusted solutions and services Awareness and understanding Community IM - IT workforce Modern workplace Digital collaboration The way forward Implementing the Strategic Plan Risks and mitigation strategies Measuring progress Staying evergreen Appendix A: Government of Canada IM / IT modernization priorities Appendix B: implementation roadmap Appendix C: key performance indicators Appendix D: roles and responsibilities Appendix E: definitions Appendix F: draft digital principles About this document / what’s new This is the first update to the Government of Canada Information Technology Strategic Plan, published in June 2016. (extracted from644.html)
Return to footnote 1 referrer © Her Majesty the Queen in Right of Canada, represented by the President of the Treasury Board, 2017, ISBN: 978-0-660-24007-7 Page details Date modified: 2020-07-27 About this site Treasury Board of Canada Secretariat (TBS) Contact us Forms Government of Canada All contacts Departments and agencies About government Themes and topics Jobs Immigration and citizenship Travel and tourism Business Benefits Health Taxes Environment and natural resources National security and defence Culture, history and sport Policing, justice and emergencies Transport and infrastructure Canada and the world Money and finances Science and innovation Indigenous Peoples Veterans and military Youth Manage life events Government of Canada Corporate Social media Mobile applications About Canada.ca Terms and conditions Privacy (extracted from644.html)
AI in the Nordic-Baltic region | Nordic cooperation Skip to main content Service menu Press Career Contact EN language dropdown DA FI IS SV NO Main navigation Nordic Council of Ministers Nordic Council Mega menu EN language dropdown DA FI IS SV NO Nordic Council of Ministers News from the Nordic Council of Ministers Our Vision 2030 Politiske prioriteter 2025-2030 The Presidency of the Nordic Council of Ministers Presidency programme for the Nordic Council of Ministers 2025 Organisation The Secretary General Secretariat Governing documents The Nordic Councils of Ministers Ministers for Co-operation Labour Environment and Climate Sustainable Growth Digitalisation Fisheries, Aquaculture, Agriculture, Food and Forestry Justice Culture Gender Equality and LGBTI Social and Health Affairs Education and Research Economic and Fiscal Policy Meetings and protocols Institutions Nordic Culture Point Nordic House in Reykjavik Nordic House in the Faroe Islands The Nordic Institute in Greenland Nordic Institute on Åland Nordic Welfare Centre NordForsk NordGen Nordic Innovation Nordic Energy Research Nordregio NIVA Education About the Nordic Council of Ministers History of the Nordic Council of Ministers Budget Nordic Council News from the Nordic Council Parliamentary co-operation Nordic Council cases Nordic Council’s international co-operation The Presidency of the Nordic Council Presidency programme for the Nordic Council 2025 Organisation Secretary General Secretariat Governing documents Nordic Council committees and members All members The Presidium Committee for Knowledge and Culture in the Nordic Region Committee for a Sustainable Nordic Region Committee for Growth and Development in the Nordic Region Committee for Welfare in the Nordic Region Control Committee Election Committee Sessions and meetings Protocols and upcoming meetings Session 2025 Theme Session 2025 2024 Session Past sessions About the Nordic Council The history of the Nordic Council Annual report News All news Events Current initiatives 2025 Session of the Nordic Council Arctic Circle 2025 EXPO 2025 Press Nordic image bank Career Knowledge and services Living, working and studying in the Nordic Region Publications Release a publication Facts and statistics Facts about the Nordic Region Nordic statistics Funding opportunities About funding from the Nordic Council of Ministers Public procurements Border database About the Nordic co-operation The history of Nordic co-operation Nordic Council prizes Environment Prize Children and Young People's Literature Prize Literature Prize Film Prize Music Prize Policy areas Environment and climate Culture Legislation and Justice Sustainable development Digitalisation and innovation Children and young people See all policy areas Service menu Press Career Contact Search Search Menu Breadcrumb Home AI in the Nordic-Baltic region 14.05.18 | Declaration Artificial intelligence (AI) can help solve major societal challenges and provide significant benefits in a variety of areas. (extracted from490.html)
Accept additional cookies Reject additional cookies View cookies Hide cookie message Skip to main content GOV.UK Navigation menu Menu Menu Services and information Benefits Births, death, marriages and care Business and self-employed Childcare and parenting Citizenship and living in the UK Crime, justice and the law Disabled people Driving and transport Education and learning Employing people Environment and countryside Housing and local services Money and tax Passports, travel and living abroad Visas and immigration Working, jobs and pensions Government activity Departments Departments, agencies and public bodies News News stories, speeches, letters and notices Guidance and regulation Detailed guidance, regulations and rules Research and statistics Reports, analysis and official statistics Policy papers and consultations Consultations and strategy Transparency Data, Freedom of Information releases and corporate reports Search GOV.UK × Search GOV.UK Search Search GOV.UK Search Home Business and industry Science and innovation Artificial intelligence National AI Strategy Department for Science, Innovation & Technology Office for Artificial Intelligence Department for Digital, Culture, Media & Sport Department for Business, Energy & Industrial Strategy Guidance National AI Strategy - HTML version Updated 18 December 2022 Contents Our ten-year plan to make Britain a global AI superpower Executive summary Summary of key actions Introduction Pillar 1: Investing in the long-term needs of the AI ecosystem Pillar 2: Ensuring AI benefits all sectors and regions Pillar 3: Governing AI effectively Next steps Print this page © Crown copyright 2022 This publication is licensed under the terms of the Open Government Licence v3.0 except where otherwise stated. (extracted from185.html)
Cancel Services and information Benefits Births, death, marriages and care Business and self-employed Childcare and parenting Citizenship and living in the UK Crime, justice and the law Disabled people Driving and transport Education and learning Employing people Environment and countryside Housing and local services Money and tax Passports, travel and living abroad Visas and immigration Working, jobs and pensions Government activity Departments News Guidance and regulation Research and statistics Policy papers and consultations Transparency How government works Get involved Support links Help Privacy Cookies Accessibility statement Contact Terms and conditions Rhestr o Wasanaethau Cymraeg Government Digital Service All content is available under the Open Government Licence v3.0 , except where otherwise stated © Crown copyright (extracted from185.html)
However, “had he been able to examine and contest the logic of the COMPAS system to prove that its score gave a distorted picture of his life, he might have gone home much earlier” (Wexler 2017b) Rodríguez’s case is an example of the discriminatory use of AI in criminal justice, which also includes prominent AI applications for the purposes of predictive policing. (extracted from689.pdf)
In such cases, which include law enforcement and criminal justice applications, the attempt to modify the data to reduce or eliminate underlying biases may inadvertently introduce new challenges. (extracted from689.pdf)
The desirability of the use of AI solutions is also something that should be duly considered– with regard to the purpose, advantages and burden imposed by them on social values, justice and the public interest. (extracted from689.pdf)
It has links to broader societal structures and the justice of our socio-economic systems and thus relates to the problem of surveillance capitalism. (extracted from689.pdf)
(2018) outline how criminal justice risk assessment tools could benefit low-risk individuals through increased pre-trial releases and shorter sentences. (extracted from689.pdf)
verification of the authenticity of travel documents); and in the administration of justice and democratic processes (e.g. (extracted from689.pdf)
Directorate-General for Justice and Consumers, European Commission, Brussels. (extracted from689.pdf)
Expert Group on Liability and New Technologies, Directorate-General for Justice and Consumers, European Commission, Brussels. (extracted from689.pdf)
For instance, the opening of the Universal Declaration of Human Rights states that “recognition of the inherent dignity and of the equal and inalienable rights of all members of the human family is the foundation of freedom, justice and peace in the world” (UN 1948). (extracted from689.pdf)
Federal Ministry of Justice and Federal Office of Justice, Berlin. (extracted from689.pdf)
One could perhaps even argue that AI has been linked directly to international justice and sustainability through the SDGs. (extracted from689.pdf)
In particular, the European Union Agency for Fundamental Rights (FRA) and the Ad Hoc Committee on Artificial Intelligence (CAHAI), a Council of Europe body responsible for examining the possibilities of establishing a legal framework on AI, have drawn up an inventory of fundamental rights likely to be threatened by AI: in particular respect for human dignity, respect for privacy and data protection, equality and non-discrimination, access to justice, access to social rights, etc. (extracted from104.pdf)
However, in addition to the observations on use cases prohibited by the proposal for an EU regulation on AI, the CNCDH would like to highlight two types of use of particular concern for the respect of human rights: predictive justice and the recognition of emotions in support of a selection process. (extracted from104.pdf)
The CNCDH also notes that the Ministry of Justice prematurely terminated, in January, its experimentation with such software, by establishing the multiplicity of criteria to be taken into account to characterise the extent of bodily injury and the excessive importance of the means to be mobilised to study and prevent algorithmic biases in order to achieve a satisfactory level of performance36. (extracted from104.pdf)
33 According to appearance theory, enshrined by the European Court of Human Rights, “justice must not only be done, it must also be seen to be done”. (extracted from104.pdf)
See the response of the Minister of Justice published in the OJ Senate of 01/10/2020, page 4462, available online: https://www.senat.fr/questions/base/2020/qSEQ200616942.html. (extracted from104.pdf)
Marzolf, “Le ministère de la Justice renonce à son algorithme DataJust”, Acteurs publics, 14 January 2022. (extracted from104.pdf)
These adverse effects are most often mentioned on the basis of a reference to the “sensitive” sectors in which the system is deployed, such as police, justice or health, or an inventory of fundamental rights and freedoms that may be challenged by AI technology43. (extracted from104.pdf)
These are AI systems that operate in “sensitive” areas: biometric identification, management and operation of critical infrastructure, education and training, employment, access to essential private services, public services and social benefits, police, justice, migration management. (extracted from104.pdf)
27 A - 2022 - 6 • EN • Opinion on the impact of artificial intelligence on fundamental rights as the preservation of the rule of law and democracy, not forgetting the environment, the National Consultative Commission on Human Rights intends to continue its work on artificial intelligence in the future, especially to examine its impacts, particularly in the areas of health, education, employment and justice. (extracted from104.pdf)
Webinar: Automating Injustice - Fair Trials Leap Members Login News Events The Right to a Fair Trial The rights of accused people Discrimination Criminalisation Misuse of state power Mass incarceration Campaigns Protecting the rights of accused people The right to counsel Pre-trial detention Plea bargaining Artificial intelligence (AI), data and criminal justice INTERPOL COVID-19 Justice Campaign Extradition reform Resources Resource Hub News Publications Strategic Litigation Legal Analysis Information & Toolkits Films Case Studies About Us Where we work How we work Building a movement Our People Trustees Get Involved Open Mobile Nav Cancel Share on Facebook Share on Twitter Share on Linkedin Share by Email Back to Resource Hub Print this page Share on Facebook Share on Twitter Share on Linkedin Share by Email Film/Video Webinar: Automating Injustice Published: September 09, 2021 Artificial intelligence (AI) and automated decision-making (ADM) systems are increasingly used by European law enforcement and criminal justice authorities to predict and profile people’s actions and assess their ‘risk’ of criminality or re-offending in the future. (extracted from306.html)
With input from speakers with first-hand experience of the harm these systems cause, experts on AI and automated systems in criminal justice, and leading European policymakers. (extracted from306.html)
Chair: Laure Baudrihaye-Gérard, Legal Director (Europe), Fair Trials Panellists: Diana Sardjoe, Founder, De Moeder is de Sleutel (The Mother is the Key), mother of children impacted by risk modelling and profiling systems Petar Vitanov MEP (S&D) (Bulgaria), Rapporteur of LIBE committee AI in criminal matters report Sarah Chander, Senior Policy Advisor, European Digital Rights (EDRi) Martin Sacleux, Legal Advisor at the Council of Bars and Law Societies of Europe (CCBE) Griff Ferris, Legal & Policy Officer, Fair Trials (presenting the Automating Injustice report) Issue AI and algorithms & data Countries European Union Related content End the use of AI to profile people in the EU Artificial intelligence in criminal justice: Fair Trials' work The unchecked expansion of policing powers in Europe Get Involved Support our work by signing up for updates about our work or making a donation. (extracted from306.html)
Donate Fairness, equality, justice Company Careers Contact Us Privacy Safeguarding Policy Cookies & Privacy Policy Connect Some activities in the following sections on this website are funded by the European Union’s Justice Programme (2021-2027): Legal Experts Advisory Panel, Defence Rights Map, Case Law Database, Advice Guides, Resources, Campaigns, Publications, News and Events. (extracted from306.html)
Hollywood, “Evaluation of the Shreveport Predictive Policing Experiment,” National Institute of Justice, 2014,https://nij.ojp.gov/topics/articles/evaluation-shreveport- predictive-policing-experiment. (extracted from702.pdf)
Artif Intell Law, 1–43 The European Court of Justice E (2011) The European court of justice ruling. (extracted from202.html)
http://ec.europa.eu/ireland/press_office/news_of_the_day/ecj-ruling-sex-discrimination-in-insurance-contracts_en.htm The US department of Justice U (2011) The us federal legislation. (extracted from202.html)
http://www.justice.gov/crt Turner M, Skidmore F (1999) Mortgage lending discrimination: a review of existing evidence. (extracted from202.html)
Institute for Information Technology, National Research Council, Ottawa US Department of Justice U (1974) Us equal credit opportunity act. (extracted from202.html)
Other Legislation Convention on Access to Information, Public Participation in Decision-Making and Access to Justice in Envi- ronmental Matters (Aarhus Convention). (extracted from258.pdf)
What constitutes a sufficient interest and impairment of a right shall be determined consistently with the objective of giving the public concerned wide access to justice. (extracted from258.pdf)
Public administration does not only administrative procedure or administrative justice use highly complex systems that would commonly legislation (compare, eg, Article III-2 (1) of the be described as AI, but also simpler systems where ReNEUAL Model Rules). (extracted from258.pdf)
developments in the field, it should become a 50 Comments and Sources Article 16: Complaints and Legal (16.5.) Paragraph 5 recalls some of the characteristics Protection necessary for effective access to justice. (extracted from258.pdf)
Indeed, several notions of fairness exist that are not only technically defined but also entangled with concepts of social justice, specifically the concept of privilege, held by virtue of belonging to certain social identity groups34. (extracted from270.pdf)
These values are common to the Member States in a society in which pluralism, non-discrimination, tolerance, justice, solidarity and equality between women and men prevail.' Furthermore, its article 3(3) states that the Union 'shall combat social exclusion and discrimination, and shall promote social justice and protection, equality between women and men, solidarity between generations and protection of the rights of the child.' Similar ideas can be found in the Treaty on the Functioning of the European Union, especially in its Part Two, entitled 'Non-Discrimination and Citizenship of the Union' (see article 19.1.) and the EU Charter of Fundamental Rights (article 21). (extracted from270.pdf)
Even worse, this contextually limited interpretation of the concept of discrimination has been endorsed by the Court of Justice of the EU43. (extracted from270.pdf)
There is a nice argument that supports such additional use of the concept: even though the European Court of Justice has never defined the notion of fairness in data protection law, it has used this notion of fairness in two different contexts: fair balance and transparency.48 If we consider that fairness has to do with the reasonable expectations of data subjects, then it should help us to avoid some of the discriminatory results that are not so easy to uncover: a data subject would hardly allow the type of processing that would cause him/her to suffer a damage that other people do not suffer. (extracted from270.pdf)
'Fairness in Criminal Justice Risk Assessments: The State of the Art.' Sociological Methods & Research, vol. (extracted from270.pdf)
Gerards, J., Xenidis, R., Algorithmic discrimination in Europe: challenges and opportunities for gender equality and non-discrimination law, European Commission, Directorate-General for Justice and Consumers, Publications Office, 2021, https://data.europa.eu/doi/10.2838/77444 Gianclaudio Malgieri and Vincenzo Tiani, How the EU Council is rewriting the AI Act, REPORT - 6 December 2021, December 2021, at: https://brusselsprivacyhub.eu/publications/how-the-eu-council-is- rewriting-the-ai-act, last accessed 24/03/2022. (extracted from270.pdf)
The jurisprudence of the Court of Justice of the EU on the use of AI 35 2.3.1. (extracted from264.pdf)
Policy recommendations for the best use of AI in the fisheries and its value chain 74 REFERENCES 76 4 Artificial Intelligence and the fisheries sector LIST OF ABBREVIATIONS AFMA Australian Fisheries Management Authority AI Artificial Intelligence AIA proposal Proposal for a Regulation of the European Parliament and of the Council laying down harmonised rules on artificial intelligence (Artificial Intelligence Act) AIS Automatic Identification System ANN Artificial Neural Network BN Bayesian Network CAP Common Agricultural Policy CCTV Closed-Circuit Television CFP Common Fisheries Policy CJEU Court of Justice of the European Union CNN Convolutional neural network COM Common Organisation of the Markets DCF Data Collection Framework DL Deep Learning EGD European Green Deal EM Electronic Monitoring EMFAF European Maritime, Fisheries and Aquaculture Fund EU European Union EUMOFA European Market Observatory for Fisheries and Aquaculture FAO Food and Agriculture Organisation of the United Nations FCR Fisheries Control Regulation 5 IPOL | Policy Department for Structural and Cohesion Policies FPS Frontal protection systems FROODS Fishing Route Optimization Decision Support System GDP Gross Domestic Product GDPR General Data Protection Regulation GES Good Environmental Status GVA Gross Value Added H2020 Horizon 2020 EU research and innovation funding programme IATTC Inter American Tropical Tuna Commission International Commission for the Conservation of Atlantic Tunas ICCAT ICES International Council for the Exploitation of the Sea IMO International Maritime Organization IOTC Indian Ocean Tuna Commission IUU Illegal, Unreported and Unregulated LO Landing obligation Minimum Conservation Reference Size MCRS Machine Learning ML MPI Ministry of Primary Industries of New Zealand MSFD Marine Strategy Framework Directive MSY Maximum Sustainable Yield NMFS National Marine and Fisheries Services NOAA North Oceanic and Atmospheric Administration OJ Official Journal PECH Committee European Parliament’s Committee on Fisheries PET Protected Endangered and Threatened species 6 Artificial Intelligence and the fisheries sector RFMO Regional fisheries management organisation SDG Sustainable Development Goal SME Small and Medium Enterprise STECF Scientific Technical Economic Committee on Fisheries SVM Support Vector Machine Total Allowable Catches TAC Treaty on European Union TEU Treaty on the Functioning of the European Union TFEU United Nations UN US United States of America VMS Vessel Monitoring System WCPFC Western and Central Pacific Fisheries Commission 7 IPOL | Policy Department for Structural and Cohesion Policies LIST OF FIGURES Figure 1: Conceptual diagram showing the role of AI in relation with other digitalisation activities 15 Figure 2: Classification of AI techniques and approaches in the AIA proposal expanded with further subcategories used in this study 42 Figure 3: Simplified diagram of an expert system 47 Figure 4: Simplified schema of the Fish Value Chain 55 Figure 5: Traceability aspects improvable by AI systems 56 Figure 6: Published papers related to fisheries, AI and traceability keywords 57 LIST OF TABLES Table 1: Non-exhaustive summary of the recitals and articles of the most relevant EU fisheries legislation containing elements making the use of AI systems possible 31 Table 2: Similarities, differences and keywords associated to specific ML methods 43 8 Artificial Intelligence and the fisheries sector EXECUTIVE SUMMARY This study reviews the main applications of Artificial Intelligence (AI) systems in fisheries and identifies current challenges for fisheries that have the potential to be dealt with through AI. (extracted from264.pdf)
The third section (2.3) includes a reference to the scarce jurisprudence of the Court of Justice of the European Union (CJEU) that has so far, directly, or indirectly, considered AI- related issues with projection on the fisheries sector. (extracted from264.pdf)
58 European Commission, Directorate-General for Justice and Consumers, Liability for artificial intelligence and other emerging digital technologies, Publications Office, 2019. (extracted from264.pdf)
The jurisprudence of the Court of Justice of the EU on the use of AI The CJEU as the judicial authority of the EU by virtue of Article 19 TEU, in cooperation with the judicial bodies of the EU Member States, ensures the uniform application and interpretation of EU law. (extracted from264.pdf)
An appeal has now been brought against this judgment before the Court of Justice 61. (extracted from264.pdf)
Case on illegal mechanical device for fish classification A Court of Justice case was found where an illegal mechanical fish classification device was used. (extracted from264.pdf)
In its judgment of 11 February 2021 in Case K.M.62, the Court of Justice ruled on the reference for a preliminary ruling received from the Irish Court of Appeal under Article 267 TFEU. (extracted from264.pdf)
62 Judgment of the Court of Justice of 11 February 2021, K.M., C-77/20, ECLI:EU:C:2021:112. (extracted from264.pdf)
The Court of Justice was asked whether Article 89 and Article 90 of FCR, read in the light of the principle of proportionality enshrined in Article 49(3) of the Charter, were to be interpreted as precluding a national provision which, in order to penalise an infringement of Article 32 of Regulation No 850/98, provided for the imposition of a fine and the mandatory confiscation of prohibited or non-compliant catches and fishing gear found on board the vessel concerned (paragraph 25). (extracted from264.pdf)
The Court of Justice found that the mandatory confiscation of prohibited or non-compliant catches and fishing gear may deter the persons concerned from infringing the prohibition on sorting equipment, laid down in Article 32(1) of Regulation No 850/98, by depriving them of the illegally obtained benefits which they could otherwise enjoy, and of the possibility of continuing to use such equipment (paragraph 44). (extracted from264.pdf)
65 Among other judgments, see: judgment of the Court of Justice of 16 July 2015, Chmielewski, C-178/03, ECLI:EU:C:2015:475, paragraph 21. (extracted from264.pdf)
ISP DIGITAL FUTURE WHITEPAPER & YJoLT SPECIAL PUBLICATION Algorithms and Economic Justice: A Taxonomy of Harms and a Path Forward for the Federal Trade Commission Rebecca Kelly Slaughter with Janice Kopec and Mohamad Batal August 2021 Contents Algorithms and Economic Justice .................................................................................... (extracted from728.pdf)
59 Algorithms and Economic Justice: A Taxonomy of Harms and a Path Forward for the Federal Trade Commission The proliferation of artificial intelligence and algorithmic decision-making has helped shape myriad aspects of our society: from facial recognition to deepfake technology to criminal justice and health care, their applications are seemingly endless. (extracted from728.pdf)
As an FTC Commissioner, I aim to promote economic and social justice through consumer protection and competition law and policy. (extracted from728.pdf)
Algorithms and Economic Justice | Rebecca Kelly Slaughter 1 I. (extracted from728.pdf)
The applications of these technologies are innumerable, from facial recognition to deepfake technology, criminal justice, and health care. (extracted from728.pdf)
Algorithmic decision-making, and the AI that fuels it, could realize its promise of promoting economic justice by distributing opportunities more broadly, resources more efficiently, and benefits more effectively. (extracted from728.pdf)
In the criminal justice system, for example, commentators note that algorithms and AI contribute to over-surveillance,5 wrongful detainment and arrest,6 and biased risk assessments used to determine pre-trial status and even sentencing.7 Mounting evidence reveals that algorithmic decisions can produce biased, discriminatory, and unfair outcomes in a variety of high-stakes economic spheres including employment, credit, health care, and housing.8 3 See, e.g., Matt Kasman & Jon Valant, The Opportunities and Risks of K-12 Student Placement Algorithms, BROOKINGS INST. (extracted from728.pdf)
7 See, e.g., Algorithms in the Criminal Justice System: Pre-Trial Risk Assessment Tools, ELEC. (extracted from728.pdf)
CTR., https://epic.org/algorithmic-transparency/crim-justice (last visited Jan. (extracted from728.pdf)
Algorithms and Economic Justice | Rebecca Kelly Slaughter 3 The COVID-19 pandemic and its attendant social and economic fallout underscore the incredible stakes of the decisions we now delegate to technology. (extracted from728.pdf)
23 4 4 of some of the algorithmic harms that threaten to undermine economic and civil justice.13 I identify three ways in which flaws in algorithm design can produce harmful results: faulty inputs, faulty conclusions, and failure to adequately test. (extracted from728.pdf)
Algorithms and Economic Justice | Rebecca Kelly Slaughter 5 promising remedy the FTC secured in recent enforcement actions. (extracted from728.pdf)
I hope to draw the attention and ingenuity of the interested public to the challenges posed by algorithms so that we can work together on creating an enforcement regime that advances economic justice and equity. (extracted from728.pdf)
David Edelman, “AI is not magic; it is math and code.”15 As we consider the threats that algorithms pose to justice, we must remember that just as the technology is not magic, neither is any cure to its shortcomings. (extracted from728.pdf)
The second subpart describes three ways in which even sophisticated algorithms still systemically undermine civil and economic justice. (extracted from728.pdf)
Often skewed training data reflect historical and enduring Algorithms and Economic Justice | Rebecca Kelly Slaughter 7 patterns of prejudice or inequality, and when they do, these faulty inputs can create biased algorithms that exacerbate injustice.16 One recent example is Amazon’s failed attempt to develop a hiring algorithm driven by machine learning, an effort ultimately abandoned before deployment because the algorithm systematically discriminated against women. (extracted from728.pdf)
Algorithms and Economic Justice | Rebecca Kelly Slaughter 9 and socioeconomic differences.23 As the BBC noted in its coverage, “it locks in all the advantages and disadvantages—and means that the talented outlier, such as the bright child in the low-achieving school, or the school that is rapidly improving, could be delivered an injustice.”24 In short, when developers use faulty data to train an algorithm, the results may replicate or even exacerbate existing inequalities and injustices. (extracted from728.pdf)
29, 2020), https://www.businessinsider.com/experts-skeptical- amazon-halo-judges-emotional-state-from-voice-2020-8; Saheli Roy Choudhury, Amazon Says Its Algorithms and Economic Justice | Rebecca Kelly Slaughter 11 start-ups—continue to sell questionable affect recognition technology, and it is sometimes deployed to grant or deny formative life opportunities. (extracted from728.pdf)
Algorithms and Economic Justice | Rebecca Kelly Slaughter 13 throwing around terms such as “AI” and “machine learning,” in many contexts the technology is still deeply imperfect.37 Indeed, an employment-screening algorithm’s assessment of a candidate can sometimes be less accurate or useful than the subjective (though still imperfect) impression an employer gets from conducting an interview.38 These risks can be compounded when certain products are emphatically marketed as producing reliable or objective predictions about potential hires when their conclusions are in fact flawed and misleading.39 This is a 37 Algorithmic hiring is problematic for a number of other reasons. (extracted from728.pdf)
Algorithms and Economic Justice | Rebecca Kelly Slaughter 15 people in the real world. (extracted from728.pdf)
Algorithms and Economic Justice | Rebecca Kelly Slaughter 17 test to reliably detect and prevent bias, early and ongoing testing of the outcomes in this instance may have caught this flaw years earlier.48 Another example on this front: A few years ago, a reporter found that typing in a number of common female names on LinkedIn would result in a prompt for a similarly spelled man’s name instead—for example, “Stephan Williams” when searching for “Stephanie Williams.”49 But, according to the reporter, when any of the 100 most common male names was entered, LinkedIn never prompted with a female alternative. (extracted from728.pdf)
Algorithms and Economic Justice | Rebecca Kelly Slaughter 19 was reluctant to acknowledge (or flat-out denied) the possibility of bias.54 This problem is not limited to algorithms, but, as in other instances of unintended bias, admitting that it might occur, despite best intentions, is imperative. (extracted from728.pdf)
Algorithms and Economic Justice | Rebecca Kelly Slaughter 21 This problem may persist across advertising algorithms, which are designed to maximize clicks and conversions. (extracted from728.pdf)
Algorithms and Economic Justice | Rebecca Kelly Slaughter 23 to believe that the hospital or manufacturer of the algorithm in question was trying to disadvantage Black patients. (extracted from728.pdf)
Algorithms and Economic Justice | Rebecca Kelly Slaughter 25 Much of today’s digital economy is fundamentally geared toward maximizing consumer attention and then monetizing it—the more eyeballs and time spent, the better. (extracted from728.pdf)
Algorithms and Economic Justice | Rebecca Kelly Slaughter 27 these efforts.81 As the researchers suggest, these are “the dangers of crowd-sourced, uncurated content combined with engagement oriented, gameable recommendation systems. (extracted from728.pdf)
Algorithms and Economic Justice | Rebecca Kelly Slaughter 29 mechanism—or do away with it altogether—at its discretion, and the public would be none the wiser. (extracted from728.pdf)
But the FTC is also responsible for promoting competition, and the threats posed by algorithms profoundly affect that mission as well; moreover, these two missions are not actually distinct, and problems—including those related to algorithms and economic justice—need to be considered with both competition and consumer protection lenses. (extracted from728.pdf)
Algorithms and Economic Justice | Rebecca Kelly Slaughter 31 implications of algorithms in antitrust law is well beyond the scope of this article, but I will briefly highlight a few of the ways competition can be imperiled by use or misuse of algorithms.93 These topics include traditional antitrust fare such as pricing and collusion, as well as more novel questions such as the implications of the use of algorithms by dominant digital firms to entrench market power and to engage in exclusionary practices. (extracted from728.pdf)
Moving forward, competition enforcers may deploy 97 In 2015, the US Department of Justice brought criminal charges against two e-commerce companies in United States v. (extracted from728.pdf)
Dep’t of Justice, Former E-Commerce Executive Charged with Price Fixing in the Antitrust Division’s First Online Marketplace Prosecution (Apr. (extracted from728.pdf)
6, 2015), https://www.justice.gov/opa/pr/former-e-commerce-executive-charged-price-fixing-antitrust- divisions-first-online-marketplace. (extracted from728.pdf)
Algorithms and Economic Justice | Rebecca Kelly Slaughter 33 their own machine-learning technology in an effort to detect collusion.99 Indeed, the United Kingdom’s Competition and Markets Authority is already deploying online price monitoring in an effort to detect illegal resale price maintenance.100 Even absent collusion, algorithms can fuel personalized pricing practices that may alter the competitive dynamics of a market in ways that harm consumers, for example through supra-competitive prices.101 As more data is collected about consumers, pricing algorithms may be able to help sellers better gauge a consumer’s maximum willingness to pay.102 For example, in 2015, the job-matching firm ZipRecruiter, changed its flat $99 subscription fee to range of fees decided customer by customer based on data provided in a survey by the potential customer. (extracted from728.pdf)
105 McSweeny & O’Dea, The Implications of Algorithmic Pricing for Coordinated Effects Analysis and Price Discrimination Markets in Antitrust Enforcement, 32 ANTITRUST 75 (Fall 2017) (“A merger that might previously have required an analysis of competitive effects in one relevant product market may Algorithms and Economic Justice | Rebecca Kelly Slaughter 35 Of course, the concerns of antitrust law extend well beyond pricing, especially in the kinds of data-dependent digital markets that broadly deploy algorithms. (extracted from728.pdf)
Using the FTC’s Current Authorities to Better Protect Consumers There is no question that the critical algorithmic problems identified—faulty inputs, faulty conclusions, failure to adequately test, proxy discrimination, surveillance capitalism, and threats to competition—undermine rather than advance economic justice. (extracted from728.pdf)
Algorithms and Economic Justice | Rebecca Kelly Slaughter 37 I consider how enforcers—and the FTC in particular—can use current authority to address these new fact patterns. (extracted from728.pdf)
The FTC has four types of enforcement authority that provide the agency with some ability to protect consumers and promote economic justice in the face of algorithmic harms: our general authority under the FTC Act; sector-specific rules and statutes, such as FCRA, ECOA, and COPPA; the study authority of section 6(b); and the rulemaking authority of section 18. (extracted from728.pdf)
Algorithms and Economic Justice | Rebecca Kelly Slaughter 39 The agency can also use its deception authority in connection with algorithmic harms where the marketers of products or services represent that they can use machine- learning technology in unsubstantiated ways, such as to identify or predict which job candidates will be successful or will outperform other candidates. (extracted from728.pdf)
Algorithms and Economic Justice | Rebecca Kelly Slaughter 41 with protected class membership, the creditor may be violating ECOA.124 The FTC should investigate such conduct and, if appropriate, vigorously pursue enforcement. (extracted from728.pdf)
Algorithms and Economic Justice | Rebecca Kelly Slaughter 43 Currently, FCRA provides several important protections for consumers. (extracted from728.pdf)
Algorithms and Economic Justice | Rebecca Kelly Slaughter 45 see the Federal Register notice to get a sense of some of the questions under consideration.141 D. (extracted from728.pdf)
Algorithms and Economic Justice | Rebecca Kelly Slaughter 47 IV. (extracted from728.pdf)
2019) (enacted) (requiring employers to (1) notify each applicant that AI may be used to analyze the applicant’s video interview and consider the applicant’s fitness for the position; (2) provide each applicant with information Algorithms and Economic Justice | Rebecca Kelly Slaughter 49 Effective transparency, however, must provide meaningful and intelligible information; it cannot simply overwhelm a user with information and trigger decision fatigue. (extracted from728.pdf)
Algorithms and Economic Justice | Rebecca Kelly Slaughter 51 in conjunction with the help and advice of subject-matter experts, empowers the FTC to address AI-driven harms prospectively. (extracted from728.pdf)
Algorithms and Economic Justice | Rebecca Kelly Slaughter 53 fulfill its statutorily directed mission. (extracted from728.pdf)
23 54 54 In the area of algorithmic justice, a section 18 rule might affirmatively impose requirements of transparency, fairness, and accountability. (extracted from728.pdf)
Algorithms and Economic Justice | Rebecca Kelly Slaughter 55 bias, discrimination, privacy, and security; (2) evaluate how their information systems protect the privacy and security of consumers’ personal information; and (3) correct any issues they discover during the impact assessments. (extracted from728.pdf)
While privacy legislation may not seem directly applicable to the problems we are discussing today, it can in fact play an important role in addressing algorithmic justice—and it is worth noting that the algorithmic-justice requirements imposed in Europe were done as a part of its privacy law, the GDPR. (extracted from728.pdf)
The Algorithmic Justice and Online Platform Transparency Act also provides strong civil rights provisions, building upon the transparency requirements of the Algorithmic Accountability Act and adding the civil rights protections Yale Information Society Project | Yale Journal of Law & Technology Vol. (extracted from728.pdf)
See Algorithmic Justice and Online Platform Transparency Act S. (extracted from728.pdf)
Algorithms and Economic Justice | Rebecca Kelly Slaughter 57 V. (extracted from728.pdf)
Algorithms could promote economic justice by helping distribute opportunities more broadly, resources more efficiently, and benefits more effectively. (extracted from728.pdf)
Algorithms and Economic Justice | Rebecca Kelly Slaughter 59 A joint publication of the Information Society Project at Yale Law School and the Yale Journal of Law & Technology. (extracted from728.pdf)
And it was in this mindset that the CNIL set a collective approach in motion, for several months overseeing a public debate with the help of partners from various sectorial fields (health, justice...). (extracted from106.pdf)
Among them, we could mention the “Ligue de l’Enseignement” (associa- tion that focused on education concerns), French Insu- rance Federation (FFA), French Ministry of Culture (DG- MIC), Open Law (association that reflects on the justice Ethical thinking concerns system) as well as trade unions such as CFE-CFC and FO Cadres (for recruitment and HR), etc. (extracted from106.pdf)
The views of the diverse stakeholders (trade unions, associations, businesses, researchers, citizens, etc.) across a wide range of sectors (from insurance to education, justice and healthcare) thus informed the writing of this report, which provides an overview of the ethical matters raised by algorithms and artificial intel- ligence in their current applications and their potential uses in the relatively short term. (extracted from106.pdf)
THE ETHICAL MATTERS RAISED BY ALGORITHMS AND ARTIFICIAL INTELLIGENCE ALGORITHMS AND ARTIFICIAL INTELLIGENCE TODAY The main functions of algorithms and AI across different sectors Education Justice Health Security Work, HR Culture Other Better identify Reveal the Tap into the Identify Understand Create cultural Fine-tune an learners’ different ways vast amount unsuspected social showpieces insurance abilities judgments are of scientific links for solving phenomena (painting, music) company Generating handed down publications gendarmerie-led in the workplace customer’s risk knowledge between regions investigations profile Allocate higher Allocate patients Match a list of Match education places for participation applicants to a “compatible” to candidates in a clinical trial job vacancy profiles (APB) on dating apps, Matching etc. (extracted from106.pdf)
The next step of what some refer to as “predic- on the sheer range of the spectrum of serious or less serious tive justice” would involve entrusting systems with the task matters in the use of such or such an algorithm. (extracted from106.pdf)
Incidentally, the CNIL-led public debate brought a legal rules and categories no longer on the grounds of controversy to light in this regard, concerning IBM’s Watson our ideal of justice, but so that they are more readily platform. (extracted from106.pdf)
And anyone deploying algorithms ligence be limited to crucial decisions, sectors where the that are likely to be used on a large scale should be urged impact on humans is undeniable, such as medicine, justice, to bear it in mind. (extracted from106.pdf)
Practical examples yet, are we absolutely certain that, within certain limits, of algorithms being used by the authorities as well as the forms of regional disparity do not, in fact, reflect a res- example of predictive justice give a clearer idea of this ponsible exercising of caution on the part of the judge? (extracted from106.pdf)
A complex chain a predictive justice. (extracted from106.pdf)
For it enables juries to take on board new insights over the course of Similarly, at the symposium on predictive justice orga- hearing the different arguments, and to change opinion, nised on 19 May 2017 by the Lille Bar, Law Department as shows more clearly than any demonstration the film of Université catholique de Lille and the Douai Court of by Sidney Lumet, Twelve Angry Men. (extracted from106.pdf)
Predictive justice applications are being subjected to particularly close public scrutiny as regards their Management Profile for Alternative Sanction) tool designed to come up with a recidivism risk score for prisoners or defendants on trial. (extracted from106.pdf)
Medicine our day-to-day lives, as long as answers are and justice are other sectors where this question might forthcoming. (extracted from106.pdf)
It is paramount that they have eness of the ethical dimensions of a decision-making the fullest possible awareness of the ethical and social process that must not exclude human intervention and by implications of their work and of the very fact that these honing critical thinking in some particularly sensitive sec- can even extend to societal choices which they should tors, such as medicine, recruitment, justice and perhaps not by rights be able to judge alone. (extracted from106.pdf)
The partners in the public debate • Allistene’s research committee on ethics (CERNA) • Institut Mines-Télécom (IMT), Research Chair “Values • Bordeaux’s Cognitique Institute (ENSC) and Politics of Personal Information” • Bordeaux University • Laboratory for Collective and Artificial Intelligence (LICA) • Caisse des dépôts et consignations (CDC) • Law Department of Université Catholique de Lille, • Club des Juristes (thinktank) Centre of research on relations between risk and law • Collège des Bernardins • Law Department of Université Catholique de Lyon • Complex Systems Institute of Paris Ile-de-France (ISC-PIF) • Ligue des Droits de l’Homme (Human Rights League/LDH) • Confédération française de l’encadrement – Confédération • Ligue de l’Enseignement (Education League) générale des cadres (CFE-CGC, trade union) • Lille 2 University • Communication Publique • Lille Association of Lawyers • Conseil National des Barreaux (national institution • Lyon’s administrative court of appeal that represents all practising lawyers in France/CNB) • Microsoft • Conseil Supérieur de l’Audiovisuel (independent authority • Ministry of Culture, via the General Directorate of Media to protect audiovisual communication freedom/CSA) and Cultural Industries (DGMIC) • Conservatoire National des Arts et Métiers (leading • Ministry of National Education, via the Directorate higher education and research institution dedicated of Digital Technology for Education (DNE) and its Numéri’lab to adult continuing education/CNAM) • National Academy of Technologies of France • Douai court of appeal • National Institute of Higher Studies on Defence (IHEDN) • ESCP Europe, IoT Chair • National Institute of Higher Studies on Security and Justice • Etalab(body that works in France on data sharing (INHESJ) in the public sector) • National Institute of Applied Sciences (INSA) • “Familles rurales” association • Necker Hospital • Federal University of Toulouse • OpenLaw (association) • French Association for Artificial intelligence (AFIA) • Paris II University • French Association for Employment Law (AFDT) • Randstad • French Development Agency(AFD) • Research Centre of the National Gendarmerie School • French governmental advisory council on bioethics issues of Officers (CREOGN) (CCNE) • Rhône Département-level Council of the Medical • French Insurance Federation (FFA) Association • French National Center for Scientific Research (CNRS)’s • Renaissance Numérique (thinktank) ethics committee (COMETS) • School of Advanced Studies in the Social Sciences (EHESS) • FO-Cadres (trade union) • Sciences Po Lille • Fondation Internet Nouvelle Génération (FING) • Sciences Po Paris • Fotonower • Société informatique de France (association devoted • Génotoul societal (bioscience and ethics platform) to computer science/SIF) • Groupe VYV (MGEN – ISTYA – Harmonie) • The Future Society at Harvard Kennedy School, AI Initiative • Imagine Institute on genetic diseases • Universcience • INNOvation Ouverte par Ordinateur (INNOOO) • Visions d’Europe (association) The other contributors The 37 citizens who took part in the public consultation • A rbre des connaissances (association) organised in Montpellier on 14 October 2017. (extracted from106.pdf)
23/01/2017 LAUNCH EVENT ROUNDTABLE SESSIONS “Algorithms and humans” and “Fairness, transparency and plurality of algorithms” > CNIL 23/03/2017 SYMPOSIUM “Towards new forms of humanity?” 25/03/2017 > Universcience 31/03/2017 CONFERENCE “Algorithms and law” > Lille II University 06/04/2017 CONFERENCE “The choice in the age of Big Data” > Sciences Po Lille and Visions d’Europe 08/04/2017 DEBATE “The governance of emerging technosciences” > German American Conference at Harvard University 18/04/2017 DEBATE “Transatlantic perspectives on: AI in the age of social media; privacy, security and the future of political campaigning” > The Future Society at Harvard Kennedy School 18/04/2017 ROUNDTABLE SESSIONS “Big Data, human resources: algorithms on the agenda” > FO-Cadres 04/05/2017 CONFERENCE “Fairness of algorithmic decision-making” > Toulouse III – Paul Sabatier University 16/05/2017 DEBATE “Will digital technology spell the end of the rule of law?” > Collège des Bernadins 19/05/2017 SYMPOSIUM “Predictive justice” > Douai Court of Appeal, Lille Association of Lawyers and Law Department of Université Catholique de Lille 02/06/2017 WORKSHOPS “Fairness of algorithmic decision-making and processing” > LabEx International Centre for Mathematics and Computer Science in Toulouse 64 HOW CAN HUMANS KEEP THE UPPER HAND? (extracted from106.pdf)
THE ETHICAL MATTERS RAISED BY ALGORITHMS AND ARTIFICIAL INTELLIGENCE EVENTS 08/06/2017 DEBATE “Algorithms in healthcare: what ethics?” > Groupe VYV (MGEN – ISTYA – Harmonie) 14/06/2017 ROUNDTABLE SESSION “Artificial intelligence: ethics, at the intersection of HR and Big Data” > Confédération française de l’encadrement – Confédération générale des cadres (CFE-CGC) 16/06/2017 DEBATE “Algorithms, employment and ethics” > French Association for Employment Law (AFDT) 19/06/2017 DAY “Ethical algorithms, a moral requirement and competitive advantage” > Allistene’s CERNA and Société Informatique de France (SIF) 19/06/2017 SYMPOSIUM “Human, non-human in the age of artificial intelligence” > Paris II University 21/06/2017 SYMPOSIUM “Artificial intelligence: autonomy, delegation and accountability” > Bordeaux’s Cognitique Institute (ENSC) 22/06/2017 WORKSHOP “Ethics of algorithms: implications for healthcare” > Genotoul (bioscience and ethics platform) 22/06/2017 CROWDSOURCING WORKSHOP “Artificial intelligence and law” > OpenLaw 22/06/2017 SYMPOSIUM “ The many dimensions of data ” 23/06/2017 > Institut Mines-Télécom, Values and Politics of Personal Information Research Chair 27/06/2017 SYMPOSIUM “Security and justice, the challenge of the algorithm” > National Institute of Higher Studies of Security and Justice (INHESJ) 28/06/2017 MOCK TRIAL AND ROUNDTABLE SESSION “ Ethique, algorithmes and justice ” > Law Department of Université Catholique de Lyon and Lyon’s Administrative Court of Appeal 28/06/2017 STUDY DAY “Admission Post-bac, textbook case of public algorithms” > Fondation Internet Nouvelle Génération (FING) and Etalab 03/07/2017 DAY “Algorithms and digital sovereignty” > Allistene’s CERNA 05/07/2017 DAY “Ethics and artificial intelligence” > French National Center for Scientific Research (CNRS)’s ethics committee (COMETS) and French Association for AI (AFIA) 22/08/2017 DEBATES on algorithms in education. (extracted from106.pdf)
å£ pÔQ ô3qäÔP6ûeju¬&·Õê'V¬$«X§ä¯+_"R !ßõrlý6  A=Õ  V¦ù6ÁÎx7Í¹; [RÜQ¥e4IÙÇï x)?n¡­gO>Osw½NWßÝÌÇ¯¼r=¶Þ¸÷7ÞkKOè½øá^ìTß"· %Y÷²#BÂo[Cï)Q!ÀËÌÜ¸yµ#NÉ0ýÎ/¬ïÏ¾&õ÷?òCÁ weEÞøMmË=Ø¤Z; ^¯£{´è¢p­GÊÛ«`i \ÚÑ+¥|Zm Jào ýá:·Ý¤ -²ûG½ÞñÙÔÔe} ´Ò,`Ýï´¬î7x.kÅÒ·bJrÒ]Ì,²^Cä&T±`fÚ Ðò'k¤þìâ¢òc"æ?'(áçn+§cPÓe« û -ygV¼ ¸¹cphSõ¯¦±°¡¢Pd6K{<$ÖUò©hlÆJo% *¿-WWöÛbµu=SÁÖÏwo¶·×Ee9êG*Éâò_»Ó ÷z'{léë  ÅìEäè#p¶ÿ3^í1m]gü{ó `^ÆüàÚ×`Û`üc c^G MÓ(éJ-ÑÔlíH)Z¦Jímª¦½RiR4M[2çÏ®R)*¶iZ©¦IÚF]7u¶%ïwÎ½ ²¨Ïç»ç{Îù¾óß÷ûFÜäXUötëyøBuÛåÀ{³c> ÆgJ·[ÚXTÍ5¢F·e³sÓV@\3í íëµzG÷»#ûÞRýîþÎtÈbëÍ&& Õ¡§ãß~~±c23ß+¸w#S]ÌæÑôpùpÿlàM­ÀQöSA*  £å[¹¾Îâ°GmM[]¡Uë §7Öï _ùæ±¿y hV0ÿî$7YFPð F8xÙóÿ+Â=°ª" pºº-¶|Îê*ê* ó¹$àHâ 8p°~Bÿ¨Hâ`H¢ ×óø[B×= ¯SR´ òVÒ4)\¸øÏÔ;Økì ,-,, û&-½c£SS£LÉdRm ¹GöjË2±©C¦bríÞ÷XÈ2:k ð jCþÃëA5§0?÷p<ë7=­gÕog ÜÓzµ~c©(mf1ýævrZ=åZåÒ2àâbØYV&©aÍÂYSd¶Ðô¨0ÎY¢Æ2Í=È»»Y #Âß,e ®~ ;;I/#ïäDÇIoÕK½+Jß´XÇ Ô=îu²N¥CxpÅ«* }qÞÄDq õm×lZÛæ¸vè6>K»9ÍâFRl ¨¿ËQ#n80¼Qi© \it¨xTÅîPØY#òÐú¤ª­kÐÕ32µtv·Á7Øá{ôzOÔöÃÚ:áÜ°¦E{î¹ûî°ÃÜÒ)5ïÆ × &íøô©Ñ Lµ;c>{­¼Û'Å; ÍÎM {õoÊÙÎ¤gÂö=áåP{µ¹·mGk£©ºÃóàªÓv£ªÅæn­ ÷ó È£Âï73ä÷9 Ï^á¶3bb'B}ÛH8Âë +f¾$"nºiù=b" M¯@á°êç ¨NÚRü*h;¹&¯òee´ ) |Pú ¬H  ØC!Ed Óø>,7AL|×2ví^QH ïpñ \ÉS²S*ýf #J%XÄXbd~þqöÂÓáÈ3¤íkp%|~iIcÔ+õùM&ä Õ M>¿oúDÿÈW³>ïÌÒðG1u¨»ûkÑ/¿>;{éxÜýä«Ïö<9ÔáÛ½ÆÐL$22vZë+Nãá½ÞÄbÚÕ}àäÐ®aLðV·÷Üïê+<Õ}ñIï%<¢ ;8Ü47JZà;SÍ*5­Úiãuã¯³Ócêv¡V¦Óè3çtù )ãéÖx+×a¼þ÷§©<¼M^D ÕÆæ/®ã Sf¢@PÑð{ÎÚÚj T[ ; VþÁù<SøæÉ$:dV ÕI¡¿Ï`Í÷*ö¾÷JúöâÌæ{Ñí¥4ìE^BVAú±°²ë*ó­aO~÷ ÙQÞØÐú¢ÍPÓqÄaÝ)k 6¾vÖPÖÞS´ö0½Mþ&|øèµÕz¶ucí*¬Ý ø"  H4óm¼R°6½½mqJ<ô¹!|5\Û×P}Í×(_U²5_g Ö>{x Ø[èØ" ÷{Üà¶c}ú*ÖI÷gÉ»<.t¨1®ß Î=9lOa×{0n¢ºíP[êFÖaT¤ [6@ôºÂ>eu¬ É³Âý5EÁô«éÙþPt¨z¬B øfm= ¬ØÊOJFúj½Éº«ÓðÌ>ÿp·¹lqµ(îàßüIºWøifw«ù Eño)°ÞWª Nò3¦®v ú ÿ`GÚèj¥åælòáÅ=ÙàÅÃvËÜÒY>þ~þ$YÛ¿jk~­åtÌ¯¦fBÎY\kíJ8Òí~#R+-'ºL42zviÎbÇüÁlæms/,+û¯È/Ð,ÎMdwqóî²Ð(.K#¸4@+héÈúý×ÅÓkß"H¯ù;â ÎQDÔ#ñùÈ/s¸Ø~ÒÅOsNS\QÚzµmU[ïæ)«klòcè¸Úa` ÖÁ8k¥Z øXiÂÒC%b¡½Ã(k MôË|Ã} fm[ ÏÙ oJm82úÈÞ«ôÃu#==fÍf páCîAFÎÏ eÒ³[ÂÿúòC ðX(W¸%¼MüIr ÝÏ!â[mT¹³ëT(¾Ê,¾F,hÙg« »à ÂC9Á R-ág#jqN q ÊKì8Ì=äâù°=qÊ(ÚV}³U×Ä â\ÞðN#oäCdµ°®¬¾:u°§çà©ÔF}jØé ~*ºkµóûöM´;«ì­ÎNU® iªºggfg»7ÚXÇÈh,zd¤cSùÒÂ ~§Õ`{ gù¿2Ho­{é-ºL¾LÎ¨þÒ½Ûï ªÝ%UâÇÌõN½ÏÛc­'××òUe  Âc«59 ºþw¤òõµõµ²ø<Â÷É_ìïSEfâ¤¡ÿ&>®}h´ uÄÍÐFî4 µ[G2âÉ°~ÁÏ¿Ë°±ô8Þ}æsßGÈýÔXè}ÒDÿE´t ØopÌ ÇHTxí0Ä¹ç·È,½HbtSÐïYÑwèS sj{íq 1Ò,üìæI£x4 GI°ìÀ^ìÂ~+ dâÈ È ÚåúoÈæ ßÓib«1Oaï¸(qjç¶;ÑvÏ¡ÿ¢EÖÖ"C¿öÏhë]ú ûåUÅñ3÷Þvû~nK!ÅêÂÖZj¡MiE -P ´%]Øm{ÛínÙÝÒ ÈÃ `¢  F>ØT Ê7 >¢H¢È4ð¡"ëfgËv-)ò­wòÛsæÜsïÌÇ»¬ãPHÅ ¾A1Ë¥ (]¡,VKu¬6ð«@ùE ÊÙ+ ¬ôV`¦zæFå OeôG©¡%Ê>*Vchû ¬ÊÒÑW+{í^¢yì,Ú²bÎ8çDÜE BÆ¢!LÝ[«¦f¶m­ÁÜïOb½x×5JQ.#®?È¢©Z£(ðÙDeì·ÀßJ Ú\É¢ìÆ}?ÆÛú)ýiTû)OU`¯¤AqJbðÓdå(üüT ü¹­óµ óèÁ½]èo ÖÓ*À×ë¸3¤÷ÀXÒÕ&Êçð½R/ýmÐóþ7éÃê|¯rR$}+¥L¥4ð+ö1ßÏ+h0ð;¸n¢^Á4ÔoAªÙMZjùÞ¸ÂsÌ+°ÞE>`Í`b/yA¢æÓÓCyb4JiªàQ| ±ß¥®~1å¹ÇC³A¹ÈA gLR àãÄA~ !rrÕXÀs]8"ç= äÂ"'>Ô½¤qD®çÞ)U°@æáÿNþÎóòg£C9}?Í VP&ë#1EÙ|¾ó3 ì ¨çgÁ÷wÅ ïÃcBýI;!øùÉiØ«3?Bð3Qc ?ãÂgÝÃàg` ¬ñØÿg©ÝÁ ½X6á\ÛÜÔ5¸,À 2XöK æ©ë·GÜÏ~­£û©ðÔE­ ,%³ê\ú2èuèwê¨7B¾ Ú`7P_=Óú~zô¨øZ$\ïÛÂäNIE_IJÂàïHIa¶Uàm»ÁpÜïS  >{%o~É ²åCþÖ.ð ð×Á%° 4I ð8 ¾ ~±N^ð5øà.Ð ¯K> îß ³­á6xo´d?ZÃàmµË÷¶ÐV»ð¹Á¿0ÉEqpÖ;´>úêU,}ÔÇ¦³mì¶Ò¦V·ò¡9´ãÚ¥¨õQFGGï¾c*7 3ÝY³#æjì±îØObïÇm; ßßðdÂk _&* ½ÌI®%¯N¾RÒÒr=uyêÁÔÓÒÒE9®¥÷¥ßËhË¸ùýê¬¬f %¥Ø¼Þì6o54÷r={röôìY#w³ÏgJNðêP&vþ« /¹ñ2^ÆËx½ðs@9Dðo#Eñ®8Øß¨'ÐAi'Ê  ¸TJC-¨kÐK¤ }ÔM´YY%¦Å¢V Ôñ¯þºBIl¢ÔUêÓ¤®A_*õhè¤n¢£ìzQá}Ñ¼¥ú|·³@ëré^£¥ÕïÓ½NÓ»Ùé(¨©©ªª®²Ái¹ËÙSgwû ÅÂßÉõ^ÝðévÝïµ; vo»îi~Ð^>Zi1|~§×éÐ ·ÞawoÑÛº¼Ïalô ·Oïöx]nÃá, ¤SÒ ÁGK!ç¼ÔIÐç EGÝ j%?ùDÍ éÜ_ 7~;D[ µ!R¯ðràw#| ¼Å-ZíïsáN7¬¼å,"q> ráÂ°@H±±ü#Õ¸rJ=9Jp±¡$Ä=ÜLÏGXðu ÎàÕiÃS%ZaQ©^³Ã, ¢í´Ã?]Z¢Ú:Ôh²´D£C43ì)¡q+<Ò¥%¾ ËíÊXZM °0­COÅÑ³he bÐÉ/Ç3|Y- ö ´øÀ(Íl@½j»a® À`dó`BrG¤Lá²KúA6¡ÌÇíYâ¾GÊd.»øóAéò°ÿ0 ¸+QY endstream endobj 379 0 obj < >stream 168586311 Townhouse No People Sword Justice Fountain Ostzeille Romerberg Romer Statue Sculpture Half-Timbered Roman Justice - Concept German Culture International Landmark Famous Place Travel Destinations Horizontal Frankfurt - Main Hesse Germany Europe Day Summer Sky House Town Square Cityscape Symbol Weight Scale Juergen Sack The 300 years old Statue of Justice (Justitia) and the RÃ¶merberg in Frankfurt. (extracted from371.html)
Copyright © United Nations Interregional Crime and Justice Research Institute (UNICRI), 2020 Viale Maestri del Lavoro,10, 10127 Torino – Italy Tel: +39 011-6537 111 / Fax: +39 011-6313 368 Website: www.unicri.it E-mail: unicri.publicinfo@un.org © The International Criminal Police Organization (INTERPOL), 2020 200, Quai Charles de Gaulle, 69006 Lyon – France Tel: +33 4 72 44 70 00 / Fax: +33 4 72 44 71 63 Website: www.interpol.int E-mail: edgci-ic@interpol.int 2 FOREWORD Crime is not stagnant. (extracted from660.pdf)
We have strived to shape this forum, giving it meaning and purpose, and positioning it to grow into a global platform for cooperation and collaboration amongst law enforcement on AI This report on AI for law enforcement is the most recent product of the collaboration on AI between the Innovation Centre of the International Criminal Police Organization (INTERPOL) and the United Nations Interregional Crime and Justice Research Institute’s (UNICRI) Centre for AI and Robotics. (extracted from660.pdf)
The increasing interest and attention these meetings are receiving is both a reward for INTERPOL and UNICRI and reveals the growing relevance of AI for the criminal justice community. (extracted from660.pdf)
Human rights, civil liberties and even the fundamental principles of law upon which our criminal justice system is based may be unacceptably exposed, or even irreparably compromised, if we do not navigate this route with extreme caution. (extracted from660.pdf)
The chapter be- gins by presenting the general principles that law enforcement should endeavour to adhere to, namely the respect for human rights, democracy, justice and rule of law, as well as the related requirements of fairness, accountability, transparency and explainability that should be adopted in order for law en- forcement to meet these principles. (extracted from660.pdf)
Equally, it is a valuable exercise for policy- and deci- sion-makers in the broader criminal justice community to, from a legal and ethical perspective, prepare frameworks for the eventual integration of such technologies into law enforcement. (extracted from660.pdf)
The seminal 2019 white paper AI and Ethics at the Police by Leiden University and TU Delft16 suggests that, from a legal perspective, to act responsibly means “to accept moral integrity and authenticity as ideals and to deploy reasonable effort toward achieving them.”17 Striving for moral integrity, in turn, implies “adhering to the values of freedom, equality, and solidarity.”18 For the purposes of this report, however, a more straightforward understanding will be adopted and the term ‘responsible’ will be framed in line with the Oxford Dictionary, which defines ‘responsibly’ as acting “in a sensible or trustworthy manner.”19 In this context, the responsible use of AI by law enforcement should be understood as use that enshrines the general principles of respect for human rights, democracy, justice and the rule of law. (extracted from660.pdf)
Justice for Hedgehogs. (extracted from660.pdf)
It has been heralded for its role in building “an area of freedom, security and justice with a high level of data protection, in accordance with the EU Charter of Fundamental Rights.” Aiming at protecting individuals’ personal data, while guaran- teeing a high level of public security, the LED provides rights for data subjects, as well as obligations for “competent authorities” when processing data for “law enforcement purposes”, i.e., prevention, investi- gation, detection, prosecution of criminal offences or the execution of criminal penalties, including the safeguarding against and the prevention of threats to public security. (extracted from660.pdf)
The development of trustworthy AI systems should be based upon established fundamental values, such as the respect for human dignity, democracy, justice and rule of law, while, at the same time, guaranteeing the freedom of the individual and citizens’ rights in order to ensure equality and non-dis- crimination. (extracted from660.pdf)
45 RECOMMENDED ACTIONS Define a statement of principles on the use of AI in law enforcement that will guide law enforcement to ensure respect for human rights, democracy, justice and the rule of law and support it to prioritize the key requirements of fairness, accountability, transparen- cy and explainability, as well as safety and robustness; › Develop guidance for law enforcement on the implementation of new technology to support and encourage law enforcement agencies to explore and invest in new AI opportunities and to develop training in new AI applications and disseminate best practices; › Create a knowledge-base with the law enforcement community on the requiremen- ts for the adoption of AI, such as what kinds of problems AI is capable of tackling, the current or inherent limitations and the resources (tools, data, expertise, com- puting power) required to implement AI solutions; › Develop guidance for law enforcement on the admissibility of AI in court that as- sesses the impact and results of the specific use of AI in courts, while ensuring the respect for human rights and rule of law; › Create an expert advisory committee that can provide guidance to law enforcement in terms of legislation and serve as a forum for discussing appropriate legislative models with legal experts and other key stakeholders; › Identify an external global body to provide advisory support to law enforcement on ethical issues and to provide support in carrying out audits to check whether a system is responsible and complies with legal requirements; › Foster a community and organize training courses and workshops to attract and connect different stakeholders from law enforcement, industry, academia, civil society and international bodies with the diverse backgrounds and essential per- spectives to gather and synthesize views from cross-sections of society, in order to provide a balanced and facts-based picture of the opportunities and challenges of the use of AI and to highlight the application of AI to law enforcement and provide hands-on support. (extracted from660.pdf)
52 ANNEX II LIST OF ABBREVIATIONS ADM Automated Decision-Making AFP Australian Federal Police AI Artificial intelligence AI-HLEG High-Level Expert Group on AI AiLECS Artificial Intelligence for Law Enforcement of Community Safety CCTV Closed-Circuit Television EU European Union FATE Fairness, Accountability, Transparency and Explainability GDPR General Data Protection Regulation GPS Global Positioning Services IC INTERPOL’s Innovation Centre IEDs Improvised Explosive Devices IGCI INTERPOL’s Global Complex for Innovation INTERPOL International Criminal Police Organization LED Law Enforcement Directive 2016/680 MAS Monetary Authority of Singapore NLP Natural Language Processing non-POI non-Person of Interest NPA Japan National Police Agency OECD Organisation for Economic Co-operation and Development R&D Research and Development UAV Unmanned Aerial Vehicles UNICRI United Nations Interregional Crime and Justice Research Institute ZITiS Central Office for Information Technology in the Security Sector 53 ABOUT INTERPOL INTERPOL is the world’s largest international police organiza- tion. (extracted from660.pdf)
ABOUT UNICRI The United Nations Interregional Crime and Justice Research Institute was established in 1968. (extracted from660.pdf)
Within the broad scope of its mandate, the Institute contributes, through research, training, field activities and the collection, exchange and dissemination of information, to the formulation and implementation of improved policies in the field of crime prevention, justice and emerging security threats, due regard being paid to the integration of such policies within broader policies for socio-economic change and development, and to the protection of human rights. (extracted from660.pdf)
In 2017, UNICRI opened its Centre for Artificial Intelligence and Robotics in The Hague, the Netherlands, with a view towards advancing understanding of artificial intelligence, robotics and related technologies vis-à-vis crime prevention, criminal justice, the rule of law and security. (extracted from660.pdf)
1 On behalf of the German Presidency of the Committee of Ministers of the Council of Europe, the German Federal Foreign Office and the Federal Ministry of Justice and Consumer Protection are proud to have hosted a virtual high- level Conference on this issue on 20 January 2021, with the support of the Council of Europe. (extracted from338.pdf)
Department of Commerce The following Federal departments and agencies are represented on the Subcommittee on Machine Learning and Artificial Intelligence and through it, work together to monitor the state of the art in machine learning (ML) and AI (within the Federal Government, in the private sector, and internationally), to watch for the arrival of important technology milestones in the development of AI, to coordinate the use of and foster the sharing of knowledge and best practices about ML and AI by the Federal Government, and to consult in the development of Federal research and development priorities in AI: Department of Commerce (Co-Chair) Department of Veterans Affairs Department of Defense United States Agency for International Department of Education Development Department of Energy Central Intelligence Agency Department of Health and Human Services General Services Administration Department of Homeland Security National Science Foundation Department of Justice National Security Agency Department of Labor National Aeronautics and Space Administration Department of State Office of the Director of National Intelligence Department of Transportation Social Security Administration Department of Treasury The following offices of the Executive Office of the President are also represented on the Subcommittee: Council of Economic Advisers Office of the Vice President Domestic Policy Council National Economic Council Office of Management and Budget National Security Council Office of Science and Technology Policy (Co- Chair) PREPARING FOR THE FUTURE OF ARTIFICIAL INTELLIGENCE PREPARING FOR THE FUTURE OF ARTIFICIAL INTELLIGENCE Contents Executive Summary ..................................................................................................................................................... (extracted from304.pdf)
30 Justice, Fairness, and Accountability ................................................................................................................. (extracted from304.pdf)
Public- and private- sector investments in basic and applied R&D on AI have already begun reaping major benefits to the public in fields as diverse as health care, transportation, the environment, criminal justice, and economic inclusion. (extracted from304.pdf)
Use of AI to make consequential decisions about people, often replacing decisions made by human-driven bureaucratic processes, leads to concerns about how to ensure justice, fairness, and accountability—the same concerns voiced previously in the Administration’s Big Data: Seizing Opportunities, Preserving Values report of 2014,1 as well as the Report to the President on Big Data and Privacy: A Technological Perspective published by the President’s Council of Advisors on Science and Technology in 2014.2 Transparency concerns focus not only on the data and algorithms involved, but also on the potential to have some form of explanation for any AI-based determination. (extracted from304.pdf)
Many areas of public policy, from education and the economic safety net, to defense, environmental preservation, and criminal justice, will see new opportunities and new challenges driven by the continued progress of AI. (extracted from304.pdf)
The promise of AI has been compared to the transformative impacts of advances in mobile computing.21 Public- and private-sector investments in basic and applied R&D on AI have already begun reaping major benefits for the public in fields as diverse as health care, transportation, the environment, criminal justice, and economic inclusion.22 At Walter Reed Medical Center, the Department of Veteran Affairs is using AI to better predict medical complications and improve treatment of severe combat wounds, leading to better patient outcomes, faster healing, and lower costs.23 The same general approach—predicting complications to enable preventive treatment—has also reduced hospital-acquired infections at Johns Hopkins University.24 Given the current transition to electronic health records, predictive analysis of health data may play a key role across many health domains like precision medicine and cancer research. (extracted from304.pdf)
Autonomous watercraft may be much cheaper to operate than manned ships, and may someday be used for enhanced weather prediction, climate monitoring, or policing illegal fishing.27 AI also has the potential to improve aspects of the criminal justice system, including crime reporting, policing, bail, sentencing, and parole decisions. (extracted from304.pdf)
The Administration is exploring how AI can responsibly benefit current initiatives such as Data Driven Justice and the Police Data Initiative that seek to provide law enforcement and the public with data that can better inform decision-making in the criminal justice system, while also taking care to minimize the possibility that AI might introduce bias or inaccuracies due to deficiencies in the available data. (extracted from304.pdf)
Social justice and public policy institutions that do not typically engage with advanced technologies and data science in their work should consider partnerships with AI researchers and practitioners that can help apply AI tactics to the broad social problems these institutions already address in other ways. (extracted from304.pdf)
The use of AI to make consequential decisions about people, often replacing decisions made by human actors and institutions, leads to concerns about how to ensure justice, fairness, and accountability—the same concerns voiced previously in the “Big Data” context.62 The use of AI to control physical-world equipment leads to concerns about safety, especially as systems are exposed to the full complexity of the human environment. (extracted from304.pdf)
Justice, Fairness, and Accountability A common theme in the Law and Governance, AI for Social Good, and Social and Economic Impacts workshops was the need to ensure that AI promotes justice and fairness, and that AI-based processes are accountable to stakeholders. (extracted from304.pdf)
In the criminal justice system, some of the biggest concerns with Big Data are the lack of data and the lack of quality data.65 AI needs good data. (extracted from304.pdf)
It is important that anyone using AI in the criminal justice context is aware of the limitations of current data. (extracted from304.pdf)
65 Matt Ford, “The Missing Statistics of Criminal Justice,” The Atlantic, May 31, 2015, http://www.theatlantic.com/politics/archive/2015/05/what-we-dont-know-about-mass-incarceration/394520/ 66 Julia Angwin, Jeff Larson, Surya Mattu, and Lauren Kirchner, “Machine Bias,” ProPublica, May 23, 2016, https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing. (extracted from304.pdf)
Many areas of public policy, from education and the economic safety net, to defense, environmental preservation, and criminal justice, will see new opportunities and new challenges driven by the continued progress of AI. (extracted from304.pdf)
Social justice and public policy institutions that do not typically engage with advanced technologies and data science in their work should consider partnerships with AI researchers and practitioners that can help apply AI tactics to the broad social problems these institutions already address in other ways. (extracted from304.pdf)
Matt Ford, “The Missing Statistics of Criminal Justice,” The Atlantic, May 31, 2015, http://www.theatlantic.com/politics/archive/2015/05/what-we-dont-know-about-mass-incarceration/394520/ Jason Furman, “Is This Time Different? (extracted from304.pdf)
An inaccurate explanation of how the algorithm 9 This publication is available free of charge from: https://doi.org/10.6028/NIST.IR.8312 ______________________________________________________________________________________________________ arrived at its outcome could result in a miscarriage of justice. (extracted from476.pdf)
Science and Justice, 57(2):144–154, 2017. (extracted from476.pdf)
oecd.org/sti/ieconomy/oecdguidelinesontheprotectionofprivacy- 40 Privacy Commissioner, Office of the Privacy Commissioner Briefing andtransborderflowsofpersonaldata.htm for the Incoming Minister of Justice: Hon Andrew Little, October 2017, 34 OECD Guidelines, Article 7 para 2.1 https://privacy.org.nz/assets/Uploads/Briefing-for-Incoming-Min- 35 Ibid. (extracted from489.pdf)
First, any public agency or business in New 42 Privacy Commissioner, Office of the Privacy Commissioner Briefing Zealand that handles personal data of individuals for the Incoming Minister of Justice: Hon Andrew Little, October 2017, residing in the EU will need to ensure that their para 4.14 https://privacy.org.nz/assets/Uploads/Briefing-for-Incom- ing-Minister-October-2017.pdf Ibid para 4.14. (extracted from489.pdf)
This the Court of Justice of the European Union on resulted in human rights considerations being the application of the right to privacy under the elevated amongst the new legislative principles European Convention on Human Rights (ECHR) and decision-making criteria that the reforms have introduced. (extracted from489.pdf)
the administration of juvenile justice (“the Beijing Rules”). (extracted from489.pdf)
That same month, the Minister of Advisory Panel https://www.hrc.co.nz/your-rights/indigenous-rights/our- Justice announced that Cabinet had agreed in principle to allow courts work/review-new-zealands-constitutional-arrangements/. (extracted from489.pdf)
As a PRM initiative requires agencies to New Zealand’s child protection and youth justice share identifiable personal information without jurisdictions. (extracted from489.pdf)
Government Communications Security Bureau (GCSB) and NZ Security and Intelligence • Have you consulted the Privacy Commissioner, the Ministry of Justice and Service (NZSIS) and their oversight regime. (extracted from489.pdf)
The most common type for the Incoming Minister of Justice: Hon Andrew Little, October 2017, of complaints relate to adverse recommendations by the NZSIS as to para 4.2 https://privacy.org.nz/assets/Uploads/Briefing-for-Incoming-Min- security clearances required for employment, http://www.igis.govt.nz/ ister-October-2017.pdf complaints/ 150 Report of Special Rapporteur for freedom of expression, Frank La 147 Intelligence and Security Act 2017, s 185. (extracted from489.pdf)
173 See http://ec.europa.eu/justice/data-protection/international-trans- 178 Ibid. (extracted from489.pdf)
Beyond the familiar, these systems are for harm caused by algorithmic processing.232 also being introduced in critical areas like law, finance, policing and the workplace, where they The Special Rapporteur further observes that: are increasingly used to predict everything from our taste in music to our likelihood of committing a crime to our fitness for a job or an educational Recommendations and decisions that result opportunity.235 from algorithmic processing appear to spring from an inscrutable and unknowable black box, a kind of twenty-first century Delphic oracle As discussed earlier in the paper, AI is increasingly that seemingly makes unchallengeable and used in the criminal justice system. (extracted from489.pdf)
For example, authoritative pronouncements divorced from by the police to target resources or high- human agency.233 risk individuals, by the courts to predict the likelihood of re-offending and prisons in targeting As with surveillance activities by intelligence and restorative justice. (extracted from489.pdf)
Five written origin” and other “special categories” warnings must also be provided to judges when (Article 9); and prohibits decisions based COMPAS assessments are used.238 solely on automated processing, including profiling, which produces legal effects concerning him or her or similarly significant Concerns regarding the inherent risk of bias effects against him or her” that is “based that arises from algorithmic risk assessments on the special categories of information in the criminal justice sector were raised by referred to in Article 9” (Article 22) U.S. (extracted from489.pdf)
Significant racial disparities “Although these measures were crafted with the best of intentions, I were found to exist, with the formula more likely to falsely flag black am concerned that they inadvertently undermine our efforts to ensure defendants as future criminals, wrongly labelling them this way at individualized and equal justice… they may exacerbate unwarranted almost twice the rate as white defendants. (extracted from489.pdf)
and unjust disparities that are already far too common in our criminal org/article/machine-bias-risk-assessments-in-criminal-sentencing justice system and in our society. (extracted from489.pdf)
See https://www.justice.gov/opa/ speech/attorney-general-eric-holder-speaks-national-association-crimi- 241 Ibid p 28. (extracted from489.pdf)
ù{dÈæm[¬¨GÕ}äÂE/-³*Ëè÷ßu¯åUøéå.0±Ro³¨=D­> ÷ÉLxªµÌ¯BÆîrâ4 EpÎá¶Ì¦Ck£ØW1uÁ8 Hü7 ýw2SÉSÁÌ Ü´'¤\!G 5MGîá¯8|\¥äv4®©å·§iÌæÎh®°Rr¨R/òÓ|/b<[ó Ó`MüÒkAbNlbyL¯t{*Rn¹ ü~æ3s\¶{ o¥ÏKCLaß`¯lh°e É\ª*=Æ]fÏÆ<Ü¥¹IóT±q^ô% `^%e&<nòÛIU2íå¤>1öòÓô¤  Lø¥èÆLÀ=<;.£'ÙPYMDÚ6Ö+'ÐW^Ñ9Ôucrç÷:Øó ã^ÅRHòÕ3Êð8Ëõ$¿Ì°#Î/æ¼cE> ZÃ£!Ål¿qñ'ïòHÑáÇ7 Î83ìð :¦à4Ïù<¡w2=Ðïµý±²2cjdz|e1!÷Sxn1U0è[¢èU-º¹o øã'>ÿxä g&ëO¹nWðA!ØµÞzê­n­26ÓHM?B!AÏ Çç«bÌ´K ÊïmoI ¹£¯ñõ±Rrn)<fË®Ë÷Á©ùv»á^ y{»¹[´ÄbÝÔøRÑ1 FSX \1Öô*ÎÑt3Úà £¯ÍpÃã¢Gí 9èhæÄ»ÅXáeß Ø5µÞMjZÏ êÕ ê;®ÝË÷1qQráwÛ| =ß/íÿWx îöXîÓ¨bê´£¼¶?ËXÊÓà{6eÏ"ÿ/P¹Â endstream endobj 239 0 obj <</Type/Font/Subtype/TrueType/Name/F11/BaseFont/Times#20New#20Roman/Encoding/WinAnsiEncoding/FontDescriptor 240 0 R/FirstChar 32/LastChar 32/Widths 1954 0 R>> endobj 240 0 obj <</Type/FontDescriptor/FontName/Times#20New#20Roman/Flags 32/ItalicAngle 0/Ascent 891/Descent -216/CapHeight 693/AvgWidth 401/MaxWidth 2568/FontWeight 400/XHeight 250/Leading 42/StemV 40/FontBBox[ -568 -216 2000 693] >> endobj 241 0 obj <</Subtype/Link/Rect[ 68.65 163.91 526.35 173.63] /BS<</W 0>>/F 4/A<</Type/Action/S/URI/URI(https://www.washingtonpost.com/opinions/big-data-may-be-reinforcing-racial-bias-in-the-criminal-justice-system/2017/02/10/d63de518-ee3a-11e6-9973-c5efb7ccfb0d_story.html?utm_term=.720084735d73) >>/StructParent 114>> endobj 242 0 obj <</Subtype/Link/Rect[ 68.65 154.18 486.19 163.91] /BS<</W 0>>/F 4/A<</Type/Action/S/URI/URI(https://www.washingtonpost.com/opinions/big-data-may-be-reinforcing-racial-bias-in-the-criminal-justice-system/2017/02/10/d63de518-ee3a-11e6-9973-c5efb7ccfb0d_story.html?utm_term=.720084735d73) >>/StructParent 115>> endobj 243 0 obj <</Subtype/Link/Rect[ 147.42 125.02 515.59 134.74] /BS<</W 0>>/F 4/A<</Type/Action/S/URI/URI(https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing) >>/StructParent 116>> endobj 244 0 obj <</Filter/FlateDecode/Length 5388>> stream x¥<ËrGwEè ú]¯®î]#HY²=3ÞõZÚõAWâc 4|ÎõÌnµC$ÑïÂÉÙfwsµXîªï¾;9ÛíËõê²úròùþáo'_ V'¿,®oî»û»ÓÓêüû÷Õùç·oN>jh®ú|õö¨Zø_T¶m­+Ûw ¼¸¨ >iQ]oß¾i«küñÃÛ7UÛ´­ dõyùeöþ¾þ[õùÏoß| 7 êU3´9*x ë¹ÝáåÍ·z.úYu_ÛÙUUÏ»ÙÚ8 üqÿ  ÂÌVu7« óN6Ìæù2ÛîÇe=W³Ò°¡meÃJ3È¡kÌq ÊÆ z f×H ®WvºÙ®îgk$ôv[-îð=Q\ýì Ü.ð3ñ ¸ÓÏ~Ånìu=¸±;|¶-c¤l.R^}øù}uRóûÝîþö%þFØJw~§VMÛ-«ZÁFÑkÔì*Ñ7BÖRü«øùýOßWí JxÃ¤Cï|^#QÂ¶Èc'jó^¸ÍX9AÝ/ÜÄ»TiÓèO0/Ádu;þµÚï3m>ÉÏcéíÒ" Îö3À#$¦]ÐYzAÁ!F$îÝ-¼ÓÑ<D ª«À õ¼¦¼î_ªnA _*IïXÌ·E ·cXÞÃº'p-w%pÞEÏ'tc8( ³u`òmQFÛÆ0*ñ( (ÙMi¾ ²o¬ÎÏÂ9 -llÓK6òKÜôo$@ Ümì¥"©· ¿ûøÇ,@D qµ 10ª¨{ÙHÁaáç$HN¹#¤ÉIêr§O~~,)h ­5|ºo¸lÏZ<NÚ ãÈ:ºÇ.[|Üjüù{]RZh¤= ª³Øå7R!mIc+Ð¥xR v ;åÚuPoé¤8ömÚî8t§&°n½ÑbFsÿ/Ø5§ÕÒ#ECqp0 +ÿü;´Î¯H)9îÚ@?ÿÝ¯])=û¡Ö4½)Óª Æý& |ë§²ÅDZÝ´Ñ]t.ú¶é %5gÖv= WÕ¬Rà¨p\q´Óðq3¶;:v;âSeÝm[ªQ|þ n¢O\ÐH%ùÐ× ü¾x¦L·qjNÊ.£«nèQäZVåll¬³ªÍÕâ:¨Öd=ï¢OFt%{ Ö Ëå ÄÐ#ã§5ÃÃ{67þ±òdÑÕSÀ{PlMM4Ó¶(SR4­å°¿ÝÐf&è¿½ßÀ geº:ð sh¸ÒÛb( ¸"X~ÿT«=þÐáÍÅ#(sëó</6dµé`\²×;«»½!e¶,[;Ó(Åg FV¸ Ê$Ã|»ºÛyÖ?îÖÁ#»t.= ,nJk0Ì`3¢â MÚ+*.bÙE ¶aÊ wOQ[ÐDÿ;G)-Ã¹ÉÒøÇ÷µa ÷'·ÙHg~Åaóëý+Fí»Ãñ}Mf °ù§$3+°"Oõ|H#WÐ!ðãôI¢]x´j×/YxõrÈQCÜQVãPÅä§Y~­ËC]5³_%ôt7°:Ï  ÅåKZWäãváat[£nÐ¶áD%«æ7#y¡§&FWa+ß :cA&Es+»»ÁT úÑ:ðÉd øp´À¦æËA eQ 0ÐY 2ô>óEî"pèR²êèvÅWr$tñ|Ëé8#°ÿ6Ü¸-\YAl þ¹h¨ú U[ö%éÄ ög¢ßéuïGýiîÜc¿Ì²Ñº|q ïiæ !14*øç8c<ÕnÎ]Ø»pòµù×²ÝÐX6]1÷`rÌ@~öÕYv&Ú²I G0r<Zm@.¶>¦Ì 6I<³µ:ý·(GSM¤¸Chmä@ò¦äN*XhÏ¹ÈáNßÓª¶9qOEJ´ 0Çóî¦ªÀk 49Ì.àbê¢ë!ÊgX´Yz8}HÞ³cñ¦[qbçµsÎÖf;>ÑµÃÏFN>v{ÒÏ¨g£¿|×¶úãé\ûß¢?<5øëìT|þ><§s|¡ÏNç ~· ^ÈßØÓ _{þ´cïÏÍ©& Å |°4½ký % sD¡¦sÓÐÄÅy4à4ÆÅUà4n ç MÅèYLÀÙø µîÃé|Ø;] Ç½ÀÙiy*,/~ð´xâ»´*­áqÀÞÚ1§¥Î÷ÂÇnS¡óFãiÔ~£{¢ã´ çZÐär>ß[ãÐZ7 - 'Ý7¥p¢oÑõx¹E ~Àà¦¼rT ØÃAäÚ!:Çd  #r_âÚÙ¥náÇÿ=fGÉjí\R1b]\¤TYIJ)~à´}HiAqØ«M´xDË½>èÔ_0®nQKÍL y2iÌ¥+ú» * ±> ÆhEÁBtY¨æøPòþ%)³ècDWñTL=YÄrÇÑa°m î 5h­§ûù¾öY¯+ÄÇV 3%ÎwO[Lé ¶·KòÖOÜï=¾Hæv/¶ÞÓ )ÁßÆý YmÑð)Ùc­5 3ZlàÔ|pÑ¹ð¥ MÃ%ñQM«cßkâÚ8Á:û3·I½jo"ô÷Þò%ýmln¢zgî# ¤ÝK´½³¢õZÝôhhÔ\M4}>5ÒÔ¹Q#Ä¨ÒM°MGhß ¿ºk4sQ;tå"è¹û9å>.8´J7Zr´Å| uYëÏø3V¹¾.£¶©VuïKåÕ¸@&d-Ìál0+iø¥GqêÍqë` X/ÂNJ¢ÿ÷ù_~s@ÛÃÑ Â)uÃwþ_0ònö®wíu&[(»FyØ±Û÷>Èöiîhä®¡ñC> = ã -p@gôÒ]N  2çÞ %î+aãhQÝIvå°Yå*¤ é\8{ãµ¹=¯¬¶Ìa °ÁºÐÚäÑ7zÆN,¶×áðæGð Ñ£bðâý=¼q~Öc çâ<åS]Áú+#p&Ü+ÿ Óåÿ\ ³Ó>ßx·Ã9ÃÃTÐOX«$S'Ã]`+C`)ñO[T<àgaaMSn£iËA_ñAì öÅåÊ*K`aMã<¢XpÛ§ð>S:yÜa´wëdï¼Î svvSät ?ó`3{ vL%$íuÆÃº jH_|ÎÔÄaÆÂµ@T¾£\y G 6²ÃLco)¤hTÏa£·c} ¹²3hÇYW/y¹ç ÜëÊã<{E¹"ð¯A ¼£iaE¡.äª ûõ¼j%S=±ÕÀ=L((ãSaÁ%äDªõ#Js®Òðq(Ø®ï ®s«¾XË¸0Ó¾ËÏè¦Ý9èÃûJÑ×h,Hfî(üýíl½àLÌÛFHpz*3¦í«^ÚéªÍêíßþTÝ%ß é¸®qí0Ò¦DÒ4Ffóý×Þ Û½×vºSüãÿ¸»h5 4ôÚ9eí(]çJNÌ¸diÕ©¨*5ÊÖkZñÒäYP ÒùýðòXPûsÿöiQ^òSÖ £X  æ_®^¾òwI CTË¬·åµíÐæÓË}BSn -xü]-?JËú@§egHlò¹VUS2GY=bÐÄ 5Då7ÏÙá ÃNµÌDÈò+üùÖ<Cx¥Ãü'|¤Ã§ó  p¤·éêp×ÕxZæ5ÔÀaó2FÈ¶LrXºU èÍ¸A NêÎ].!ë¢;&G©äjUInºÉ ì B®ZÌìø+×XdZdø¹3º¸ û|  rfRßÌe6~T­´ûªð°¬Ëý  ÌÙ²,þÓí=²Y|:â>ÂÕ¡ÁâR°¨ºÝE£uËðçí øWQÂì%å0ûa·mp(xÇG7îêaE>ÞäuÃ¨*4·mJ¨Î»Î%±ü&õ¸Ü­²n.oça7L÷ ;X [RÝpÌi9*ñÓ÷1ÊÙstÊZµ¶gm¹ªQüìDú·()ÅÆ&°<µîg,l×mä&ä®½¦üÝ8ÕãN wÆVµ+ '\§yÜöåedêzDvÛ µnfí +g<§­àxú-¤åCÑ2ò½à.w®;1÷iU$nRê5^+½ú*tÆÞÓýtðJJÅÓ |£l@(#Î`GGQ§Ó§Ùá» |Ê}»g]g u{Lj½î1?RÓJ¬P2Xg½ceþ`L0Kiq0§è«´k­l¸,Á ÑÑTÍïuµTØ´ÄÀ?b\  êjÎvàå]MN Ù% xK6~¼D ,ïsÃT"Bßt²ýS×Tô¯7£·¼}OÕ= ÖÀæÌ*vÂ>"e°ånV* 6Â oI«/÷ÐÂ)ÇHzo Ña!5=öëdùòG×Ö7äe¤%Ö >»7<ö Sî?PÞf¹óÔ¨lEüx´­7Å ßÔÊ+A}ÇC@ï ;_ÝÓé½¦¤úÚÊ×¾Ý¡ÏòXnIë¨ J¦·1¥¡R#®#ï#;PV uùÆj,^gKue\âÁâeÜf¼osLû5`±6¶OdÙzÖ¤à3UþYý@­¡l7AÆ<ºðí <~â,¥ÇÎuêhh³F=¸Ô[*WÅÜÜ+©_ï Ôw6OÎÅ ,íAeí! (extracted from86.html)
äºÜ:c'r[þß ä¸M,Ñ¿³Ào¯µÑ2ÇaOU¿Õçdi ¼róZAÃNéDì[oåEó:@¶×¶ÛÈJuL»Mø`LâQó­-ÛtÞ¾z|GDÍZ}´JÈ ©0v&É#õnùüëWPCÚP~ádðG¶ Lí¯è:¤v XxfÍïñ¸ø {WàÛ´_=duàÃïÉí[<&í'ë ±| ¡'þz±:Ë_+¨OR> ûh.À¬MéÏ¸ {úMÝ&75vð_ëæáN­1·[½Ïï^|Ý÷sÅ"6´þN»ÔV.Yªûú©EZjNÌÛl-$;o¿ ¾ÌhKûÂÑË2¬?¼Ç©w«s<º´%ÁE·x\,©ñ²û tNq"'Ñ1=8lxþ%ô5U~ç4gwø5]á<Jqæ9i·s;KIâvímZ?RGÌmA|Gù¤?:Ðuä%:ÇÜÐÞ-é Íè ×atP|-prµñE-,.Ë¢eâÈìBæSÎ?ÉFAQb¢×çÐuÓ 5¢¹êHó¬KA(É¾û dÛÎm0ìRØÙýNQ-Ô ÝNÁnÚãÝ/ðëïdçÛ¶{Ûë¿ÕüüÛdÖfCDÙ o,kS Ë¢¯þù;XÎº¿ Ë¦mî MÍë ,u&ã²»Qö8dÉÊZâ­½p³Ìñ4ÕTvÆ9ßÕÀ !;²Éy2íj#Ö%îÌÂ÷/ßäÿÊôkß4 Ý¤ù Ì1(:Í ã¡8Þºýô ·Ã 'ClqtKäJ\ÓhÒk>æ±JxgH Kú"cºÕXjÏ .È­?¡½YEÁa¿÷»¶9ûô F¤êD¬sËzz°émÂõËª]{ øh0ãÔ+ÀN X»µÆ<u»ç`OvvÏõø¢kj÷Á Ò¦#$EMA`P_Ú½J±aèâK4°,év£ôß;wn¸ýðéúnqkjr 4÷íñ ük9%¬_ª'ÍxöaìÔ, ª^ÄAªÓS[è¬{-GÌEÇú Ü¬?Î£þ;Pdx±xµÑ£[ºv²è¾|qà½B"ÒÄ¶ã~ýsÇW9¥½B¶Åý3bE6üvðF:' ×E%ü)Ðk1e vN I "ãKäXD7è£)Nå!xèxÜí?è åMý cv2M ªïÓ':fÛïn´ûû ÓB ôp¨VxLOÏ¶zðü&Ðá«ÿúÚ üÿ¦x4±ü×ý+Ø¶é¢¤kÿ ðC' endstream endobj 313 0 obj <</Subtype/Link/Rect[ 68.65 735.06 526.35 746] /BS<</W 0>>/F 4/A<</Type/Action/S/URI/URI(https://www.theguardian.com/technology/2017/apr/13/ai-programs-exhibit-racist-and-sexist-biases-research-reveals) >>/StructParent 184>> endobj 314 0 obj <</Subtype/Link/Rect[ 68.65 718.13 180.88 735.06] /BS<</W 0>>/F 4/A<</Type/Action/S/URI/URI(https://www.theguardian.com/technology/2017/apr/13/ai-programs-exhibit-racist-and-sexist-biases-research-reveals) >>/StructParent 185>> endobj 315 0 obj <</Subtype/Link/Rect[ 68.65 690.25 489.1 707.19] /BS<</W 0>>/F 4/A<</Type/Action/S/URI/URI(https://www.theguardian.com/science/2016/sep/01/how-algorithms-rule-our-working-lives) >>/StructParent 186>> endobj 316 0 obj <</Subtype/Link/Rect[ 84.853 668.37 526.35 679.31] /BS<</W 0>>/F 4/A<</Type/Action/S/URI/URI(https://www.washingtonpost.com/opinions/big-data-may-be-reinforcing-racial-bias-in-the-criminal-justice-system/2017/02/10/d63de518-ee3a-11e6-9973-c5efb7ccfb0d_story.html?utm_term=.720084735d73) >>/StructParent 187>> endobj 317 0 obj <</Subtype/Link/Rect[ 68.65 657.44 526.35 668.37] /BS<</W 0>>/F 4/A<</Type/Action/S/URI/URI(https://www.washingtonpost.com/opinions/big-data-may-be-reinforcing-racial-bias-in-the-criminal-justice-system/2017/02/10/d63de518-ee3a-11e6-9973-c5efb7ccfb0d_story.html?utm_term=.720084735d73) >>/StructParent 188>> endobj 318 0 obj <</Subtype/Link/Rect[ 68.65 640.5 314.53 657.44] /BS<</W 0>>/F 4/A<</Type/Action/S/URI/URI(https://www.washingtonpost.com/opinions/big-data-may-be-reinforcing-racial-bias-in-the-criminal-justice-system/2017/02/10/d63de518-ee3a-11e6-9973-c5efb7ccfb0d_story.html?utm_term=.720084735d73) >>/StructParent 189>> endobj 319 0 obj <</Subtype/Link/Rect[ 254.37 629.56 526.35 640.5] /BS<</W 0>>/F 4/A<</Type/Action/S/URI/URI(https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing) >>/StructParent 190>> endobj 320 0 obj <</Subtype/Link/Rect[ 68.65 606.62 235.75 629.56] /BS<</W 0>>/F 4/A<</Type/Action/S/URI/URI(https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing) >>/StructParent 191>> endobj 321 0 obj <</Subtype/Link/Rect[ 84.853 572.75 481.65 595.68] /BS<</W 0>>/F 4/A<</Type/Action/S/URI/URI(https://motherboard.vice.com/en_us/article/mg9vvn/how-our-likes-helped-trump-win) >>/StructParent 192>> endobj 322 0 obj <</Subtype/Link/Rect[ 68.65 527.93 368.73 550.87] /BS<</W 0>>/F 4/A<</Type/Action/S/URI/URI(http://journals.uic.edu/ojs/index.php/fm/article/view/7090/5653) >>/StructParent 193>> endobj 323 0 obj <</Subtype/Link/Rect[ 68.65 506.06 526.35 516.99] /BS<</W 0>>/F 4/A<</Type/Action/S/URI/URI(https://www.theguardian.com/world/2016/oct/27/angela-merkel-internet-search-engines-are-distorting-our-perception) >>/StructParent 194>> endobj 324 0 obj <</Subtype/Link/Rect[ 68.65 483.12 186.9 506.06] /BS<</W 0>>/F 4/A<</Type/Action/S/URI/URI(https://www.theguardian.com/world/2016/oct/27/angela-merkel-internet-search-engines-are-distorting-our-perception) >>/StructParent 195>> endobj 325 0 obj <</Subtype/Link/Rect[ 127.94 426.3 369.24 449.24] /BS<</W 0>>/F 4/A<</Type/Action/S/URI/URI(http://www.unesco.org/new/en/world-media-trends) >>/StructParent 196>> endobj 326 0 obj <</Subtype/Link/Rect[ 322.87 404.43 526.35 415.37] /BS<</W 0>>/F 4/A<</Type/Action/S/URI/URI(http://isc.independent.gov.uk/committee-reports/special-reports) >>/StructParent 197>> endobj 327 0 obj <</Subtype/Link/Rect[ 68.65 381.49 176.71 404.43] /BS<</W 0>>/F 4/A<</Type/Action/S/URI/URI(http://isc.independent.gov.uk/committee-reports/special-reports) >>/StructParent 198>> endobj 328 0 obj <</Subtype/Link/Rect[ 375.67 348.68 526.35 359.61] /BS<</W 0>>/F 4/A<</Type/Action/S/URI/URI(https://ec.europa.eu/home-affairs/sites/homeaffairs/files/what-we-do/policies/european-agenda-security/legislative-documents/docs/20160420/communication_eas_progress_since_april_2015_en.pdf) >>/StructParent 199>> endobj 329 0 obj <</Subtype/Link/Rect[ 68.65 337.74 526.35 348.68] /BS<</W 0>>/F 4/A<</Type/Action/S/URI/URI(https://ec.europa.eu/home-affairs/sites/homeaffairs/files/what-we-do/policies/european-agenda-security/legislative-documents/docs/20160420/communication_eas_progress_since_april_2015_en.pdf) >>/StructParent 200>> endobj 330 0 obj <</Subtype/Link/Rect[ 68.65 314.8 453.1 337.74] /BS<</W 0>>/F 4/A<</Type/Action/S/URI/URI(https://ec.europa.eu/home-affairs/sites/homeaffairs/files/what-we-do/policies/european-agenda-security/legislative-documents/docs/20160420/communication_eas_progress_since_april_2015_en.pdf) >>/StructParent 201>> endobj 331 0 obj <</Subtype/Link/Rect[ 68.65 192.36 486.6 215.3] /BS<</W 0>>/F 4/A<</Type/Action/S/URI/URI(http://www.venice.coe.int/webforms/documents/default.aspx?pdffile=CDL-AD\(2017\)009-e) >>/StructParent 202>> endobj 332 0 obj <</Type/Page/Parent 2 0 R/Resources<</Font<</F1 5 0 R/F11 239 0 R>>/ExtGState<</GS41 41 0 R>>/ProcSet[/PDF/Text/ImageB/ImageC/ImageI] >>/MediaBox[ 0 0 594.96 842.04] /Contents 333 0 R/Group<</Type/Group/S/Transparency/CS/DeviceRGB>>/Tabs/S/StructParents 203>> endobj 333 0 obj <</Filter/FlateDecode/Length 199>> stream x}= Â0÷@þÃº$ii"´õ BÁlÅ!øQ;ÔfñßÛî]»çx|õÕÑí<& {ïv§Ã ·ÍeËíýrà¹+«³óUsNÌR$¾`Ñ°GJö¥bR#4#¦#Øºµ-7*@y£D ìdII1Àp »¦dÞa¥à=Iã}SÿãxWK²ñÊ0ù,þ.ùt¾ ¥`QôÃúäÏ;,]Í ¾Wÿ fB¨5k_Oä äSß endstream endobj 334 0 obj <</Type/Page/Parent 2 0 R/Resources<</Font<</F1 5 0 R/F11 239 0 R>>/ExtGState<</GS41 41 0 R>>/XObject<</Image336 336 0 R/Image338 338 0 R/Image340 340 0 R/Image342 342 0 R/Image344 344 0 R/Image346 346 0 R/Image348 348 0 R/Image350 350 0 R/Image352 352 0 R/Image354 354 0 R/Image356 356 0 R/Image358 358 0 R/Image360 360 0 R/Image362 362 0 R/Image364 364 0 R/Image366 366 0 R/Image368 368 0 R/Image370 370 0 R/Image372 372 0 R/Image374 374 0 R/Image376 376 0 R/Image378 378 0 R/Image380 380 0 R/Image382 382 0 R/Image384 384 0 R/Image386 386 0 R/Image388 388 0 R/Image390 390 0 R/Image392 392 0 R/Image394 394 0 R/Image396 396 0 R/Image398 398 0 R/Image400 400 0 R/Image402 402 0 R/Image404 404 0 R/Image406 406 0 R/Image408 408 0 R/Image410 410 0 R/Image412 412 0 R/Image414 414 0 R/Image416 416 0 R/Image418 418 0 R/Image420 420 0 R/Image422 422 0 R/Image424 424 0 R/Image426 426 0 R/Image428 428 0 R/Image430 430 0 R/Image13 13 0 R>>/ProcSet[/PDF/Text/ImageB/ImageC/ImageI] >>/MediaBox[ 0 0 594.96 842.04] /Contents 335 0 R/Group<</Type/Group/S/Transparency/CS/DeviceRGB>>/Tabs/S/StructParents 3>> endobj 335 0 obj <</Filter/FlateDecode/Length 1303>> stream xMk Gï û úèøÐÛU]Õ ` ä 6^ÈAè dID£,üûTÍîJÓ3S&mµx´5O¼õVu{óîiwwsyµsoÞlÞívW\sçíã÷Íößï×Ï·w»»Ç·oÝÉÙ©;Ù®W_ÁU_ÛÞ¬WàüÇH Ë5úTÜö^¾öá+»ý{½ îV|X¯Î_¹_.Üö·õê½¼È½ÿtê6Æ N w»Çû ãÆ=GGÕã n!ão Á²ðÕa<nóYGòéôã í¬¦¯D.§äåã¼¯!@$¡{3ù.ðÅqÊòë§ëõêæõáM ³Ïà"G_Ña©>Â<|÷÷×îA¾=Bây ÞÇÆ},Ïc#ù h?A@"_e¦ì¹¸+ÝÙ÷·×1&wöè¾,L2ö£lË(**¼,:Å§"Æà,"÷ ê¯'Ë*Èþ7D´©(ÿÆfÄ|nds'1k¼,*²O¼4Sº·±J"ÕÇ¼4S»÷1z¥GJ¨Ö2"²© ÝHÁ IÙiJ   síH¦R4µ½¾YjG í¥a%¦) èu ZÀó¤U iÊºGô#9>ArEd²õÓk=@¸0KÎÅKM#mýôzOëQ Z DÛ ×{¢8f²§¼4K[²Ýö£^0_ØµDö4ûíôù)ÝOl¦b±×~¤! ,3¤<® 2Å~ûaí#@|}iJ{ýG73Î\CñÜ"Mý`¯ýH^VyÉdkm¦~°×~&%:õÜ9ÑNLìu¢]@3j¶0K[²½ö©¨áLV:jnÅl¯ÿPÛÞE)[!/!mÅöÚÏÐå<Æ%¦-Ù^ÿIìkI´HûtÐÝþàüM_©3MÅ^Â5í§Ó"«YMÅ^ÿÉ~ävhÊ'öº´¬D³Nä [¤©ØÝü 4¾tk¶|zýGp>É çÜ míô4Ô2\PLñÈ§Rð%/«>³¸,y$ãx ÇF¥Á´Îû`£<käaE¤CÒ~)é(ÇÃÃÉðìÑ¬t¸RFô.aÁê<ôð¤W4bá yvÁ²vrjAiÒ¢ÞføP¤ ûÁdGv5F¬Póa'ÞÝpú@³;·s¥ùVHê%Y(¾ XÎ¤íØº¼gyt4r6 ÞÒ>t ¢ý Qg yÐÊs<°´ ®.@AÍUYÎ[¨cdÿItÙG3ÅøpCÑHÒä 3ãáÙ72Ø¬tLÔ , â¦áRì½< Q2a h.Q/(yHÏHñÄKHÓp»÷HÓc²¬rÀM-Òô[JýÒa½ ¥ë -ÒNî^X.zýÒ"¡3hk§ôÓóF zÐÓláÔî%ÅìÃ4Aü½©'hêC7QJð(I Ñ ÿãÔçû"o MÕp·áÚîGzç ç¼8­Ñ rd9zg=¼LZÎ´ä5·HRK¬¡Dj¨ûG?"Uý?w.UA]È«Ý7'Sc­¤£6÷ëãÃ.àþ\¯¾®WGÈ6V¦ endstream endobj 336 0 obj <</Type/XObject/Subtype/Image/Width 2/Height 2/ColorSpace[/Indexed/DeviceRGB 1 <FFFFFFFFFFFF>] /BitsPerComponent 1/Interpolate false/SMask 337 0 R/Length 2>> stream    endstream endobj 337 0 obj <</Type/XObject/Subtype/Image/Width 289/Height 120/ColorSpace/DeviceGray/BitsPerComponent 1/Filter/FlateDecode/Length 313>> stream xíÖÁ  PÿH BiP¶-a)Á£Ç|À¯®BHöæü%Î D Ã2LG uôgdýäðo¤9È8ª¿õ Ü ¡¦cÅ,ìî<ñ3ªÇAÃô¾¥4¸õ°àÒáF4ÄÀÐ`ÌHAQ ;ú2Ò ô 0gd¯hÉÈ6ÐOÐ°¢ Pî±"pÒah{w ¢YÈ ]cGñl½,òý LdI¸(PÈÇÎÌ¨µ» TV4ò 42PÌ=L¹¨érHÁD£G«¥6$¡HgRÒHãp ï»Ã¼RÈp¾f³ÔG#=È1)CîfÍ>` Áº¡øA +3!Á@nNßj_b(GÝÿ};êècÑ ,êëÑ endstream endobj 338 0 obj <</Type/XObject/Subtype/Image/Width 2/Height 2/ColorSpace[/Indexed/DeviceRGB 1 <FFFFFFFFFFFF>] /BitsPerComponent 1/Interpolate false/SMask 339 0 R/Length 2>> stream    endstream endobj 339 0 obj <</Type/XObject/Subtype/Image/Width 277/Height 120/ColorSpace/DeviceGray/BitsPerComponent 1/Filter/FlateDecode/Length 559>> stream xíÕM²¤  `x,XzÂ» Í£x,(2IÅNOÍb6ZÕ %_Ë_.+¼ä% âK¾&¿"±³HÂKþ ñsOLº YyÂÏéç °`_Ù Xð ¼0X¬'â/ß!:$S#á2Ðt´¸ ¸ Ig2»ø³ÔMo"¢Aº/ Ç4¥{²àXÑöDuf1Ú"&÷Dóº#íì3ÁShJ=¼1 Oût}Ù7$nä²">»:ë-jhëFDu$>ÕÈ2$aÖ(H áã­¤4@$ e@übvâ`} ¹ d@ÜzeLljW$ç%S:KiÝ {& `@òepF6»å âwb:â®Ä7ßÉL}¶¦ÒÅßI`b¾ 5IðÖÇ;Y86¢ \ÈÂÄî KD½púâÚ'"öªF#Íg¨jÐ^»VKÒåzè8,M#$C$Õ} *Ø©TìóµQ>7eT#9pÅ¤Æ=¾b ïRàºn$SäeÜ0X±okKû¢òÈÃJ ø@M+Ì5aé¸xàª³5-èècÃcRül!ok;ÈÓÕîñ_Y$A&ûWö_H+ -"²HLB ÈgbùÓðL Eò@J$E&I&«HLÉú;òæ} endstream endobj 340 0 obj <</Type/XObject/Subtype/Image/Width 2/Height 2/ColorSpace[/Indexed/DeviceRGB 1 <FFFFFFFFFFFF>] /BitsPerComponent 1/Interpolate false/SMask 341 0 R/Length 2>> stream    endstream endobj 341 0 obj <</Type/XObject/Subtype/Image/Width 1821/Height 120/ColorSpace/DeviceGray/BitsPerComponent 1/Filter/FlateDecode/Length 2730>> stream xíÜK¤È `$ÇÆr ß$®âcxa Z³ðÒG£Q/¼ôÖ,z9XÞPðÿàÿ¬¤²³Z :«xü ¯x@MQ>o À;¤ËxtÏ.ã9Òe<GºçHñ é2#]Æs¤ËxtÏ.#¦ùùAØþ©Õ/ÆÒç2øp¯o¶ýxGîl2Sæûð5ºç+ÿÍóæ[½ïÞÞ*¦×£¶~ î9Æ¿ÑgÒXã÷¾ßÞlqOÔõcÆ5¼¼Ï¤Ñµp­ø3EïL¯Gí 2ká¬1àÆ¦k¥ýøXïñzÔÍøÇ¹ãT­áeoñ3iÛy´Yî0 Ç[xo5ÚéùFyW{_ãÙ8¾±=;N{cçÈ«þ»5Öqc=<Ý¨þ Æ %{_ÅÆà¾ ÆêÉ÷Õ»ûêÞë ±1è ÁX>¹|Ð¸ë ÆÌ8»îÅ@âMéu£<O¾Â8'a,ÒBc UEùT=h\Óe¼¥ËX\ÆCé26Lf(vFK½ëdÙzêR¿×- ¸YTUÊÞºÆÈ¨  ÕÔjÕºVµª³[ÙØlùx,çfúh° OKµ÷ðC 3Åàq{éeûW;ËQ(¾-¡Â×-]oiÙ«ÂÂf0ÒXÃZê ÑâTFY=ÚLa¤¬ð¯+?È= ÕQxÐ4Lt'Ê õÐÚÖ¡ù*ð¸+Ãx9E°z9Ë­F$ËõÚ³XØ µ4b4±±Á ãíü×ÖË£ÕaxPÇ4ÒV2Zq ÈO¸ ¶-ñk¥r^íS.·±¹ë= ~ ÍèÚwFÏ;?Â©=ïÜE£S'VáAÓÐHYÉèÄá #^øwy"cMq:e¤ºÄ¯¼ #h©>÷É{¾ðì QÆ`àñ·m.fÅ]Ô¨S « áýâjbcÍFr¾Æ<ÑQ¸ºïÕåÉåV£Ãó ës¿Ò!à¦Í(÷ÅÀ#ãgÞ1ÁøØ1 é(W»« á½ ¶RC>>W!Ëü îçéRRÆf°|h£¼6¸ÜjÈ>R~Z.}ç¦Â¼H£ÃÀ÷F¿#ë</*3UWñÎÿ=Ë±è Ù@{`l:7ÇFßUhìjalàJF*·áôy2s¿x òvgá½Ù®ÆvÍÇ[t5xüN÷ëÑ_h+[§´Â£C]Øy] ï ð \9E%PÅ¸åa¥rÁÔø¡< {ì¼p ówyè¡ËS÷¸77£/ÜËù*¸ÕãÒÒ¿lÑCu yÙJwÑ¹øCdÄHhÔëyµQé|è ÇK^A·0r¹Å·¶W |-[ì% (!­íí`á à"ÂKË }!oÌX [Ì¶q*~PFÌ§]ÉG:ÐØ8cË-F¼5·¸<ìèJÇåÕíBàÚ8ó.Ffqgï ÙÁÒE0s¼ó²5E.<©ß ïaxæj\ uYÕna2lF.·± BE -ã¢o¼+b0f­/Æ·Ô0ÆÁ{E;úX·Rµ²W##õ¢d½MÃ«±~ÍØ+c»fhlZ#|2Ê§¢6±¥¯±ukÒhØß³±8ä½ÃXhãLçF -ãp>6Ò qotÚØ¹B0½ÑnF3NÔÇ" :÷ÆaoóPn1¶Òh±SëÑØneì ±äR"³n½i hÅÚEÆ qÂØGÆN±Z¸1jh g=®3jD3Â.Ú²BdÇ#ã6büÆvÁmÆ»ûª2r¹ÅÈËF(Ùì4'õín\oÆnéíqÒÓÍ`¬¹°µNèCÆý³­»ý70ª:¥±§¸Æ#R]>${»×Pû÷±ô ã3ªëþQ 6Üi_ 66Ç6büð4[Jqy%ÓÄã~#Öà#ã¡w÷qâ5ö±QÜuAVë#ÆúÑí ö 2:µcßÕØ 36âÞqÈX« ÷Fe 1ÊÛá!£U°«îí; 7V~Jõ Yc ­=Þm¬ôâÆxwÿø¸ÑàLûÑÃ!½U»à7e*{¯q êMF;'Ç Yc)Ç1£Wò2{'ãtÌ(ûcF9Å] ÞÇÈóûF«×Wï7Ò8 0Îyã}ãz8h¬Þj,ä y:4&Oå\Ã¸ Ë©ÃÍ;2Æz±4Â8 w<bì7ÝðÝ·Ã±«ûâ±ªý*ÆòNcsÓ8ò:2n±×:øQqÚ¸Ðî0b0ñ: üÈË[z~ZëÆÄFûºQ¬Ye6cçR®ÉÝ!c0VycX»e kSÞ¸ þÞb yí±ßy¥â=¡pÊ¸EW²{ìbãJ*cr 9¹re4Yã²N~Ã¸< ðÊ8HãR¯4N9£ÑF^\YhãîyÇj,µ±¿e Ï;c2Ò302±ÖÊèE[ýeÂûÆjãL]N=íô,)mDº2Â÷6k¬Âs«° |[üp¤Ì/õ`¦0vFJ+#=½¥UóÈHOîâgsÁÈOÚåËóÇÍøC<À¶f~ÔZv«qF<`U[uòÏàØÚ =c í=ÑHCåÂH«IYãO[W#\»­X(eãT¡±¡EÆ-ÖÍP][rI~v¿$0Åhñ ¡hÍgBÚH1ù\ÎY#NÀ¥±Ç"cóÏ 4«Þ¯Y¸bÔVêüí§½Ñz?kã¿©XI¯Hh#¯¥¡ra¤sFï¿³øm0©÷:FÏË{#FÓªtÚ8iã¿¸XÂX>g¬Õ ù¦ÂûU­Ö°­Á/!V£Ä7ÑË*~^Ç±_ÖÒÆ>¼ æ}d´¤OK®\®ç`9#Tôã,ÄhÔë9XAÏ ±RFz]nJ+|kH?ðûXno¤wÉFØ2VbÎmó\"µò °å}¹Ö/ï¤ XKeÄs§(õ rÎbH°Ã?|ÕÆÑ®ÕFz'0mc¡øÂ!\-#Ý5ÅQ *¹ôËïv6<OwËRk#pÐ°/·U¬ªK¡?5÷®Øld¤ ÷¤+F 1o¬;a¬è}£vËÅá2Z¬Û\óù"îÁ÷¯²pÎ1þ&¼u[O»¼û'ÓÙÊu8û$«¹ªQæÿóß]ñ{Jæ2"íouÛ¦w ã©É^ÆS$9 Òib?Nç0¶ z;Jç0öz0Jç0úÅª(ÂXúÖÊéDNbT+Mq:1¿ùÆÊëU(ÂXzý'|Q: ×üúüÖsí­Ëñ$Fõ°wNb¼.ã9Òÿ ñ|«ì endstream endobj 342 0 obj <</Type/XObject/Subtype/Image/Width 2/Height 2/ColorSpace[/Indexed/DeviceRGB 1 <FFFFFFFFFFFF>] /BitsPerComponent 1/Interpolate false/SMask 343 0 R/Length 2>> stream    endstream endobj 343 0 obj <</Type/XObject/Subtype/Image/Width 2357/Height 120/ColorSpace/DeviceGray/BitsPerComponent 1/Filter/FlateDecode/Length 4450>> stream xíM®ä¶Ç%ËgX^zD>FVÑUr,³@^xé#ä(ÖÀYÎF^Z£e1¬ú?¤VwSR÷³VoØO"U?~,±ßdÙ)÷EýÖ| rbJSäÄ$'¦$91%É)INLIrbJSäÄ$'¦$91%É)INLIrbJSäÄ$'¦$ù`ÊÇã:)7\Í©åðøÚL³RiSª¬úzÿ ôi)kÌi)+r.þ&Êæn-ò>A]ÑÝ¼½©¸Iµ< SÕ&dJÇT¤ô.Õß¼½IÝÃTv< SÝ%dJÇ´çn¡ ¬`*Mî/®>OÃ¤o)v©ÔU·Y®aJ¬bªqñ©ªyYL#9éÃÔGU\Ãôê Ü#é¦¿ÿ¾0 OÀä ÀTL/)~Ð\ÃT>Ó'Ý¦'=éîb²î<ÓuÓ¦'­îb²ë¦§b:° _Ãô¤UxÝÏ×VáOÅ$é !+t>_«òÄtbZSäÄ$'¦$y&&^)Ñ ¹! (extracted from86.html)
Making government services more efficient and accessible: Despite often being slow to adopt new technologies, governments around the world are using AI, from the local to the national levels, to make public services more efficient and accessible, with an emphasis on developing “smart cities.” AI is also being used to allocate government resources and optimize budgets.37 HARMFUL AI Perpetuating bias in criminal justice: There are many documented cases of AI gone wrong in the criminal justice system. (extracted from12.pdf)
criminal justice system, was inaccurate at forecasting future crime and heavily biased against black defendants. (extracted from12.pdf)
Considering ethical concepts such as justice, fairness, transparency, and accountability allows for valuable debate about the societal impacts of AI, and the role of AI in our lives.52 There is also an academic research community devoted to addressing ethical issues.53 Ethics have helped those researching and developing AI to define boundaries for themselves. (extracted from12.pdf)
In countries which have not abolished the death penalty, sentence of death may be imposed only for the most serious crimes in accordance with the law in force at the time of the commission of the crime and not contrary to the provisions of the present Covenant.” - Article 6 of the ICCPR The growing use of AI in the criminal justice system risks interfering with rights to be free from interferences with personal liberty. (extracted from12.pdf)
criminal justice system to inform detainment decisions at nearly every stage, from assigning bail to criminal sentencing.62 The software has led to more black defendants falsely labeled as high risk and given higher bail conditions, kept in pre-trial detention, and sentenced to longer prison terms. (extracted from12.pdf)
In criminal justice, this discrimination is often the result of forms of bias. (extracted from12.pdf)
32 accessnow.org HUMAN RIGHTS IN THE AGE OF ARTIFICIAL INTELLIGENCE the uses of AI for these purposes.118 Any assessment process should include: • Testing and audits by independent experts • Identifying measures to mitigate identified risks and prevent any rights violations from occurring, and measuring compliance and efficacy • A failsafe to terminate acquisition, deployment, or any continued use if at any point an identified human rights violation is too high or unable to be mitigated • Identification of any new legal safeguards needed to protect human rights in specific applications of AI tools • Special determinations of bias, particularly in the criminal justice sector due to the risks to fair trial, right to liberty, and non-discrimination • If a third party is used to develop and/or implement the system, a requirement for the third party to participate in the human rights assessment process 3. (extracted from12.pdf)
There should always be a human in the loop, and for high-risk areas, including criminal justice, significant human oversight is necessary. (extracted from12.pdf)
This is particularly important in areas such as in law enforcement and the justice system. (extracted from12.pdf)
Empowerment TRUST: How can Reliability and safety: Throughout their Data Agency: A/IS EQUITABLE: DoD should Fairness and non- Justice and AI be trusted? (extracted from113.pdf)
23 Nov 18 Debate IQ2 IQ2 Debate: Humanity is Designing its Own Demise 28 Oct 17 Article SCIENCE + TECHNOLOGY The “good enough” ethical setting for self-driving cars 19 Jul 16 Say Hello GET IN TOUCH SUBSCRIBE FOLLOW US With respect for the people of our First Nations and the justice of their claims, The Ethics Centre acknowledge their unbroken care for country, since time immemorial. (extracted from609.html)
Projet AJC | ACT Project | Autonomisation des acteurs Judiciaires par la Cyberjustice et l’intelligence artificielle | Autonomy Through Cyberjustice Technologies and AI Laboratoire de CYBERJUSTICE Laboratory Espace exclusif – membres AJC en AJC Présentation Entrevues des chefs de groupe Entrevues des chercheurs Infrastructure de recherche Gouvernance Comité scientifique Actualités Nous joindre Organisation Groupes de travail Chantier 1 – Subproject 1 Chantier 2 – Subproject 2 Chantier 3 – Subproject 3 Chantier 4 – Subproject 4 Chantier 5 – Subproject 5 Chantier 6 – Subproject 6 Chantier 7 – Subproject 7 Chantier 8 – Subproject 8 Chantier 9 – Subproject 9 Chantier 10 – Subproject 10 Chantier 11 – Subproject 11 Chantier 12 – Subproject 12 Chantier 13 – Subproject 13 Chantier 14 – Subproject 14 Chantier 15 – Subproject 15 Chantier 16 – Subproject 16 Chercheurs Partenaires Partenaires institutionnel Partenaires issus du milieu de l’action sociale et communautaire Partenaires issus du milieu universitaire Partenaires issus du monde professionnel Partenaires issus de l’industrie Publications Bibliothèque numérique sur la cyberjustice – Cyberjustice Digital Library Rapport mi-parcours | Annexe bibliographique Articles scientifiques Guides de meilleures pratiques Livres ou chapitres de livres Documents de travail Inventaires Blogues Articles de presse Présentations Conférences et activités Études de cas & Évaluations Gouvernance juridique Conférences Conférence 2025 du partenariat AJC AI, Large Language Models and Justice L’accessibilité : la nouvelle frontière de la justice en ligne Fintech: Réguler aujourd’hui les technologies financières de demain Vidéos Conférences enregistrées Rencontre annuelle | Juin 2020 Webinaires des partenaires Webconférences cyberjustice AJC Présentation Entrevues des chefs de groupe Entrevues des chercheurs Infrastructure de recherche Gouvernance Comité scientifique Actualités Nous joindre Organisation Groupes de travail Chantier 1 – Subproject 1 Chantier 2 – Subproject 2 Chantier 3 – Subproject 3 Chantier 4 – Subproject 4 Chantier 5 – Subproject 5 Chantier 6 – Subproject 6 Chantier 7 – Subproject 7 Chantier 8 – Subproject 8 Chantier 9 – Subproject 9 Chantier 10 – Subproject 10 Chantier 11 – Subproject 11 Chantier 12 – Subproject 12 Chantier 13 – Subproject 13 Chantier 14 – Subproject 14 Chantier 15 – Subproject 15 Chantier 16 – Subproject 16 Chercheurs Partenaires Partenaires institutionnel Partenaires issus du milieu de l’action sociale et communautaire Partenaires issus du milieu universitaire Partenaires issus du monde professionnel Partenaires issus de l’industrie Publications Bibliothèque numérique sur la cyberjustice – Cyberjustice Digital Library Rapport mi-parcours | Annexe bibliographique Articles scientifiques Guides de meilleures pratiques Livres ou chapitres de livres Documents de travail Inventaires Blogues Articles de presse Présentations Conférences et activités Études de cas & Évaluations Gouvernance juridique Conférences Conférence 2025 du partenariat AJC AI, Large Language Models and Justice L’accessibilité : la nouvelle frontière de la justice en ligne Fintech: Réguler aujourd’hui les technologies financières de demain Vidéos Conférences enregistrées Rencontre annuelle | Juin 2020 Webinaires des partenaires Webconférences cyberjustice Conférence 2025 du partenariat AJC À la une À la une Actualités Quoi savoir avant d’aller à la Conférence AJC 2025 16 septembre 2025 Merci de vous être inscrit à la conférence annuelle du partenariat AJC (Autonomisation des acteurs judiciaires par la cyberjustice) 2025 ! (extracted from170.html)
À tout moment, si vous avez besoin d’aide, le personnel de la conférence sera […] Lire la suite À la une Actualités Événements Conférence le mercredi 5 mars : « Considerations on AI Governance and concrete applications in the field of Law and Justice » 3 mars 2025 Le mercredi 5 mars 2025 à 13:00, heure de Montréal (19:00, heure de Madrid) se tiendra en hybride la conférence « Consideraciones sobre la gobernanza de la IA y aplicaciones concretas en el ámbito del Derecho y la Justicia » (Considerations on AI Governance and concrete applications in the field of Law and Justice), organisée par la […] Lire la suite À la une Actualités IA générative & Résolution des litiges | Publication de Karim Benyekhlef, Valentin Callipel et Aurore Clément 10 décembre 2024 Le Pr Karim Benyekhlef, Valentin Callipel et Aurore Clément viennent de publier « IA générative & Résolution des litiges ». (extracted from170.html)
publication intégrale ﻿IERDJ_EDC-volume-2-301024Télécharger Lire la suite À la une Actualités Offre d’emploi | 2 auxiliaires de recherche au Laboratoire de cyberjustice 14 novembre 2024 Le Laboratoire de cyberjustice est un espace unique de réflexion et de création où les processus de justice sont modélisés et réimaginés afin d’améliorer l’accès à la justice. (extracted from170.html)
Nos travaux se distinguent par l’expérimentation d’innovations technologiques, comme l’intelligence artificielle, pour optimiser le fonctionnement actuel de la justice, de manière à en accroître l’efficacité, à en […] Lire la suite Voir les précédents Nouvelles À la une Actualités Quoi savoir avant d’aller à la Conférence AJC 2025 16 septembre 2025 Merci de vous être inscrit à la conférence annuelle du partenariat AJC (Autonomisation des acteurs judiciaires par la cyberjustice) 2025 ! (extracted from170.html)
À tout moment, si vous avez besoin d’aide, le personnel de la conférence sera […] Lire la suite À la une Actualités Événements Conférence le mercredi 5 mars : « Considerations on AI Governance and concrete applications in the field of Law and Justice » 3 mars 2025 Le mercredi 5 mars 2025 à 13:00, heure de Montréal (19:00, heure de Madrid) se tiendra en hybride la conférence « Consideraciones sobre la gobernanza de la IA y aplicaciones concretas en el ámbito del Derecho y la Justicia » (Considerations on AI Governance and concrete applications in the field of Law and Justice), organisée par la […] Lire la suite À la une Actualités IA générative & Résolution des litiges | Publication de Karim Benyekhlef, Valentin Callipel et Aurore Clément 10 décembre 2024 Le Pr Karim Benyekhlef, Valentin Callipel et Aurore Clément viennent de publier « IA générative & Résolution des litiges ». (extracted from170.html)
Cet article explore la controverse entourant la demande de traduction des plus […] Lire la suite À la une Actualités Événements Découvrez la conférence ‘Montreal 2024, Generative AI and Justice’ à travers nos vidéos 7 août 2024 Le Laboratoire de cyberjustice a le plaisir de vous inviter à la conférence Montreal 2024: Generative AI and Justice, qui se tiendra le 1er octobre 2024 au laboratoire. (extracted from170.html)
Informations pratiques Lieu de la conférence : Laboratoire de cyberjustice (B-2215), Pavillon Jean-Brillant, 3200 rue Jean-Brillant, Montréal, Québec H3T 1N8 Date : 2024/10/01 de 9:00 à 17:30 Inscription […] Lire la suite À la une Événements Conférence à venir : La justice à l’épreuve de l’IA. (extracted from170.html)
(11 septembre 2024) 24 juillet 2024 Le Laboratoire de cyberjustice a le plaisir de vous inviter à la conférence La justice à l’épreuve de l’IA. (extracted from170.html)
Informations pratiques Quand : 11 septembre 2024 de 16:30 à 19:30 Où: Salon François-Chevrette, A-3464, Pavilion Maximilien-Caron, 3101 chemin de la Tour, Montréal, Québec, H3T 1N8 […] Lire la suite À la une Actualités Conférence à venir : « Montreal 2024, Generative AI and Justice » (1 octobre 2024) 18 juin 2024 Le Laboratoire de cyberjustice a le plaisir de vous inviter à la conférence Montreal 2024, Generative AI and Justice, qui se tiendra le 1er octobre 2024 au Laboratoire. (extracted from170.html)
Cette conférence est une occasion unique pour les acteurs du droit et de la justice de mieux comprendre et de se positionner sur les usages émergents […] Lire la suite À la une Actualités Dans les médias Prof. (extracted from170.html)
Hannes Westermann, premier intervenant des webinaires « AI & Access to Justice » du Stanford Legal Design Lab 18 juin 2024 Le Laboratoire de cyberjustice a le plaisir de vous annoncer que Prof. (extracted from170.html)
Hannes Westermann a été le premier intervenant des webinaires “AI & Access to Justice” du Stanford Legal Design Lab. (extracted from170.html)
Dans ce premier webinaire intitulé “Generative AI for Access to Justice: Challenges and Opportunities”, Prof. (extracted from170.html)
Jinzhe Tan (Laboratoire de cyberjustice) dans le cadre du cycle de conférences de 2024 « AI, Large Language Models, and Justice Considerations for Legal Practitioners Judges, Law Schools and Public Legal Educators » Biographie […] Lire la suite À la une Actualités Web-conférences cyberjustice Vidéo – Conférence « L’IA générative dans le domaine juridique : une approche nuancée » (DL4T) de Prof. (extracted from170.html)
Amy Salyzyn (UOttawa) & Dre Katie Szilagyi (University of Manitoba) dans le cadre du cycle de conférences de 2024 « AI, Large Language Models, and Justice Considerations for Legal Practitioners Judges, Law Schools and Public Legal Educators » […] Lire la suite À la une Actualités Événements Nouvelles Conférence à venir : « AI and Judging » de Pre Tania Sourdin (7 mai 2024) 6 mai 2024 Le Laboratoire de cyberjustice a le plaisir de vous inviter à la conférence en ligne « AI and Judging« de Pre Tania Sourdin. (extracted from170.html)
Cliquer ici pour vous inscrire Informations pratiques Quand : 7 mai 2024, 16h30 F Format : Sur Zoom Biographie Pre Tania Sourdin is President of the Academic Senate at the University […] Lire la suite À la une Actualités Rapport de recherche – « Le numérique et l’intelligence artificielle comme outil de justice sociale : les initiatives par et pour les groupes et communautés marginalisés » 1 mai 2024 Le Laboratoire de cyberjustice a le plaisir de vous présenter le rapport de recherche intitulée Le numérique et l’intelligence artificielle comme outil de justice sociale : les initiatives par et pour les groupes et communautés marginalisés par Pre Karine Gentelet (UQO), membre du Laboratoire de cyberjustice et du projet AJC. (extracted from170.html)
Ce rapport met en lumière les initiatives […] Lire la suite À la une Actualités Offre d’emploi | Étudiant.e en informatique aux cycles supérieurs 1 mai 2024 Le Laboratoire de cyberjustice est un espace unique de réflexion et de création où les processus de justice sont modélisés et réimaginés afin d’améliorer l’accès à la justice. (extracted from170.html)
Nos travaux se distinguent par l’expérimentation d’innovations technologiques, comme l’intelligence artificielle, pour optimiser le fonctionnement actuel de la justice, de manière à en accroître l’efficacité, à en […] Lire la suite À la une Actualités Nouvelles Offre d’emploi | 2 auxiliaires de recherche au Laboratoire de cyberjustice 22 avril 2024 Le Laboratoire de cyberjustice est un espace unique de réflexion et de création où les processus de justice sont modélisés et réimaginés afin d’améliorer l’accès à la justice. (extracted from170.html)
Nos travaux se distinguent par l’expérimentation d’innovations technologiques, comme l’intelligence artificielle, pour optimiser le fonctionnement actuel de la justice, de manière à en accroître l’efficacité, à en […] Lire la suite À la une Actualités Événements Nouvelles Conférence à venir: Colloque sur l’intelligence artificielle et le droit pénal « Des ordres et (désordres) de l’intelligence artificielle : Propagande – Pornographie – Profilage » (25 avril 2024) 22 avril 2024 La Chaire Lexum et le Laboratoire de cyberjustice ont le plaisir de vous inviter au colloque sur l’intelligence artificielle et le droit pénal « Des ordres et (désordres) de l’intelligence artificielle : Propagande – Pornographie – Profilage. (extracted from170.html)
Marina Pavlović dans le cadre du cycle de conférences de 2024 « AI, Large Language Models, and Justice Considerations for Legal Practitioners Judges, Law Schools and Public Legal Educators » Biographie des conférenciers Prof. (extracted from170.html)
La Bourse doctorale de la Chaire LexUM en information juridique et du Laboratoire de cyberjustice est conçue pour encourager deux […] Lire la suite À la une Actualités Appel à candidatures – Projet « Intelligence artificielle pour la découvrabilité » 2 avril 2024 POSTE : Étudiant en droit des nouvelles technologies Le Laboratoire de cyberjustice est un espace unique de réflexion et de création où les processus de justice sont modélisés et réimaginés afin d’améliorer l’accès à la justice. (extracted from170.html)
Nos travaux se distinguent par l’expérimentation d’innovations technologiques, comme l’intelligence artificielle, pour optimiser le fonctionnement actuel de la justice, de […] Lire la suite À la une Actualités Web-conférences cyberjustice Conférence à venir: (Free)lance content and GenerativeAI: The persisting plight of freelance authors across the creative industries de Dre Pina D’Agostino (9 avril 2024) 2 avril 2024 date 9 avril 2024, 13h00 Réunion zoom https://mcgill.zoom.us/meeting/register/tZEqcOyppzguHd3ddDeKt5Ylo5x5BS9dDBLq description sommaire Recycling existing copyright-protected works in new media is an age-old recurrence, which continues to challenge copyright law and its future on a global scale. (extracted from170.html)
Mark Daley (Western University) dans le cadre du cycle de conférences de 2024 « AI, Large Language Models, and Justice Considerations for Legal Practitioners Judges, Law Schools and Public Legal Educators » Biographie du conférencier Mark is the Chief AI Officer at Western University and a […] Lire la suite À la une Événements Conférence à venir: Understanding Large Language Models de Dr. (extracted from170.html)
Plusieurs fonctionnalités ont été évoquées, telles que la détection de messages incendiaires, la capacité de proposer une reformulation de ceux-ci, ainsi que la possibilité de demander à l’agent […] Lire la suite À la une Appel à communications: Conférence « Can Artificial Intelligence Contribute to Justice? (extracted from170.html)
Ce numéro de la RIDP, intitulé « Artificial Intelligence and Administration of Criminal Justice », présente les résultats d’une recherche collective entreprise en 2020. (extracted from170.html)
Il s’inspire des travaux antérieurs influents de l’OCDE sur l’accès à la justice, par exemple les […] Lire la suite À la une Actualités Offre d’emploi | Auxiliaire.s de recherche au Laboratoire de cyberjustice 4 décembre 2023 Le Laboratoire de cyberjustice est un espace unique de réflexion et de création où les processus de justice sont modélisés et réimaginés afin d’améliorer l’accès à la justice. (extracted from170.html)
Nos travaux se distinguent par l’expérimentation d’innovations technologiques, comme l’intelligence artificielle, pour optimiser le fonctionnement actuel de la justice, de manière à en accroître l’efficacité, à en […] Lire la suite À la une Conférences de la Chaire Lexum Conférence à venir : IA, enjeux de pouvoir, de puissance et de souveraineté dans un monde en transition de Dre Asma Mhalla (4 décembre 2023) 1 décembre 2023 La Chaire Lexum et le Centre de recherche en droit public ont le plaisir de vous inviter à la conférence « IA, enjeux de pouvoir, de puissance et de souveraineté dans un monde en transition » de Dre Asma Mhalla. (extracted from170.html)
Cliquer ici pour vous inscrire Informations pratiques Quand : 4 décembre 2023, 16h30 Format : Présentiel et distanciel Où […] Lire la suite À la une Conférences et colloques La Conférence Cyberjustice Europe 2023 en images 24 novembre 2023 La conférence Cyberjustice Europe 2023 a été un succès, et nous souhaitons exprimer nos remerciements à l’équipe de l’Institut des Études et de la Recherche sur le Droit et la Justice, ainsi qu’au Conseil de l’Europe, avec qui nous avons collaboré pour organiser cet événement. (extracted from170.html)
Cette conférence a été une opportunité unique de discuter de l’impact […] Lire la suite À la une Actualités Publication à venir (15 novembre 2023) – Les intelligences artificielles au prisme de la justice sociale – Considering Artificial Intelligence Through the Lens of Social Justice 10 novembre 2023 Le livre « Les intelligences artificielles au prisme de la justice sociale – Considering Artificial Intelligence Through the Lens of Social Justice » sous la direction de Pre Karine Gentelet sera publiée le 15 novembre 2023 par PUL (Presses de l’Université Laval). (extracted from170.html)
La Commission avait également sollicité certains acteurs à partager […] Lire la suite À la une Rapport du sous-projet 11: L’accès à la justice par l’IA – Par et pour les communautés marginalisées et/ou sous-représentées 30 octobre 2023 Karine Gentelet Ce rapport écrit par Pre Karine Gentelet (UQO), Mme Marie Zumstein & Mme Lily-Cannelle Mathieu a pour objectif de détailler les raisons pour lesquelles des tensions peuvent survenir lors du déploiement d’un outil d’IA dédiée à l’accès à la justice auprès de populations marginalisées. (extracted from170.html)
Ce rapport a pour objectif d’établir une cartographie des différentes Legaltechs œuvrant dans le domaine de la « Justice prédictive » en France et à l’étranger, dans le but d’informer […] Lire la suite À la une Événements Visite de la délégation du Conseil national des barreaux et du cabinet d’avocats FÉRAL au Laboratoire de cyberjustice 26 octobre 2023 Nous sommes ravis d’avoir accueilli la délégation du Conseil national des barreaux et l’équipe du cabinet d’avocats FÉRAL dans notre laboratoire. (extracted from170.html)
A lawyer […] Lire la suite À la une Actualités From Wetware to Robots to Airware: Imagining a Trajectory for AI-facilitated Justice Alexis Leblanc-Roy 20 septembre 2023 This installation is intended to interrogate the involvement of AI in the justice system. (extracted from170.html)
predictive justice systems. (extracted from170.html)
Télécharger ici Lire la suite À la une Actualités Lunch and Learn: Large Language Models – Applications in the legal field 10 février 2023 Le mercredi, 8 février dernier a eu lieu le Lunch and Learn: Large Language Models – Applications in the legal field où l’équipe du Laboratoire a eu l’occasion de participer à une séance de brainstorming sur l’utilisation de ChatGPT dans le domaine juridique avec des contributions de Karim Benyekhlef, Valentin Callipel, Mark Likhten, Philippe Langlais (Département d’informatique […] Lire la suite Actualités À l’invitation du ministère de la Justice du Canada, le Pr. (extracted from170.html)
Karim Benyekhlef et Me Valentin Callipel représenteront le Canada dans le cadre du dialogue Canada-Europe sur la numérisation de la justice 13 janvier 2023 Karim Benyekhlef, directeur du Laboratoire de cyberjustice, et Valentin Callipel, chargé de mission, rencontreront des experts européens et canadiens dans le cadre d’une discussion portant sur l’échange des meilleures pratiques en matière de numérisation de la justice. (extracted from170.html)
Il s’agit du Martin Felsky Award pour leur article intitulé « Judging by the Numbers : Judicial Analytics, the Justice System and its Stakeholders ». (extracted from170.html)
Le Martin Felsky Award a été […] Lire la suite À la une Événements La justice en ligne comme solution aux barrières à l’accès à la justice 21 septembre 2022 • 15:00 Laboratoire de cyberjustice & En ligne (Hybride) 3 août 2022 Présentateur(trice)s Julia Atack, Yannick Labelle, Responsable des affaires juridiques et auteure de la recherche Résumé de la présentation De nombreuses barrières se dressent devant les consommateurs qui désirent avoir accès à la justice. (extracted from170.html)
Malgré les multiples mesures envisagées ou tentées par les législateurs canadiens, les solutions proposées jusqu’à présent ne semblent pas être à la […] Lire la suite À la une Table ronde sur la « Justice décentralisée et Web 3.0 » – Que pouvons-nous apprendre des projets de résolution de conflits en ligne par blockchain ? (extracted from170.html)
29 juillet 2022 Dans le cadre de la rencontre annuelle du Projet AJC Issue du Web 3.0 – une idée de nouvelle itération du World Wide Web basée sur la technologie blockchain qui intègre des concepts tels que la décentralisation et l’économie basée sur les jetons – la « justice décentralisée » est une nouvelle approche de la résolution des […] Lire la suite À la une Actualités Événements Conférence internationale du partenariat AJC 4 octobre 2022 6 octobre 2022 Laboratoire de cyberjustice 7 juillet 2022 La conférence internationale de mi-parcours du partenariat AJC, qui se déroulera du 4 au 6 octobre 2022, arrive à grands pas! (extracted from170.html)
Elle se déroulera maintenant du 4 […] Lire la suite Actualités Existe-t-il un juste usage et une réelle utilité de l’IA en justice ?| Karim Benyekhlef 30 juin 2022 Karim Benyekhlef Dans le cadre du congrès Time World 2022, ayant eu lieu du 5 au 7 mai 2022, Pr Benyekhlef s’est penché sur la question de l’impact de l’utilisation de l’intelligence artificielle sur le droit. (extracted from170.html)
Il a présenté une conférence intitulée « Existe-t-il un juste usage et une réelle utilité de l’IA en justice ? (extracted from170.html)
Description Les promesses […] Lire la suite À la une Événements Legal Literacy: The New frontier for Online Justice 21 juin 2022 • 10:00 11:00 En ligne - Online 10 juin 2022 Cette activité se déroulera en anglais. (extracted from170.html)
La discussion portera sur comment un cadre d’alphabétisation fonctionnelle peut être utilisé pour réduire la complexité des formulaires des cours et des tribunaux et ultimement améliorer l’accès à la justice. (extracted from170.html)
Depuis sa création, il a […] Lire la suite Événements The role of courts and access to justice in the digital era 9 juin 2022 10 juin 2022 En ligne - Online & Radboud University Nijmegen 29 avril 2022 Les inscriptions sont ouvertes pour la conférence hybride « The role of courts and access to justice in the digital era » (Université Radboud de Nimègue) qui se déroulera les 9 et 10 juin 2022. (extracted from170.html)
Dans le cadre du Cycle d’ateliers « L’accessibilité : la nouvelle frontière de la justice en ligne », nous vous invitons à l’atelier « Cultural Accessibility ». (extracted from170.html)
Elle est associée au Centre de recherche en droit, technologie et société et professeure agrégée au Département des sciences sociales […] Lire la suite À la une Actualités Justice et IA | Karim Benyekhlef participe à la série de balados UtopIA 19 avril 2022 Karim Benyekhlef Pr Karim Benyekhlef, directeur du Laboratoire de cyberjustice, a participé, avec Pr Jocelyn Maclure, au balado « Justice et IA », animé par Christian Auger. (extracted from170.html)
Description de l’épisode Une société gouvernée par les algorithmes peut-elle rendre justice? (extracted from170.html)
Comment les acteur.rice.s de la justice perçoivent-ils l’avènement de cette technologie? (extracted from170.html)
Une justice automatisée est-elle en marche? (extracted from170.html)
Summary Online Dispute Resolution (ODR) can increase access to justice, but the expense and scarcity of facilitators […] Lire la suite À la une Événements Artificial Intelligence and Access to Justice: Perspectives from the legal and technical domain 29 mars 2022 • 16:30 Laboratoire de cyberjustice & En ligne - Online 11 mars 2022 Cet atelier se déroulera en anglais. (extracted from170.html)
Nye a plus de 20 ans d’expérience dans la direction de projets sophistiqués et multidisciplinaires dans le secteur de la justice en Ontario. (extracted from170.html)
En anglais Conférencier Pr Kieran Tranter is the Chair of […] Lire la suite À la une Nouvelles Le Laboratoire de cyberjustice obtient le statut d’observateur auprès du Réseau européen sur la cyberjustice 8 février 2022 Le Laboratoire de cyberjustice est heureux d’annoncer qu’il a obtenu le statut d’observateur auprès du Réseau européen sur la cyberjustice (European Cyberjustice Network) de la CEPEJ (Commission européenne pour l’efficacité de la justice) du Conseil de l’Europe. (extracted from170.html)
Les bases de données sur lesquelles […] Lire la suite À la une Événements AJC|ACT Workshop: «What have we learned from the accelerated experience of remote hearings during the pandemic?» 8 février 2022 • 9:00 11:30 En ligne - Online 17 janvier 2022 En anglais Introduction The pandemic had an unprecedented accelerating effect on the transition towards remote hearings within the justice system. (extracted from170.html)
[…] Lire la suite À la une Nouvelles Nomination du professeur Karim Benyekhlef au Conseil scientifique du nouvel Institut des Études et de la Recherche sur le Droit et la Justice (IERDJ) 10 janvier 2022 Karim Benyekhlef Le Laboratoire de cyberjustice est heureux d’annoncer la nomination du professeur Karim Benyekhlef au Conseil scientifique du nouvel Institut des Études et de la Recherche sur le Droit et la Justice (IERDJ), issu de la fusion de l’Institut des hautes études sur la justice et de la Mission de recherche Droit et justice, deux organismes de recherche […] Lire la suite À la une Événements Afrofuturism, Critical Race Theory, and the Future of Policing 3 février 2022 • 16:30 En ligne - Online 7 janvier 2022 Dans le cadre du Cycle de conférences « Droit et Littérature: Représentations littéraires des identités et transcriptions juridiques », le Laboratoire de cyberjustice vous invite à la conférence « Afrofuturism, Critical Race Theory, and the Future of Policing » présentée par Pr Bennett Capers (Fordham Law School). (extracted from170.html)
Cette subvention est accordée pour 3 ans (2021-2024) pour le projet « Les incidences de l’architecture logicielle des tribunaux en ligne sur l’accès à la justice », avec les professeurs Karim Benyekhlef et Pierre-Luc […] Lire la suite Nouvelles Le professeur Pierre-Luc Déziel publie une étude sur l’incidence des technologies sur la formation des juristes au Québec 24 mars 2021 Une équipe de la Faculté de droit de l’Université Laval dirigée par le professeur Pierre-Luc Déziel et composée de Hélène Zimmermann et Satchel Dell’olio Delpech publie une étude relative à l’incidence des technologies de l’information et des communications sur la formation des juristes au Québec. (extracted from170.html)
Financée par le ministère de la Justice du Québec, l’étude […] Lire la suite À la une Le Laboratoire de cyberjustice sélectionné pour le Comité en gouvernement ouvert du Québec 23 mars 2021 Le Laboratoire de cyberjustice est heureux d’annoncer qu’il a été sélectionné en tant qu’organisme de la société civile pour participer au Comité en gouvernement ouvert du Québec du Secrétariat du Conseil du trésor. (extracted from170.html)
Le mandat du comité sera : *de conseiller le Secrétariat du Conseil du trésor quant à l’élaboration de nouveaux plans d’action pour un […] Lire la suite Nouvelles Podcast | Jena McGill and Amy Salyzyn on Judicial Analytics 8 mars 2021 Amy Salyzyn Listen to Jena McGill and Amy Salyzyn talk about their upcoming paper “Judging by Numbers: How will Judicial Analytics Impact the Justice System and its Stakeholders?” Jena and Amy explain some of the benefits of judicial analytics software and a few concerns that arise with its use. (extracted from170.html)
Also available on Spotify Read this paper on SSRN McGill, Jena and […] Lire la suite Nouvelles Kevin Ashley, chercheur du projet AJC, reçoit une subvention de 357 000$ pour le projet Using AI to Increase Fairness by Improving Access to Justice 29 janvier 2021 Kevin Ashley Le Laboratoire de cyberjustice est heureux d’annoncer que la National Science Foundation (USA) attribue, dans le cadre du programme Fairness in Artificial Intelligence, une subvention de 357 000$ (3 ans) au projet Using AI to Increase Fairness by Improving Access to Justice mené par le professeur Kevin Ashley (University of Pittsburgh), chef du Chantier 2 […] Lire la suite À la une Publication de l’ouvrage collectif AI and Law: a Critical Overview dirigé par Karim Benyekhlef 28 janvier 2021 Karim Benyekhlef Le Laboratoire de cyberjustice et la Chaire LexUM en information juridique sont heureux d’annoncer la publication de l’ouvrage collectif AI and Law: a Critical Overview aux éditions Thémis. (extracted from170.html)
Le livre sera disponible début 2021 en version imprimée, […] Lire la suite Nouvelles Chaire Abeona-ENS-OBVIA | Appel à contribution pour un ouvrage collectif sur « Justice sociale et IA » 17 novembre 2020 Karine Gentelet Cet ouvrage collectif s’inscrit dans le cadre des travaux de la Chaire Abeona-ENS-OBVIA et aura pour objectif de proposer une réflexion renouvelée et multidisciplinaire sur les enjeux des usages de l’intelligence artificielle à partir d’une perspective de justice sociale. (extracted from170.html)
Devant les enjeux de mobilité, […] Lire la suite Nouvelles Rapport | To Surveil and Predict : A Human Rights Analysis of Algorithmic Policing in Canada 5 octobre 2020 Ce rapport rédigé par Kate Robertson — avocate et chercheuse au Citizien Lab, Cynthia Khoo — chercheuse au Citizen Lab et avocate spécialisée dans la technologie et les droits de l’homme — et Yolanda Song — avocate chez Stevenson Whelton LLP et associée de recherche pro bono à l’IHRP — examine les technologies algorithmiques qui […] Lire la suite Nouvelles Tribune de Karim Benyekhlef et Valentin Callipel |Algorithmes et Justice : une prudente avancée 1 octobre 2020 Karim Benyekhlef / Valentin Callipel Karim Benyekhlef — directeur du Laboratoire de cyberjustice — et Valentin Callipel — chargé de mission au Laboratoire — s’intéressent dans une tribune publiée par Business & Legal Forum For Ethics & Performance aux enjeux des algorithmes dans le domaine judiciaire. (extracted from170.html)
Le Privacy Shield en est l’illustration parfaite, reflétant à lui seul les difficultés d’un couple américano-européen tentant tant bien que mal de recoller les morceaux d’un compromis juridiquement […] Lire la suite Nouvelles Winkler Institute for Dispute Resolution ANNUAL REPORT 2019-2020 17 septembre 2020 Trevor Farrow / Valentin Callipel Basé à la Osgoode Hall Law School à Toronto et nommé en l’honneur de l’ancien juge en chef de l’Ontario Warren Winkler, le Winkler Institute for Dispute Resolution est un centre de recherche qui travaille depuis 2014 dans le domaine du règlement des litiges, l’accès à la justice, et l’avenir de la profession juridique et […] Lire la suite Nouvelles Prof. (extracted from170.html)
Karine Gentelet, nouvelle titulaire de la Chaire Abeona-ENS-OBVIA Intelligence artificielle et justice sociale 10 septembre 2020 Karine Gentelet Prof. (extracted from170.html)
Karine Gentelet – chercheuse AJC – est récipiendaire de la Chaire 2020-2021 Abeona-École normale supérieure (ENS)-Observatoire international sur les impacts sociétaux de l’IA et du numérique (OBVIA) au concours IA et justice social. (extracted from170.html)
Cette chaire permet à un·e professeur·e invité·e de développer, pendant une année, des travaux sur l’intelligence artificielle (IA) et la justice […] Lire la suite Nouvelles CFCJ | The Justice Crisis : Un nouveau livre sur l’accès à la Justice au Canada 3 septembre 2020 Les Jacobs / Trevor Farrow The Justice Crisis: The Cost and Value of Accessing Law publié sous la direction de Trevor Farrow et Lesley A. (extracted from170.html)
Jacobs – chercheurs AJC – fournit un aperçu approfondi, basé sur de nouvelles recherches empiriques, de ce qui fonctionne et ne fonctionne pas pour améliorer l’accès à la justice civile et familiale au Canada. (extracted from170.html)
Alors que les algorithmes sont censés améliorer l’efficience et la qualité des services publics, certains se sont […] Lire la suite Blogue Retour sur l’expérience judiciaire en temps de pandémie : quelle technologisation de notre justice ? (extracted from170.html)
La Bourse doctorale de la Chaire LexUM en information juridique et du Laboratoire de cyberjustice est conçue pour encourager deux […] Lire la suite À la une Actualités Offre d’emploi | 2 auxiliaires de recherche au Laboratoire de cyberjustice 14 novembre 2024 Le Laboratoire de cyberjustice est un espace unique de réflexion et de création où les processus de justice sont modélisés et réimaginés afin d’améliorer l’accès à la justice. (extracted from170.html)
Nos travaux se distinguent par l’expérimentation d’innovations technologiques, comme l’intelligence artificielle, pour optimiser le fonctionnement actuel de la justice, de manière à en accroître l’efficacité, à en […] Lire la suite Voir les précédents Calendrier Actualités Conférence annuelle 2025 : Autonomisation des acteurs judiciaires par la cyberjustice (AJC) 15 octobre 2025 16 octobre 2025 Laboratoire de cyberjustice (B-2215) 30 juin 2025 Actualités Nous joindre Espace exclusif – membres AJC Inscription - Infolettre Adresse courriel © 2018 AJC • Crédits et mentions légales propulsé par force rouge sur OpenUM.ca , un projet de la Chaire L.R. (extracted from170.html)
Case Law 35 European Court of Human Rights (ECtHR) 35 Court of Justice of the European Union (CJEU) 37 National courts/data protection authorities 38 ETHICAL ASPECTS OF BIOMETRIC IDENTIFICATION 42 3.1. (extracted from271.pdf)
Recommendations with regard to consent management 87 REFERENCES 89 ANNEX: PROPOSED WORDING OF TITLE II AND TITLE IIA 96 4 PE 696.968 Biometric Recognition and Behavioural Detection LIST OF ABBREVIATIONS AI Artificial Intelligence AIA Artificial Intelligence Act AFIS Automated Fingerprint Identification System Art(s) Article(s) BCI Brain-Computer-Interface BDSG Bundesdatenschutzgesetz - German Federal Data Protection Act BIPA Illinois Biometric Information Privacy Act BVerfG Bundesverfassungsgericht - German Constitutional Court BVerwG Bundesverwaltungsgericht – German Federal Administrative Court CCPA California Consumer Privacy Act CCTV Closed-Circuit Television CFR Charter of Fundamental Rights of the European Union CJEU Court of Justice of the European Union DNA Deoxyribonucleic acid DSA Digital Services Act DSG Datenschutzgesetz - Austrian Data Protection Act EC European Commission ECG Electrocardiography ECHR European Convention on Human Rights ECtHR European Court of Human Rights Ed(s) Editor(s) Edn Edition PE 696.968 5 IPOL | Policy Department for Citizens’ Rights and Constitutional Affairs EEG Electroencephalography EES Entry-Exit-System e.g. (extracted from271.pdf)
id est (that is) IoT Internet of Things LED Law Enforcement Directive – Directive (EU) 2016/680 OGH Oberster Gerichtshof - Austrian Supreme Court of Justice OJ Official Journal of the European Union Para(s) Paragraph(s) PIPL Personal Information Protection Law of the People’s Republic of China SIS Schengen Information System SPG Sicherheitspolizeigesetz- Federal Security Police Act TEU Treaty of the European Union (TEU) TTDSG German Act on Data Protection and Privacy in Telecommunications and Telemedia UDHR Universal Declaration of Human Rights UK-DPA Data Protection Act of the United Kingdom UN United Nations 6 PE 696.968 Biometric Recognition and Behavioural Detection LIST OF BOXES WITH ILLUSTRATIONS Illustration 1: Differentiating biometrics-based data and other personal data 68 Illustration 2: Differentiating ‘real-time’ and ‘post’ remote identification 70 Illustration 3: Relationship between ‘biometric categorisation’ and ‘biometric inferences’ 71 Illustration 4: Undesirable remote biometric identification beyond law enforcement 77 Illustration 5: Remote biometric identification in grey zones around law enforcement 78 Illustration 6: Data collection and storage in the context of biometric identification 80 Illustration 7: Justification of emotion recognition or biometric categorisation 82 Illustration 8: Emotion recognition or biometric categorisation used as legal evidence 83 Illustration 9: Personality profiles created with the help of a video game 85 Illustration 10: Biometric inferences drawn with regard to third parties 86 LIST OF FIGURES Figure 1: Authentication/identification, categorisation, and detection 21 Figure 2: Steps involved in biometric identification 43 Figure 3: Steps involved in biometric categorisation 54 Figure 4: Steps involved in biometric detection 58 Figure 5: Risk-based approach of the AIA Proposal 63 Figure 6: Biometric techniques under the risk levels of the AIA Proposal 64 LIST OF TABLES Table 1: Admissibility of biometric techniques (based on simplified assumptions) 65 Table 2: Limitations on scope with regard to identification measures 75 PE 696.968 7 IPOL | Policy Department for Citizens’ Rights and Constitutional Affairs EXECUTIVE SUMMARY Background Biometric identification together with biometric categorisation, behavioural detection, emotion recognition, brain-computer-interfaces (BCIs), and similar techniques are being used to an increasing extent by public and private bodies. (extracted from271.pdf)
58 Recommendations 3 and 7 European Parliament resolution of 20 January 2021 on artificial intelligence: questions of interpretation and application of international law in so far as the EU is affected in the areas of civil and military uses and of state authority outside the scope of criminal justice (2020/2013(INI)). (extracted from271.pdf)
63 See Legislative Train Schedule, ‘Completion of EU Accession to the European Convention on Human Rights’, available at <https://www.europarl.europa.eu/legislative-train/theme-area-of-justice-and-fundamental-rights/file-completion-of-eu- accession-to-the-echr> (last accessed 09 July 2021). (extracted from271.pdf)
102 Regulation (EU) No 603/2013 of the European Parliament and of the Council of 26 June 2013 on the establishment of 'Eurodac' for the comparison of fingerprints for the effective application of Regulation (EU) No 604/2013 establishing the criteria and mechanisms for determining the Member State responsible for examining an application for international protection lodged in one of the Member States by a third-country national or a stateless person and on requests for the comparison with Eurodac data by Member States' law enforcement authorities and Europol for law enforcement purposes, and amending Regulation (EU) No 1077/2011 establishing a European Agency for the operational management of large-scale IT systems in the area of freedom, security and justice, OJ L 180, 1-30. (extracted from271.pdf)
36 PE 696.968 Biometric Recognition and Behavioural Detection Court of Justice of the European Union (CJEU) Currently, there is no EU case law regarding highly sophisticated identification techniques. (extracted from271.pdf)
The Austrian Supreme Court of Justice (Oberster Gerichtshof, OGH) ruled in favour of the employees and held that the biometric templates are obtained for the comparatively trivial aim of determining the employee’s times of coming and going. (extracted from271.pdf)
182 Austrian Supreme Court of Justice 18 October 2006, 9 Ob 109/06d; Austrian Supreme Court of Justice 22 January 2020, 9 Ob 120/19s. (extracted from271.pdf)
196 High Court of Justice (Divisional Court of Cardiff) 4 September 2019, EWCH 2341 (Admin), para 159. (extracted from271.pdf)
Any deficiencies in this regard may lead to severe unfairness or even to massive discrimination, including on racial or ethnic grounds, and to the undermining of procedural rights, including access to justice and the right to a fair trial. (extracted from271.pdf)
Gutheil M and others, ‘Interoperability of Justice and Home Affairs Information Systems’ (European Parliament 2018). (extracted from271.pdf)
Mladenovic, Milos N./McPherson, Tristram: Engineering social justice into traffic con- trol for self-driving vehicles? (extracted from532.pdf)
Civil Rights Principles for the Era of Big Data - The Leadership Conference on Civil and Human Rights Mobile Menu Overlay Home About Our Work The Coalition Take Action Media & Resources Podcast Blog Library Contact Us Switch to: Education Fund Careers Donate Right Arrow An arrow pointing to the right Switch to: Education Fund Careers Donate About Us History Task Forces Board of Directors Staff Careers Internships Voting Record 75th Anniversary Ways to Give Our Work Democracy Census and Data Equity Courts Voting Rights Justice Immigrant Rights Justice Reform Policing Inclusion & Opportunity Center for Civil Rights and Technology Economic Justice Education Equity Fighting Hate & Bias Protect DEIA The Coalition Media & Resources Advocacy Letters Amicus Briefs Beyond 100 Days Biden Timeline Comments Fact Sheets Press Releases Project 2025 Reports Resources Testimony Trump Timeline Take Action Podcast Blog Civil Rights Principles for the Era of Big Data Center for Civil Rights and Technology News 02.27.14 Share Technological progress should bring greater safety, economic opportunity, and convenience to everyone. (extracted from364.html)
At the same time, as new technologies allow companies and government to gain greater insight into our lives, it is vitally important that these technologies be designed and used in ways that respect the values of equal opportunity and equal justice. (extracted from364.html)
This requires disclosure of the underlying data, and the right to correct it when inaccurate.Signatories: American Civil Liberties Union Asian Americans Advancing Justice — AAJC Center for Media Justice ColorOfChange Common Cause Free Press The Leadership Conference on Civil and Human Rights NAACP National Council of La Raza National Hispanic Media Coalition National Urban League NOW Foundation New America Foundation’s Open Technology Institute Public Knowledge Back to Press & Media Civil and Human Rights Coalition Applauds Priorities of Presidential Budget Join the fight for justice, inclusion, and fairness for all. (extracted from364.html)
Accept additional cookies Reject additional cookies View cookies Hide cookie message Skip to main content GOV.UK Navigation menu Menu Menu Services and information Benefits Births, death, marriages and care Business and self-employed Childcare and parenting Citizenship and living in the UK Crime, justice and the law Disabled people Driving and transport Education and learning Employing people Environment and countryside Housing and local services Money and tax Passports, travel and living abroad Visas and immigration Working, jobs and pensions Government activity Departments Departments, agencies and public bodies News News stories, speeches, letters and notices Guidance and regulation Detailed guidance, regulations and rules Research and statistics Reports, analysis and official statistics Policy papers and consultations Consultations and strategy Transparency Data, Freedom of Information releases and corporate reports Search GOV.UK × Search GOV.UK Search Search GOV.UK Search Home Government Government efficiency, transparency and accountability Data Ethics Framework Government Digital Service Central Digital & Data Office Guidance Data Ethics Framework Updated 16 September 2020 Contents How to use the Data Ethics Framework Overarching principles Specific actions Next steps Print this page © Crown copyright 2020 This publication is licensed under the terms of the Open Government Licence v3.0 except where otherwise stated. (extracted from50.html)
Cancel Services and information Benefits Births, death, marriages and care Business and self-employed Childcare and parenting Citizenship and living in the UK Crime, justice and the law Disabled people Driving and transport Education and learning Employing people Environment and countryside Housing and local services Money and tax Passports, travel and living abroad Visas and immigration Working, jobs and pensions Government activity Departments News Guidance and regulation Research and statistics Policy papers and consultations Transparency How government works Get involved Support links Help Privacy Cookies Accessibility statement Contact Terms and conditions Rhestr o Wasanaethau Cymraeg Government Digital Service All content is available under the Open Government Licence v3.0 , except where otherwise stated © Crown copyright (extracted from50.html)
Accept additional cookies Reject additional cookies View cookies Hide cookie message Skip to main content GOV.UK Navigation menu Menu Menu Services and information Benefits Births, death, marriages and care Business and self-employed Childcare and parenting Citizenship and living in the UK Crime, justice and the law Disabled people Driving and transport Education and learning Employing people Environment and countryside Housing and local services Money and tax Passports, travel and living abroad Visas and immigration Working, jobs and pensions Government activity Departments Departments, agencies and public bodies News News stories, speeches, letters and notices Guidance and regulation Detailed guidance, regulations and rules Research and statistics Reports, analysis and official statistics Policy papers and consultations Consultations and strategy Transparency Data, Freedom of Information releases and corporate reports Search GOV.UK × Search GOV.UK Search Search GOV.UK Search Home Government Public services Guidelines for AI procurement Department for Science, Innovation & Technology Office for Artificial Intelligence Department for Digital, Culture, Media & Sport Department for Business, Energy & Industrial Strategy Guidance Guidelines for AI procurement Published 8 June 2020 Contents Introduction Top 10 Considerations AI-specific considerations within the procurement process 1. (extracted from108.html)
Cancel Services and information Benefits Births, death, marriages and care Business and self-employed Childcare and parenting Citizenship and living in the UK Crime, justice and the law Disabled people Driving and transport Education and learning Employing people Environment and countryside Housing and local services Money and tax Passports, travel and living abroad Visas and immigration Working, jobs and pensions Government activity Departments News Guidance and regulation Research and statistics Policy papers and consultations Transparency How government works Get involved Support links Help Privacy Cookies Accessibility statement Contact Terms and conditions Rhestr o Wasanaethau Cymraeg Government Digital Service All content is available under the Open Government Licence v3.0 , except where otherwise stated © Crown copyright (extracted from108.html)
Several witnesses highlighted the growing use of AI within the US justice system, in particular the Correctional Offender Management Profiling for Alternative Sanctions (COMPAS) system, developed by Northpointe, and used across several US states to assign risk ratings to defendants, which help to assist judges in sentences and parole decisions. (extracted from650.pdf)
Will Crosthwait, co-founder of AI start-up Kensai, highlighted investigations which found that the system commonly overestimated the recidivism risk of black defendants, and underestimated that of white defendants.143 Big Brother Watch observed that here in the UK, Durham Police have started to investigate the use of similar AI systems for determining whether suspects should be kept in custody, and described this and other developments as a “very worrying trend, particularly when the technology is being trialled when its abilities are far from accurate”.144 Evidence from Sheena Urwin, Head of Criminal Justice at Durham Constabulary, emphasised the considerable lengths that Durham Constabulary have taken to ensure their use of these tools is open, fair and ethical, in particular the development of their ‘ALGO-CARE’ framework for the ethical use of algorithms in policing.145 138 Written evidence from Dr Ansgar Koene (AIC0208) 139 Q 74 (Olivier Thereaux) 140 Written evidence from Research Councils UK (AIC0142) 141 Written evidence from Leverhulme Centre for the Future of Intelligence (AIC0182) 142 Tom Simonite, ‘When it comes to gorillas, Google Photos remains blind’, Wired (11 January 2018): https://www.wired.com/story/when-it-comes-to-gorillas-google-photos-remains-blind/ [accessed 31 January 2018] 143 Written evidence from Will Crosthwait (AIC0094) 144 Written evidence from Big Brother Watch (AIC0154) 145 Written evidence from Marion Oswald and Sheena Urwin (AIC0068) AI IN THE UK: READY, WILLING AND ABLE? (extracted from650.pdf)
* Libby Kinsey, Co-founder, Project Juno ** Dr Mercedes Bunz, Senior Lecturer, Communications QQ 55–64 and Media Research Institute, University of Westminster ** Elizabeth Denham, UK Information Commissioner, Information Commissioner’s Office * Dr Sandra Wachter, Postdoctoral Researcher in Data Ethics and Algorithms, Oxford Internet Institute * Olivier Thereaux, Head of Technology, The Open Data QQ 65–75 Institute * Javier Ruiz Diaz, Policy Director, The Open Rights Group ** Frederike Kaltheuner, Policy Officer, Privacy International ** Dr James Luke, Chief Technology Officer for the Public QQ 76–84 Sector, IBM ** Kriti Sharma, Vice President of Artificial Intelligence and Bots, Sage * Andrew de Rozairo, Vice President, Customer Innovation and Enterprise Platform, SAP * Colin Griffiths, Policy Manager, Citizens Advice QQ 85–94 ** Will Hayter, Project Director, Competition and Markets Authority ** Olly Buston, CEO and Founder, Future Advocacy QQ 95–104 * Professor Dame Henrietta Moore, Director, Institute for Global Prosperity, UCL ** Professor Richard Susskind OBE, IT Adviser to the Lord Chief Justice of England and Wales * Dr Mark Taylor, Global Strategy & Research Director, QQ 105–115 Dyson ** Dr Joseph Reger, Chief Technology Officer EMEIA, Fujitsu ** Paul Clarke, Chief Technology Officer, Ocado * Dr Julian Huppert, Chair, Independent Review Panel for QQ 116–127 DeepMind Health ** Dr Sobia Raza, Head of Science, PHG Foundation * Nicola Perrin, Head, Understanding Patient Data, Wellcome Trust ** Dr Hugh Harvey, Clinical Artificial Intelligence QQ 128–142 Researcher and Consultant Radiologist, Guy’s and St Thomas’ NHS Foundation Trust * Dame Fiona Caldicott, National Data Guardian for Health and Care, Office of the National Data Guardian AI IN THE UK: READY, WILLING AND ABLE? (extracted from650.pdf)
There were three overarching principles to this: • partnership with, not replacement of, humans; • putting human values at the centre of their applications; and • a strong focus on a wide-ranging set of ethical considerations, including the preservation of human autonomy, beneficence, non-maleficence, and justice. (extracted from650.pdf)
for use in the criminal justice system, so defendants and their lawyers can understand and challenge evidence used against them); and • Tools for investigators and auditors for use when things go wrong. (extracted from650.pdf)
processes, the problems may also be considered to result from a lack of clarity as to the connection between the In the liberal tradition – a key influence on both pub- input data and the correlations resulting from it; this, in lic debate and the administration of justice in Ger- turn, may either be because the complexity of the algo- many and the European Union – privacy is generally rithms and the calculations they perform are “black boxed” regarded as a condition for enabling autonomy and or because the algorithms have worked with poor data so as its expression, in the sense of being able to think that the “evidence” derived from them is likewise flawed. (extracted from37.pdf)
scoring or support systems used to Collective goods, rather like the terms “common good” make a decision on granting a visa) and “justice”, cannot be defined in terms of their sub- stance. (extracted from37.pdf)
Department of Commerce Subcommittee on Networking and Information Technology Research and Development Co-Chair Co-Chair Bryan Biegel James Kurose Director, National Coordination Office for Assistant Director, Computer and Information Networking and Information Technology Science and Engineering Research and Development National Science Foundation Networking and Information Technology Research and Development Task Force on Artificial Intelligence Co-Chair Co-Chair Lynne Parker Jason Matheny Division Director Director Information and Intelligent Systems Intelligence Advanced Research Projects Activity National Science Foundation Members Milton Corn Nikunj Oza National Institutes of Health National Aeronautics and Space Administration William Ford Robinson Pino National Institute of Justice Department of Energy Michael Garris Gregory Shannon National Institute of Standards and Technology Office of Science and Technology Policy Steven Knox Scott Tousley National Security Agency Department of Homeland Security vii John Launchbury Faisal D’Souza Defense Advanced Research Projects Agency Technical Coordinator National Coordination Office for Networking and Richard Linderman Information Technology Research and Development Office of the Secretary of Defense viii NATIONAL ARTIFICIAL INTELLIGENCE RESEARCH AND DEVELOPMENT STRATEGIC PLAN Contents About the National Science and Technology Council .......................................................................................... (extracted from485.pdf)
The Federal government should therefore emphasize AI investments in areas of strong societal importance that are not aimed at consumer markets—areas such as AI for public health, urban systems and smart communities, social welfare, criminal justice, environmental sustainability, and national security, as well as long-term research that accelerates the production of AI knowledge and technologies. (extracted from485.pdf)
Scientists must also study to what extent justice and fairness considerations can be designed into the system, and how to accomplish this within the bounds of current engineering techniques. (extracted from485.pdf)
26 NATIONAL ARTIFICIAL INTELLIGENCE RESEARCH AND DEVELOPMENT STRATEGIC PLAN Building ethical AI Beyond fundamental assumptions of justice and fairness are other concerns about whether AI systems can exhibit behavior that abides by general ethical principles. (extracted from485.pdf)
Department of Commerce The following Federal departments and agencies are represented on the Subcommittee on Machine Learning and Artificial Intelligence and through it, work together to monitor the state of the art in machine learning (ML) and AI (within the Federal Government, in the private sector, and internationally), to watch for the arrival of important technology milestones in the development of AI, to coordinate the use of and foster the sharing of knowledge and best practices about ML and AI by the Federal Government, and to consult in the development of Federal research and development priorities in AI: Department of Commerce (Co-Chair) Department of Veterans Affairs Department of Defense United States Agency for International Department of Education Development Department of Energy Central Intelligence Agency Department of Health and Human Services General Services Administration Department of Homeland Security National Science Foundation Department of Justice National Security Agency Department of Labor National Aeronautics and Space Administration Department of State Office of the Director of National Intelligence Department of Transportation Social Security Administration Department of Treasury The following offices of the Executive Office of the President are also represented on the Subcommittee: Council of Economic Advisers Office of the Vice President Domestic Policy Council National Economic Council Office of Management and Budget National Security Council Office of Science and Technology Policy (Co- Chair) PREPARING FOR THE FUTURE OF ARTIFICIAL INTELLIGENCE PREPARING FOR THE FUTURE OF ARTIFICIAL INTELLIGENCE Contents Executive Summary ..................................................................................................................................................... (extracted from484.pdf)
30 Justice, Fairness, and Accountability ................................................................................................................. (extracted from484.pdf)
Public- and private- sector investments in basic and applied R&D on AI have already begun reaping major benefits to the public in fields as diverse as health care, transportation, the environment, criminal justice, and economic inclusion. (extracted from484.pdf)
Use of AI to make consequential decisions about people, often replacing decisions made by human-driven bureaucratic processes, leads to concerns about how to ensure justice, fairness, and accountability—the same concerns voiced previously in the Administration’s Big Data: Seizing Opportunities, Preserving Values report of 2014,1 as well as the Report to the President on Big Data and Privacy: A Technological Perspective published by the President’s Council of Advisors on Science and Technology in 2014.2 Transparency concerns focus not only on the data and algorithms involved, but also on the potential to have some form of explanation for any AI-based determination. (extracted from484.pdf)
Many areas of public policy, from education and the economic safety net, to defense, environmental preservation, and criminal justice, will see new opportunities and new challenges driven by the continued progress of AI. (extracted from484.pdf)
The promise of AI has been compared to the transformative impacts of advances in mobile computing.21 Public- and private-sector investments in basic and applied R&D on AI have already begun reaping major benefits for the public in fields as diverse as health care, transportation, the environment, criminal justice, and economic inclusion.22 At Walter Reed Medical Center, the Department of Veteran Affairs is using AI to better predict medical complications and improve treatment of severe combat wounds, leading to better patient outcomes, faster healing, and lower costs.23 The same general approach—predicting complications to enable preventive treatment—has also reduced hospital-acquired infections at Johns Hopkins University.24 Given the current transition to electronic health records, predictive analysis of health data may play a key role across many health domains like precision medicine and cancer research. (extracted from484.pdf)
Autonomous watercraft may be much cheaper to operate than manned ships, and may someday be used for enhanced weather prediction, climate monitoring, or policing illegal fishing.27 AI also has the potential to improve aspects of the criminal justice system, including crime reporting, policing, bail, sentencing, and parole decisions. (extracted from484.pdf)
The Administration is exploring how AI can responsibly benefit current initiatives such as Data Driven Justice and the Police Data Initiative that seek to provide law enforcement and the public with data that can better inform decision-making in the criminal justice system, while also taking care to minimize the possibility that AI might introduce bias or inaccuracies due to deficiencies in the available data. (extracted from484.pdf)
Social justice and public policy institutions that do not typically engage with advanced technologies and data science in their work should consider partnerships with AI researchers and practitioners that can help apply AI tactics to the broad social problems these institutions already address in other ways. (extracted from484.pdf)
The use of AI to make consequential decisions about people, often replacing decisions made by human actors and institutions, leads to concerns about how to ensure justice, fairness, and accountability—the same concerns voiced previously in the “Big Data” context.62 The use of AI to control physical-world equipment leads to concerns about safety, especially as systems are exposed to the full complexity of the human environment. (extracted from484.pdf)
Justice, Fairness, and Accountability A common theme in the Law and Governance, AI for Social Good, and Social and Economic Impacts workshops was the need to ensure that AI promotes justice and fairness, and that AI-based processes are accountable to stakeholders. (extracted from484.pdf)
In the criminal justice system, some of the biggest concerns with Big Data are the lack of data and the lack of quality data.65 AI needs good data. (extracted from484.pdf)
It is important that anyone using AI in the criminal justice context is aware of the limitations of current data. (extracted from484.pdf)
65 Matt Ford, “The Missing Statistics of Criminal Justice,” The Atlantic, May 31, 2015, http://www.theatlantic.com/politics/archive/2015/05/what-we-dont-know-about-mass-incarceration/394520/ 66 Julia Angwin, Jeff Larson, Surya Mattu, and Lauren Kirchner, “Machine Bias,” ProPublica, May 23, 2016, https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing. (extracted from484.pdf)
Many areas of public policy, from education and the economic safety net, to defense, environmental preservation, and criminal justice, will see new opportunities and new challenges driven by the continued progress of AI. (extracted from484.pdf)
Social justice and public policy institutions that do not typically engage with advanced technologies and data science in their work should consider partnerships with AI researchers and practitioners that can help apply AI tactics to the broad social problems these institutions already address in other ways. (extracted from484.pdf)
Matt Ford, “The Missing Statistics of Criminal Justice,” The Atlantic, May 31, 2015, http://www.theatlantic.com/politics/archive/2015/05/what-we-dont-know-about-mass-incarceration/394520/ Jason Furman, “Is This Time Different? (extracted from484.pdf)
Civil Rights Principles for the Era of Big Data - The Leadership Conference on Civil and Human Rights Mobile Menu Overlay Home About Our Work The Coalition Take Action Media & Resources Podcast Blog Library Contact Us Switch to: Education Fund Careers Donate Right Arrow An arrow pointing to the right Switch to: Education Fund Careers Donate About Us History Task Forces Board of Directors Staff Careers Internships Voting Record 75th Anniversary Ways to Give Our Work Democracy Census and Data Equity Courts Voting Rights Justice Immigrant Rights Justice Reform Policing Inclusion & Opportunity Center for Civil Rights and Technology Economic Justice Education Equity Fighting Hate & Bias Protect DEIA The Coalition Media & Resources Advocacy Letters Amicus Briefs Beyond 100 Days Biden Timeline Comments Fact Sheets Press Releases Project 2025 Reports Resources Testimony Trump Timeline Take Action Podcast Blog Civil Rights Principles for the Era of Big Data Center for Civil Rights and Technology News 02.27.14 Share Technological progress should bring greater safety, economic opportunity, and convenience to everyone. (extracted from427.html)
At the same time, as new technologies allow companies and government to gain greater insight into our lives, it is vitally important that these technologies be designed and used in ways that respect the values of equal opportunity and equal justice. (extracted from427.html)
This requires disclosure of the underlying data, and the right to correct it when inaccurate.Signatories: American Civil Liberties Union Asian Americans Advancing Justice — AAJC Center for Media Justice ColorOfChange Common Cause Free Press The Leadership Conference on Civil and Human Rights NAACP National Council of La Raza National Hispanic Media Coalition National Urban League NOW Foundation New America Foundation’s Open Technology Institute Public Knowledge Back to Press & Media Civil and Human Rights Coalition Applauds Priorities of Presidential Budget Join the fight for justice, inclusion, and fairness for all. (extracted from427.html)
We share the view that the digital society must be built on trust among all stakeholders including governments, civil society, international organizations, academics and businesses through sharing common values and principles including equality, justice, transparency and accountability taking into account the global economy and interoperability. (extracted from335.pdf)
These include freedom, dignity and autonomy, privacy and data protection, non-discrimination and equality, diversity, fairness, social justice, and internationally recognized labor rights. (extracted from335.pdf)
And it was in this mindset that the CNIL set a collective approach in motion, for several months overseeing a public debate with the help of partners from various sectorial fields (health, justice...). (extracted from321.pdf)
Among them, we could mention the “Ligue de l’Enseignement” (associa- tion that focused on education concerns), French Insu- rance Federation (FFA), French Ministry of Culture (DG- MIC), Open Law (association that reflects on the justice Ethical thinking concerns system) as well as trade unions such as CFE-CFC and FO Cadres (for recruitment and HR), etc. (extracted from321.pdf)
The views of the diverse stakeholders (trade unions, associations, businesses, researchers, citizens, etc.) across a wide range of sectors (from insurance to education, justice and healthcare) thus informed the writing of this report, which provides an overview of the ethical matters raised by algorithms and artificial intel- ligence in their current applications and their potential uses in the relatively short term. (extracted from321.pdf)
THE ETHICAL MATTERS RAISED BY ALGORITHMS AND ARTIFICIAL INTELLIGENCE ALGORITHMS AND ARTIFICIAL INTELLIGENCE TODAY The main functions of algorithms and AI across different sectors Education Justice Health Security Work, HR Culture Other Better identify Reveal the Tap into the Identify Understand Create cultural Fine-tune an learners’ different ways vast amount unsuspected social showpieces insurance abilities judgments are of scientific links for solving phenomena (painting, music) company Generating handed down publications gendarmerie-led in the workplace customer’s risk knowledge between regions investigations profile Allocate higher Allocate patients Match a list of Match education places for participation applicants to a “compatible” to candidates in a clinical trial job vacancy profiles (APB) on dating apps, Matching etc. (extracted from321.pdf)
The next step of what some refer to as “predic- on the sheer range of the spectrum of serious or less serious tive justice” would involve entrusting systems with the task matters in the use of such or such an algorithm. (extracted from321.pdf)
Incidentally, the CNIL-led public debate brought a legal rules and categories no longer on the grounds of controversy to light in this regard, concerning IBM’s Watson our ideal of justice, but so that they are more readily platform. (extracted from321.pdf)
And anyone deploying algorithms ligence be limited to crucial decisions, sectors where the that are likely to be used on a large scale should be urged impact on humans is undeniable, such as medicine, justice, to bear it in mind. (extracted from321.pdf)
Practical examples yet, are we absolutely certain that, within certain limits, of algorithms being used by the authorities as well as the forms of regional disparity do not, in fact, reflect a res- example of predictive justice give a clearer idea of this ponsible exercising of caution on the part of the judge? (extracted from321.pdf)
A complex chain a predictive justice. (extracted from321.pdf)
For it enables juries to take on board new insights over the course of Similarly, at the symposium on predictive justice orga- hearing the different arguments, and to change opinion, nised on 19 May 2017 by the Lille Bar, Law Department as shows more clearly than any demonstration the film of Université catholique de Lille and the Douai Court of by Sidney Lumet, Twelve Angry Men. (extracted from321.pdf)
Predictive justice applications are being subjected to particularly close public scrutiny as regards their Management Profile for Alternative Sanction) tool designed to come up with a recidivism risk score for prisoners or defendants on trial. (extracted from321.pdf)
Medicine our day-to-day lives, as long as answers are and justice are other sectors where this question might forthcoming. (extracted from321.pdf)
It is paramount that they have eness of the ethical dimensions of a decision-making the fullest possible awareness of the ethical and social process that must not exclude human intervention and by implications of their work and of the very fact that these honing critical thinking in some particularly sensitive sec- can even extend to societal choices which they should tors, such as medicine, recruitment, justice and perhaps not by rights be able to judge alone. (extracted from321.pdf)
The partners in the public debate • Allistene’s research committee on ethics (CERNA) • Institut Mines-Télécom (IMT), Research Chair “Values • Bordeaux’s Cognitique Institute (ENSC) and Politics of Personal Information” • Bordeaux University • Laboratory for Collective and Artificial Intelligence (LICA) • Caisse des dépôts et consignations (CDC) • Law Department of Université Catholique de Lille, • Club des Juristes (thinktank) Centre of research on relations between risk and law • Collège des Bernardins • Law Department of Université Catholique de Lyon • Complex Systems Institute of Paris Ile-de-France (ISC-PIF) • Ligue des Droits de l’Homme (Human Rights League/LDH) • Confédération française de l’encadrement – Confédération • Ligue de l’Enseignement (Education League) générale des cadres (CFE-CGC, trade union) • Lille 2 University • Communication Publique • Lille Association of Lawyers • Conseil National des Barreaux (national institution • Lyon’s administrative court of appeal that represents all practising lawyers in France/CNB) • Microsoft • Conseil Supérieur de l’Audiovisuel (independent authority • Ministry of Culture, via the General Directorate of Media to protect audiovisual communication freedom/CSA) and Cultural Industries (DGMIC) • Conservatoire National des Arts et Métiers (leading • Ministry of National Education, via the Directorate higher education and research institution dedicated of Digital Technology for Education (DNE) and its Numéri’lab to adult continuing education/CNAM) • National Academy of Technologies of France • Douai court of appeal • National Institute of Higher Studies on Defence (IHEDN) • ESCP Europe, IoT Chair • National Institute of Higher Studies on Security and Justice • Etalab(body that works in France on data sharing (INHESJ) in the public sector) • National Institute of Applied Sciences (INSA) • “Familles rurales” association • Necker Hospital • Federal University of Toulouse • OpenLaw (association) • French Association for Artificial intelligence (AFIA) • Paris II University • French Association for Employment Law (AFDT) • Randstad • French Development Agency(AFD) • Research Centre of the National Gendarmerie School • French governmental advisory council on bioethics issues of Officers (CREOGN) (CCNE) • Rhône Département-level Council of the Medical • French Insurance Federation (FFA) Association • French National Center for Scientific Research (CNRS)’s • Renaissance Numérique (thinktank) ethics committee (COMETS) • School of Advanced Studies in the Social Sciences (EHESS) • FO-Cadres (trade union) • Sciences Po Lille • Fondation Internet Nouvelle Génération (FING) • Sciences Po Paris • Fotonower • Société informatique de France (association devoted • Génotoul societal (bioscience and ethics platform) to computer science/SIF) • Groupe VYV (MGEN – ISTYA – Harmonie) • The Future Society at Harvard Kennedy School, AI Initiative • Imagine Institute on genetic diseases • Universcience • INNOvation Ouverte par Ordinateur (INNOOO) • Visions d’Europe (association) The other contributors The 37 citizens who took part in the public consultation • A rbre des connaissances (association) organised in Montpellier on 14 October 2017. (extracted from321.pdf)
23/01/2017 LAUNCH EVENT ROUNDTABLE SESSIONS “Algorithms and humans” and “Fairness, transparency and plurality of algorithms” > CNIL 23/03/2017 SYMPOSIUM “Towards new forms of humanity?” 25/03/2017 > Universcience 31/03/2017 CONFERENCE “Algorithms and law” > Lille II University 06/04/2017 CONFERENCE “The choice in the age of Big Data” > Sciences Po Lille and Visions d’Europe 08/04/2017 DEBATE “The governance of emerging technosciences” > German American Conference at Harvard University 18/04/2017 DEBATE “Transatlantic perspectives on: AI in the age of social media; privacy, security and the future of political campaigning” > The Future Society at Harvard Kennedy School 18/04/2017 ROUNDTABLE SESSIONS “Big Data, human resources: algorithms on the agenda” > FO-Cadres 04/05/2017 CONFERENCE “Fairness of algorithmic decision-making” > Toulouse III – Paul Sabatier University 16/05/2017 DEBATE “Will digital technology spell the end of the rule of law?” > Collège des Bernadins 19/05/2017 SYMPOSIUM “Predictive justice” > Douai Court of Appeal, Lille Association of Lawyers and Law Department of Université Catholique de Lille 02/06/2017 WORKSHOPS “Fairness of algorithmic decision-making and processing” > LabEx International Centre for Mathematics and Computer Science in Toulouse 64 HOW CAN HUMANS KEEP THE UPPER HAND? (extracted from321.pdf)
THE ETHICAL MATTERS RAISED BY ALGORITHMS AND ARTIFICIAL INTELLIGENCE EVENTS 08/06/2017 DEBATE “Algorithms in healthcare: what ethics?” > Groupe VYV (MGEN – ISTYA – Harmonie) 14/06/2017 ROUNDTABLE SESSION “Artificial intelligence: ethics, at the intersection of HR and Big Data” > Confédération française de l’encadrement – Confédération générale des cadres (CFE-CGC) 16/06/2017 DEBATE “Algorithms, employment and ethics” > French Association for Employment Law (AFDT) 19/06/2017 DAY “Ethical algorithms, a moral requirement and competitive advantage” > Allistene’s CERNA and Société Informatique de France (SIF) 19/06/2017 SYMPOSIUM “Human, non-human in the age of artificial intelligence” > Paris II University 21/06/2017 SYMPOSIUM “Artificial intelligence: autonomy, delegation and accountability” > Bordeaux’s Cognitique Institute (ENSC) 22/06/2017 WORKSHOP “Ethics of algorithms: implications for healthcare” > Genotoul (bioscience and ethics platform) 22/06/2017 CROWDSOURCING WORKSHOP “Artificial intelligence and law” > OpenLaw 22/06/2017 SYMPOSIUM “ The many dimensions of data ” 23/06/2017 > Institut Mines-Télécom, Values and Politics of Personal Information Research Chair 27/06/2017 SYMPOSIUM “Security and justice, the challenge of the algorithm” > National Institute of Higher Studies of Security and Justice (INHESJ) 28/06/2017 MOCK TRIAL AND ROUNDTABLE SESSION “ Ethique, algorithmes and justice ” > Law Department of Université Catholique de Lyon and Lyon’s Administrative Court of Appeal 28/06/2017 STUDY DAY “Admission Post-bac, textbook case of public algorithms” > Fondation Internet Nouvelle Génération (FING) and Etalab 03/07/2017 DAY “Algorithms and digital sovereignty” > Allistene’s CERNA 05/07/2017 DAY “Ethics and artificial intelligence” > French National Center for Scientific Research (CNRS)’s ethics committee (COMETS) and French Association for AI (AFIA) 22/08/2017 DEBATES on algorithms in education. (extracted from321.pdf)
This can most clearly be shown by comparing the sets of principles with the set of four core principles commonly used in bioethics: beneficence, non-maleficence, autonomy, and justice. (extracted from22.pdf)
4.4 Justice: Promoting Prosperity and Preserving Solidarity The last of the four classic bioethics principles is justice, which is typically invoked in relation to the distribution of resources, such as new and experimental treatment options or simply the general availability of conventional healthcare. (extracted from22.pdf)
The importance of “justice” is explicitly cited in the Montreal Declaration, which argues that “the development of AI should promote justice and seek to eliminate all 1 3 AI4People—An Ethical Framework for a Good AI Society:… 699 types of discrimination”, while the Asilomar Principles include the need for both “shared benefit” and “shared prosperity” from AI. (extracted from22.pdf)
Under its principle named “Jus- tice, equity and solidarity”, the EGE argues that AI should “contribute to global justice and equal access to the benefits” of AI technologies. (extracted from22.pdf)
Across the documents, justice variously relates to (a) Using AI to correct past wrongs such as eliminating unfair discrimination; (b) Ensuring that the use of AI creates benefits that are shared (or at least shareable); and (c) Preventing the creation of new harms, such as the undermining of existing social structures. (extracted from22.pdf)
Notable also are the different ways in which the position of AI, vis-à-vis people, is characterised in relation to justice. (extracted from22.pdf)
In Asilomar and EGE respectively, it is AI technologies themselves that “should benefit and empower as many people as pos- sible” and “contribute to global justice”, whereas in Montreal, it is “the develop- ment of AI” that “should promote justice” (italics added). (extracted from22.pdf)
Develop appropriate legal procedures and improve the IT infrastructure of the justice system to permit the scrutiny of algorithmic decisions in court. (extracted from22.pdf)
In a digitally connected world, the question of how to respect, protect and implement human rights and access to environmental justice is becoming paramount. (extracted from679.pdf)
Human rights and access to environmental justice need to be safeguarded in the development, implementation, legislation, 22 and governance of digital technologies. (extracted from679.pdf)
It might also compound further the problems with international transfer of data after that the US Safe Harbour and later the EU-US Privacy Shield were invalidated by the Court of Justice (Schrems I and II case-law, C-362/14 and C-311/18), as illustrated by Kiner (2020). (extracted from282.pdf)
The Schrems II judgment of the Court of Justice and the future of data transfer regulation. (extracted from282.pdf)
European Law Blog: https://europeanlawblog.eu/2020/07/17/the-schrems-ii- judgment-of-the-court-of-justice-and-the-future-of-data-transfer-regulation/. (extracted from282.pdf)
AI tightly developed under human oversight and control), embed the principles of fairness and justice in algorithms, applied in all phases of AI systems’ design, implementation and testing, while following through bias complaints and other undesired effects reporting. (extracted from296.pdf)
During the 71st session of the United Nations General Assembly, on 29 September 2016, UNICRI (United Nations International Crime and Justice research Institute) announced the opening of the first Centre on AI and Robotics 30 PE 626.074 European Artificial Intelligence (AI) leadership, the path for an integrated vision within the United Nations system. (extracted from296.pdf)
AI tightly developed under human oversight and control), embed the principles of fairness and justice in algorithms, applied in all phases of AI systems’ design, implementation and testing, while following through bias complaints and other undesired effects reporting. (extracted from296.pdf)
The development of artificial intelligence should ensure fairness and justice, avoid bias or discrimination against specific groups or individuals, and avoid placing disadvantaged people in an even more unfavorable position. (extracted from93.html)
European Parliament 2019-2024 TEXTS ADOPTED P9_TA(2021)0009 Artificial intelligence: questions of interpretation and application of international law European Parliament resolution of 20 January 2021 on artificial intelligence: questions of interpretation and application of international law in so far as the EU is affected in the areas of civil and military uses and of state authority outside the scope of criminal justice (2020/2013(INI)) The European Parliament, – having regard to the preamble to the Treaty on European Union, and to Articles 2, 3, 10, 19, 20, 21, 114,167, 218, 225 and 227 thereof, – having regard to the right to petition enshrined in Articles 20 and 227 of the Treaty on the Functioning of the European Union, – having regard to the Charter of Fundamental Rights of the European Union, – having regard to Council Directive 2000/43/EC of 29 June 2000 implementing the principle of equal treatment between persons irrespective of racial or ethnic origin1 (Racial Equality Directive), – having regard to Council Directive 2000/78/EC of 27 November 2000 establishing a general framework for equal treatment in employment and occupation2 (Equal Treatment in Employment Directive), – having regard to Regulation (EU) 2016/679 of the European Parliament and of the Council of 27 April 2016 on the protection of natural persons with regard to the processing of personal data and on the free movement of such data, and repealing Directive 95/46/EC (General Data Protection Regulation)3 (GDPR), and to Directive (EU) 2016/680 of the European Parliament and of the Council of 27 April 2016 on the protection of natural persons with regard to the processing of personal data by competent authorities for the purposes of the prevention, investigation, detection or prosecution of criminal offences or the execution of criminal penalties, and on the free 1 OJ L 180, 19.7.2000, p. (extracted from269.pdf)
Languages, – having regard to the European ethical Charter on the use of Artificial Intelligence in judicial systems and their environment adopted by the Council of Europe Working Group on quality of justice (CEPEJ-GT-QUAL) in December 2018, – having regard to the OECD Council Recommendation on Artificial Intelligence adopted on 22 May 2019, – having regard to Rule 54 of its Rules of Procedure, – having regard to the opinions of the Committee on Foreign Affairs, the Committee on the Internal Market and Consumer Protection, the Committee on Transport and Tourism and the Committee on Civil Liberties, Justice and Home Affairs, – having regard to the report of the Committee on Legal Affairs (A9-0001/2021), Introduction A. (extracted from269.pdf)
whereas this particular responsibility implies a need to examine questions of interpretation and application of international law related to the active participation of the EU in international negotiations, in so far as the EU is affected by the civil and military uses of this kind of AI, robotics and related technologies, and questions of state authority over such technologies lie outside the scope of criminal justice; I. (extracted from269.pdf)
Recalls that according to the Advisory Opinion of the International Court of Justice of 8 July 1996, the principle of originality cannot be cited in support of any derogation regarding compliance with current norms of international humanitarian law; 13. (extracted from269.pdf)
Insists on the importance of investing in human skills, including digital skills, in order to adapt to scientific progress involving AI-driven solutions, for individuals exercising regulated professions, including activities connected with the exercise of state authority, such as the administration of justice; calls on the Member States and the Commission to duly take this into account as part of the implementation of Directive 2005/36/EC1; 25. (extracted from269.pdf)
Believes that an effective mechanism for enforcing the rules on non-proliferation of LAWS and any future offensive AI-enabled technologies is of paramount importance for global security; State authority: examples from civil areas, including health and justice 51. (extracted from269.pdf)
Urges the Member States to assess the risks related to AI-driven technologies before automating activities connected with the exercise of state authority, such as the administration of justice; calls on the Member States to consider the need to provide for safeguards such as supervision by a qualified professional and strict rules on professional ethics; 54. (extracted from269.pdf)
Notes that AI is increasingly being used in the field of justice in order to take decisions which are more rational, more in keeping with the law in force, and quicker; welcomes the fact that the use of AI is expected to speed up judicial proceedings; 68. (extracted from269.pdf)
Stresses that the use of AI in justice could improve the analysis and collection of data and the protection of victims, and that this could be explored in research and development and accompanied by impact assessments, in particular regarding safeguards for due process and against bias and discrimination, with the precautionary principle being applied; recalls, however, that this is no substitute for human involvement in sentencing or decision-making; 70. (extracted from269.pdf)
Recalls the importance of the principles of governance, transparency, impartiality, accountability, fairness and intellectual integrity in the use of AI in criminal justice; 71. (extracted from269.pdf)
Urges the Member States to assess the risks related to AI-driven technologies before automating activities connected with the exercise of state authority, especially in the area of justice; calls on them to consider the need to provide safeguards, such as supervision by a qualified professional and rules on professional ethics; 72. (extracted from269.pdf)
Requests that the public is kept informed about the use of AI in the field of justice, and that such uses do not give rise to discrimination resulting from programming biases; stresses that the right of every individual to have access to a public official must be respected, as well as the right of the responsible official to personally take the decision and deviate from the information received from the AI when they deem it necessary in the light of the details of the matter in question; highlights the right of the defendant to appeal the decision in accordance with national legislation, without ever eliminating the final responsibility of the judiciary; 74. (extracted from269.pdf)
Moreover, a few specific cases, mentioned as running at the time of first gathering, were later found to have been discontinued because of various reasons, including significant criticism received from the general public, pressure from adversarial political forces or even executive orders from local courts of criminal justice. (extracted from241.pdf)
— Fairness and justice: The development of AI should promote fairness and justice, the rights and interests of stakeholders and promote equality of opportunity. (extracted from241.pdf)
Previous research had analysed 84 ethical AI documents published by various businesses, NGOs and (international) governmental organizations, highlighting that some principles such as transparency, fairness, justice, and responsibility were quite common in all. (extracted from241.pdf)
The impact of using algorithms for managerial decisions on public employees’ procedural justice. (extracted from241.pdf)
To further ensure trustworthy AI, most governments are adopting new legislative frameworks for AI technologies, which are often sector- specific and cover areas such as autonomous driving, healthcare and e-justice. (extracted from243.pdf)
The Federal Ministry also highlights the use of AI for evaluating judicial proceedings, as mentioned in its National e-justice strategy. (extracted from243.pdf)
ensuring that citizens can make informed and independent decisions) and to human dignity, equality and justice (i.e. (extracted from243.pdf)
In addition, public sector operators should be secured sufficient resources and incentives to engage in such development, paying particular attention to sort out the rights of the outcomes of co-development;  The Ministry of Justice and the Ministry of Finance are currently examining national regulation of automated decision-making. (extracted from243.pdf)
5.13.4 Regulation Hungary’s national AI strategy aims to ensure a responsible, reliable and human-centred utilisation of AI technologies by means of the following policies:  Creating an ethical framework: developing an AI code of conduct by the first half of 2021 in collaboration between the Ministry of Justice, the Ministry for Innovation and Technology, AI Innovation Hub and the Central Statistical Office;  Setting up an Artificial Intelligence Regulation and Ethics Knowledge Centre: the aim is to create and coordinate an extensive pool of experts to help resolve legal issues and matters of ethics relating to the regulation of AI and the implementation of the strategy;  Establishing a regulatory framework for AI: the objective is to amend the current regulatory system to suit AI and to align it to EU regulations;  Building data management regulation: the objective is to set up regulations for the use and exchange of public and private data and to define rules regarding data monetisation. (extracted from243.pdf)
The Latvian strategy identifies priority sectors with a high potential for AI applications in the country, such as transport (Intelligent transport systems), culture, justice (AI as support for decision making and drafting legislation), agriculture (automated control), and translation. (extracted from243.pdf)
Regarding ethics, the government adopts the European ethical guidelines outlined by the European Commission for the Efficiency of Justice. (extracted from243.pdf)
66 1) International bodies and AI, 2) Swiss intelligence of interests in the European AI (Digital Europe Programme) activities, 3) Changes in the world of work, 4) AI in industry and services, 5) AI in education, 6) Application of AI in science and research, 7) AI in cybersecurity and security policy, 8) AI, Media & Public, 9) Automated mobility and AI, 10) AI in healthcare, 11) AI in the financial sector, 12) AI in agriculture, 13) Energy, climate, environment and AI, 14) AI in administration, 15) Further development of the general legal framework on AI, 16) AI in justice, 17) AI, data and intellectual property law. (extracted from243.pdf)
While the establishment of the legal basis is ensured by a wide range of institutions, the FDFA will specifically focus on the following policies to further develop the general legal framework on AI:  Examining the emergence of AI-specific international law and its impact on Switzerland;  Following-up developments with regard to the visibility of AI systems in interaction with consumers;  Monitoring developments in AI-based decision-making in the justice system (predictive justice). (extracted from243.pdf)
President: Pascal Pichonnaz First Vice-President: Lord John Thomas Second Vice-President: Anne Birgitte Gammeljord Treasurer: Pietro Sirena Speaker of the Senate: Reinhard Zimmermann Secretary-General: Vanessa Wilcox Scientific Director: Christiane Wendehorst European Law Institute Secretariat Schottenring 16/175 1010 Vienna Austria Tel.: + 43 1 4277 22101 Mail: secretariat@europeanlawinstitute.eu Website: www.europeanlawinstitute.eu ISBN: 978-3-9505192-7-3 © European Law Institute 2022 Cover image: Shutterstock This publication was co-funded by the European Union’s Justice Programme. (extracted from257.pdf)
Guiding Principle 8: No limitations to the exercise of rights and access to justice 21 Automation shall not prevent, limit, or render unfeasible the exercise of rights and access to justice by affected persons. (extracted from257.pdf)
The operator is the person dispute resolution as access to justice would thus in control of the risks connected with the ADM and be deprived of human intervention. (extracted from257.pdf)
The risk- No limitations to the allocation model proposed by this Guiding Principle is exclusively based on the operation of the ADM exercise of rights and under the parameters of control and benefit and has access to justice to be complemented by other liability regimes. (extracted from257.pdf)
A basic smart home system is installed in a house to control the heating, the unfeasible the exercise of rights and access shutters, the sunshades, and the sprinklers in to justice by affected persons. (extracted from257.pdf)
For unknown reasons, to justice may be prevented, hampered or limited the ADM instructs the system to unfurl the by the inadequate use of automation. (extracted from257.pdf)
garage are completely flooded, the sunshades Second, where the affected person is deprived of the collapse due to the weight of the water, and the possibility of exercising a right or access to justice water starts to seep through the windows of the solely on the grounds that the contested decision living room and the hall. (extracted from257.pdf)
European Parliament 2014-2019 TEXTS ADOPTED P8_TA(2019)0081 A comprehensive European industrial policy on artificial intelligence and robotics European Parliament resolution of 12 February 2019 on a comprehensive European industrial policy on artificial intelligence and robotics (2018/2088(INI)) The European Parliament, – having regard to its resolution of 16 February 2017 with recommendations to the Commission on Civil Law Rules on Robotics1, – having regard to its resolution of 1 June 2017 on digitising European industry2, – having regard to its resolution of 12 September 2018 on autonomous weapon systems3, – having regard to its resolution of 11 September 2018 on language equality in the digital age4, – having regard to the Commission proposal of 6 June 2018 establishing the Digital Europe programme for the period 2021-2027 (COM(2018)0434), – having regard to Council Regulation (EU) 2018/1488 of 28 September 2018 establishing the European High Performance Computing Joint Undertaking5, – having regard to Rule 52 of its Rules of Procedure, – having regard to the report of the Committee on Industry, Research and Energy and the opinions of the Committee on the Internal Market and Consumer Protection, the Committee on Legal Affairs, the Committee on Civil Liberties, Justice and Home Affairs and the Committee on the Environment, Public Health and Food Safety (A8- 0019/2019), A. (extracted from280.pdf)
Points out that the guiding ethical framework should be based on the principles of beneficence, non-maleficence, autonomy and justice, on the principles and values enshrined in Article 2 of the Treaty on European Union and in the Charter of Fundamental Rights, such as human dignity, equality, justice and equity, non- discrimination, informed consent, private and family life and data protection, as well as on other underlying principles and values of Union law, such as non-stigmatisation, transparency, autonomy, individual responsibility and social responsibility, and on existing ethical practices and codes; 148. (extracted from280.pdf)
How Responsible AI can improve business and preserve value | PwC Skip to content Skip to footer Industries Services Issues About us Careers More Search Menu Industries Industries See all industries Consumer markets Energy, utilities and resources Financial services Government and public sector Health industries Industrials and services Private equity and principal investors Technology, media and telecommunications Menu Industries See all industries Menu Industries Consumer markets Consumer goods Hospitality and leisure Retail Transportation and logistics Menu Industries Energy, utilities and resources Chemicals Mining and metals Oil and gas Power and utilities Menu Industries Financial services Asset and wealth management Banking and capital markets Insurance Real estate Menu Industries Government and public sector Defence Education and skills Public safety, justice and security Menu Industries Health industries Health services Pharmaceuticals and life sciences Menu Industries Industrials and services Aerospace, defence and security Automotive Business services Engineering and construction Industrial manufacturing Space Menu Industries Private equity and principal investors Capital projects and infrastructure Menu Industries Technology, media and telecommunications Entertainment and media Technology Telecommunications Featured Value in motion Industry edge Engine by Starling: From launching a bank to launching a software business Menu Services Services See all services Alliances Artificial Intelligence Audit and assurance services Business model reinvention Business transformation Consulting Crisis management Deals Entrepreneurial and private business Family business Forensics Legal business solutions Managed Services Risk services Strategy Sustainability and climate change Tax Trust Workforce Menu Services See all services Menu Services Alliances Menu Services Artificial Intelligence Menu Services Audit and assurance services Actuarial services Capital markets Corporate reporting Financial audit IFRS Internal audit Next Generation Audit Risk assurance Sustainability assurance Menu Services Business model reinvention Menu Services Business transformation Menu Services Consulting Cloud transformation Digital operations Deals Finance transformation Forensics Front office transformation HR transformation Risk and regulation Strategy Technology Workforce Menu Services Crisis management Menu Services Deals Acquisitions Capital markets Corporate finance Deals strategy Joint ventures and alliances M&A legal M&A tax Performance and Restructuring​ Sovereign wealth funds Menu Services Entrepreneurial and private business Menu Services Family business Menu Services Forensics Menu Services Legal business solutions Employment Entity governance and compliance International business reorganisations Mergers and acquisitions NewLaw Menu Services Managed Services Menu Services Risk services Menu Services Strategy Menu Services Sustainability and climate change Climate risk, resilience and adaptation Energy solutions Impact management for sustainable business strategy Legal and sustainability Net zero transformation Social sustainability Sustainability assurance Sustainable capital Sustainability reporting Tax and sustainability Menu Services Tax Indirect taxes International tax services Mergers and acquisitions Sightline Tax code of conduct Tax controversy and dispute resolution Tax policy and administration Tax reporting and strategy Transfer pricing Menu Services Trust Menu Services Workforce Culture, leadership and change Inclusion Employment law Employment tax and payroll HR transformation and technology Organisational design People analytics and insights People in deals Retirement and pensions Reward and benefits Workforce risk Workforce strategy Featured Climate risk, resilience and adaptation Business transformation Sustainability assurance Menu Issues Issues See all issues Business model reinvention Business transformation C-suite insights Cybersecurity Climate and sustainability Megatrends Risk and regulation Technology Trust Upskilling Workforce Menu Issues See all issues Menu Issues Business model reinvention Menu Issues Business transformation Menu Issues C-suite insights Accelerating performance Global CEO Survey PwC at Davos strategy+business digital issue Take on Tomorrow: a strategy+business podcast The Leadership Agenda Menu Issues Cybersecurity Menu Issues Climate and sustainability Menu Issues Megatrends Menu Issues Risk and regulation Menu Issues Technology Menu Issues Trust Menu Issues Upskilling Menu Issues Workforce Featured Value in motion PwC’s 28th Annual Global CEO Survey The Fearless Future: 2025 Global AI Jobs Barometer Menu About us About us See more About Us Alumni Analyst relations Client case studies Ethics and compliance Committing to Net Zero Corporate sustainability Inclusion PwC's Global Annual Review Global regulatory affairs Human rights policy Network Leadership, Governance, and Structure New Ventures and Innovation News room Purpose, values and behaviours PwC office locations PwC's Code of Conduct Strategy& Tax Code of Conduct Third party code of conduct Transparency Report Menu About us See more About Us Menu About us Alumni Menu About us Analyst relations Menu About us Client case studies Menu About us Ethics and compliance Menu About us Committing to Net Zero Menu About us Corporate sustainability Menu About us Inclusion Menu About us PwC's Global Annual Review Menu About us Global regulatory affairs Menu About us Human rights policy Menu About us Network Leadership, Governance, and Structure Menu About us New Ventures and Innovation Menu About us News room Menu About us Purpose, values and behaviours Menu About us PwC office locations Menu About us PwC's Code of Conduct Menu About us Strategy& Menu About us Tax Code of Conduct Menu About us Third party code of conduct Menu About us Transparency Report Featured Value in motion Committing to Net Zero Global Transparency Report 2024 Menu Careers Careers Find out more about careers Search for a job Menu Careers Find out more about careers Menu Careers Search for a job Featured PwC’s Global Annual Review Committing to net zero The CEO’s ESG dilemma Loading Results No Match Found View All Results Designing, building and operating AI that delivers real-world impact Responsible AI Artificial intelligence is transforming business by streamlining activities, enhancing customer offerings, making workers more effective and speeding up innovation—prompting executives to deploy intelligent applications and agentic systems. (extracted from554.html)
Quick links Work programme Drafts and new work items Working area Working documents (user account required) ISO Electronic applications IT Tools that help support the standards development process Public material Browse documents made available by this group This committee contributes with 54 standards to the following Sustainable Development Goals : 1 No Poverty 3 Good Health and Well-being 4 Quality Education 5 Gender Equality 6 Clean Water and Sanitation 7 Affordable and Clean Energy 8 Decent Work and Economic Growth 9 Industry, Innovation and Infrastructure 10 Reduced Inequalities 11 Sustainable Cities and Communities 12 Responsible Consumption and Production 13 Climate Action 14 Life Below Water 15 Life on Land 16 Peace, Justice and Strong Institutions 39 Published ISO standards * 46 ISO standards under development * 50 Participating members 25 Observing members * number includes updates Structure Liaisons Meetings Reference Title Type ISO/IEC JTC 1/SC 42/AHG 4 Liaison with SC 27 Working group ISO/IEC JTC 1/SC 42/AHG 8 Best practices for new proposals Working group ISO/IEC JTC 1/SC 42/JAG Joint Advisory Group on AI and sustainability with ISO/IEC JTC1/SC 39 and JTC1/SC 42 Working group ISO/IEC JTC 1/SC 42/JWG 2 Joint Working Group ISO/IEC JTC1/SC 42 - ISO/IEC JTC1/SC 7 Working group ISO/IEC JTC 1/SC 42/JWG 3 Joint Working Group ISO/IEC JTC1/SC42 - ISO/TC 215 WG : AI enabled health informatics Working group ISO/IEC JTC 1/SC 42/JWG 4 Joint Working Group ISO/IEC JTC1/SC42 - IEC TC65/SC65A: Functional safety and AI systems Working group ISO/IEC JTC 1/SC 42/JWG 5 Joint Working Group ISO/IEC JTC1/SC42 - ISO/TC 37 WG: Natural language processing Working group ISO/IEC JTC 1/SC 42/JWG 6 Joint Working Group ISO/IEC JTC1/SC42 - ISO/CASCO: Conformity assessment schemes for AI systems Working group ISO/IEC JTC 1/SC 42/JWG 7 Joint Working Group ISO/IEC JTC1/SC 42 - ISO/TC 68: Artificial intelligence Working group ISO/IEC JTC 1/SC 42/WG 1 Foundational standards Working group ISO/IEC JTC 1/SC 42/WG 2 Data Working group ISO/IEC JTC 1/SC 42/WG 3 Trustworthiness Working group ISO/IEC JTC 1/SC 42/WG 4 Use cases and applications Working group ISO/IEC JTC 1/SC 42/WG 5 Computational approaches and computational characteristics of AI systems Working group Liaison Committees to ISO/IEC JTC 1/SC 42 The committees below can access the documents of ISO/IEC JTC 1/SC 42: Reference Title ISO/IEC IEC/SC 45A Instrumentation, control and electrical power systems of nuclear facilities IEC IEC/SC 62C Equipment for radiotherapy, nuclear medicine and radiation dosimetry IEC IEC/SC 65A System aspects IEC IEC/SyC AAL Active Assisted Living IEC IEC/SyC SM Smart Manufacturing IEC IEC/SyC Smart Cities Electrotechnical aspects of Smart Cities IEC IEC/TC 9 Electrical equipment and systems for railways IEC IEC/TC 56 Dependability IEC IEC/TC 62 Medical equipment, software, and systems IEC IEC/TC 65 Industrial-process measurement, control and automation IEC IEC/TC 100 Audio, video and multimedia systems and equipment IEC ISO/CASCO Committee on conformity assessment ISO ISO/IEC JTC 1 Information technology ISO/IEC ISO/IEC JTC 1/SC 6 Telecommunications and information exchange between systems ISO/IEC ISO/IEC JTC 1/SC 7 Software and systems engineering ISO/IEC ISO/IEC JTC 1/SC 24 Computer graphics, image processing and environmental data representation ISO/IEC ISO/IEC JTC 1/SC 27 Information security, cybersecurity and privacy protection ISO/IEC ISO/IEC JTC 1/SC 29 Coding of audio, picture, multimedia and hypermedia information ISO/IEC ISO/IEC JTC 1/SC 31 Automatic identification and data capture techniques ISO/IEC ISO/IEC JTC 1/SC 32 Data management and interchange ISO/IEC ISO/IEC JTC 1/SC 34 Document description and processing languages ISO/IEC ISO/IEC JTC 1/SC 35 User interfaces ISO/IEC ISO/IEC JTC 1/SC 36 Information technology for learning, education and training ISO/IEC ISO/IEC JTC 1/SC 37 Biometrics ISO/IEC ISO/IEC JTC 1/SC 38 Cloud computing and distributed platforms ISO/IEC ISO/IEC JTC 1/SC 39 Sustainability, IT and data centres ISO/IEC ISO/IEC JTC 1/SC 40 IT service management and IT governance ISO/IEC ISO/IEC JTC 1/SC 41 Internet of things and digital twin ISO/IEC ISO/IEC JTC 1/SC 43 Brain-computer interfaces ISO/IEC ISO/IEC JTC 1/SC 44 Consumer protection in the field of privacy by design ISO/IEC ISO/TC 20 Aircraft and space vehicles ISO ISO/TC 20/SC 16 Uncrewed aircraft system ISO ISO/TC 22/SC 32 Electrical and electronic components and general system aspects ISO ISO/TC 24/SC 4 Particle characterization ISO ISO/TC 36 Cinematography ISO ISO/TC 37 Language and terminology ISO ISO/TC 37/SC 3 Management of terminology resources ISO ISO/TC 37/SC 5 Translation, interpreting and related technology ISO ISO/TC 42 Photography ISO ISO/TC 46/SC 11 Archives/records management ISO ISO/TC 68 Financial services ISO ISO/TC 69 Applications of statistical methods ISO ISO/TC 108/SC 5 Condition monitoring and diagnostics of machine systems ISO ISO/TC 172/SC 5 Microscopes and endoscopes ISO ISO/TC 199 Safety of machinery ISO ISO/TC 204 Intelligent transport systems ISO ISO/TC 210 Quality management and corresponding general aspects for products with a health purpose including medical devices ISO ISO/TC 211 Geographic information/Geomatics ISO ISO/TC 215 Health informatics ISO ISO/TC 225 Market, opinion and social research ISO ISO/TC 232 Education and learning services ISO ISO/TC 258 Project, programme and portfolio management ISO ISO/TC 260 Human resource management ISO ISO/TC 261 Additive manufacturing ISO ISO/TC 262 Risk management ISO ISO/TC 267 Facility management ISO ISO/TC 268 Sustainable cities and communities ISO ISO/TC 269 Railway applications ISO ISO/TC 279 Innovation management ISO ISO/TC 299 Robotics ISO ISO/TC 307 Blockchain and distributed ledger technologies ISO ISO/TC 309 Governance of organizations ISO ISO/TC 312 Excellence in service ISO ISO/TC 321 Transaction assurance in E-commerce ISO ISO/TC 322 Sustainable finance ISO ISO/TC 324 Sharing economy ISO ISO/TC 347 Data-driven agrifood systems ISO Liaison Committees from ISO/IEC JTC 1/SC 42 ISO/IEC JTC 1/SC 42 can access the documents of the committees below: Reference Title ISO/IEC IEC/ISO JTC 3 Quantum technologies ISO/IEC IEC/SyC AAL Active Assisted Living IEC IEC/SyC SM Smart Manufacturing IEC IEC/SyC Smart Cities Electrotechnical aspects of Smart Cities IEC IEC/TC 44 Safety of machinery - Electrotechnical aspects IEC IEC/TC 62 Medical equipment, software, and systems IEC IEC/TC 65 Industrial-process measurement, control and automation IEC IEC/TC 100 Audio, video and multimedia systems and equipment IEC ISO/CASCO Committee on conformity assessment ISO ISO/IEC JTC 1 Information technology ISO/IEC ISO/IEC JTC 1/SC 7 Software and systems engineering ISO/IEC ISO/IEC JTC 1/SC 24 Computer graphics, image processing and environmental data representation ISO/IEC ISO/IEC JTC 1/SC 27 Information security, cybersecurity and privacy protection ISO/IEC ISO/IEC JTC 1/SC 29 Coding of audio, picture, multimedia and hypermedia information ISO/IEC ISO/IEC JTC 1/SC 32 Data management and interchange ISO/IEC ISO/IEC JTC 1/SC 34 Document description and processing languages ISO/IEC ISO/IEC JTC 1/SC 35 User interfaces ISO/IEC ISO/IEC JTC 1/SC 36 Information technology for learning, education and training ISO/IEC ISO/IEC JTC 1/SC 37 Biometrics ISO/IEC ISO/IEC JTC 1/SC 38 Cloud computing and distributed platforms ISO/IEC ISO/IEC JTC 1/SC 39 Sustainability, IT and data centres ISO/IEC ISO/IEC JTC 1/SC 40 IT service management and IT governance ISO/IEC ISO/IEC JTC 1/SC 41 Internet of things and digital twin ISO/IEC ISO/IEC JTC 1/SC 43 Brain-computer interfaces ISO/IEC ISO/IEC JTC 1/SC 44 Consumer protection in the field of privacy by design ISO/IEC ISO/TC 20/SC 16 Uncrewed aircraft system ISO ISO/TC 22/SC 32 Electrical and electronic components and general system aspects ISO ISO/TC 24/SC 4 Particle characterization ISO ISO/TC 36 Cinematography ISO ISO/TC 37/SC 3 Management of terminology resources ISO ISO/TC 46 Information and documentation ISO ISO/TC 46/SC 11 Archives/records management ISO ISO/TC 68 Financial services ISO ISO/TC 69 Applications of statistical methods ISO ISO/TC 106 Dentistry ISO ISO/TC 145 Graphical symbols ISO ISO/TC 159 Ergonomics ISO ISO/TC 172/SC 5 Microscopes and endoscopes ISO ISO/TC 199 Safety of machinery ISO ISO/TC 204 Intelligent transport systems ISO ISO/TC 210 Quality management and corresponding general aspects for products with a health purpose including medical devices ISO ISO/TC 215 Health informatics ISO ISO/TC 225 Market, opinion and social research ISO ISO/TC 260 Human resource management ISO ISO/TC 262 Risk management ISO ISO/TC 268 Sustainable cities and communities ISO ISO/TC 299 Robotics ISO ISO/TC 307 Blockchain and distributed ledger technologies ISO ISO/TC 309 Governance of organizations ISO ISO/TC 312 Excellence in service ISO ISO/TC 321 Transaction assurance in E-commerce ISO ISO/TC 324 Sharing economy ISO Organizations in liaison (Category A and B) Acronym Title Category BCI Blockchain & Climate Institute A BDVA Big Data Value AISBL A CI Consumers International A Cloud security alliance Cloud security alliance A EC - European Commission European Commission A ETSI European Telecommunications Standards Institute A ETUC European Trade Union Confederation A euRobotics AISBL euRobotics AISBL A EUROCAE The European Organization for Civil Aviation Equipment A HL7 Health Level Seven International A IEEE Institute of Electrical and Electronics Engineers, Inc A IIOA Independent International Organisation for Assurance A ITU International Telecommunication Union A MedTech Europe Alliance of European medical technology industry associations A ML Commons MLCommons Association A OECD Organisation for Economic Co-operation and Development, OECD A OGC Open Geospatial Consortium, Inc. (extracted from411.html)
on democracy, the rule of law and distributive justice, or on the human mind itself.) Adopt adequate measures to mitigate these risks when appropriate, and proportionately to the magnitude of the risk. (extracted from135.pdf)
9 by reference to dignity, freedoms, equality and solidarity, citizens’ rights and justice. (extracted from135.pdf)
Respect for democracy, justice and the rule of law. (extracted from135.pdf)
Additionally, fairness implies that AI practitioners should respect the principle of proportionality between means and ends, and consider carefully how to Fairness is closely linked to the rights to Non-discrimination, Solidarity and Justice (reflected in Articles 21 and following). (extracted from135.pdf)
Explicability and Responsibility are closely linked to the rights relating to Justice (as reflected in Article 47). (extracted from135.pdf)
on democracy, the rule of law and distributive justice, or on the human mind itself.) Adopt adequate measures to mitigate these risks when appropriate, and proportionately to the magnitude of the risk. (extracted from135.pdf)
In general terms, it deals with questions like “What is a good action?”, “What is the value of a human life?”, “What is justice?”, or “What is the good life?”. (extracted from135.pdf)
Part of the reason we must hold humans accountable is because only humans can be dissuaded by human justice. (extracted from407.html)
We often mistakenly think that justice is largely about compensation, but in fact not every wrong can be righted, so wrongs must also be prevented by disproportionate dissuasion for those wrongs successfully addressed. (extracted from407.html)
1 Introduction There has been an increasing trend in healthcare and criminal justice to leverage machine learning (ML) for high-stakespredictionapplicationsthatdeeplyimpacthumanlives. (extracted from690.pdf)
Asofyet,Ihavenot encounteredsuchanapplication,despitehavingworkedonnumerousapplicationsinhealthcareandcriminal justice[e.g.,21],energyreliability[e.g.,20],andfinancialriskassessment[e.g.,22]. (extracted from690.pdf)
COMPAS(CorrectionalOffender ManagementProfilingforAlternativeSanctions)isaproprietarymodelthatisusedwidelyintheU.S.Justice systemforparoleandbaildecisions. (extracted from690.pdf)
Justice System for recidivism risk prediction does not depend on the seriousness of the current crime [27, 29]. (extracted from690.pdf)
OptimizedScoringSystems: TowardTrustinMachineLearningforHealthcareandCriminal Justice. (extracted from690.pdf)
Part of the reason we must hold humans accountable is because only humans can be dissuaded by human justice. (extracted from684.html)
We often mistakenly think that justice is largely about compensation, but in fact not every wrong can be righted, so wrongs must also be prevented by disproportionate dissuasion for those wrongs successfully addressed. (extracted from684.html)
care or justice)? (extracted from20.pdf)
Retrieved 23 Note to EGE (2018): AI should contribute to global justice and equal 05 01, 2018, from https://ec.europa.eu/research/ege/pdf/ege_ ai_ access to the benefits and advantages that AI, robotics and ‘autonomous’ statement_2018.pdf systems can bring. (extracted from20.pdf)
26 See: https://www.theguardian.com/news/series/cambridge-analytica-files 31 Note to EGE (2018): Sustainability: AI technology must be in line with the human responsibility to ensure the basic preconditions for life on 27 Note to EGE (2018): Rule of law, access to justice and the right to our planet, continued prospering for mankind and preservation of a redress and a fair trial provide the necessary framework for ensuring the good environment for future generations. (extracted from20.pdf)
ù{dÈæm[¬¨GÕ}äÂE/-³*Ëè÷ßu¯åUøéå.0±Ro³¨=D­> ÷ÉLxªµÌ¯BÆîrâ4 EpÎá¶Ì¦Ck£ØW1uÁ8 Hü7 ýw2SÉSÁÌ Ü´'¤\!G 5MGîá¯8|\¥äv4®©å·§iÌæÎh®°Rr¨R/òÓ|/b<[ó Ó`MüÒkAbNlbyL¯t{*Rn¹ ü~æ3s\¶{ o¥ÏKCLaß`¯lh°e É\ª*=Æ]fÏÆ<Ü¥¹IóT±q^ô% `^%e&<nòÛIU2íå¤>1öòÓô¤  Lø¥èÆLÀ=<;.£'ÙPYMDÚ6Ö+'ÐW^Ñ9Ôucrç÷:Øó ã^ÅRHòÕ3Êð8Ëõ$¿Ì°#Î/æ¼cE> ZÃ£!Ål¿qñ'ïòHÑáÇ7 Î83ìð :¦à4Ïù<¡w2=Ðïµý±²2cjdz|e1!÷Sxn1U0è[¢èU-º¹o øã'>ÿxä g&ëO¹nWðA!ØµÞzê­n­26ÓHM?B!AÏ Çç«bÌ´K ÊïmoI ¹£¯ñõ±Rrn)<fË®Ë÷Á©ùv»á^ y{»¹[´ÄbÝÔøRÑ1 FSX \1Öô*ÎÑt3Úà £¯ÍpÃã¢Gí 9èhæÄ»ÅXáeß Ø5µÞMjZÏ êÕ ê;®ÝË÷1qQráwÛ| =ß/íÿWx îöXîÓ¨bê´£¼¶?ËXÊÓà{6eÏ"ÿ/P¹Â endstream endobj 239 0 obj <</Type/Font/Subtype/TrueType/Name/F11/BaseFont/Times#20New#20Roman/Encoding/WinAnsiEncoding/FontDescriptor 240 0 R/FirstChar 32/LastChar 32/Widths 1954 0 R>> endobj 240 0 obj <</Type/FontDescriptor/FontName/Times#20New#20Roman/Flags 32/ItalicAngle 0/Ascent 891/Descent -216/CapHeight 693/AvgWidth 401/MaxWidth 2568/FontWeight 400/XHeight 250/Leading 42/StemV 40/FontBBox[ -568 -216 2000 693] >> endobj 241 0 obj <</Subtype/Link/Rect[ 68.65 163.91 526.35 173.63] /BS<</W 0>>/F 4/A<</Type/Action/S/URI/URI(https://www.washingtonpost.com/opinions/big-data-may-be-reinforcing-racial-bias-in-the-criminal-justice-system/2017/02/10/d63de518-ee3a-11e6-9973-c5efb7ccfb0d_story.html?utm_term=.720084735d73) >>/StructParent 114>> endobj 242 0 obj <</Subtype/Link/Rect[ 68.65 154.18 486.19 163.91] /BS<</W 0>>/F 4/A<</Type/Action/S/URI/URI(https://www.washingtonpost.com/opinions/big-data-may-be-reinforcing-racial-bias-in-the-criminal-justice-system/2017/02/10/d63de518-ee3a-11e6-9973-c5efb7ccfb0d_story.html?utm_term=.720084735d73) >>/StructParent 115>> endobj 243 0 obj <</Subtype/Link/Rect[ 147.42 125.02 515.59 134.74] /BS<</W 0>>/F 4/A<</Type/Action/S/URI/URI(https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing) >>/StructParent 116>> endobj 244 0 obj <</Filter/FlateDecode/Length 5388>> stream x¥<ËrGwEè ú]¯®î]#HY²=3ÞõZÚõAWâc 4|ÎõÌnµC$ÑïÂÉÙfwsµXîªï¾;9ÛíËõê²úròùþáo'_ V'¿,®oî»û»ÓÓêüû÷Õùç·oN>jh®ú|õö¨Zø_T¶m­+Ûw ¼¸¨ >iQ]oß¾i«küñÃÛ7UÛ´­ dõyùeöþ¾þ[õùÏoß| 7 êU3´9*x ë¹ÝáåÍ·z.úYu_ÛÙUUÏ»ÙÚ8 üqÿ  ÂÌVu7« óN6Ìæù2ÛîÇe=W³Ò°¡meÃJ3È¡kÌq ÊÆ z f×H ®WvºÙ®îgk$ôv[-îð=Q\ýì Ü.ð3ñ ¸ÓÏ~Ånìu=¸±;|¶-c¤l.R^}øù}uRóûÝîþö%þFØJw~§VMÛ-«ZÁFÑkÔì*Ñ7BÖRü«øùýOßWí JxÃ¤Cï|^#QÂ¶Èc'jó^¸ÍX9AÝ/ÜÄ»TiÓèO0/Ádu;þµÚï3m>ÉÏcéíÒ" Îö3À#$¦]ÐYzAÁ!F$îÝ-¼ÓÑ<D ª«À õ¼¦¼î_ªnA _*IïXÌ·E ·cXÞÃº'p-w%pÞEÏ'tc8( ³u`òmQFÛÆ0*ñ( (ÙMi¾ ²o¬ÎÏÂ9 -llÓK6òKÜôo$@ Ümì¥"©· ¿ûøÇ,@D qµ 10ª¨{ÙHÁaáç$HN¹#¤ÉIêr§O~~,)h ­5|ºo¸lÏZ<NÚ ãÈ:ºÇ.[|Üjüù{]RZh¤= ª³Øå7R!mIc+Ð¥xR v ;åÚuPoé¤8ömÚî8t§&°n½ÑbFsÿ/Ø5§ÕÒ#ECqp0 +ÿü;´Î¯H)9îÚ@?ÿÝ¯])=û¡Ö4½)Óª Æý& |ë§²ÅDZÝ´Ñ]t.ú¶é %5gÖv= WÕ¬Rà¨p\q´Óðq3¶;:v;âSeÝm[ªQ|þ n¢O\ÐH%ùÐ× ü¾x¦L·qjNÊ.£«nèQäZVåll¬³ªÍÕâ:¨Öd=ï¢OFt%{ Ö Ëå ÄÐ#ã§5ÃÃ{67þ±òdÑÕSÀ{PlMM4Ó¶(SR4­å°¿ÝÐf&è¿½ßÀ geº:ð sh¸ÒÛb( ¸"X~ÿT«=þÐáÍÅ#(sëó</6dµé`\²×;«»½!e¶,[;Ó(Åg FV¸ Ê$Ã|»ºÛyÖ?îÖÁ#»t.= ,nJk0Ì`3¢â MÚ+*.bÙE ¶aÊ wOQ[ÐDÿ;G)-Ã¹ÉÒøÇ÷µa ÷'·ÙHg~Åaóëý+Fí»Ãñ}Mf °ù§$3+°"Oõ|H#WÐ!ðãôI¢]x´j×/YxõrÈQCÜQVãPÅä§Y~­ËC]5³_%ôt7°:Ï  ÅåKZWäãváat[£nÐ¶áD%«æ7#y¡§&FWa+ß :cA&Es+»»ÁT úÑ:ðÉd øp´À¦æËA eQ 0ÐY 2ô>óEî"pèR²êèvÅWr$tñ|Ëé8#°ÿ6Ü¸-\YAl þ¹h¨ú U[ö%éÄ ög¢ßéuïGýiîÜc¿Ì²Ñº|q ïiæ !14*øç8c<ÕnÎ]Ø»pòµù×²ÝÐX6]1÷`rÌ@~öÕYv&Ú²I G0r<Zm@.¶>¦Ì 6I<³µ:ý·(GSM¤¸Chmä@ò¦äN*XhÏ¹ÈáNßÓª¶9qOEJ´ 0Çóî¦ªÀk 49Ì.àbê¢ë!ÊgX´Yz8}HÞ³cñ¦[qbçµsÎÖf;>ÑµÃÏFN>v{ÒÏ¨g£¿|×¶úãé\ûß¢?<5øëìT|þ><§s|¡ÏNç ~· ^ÈßØÓ _{þ´cïÏÍ©& Å |°4½ký % sD¡¦sÓÐÄÅy4à4ÆÅUà4n ç MÅèYLÀÙø µîÃé|Ø;] Ç½ÀÙiy*,/~ð´xâ»´*­áqÀÞÚ1§¥Î÷ÂÇnS¡óFãiÔ~£{¢ã´ çZÐär>ß[ãÐZ7 - 'Ý7¥p¢oÑõx¹E ~Àà¦¼rT ØÃAäÚ!:Çd  #r_âÚÙ¥náÇÿ=fGÉjí\R1b]\¤TYIJ)~à´}HiAqØ«M´xDË½>èÔ_0®nQKÍL y2iÌ¥+ú» * ±> ÆhEÁBtY¨æøPòþ%)³ècDWñTL=YÄrÇÑa°m î 5h­§ûù¾öY¯+ÄÇV 3%ÎwO[Lé ¶·KòÖOÜï=¾Hæv/¶ÞÓ )ÁßÆý YmÑð)Ùc­5 3ZlàÔ|pÑ¹ð¥ MÃ%ñQM«cßkâÚ8Á:û3·I½jo"ô÷Þò%ýmln¢zgî# ¤ÝK´½³¢õZÝôhhÔ\M4}>5ÒÔ¹Q#Ä¨ÒM°MGhß ¿ºk4sQ;tå"è¹û9å>.8´J7Zr´Å| uYëÏø3V¹¾.£¶©VuïKåÕ¸@&d-Ìál0+iø¥GqêÍqë` X/ÂNJ¢ÿ÷ù_~s@ÛÃÑ Â)uÃwþ_0ònö®wíu&[(»FyØ±Û÷>Èöiîhä®¡ñC> = ã -p@gôÒ]N  2çÞ %î+aãhQÝIvå°Yå*¤ é\8{ãµ¹=¯¬¶Ìa °ÁºÐÚäÑ7zÆN,¶×áðæGð Ñ£bðâý=¼q~Öc çâ<åS]Áú+#p&Ü+ÿ Óåÿ\ ³Ó>ßx·Ã9ÃÃTÐOX«$S'Ã]`+C`)ñO[T<àgaaMSn£iËA_ñAì öÅåÊ*K`aMã<¢XpÛ§ð>S:yÜa´wëdï¼Î svvSät ?ó`3{ vL%$íuÆÃº jH_|ÎÔÄaÆÂµ@T¾£\y G 6²ÃLco)¤hTÏa£·c} ¹²3hÇYW/y¹ç ÜëÊã<{E¹"ð¯A ¼£iaE¡.äª ûõ¼j%S=±ÕÀ=L((ãSaÁ%äDªõ#Js®Òðq(Ø®ï ®s«¾XË¸0Ó¾ËÏè¦Ý9èÃûJÑ×h,Hfî(üýíl½àLÌÛFHpz*3¦í«^ÚéªÍêíßþTÝ%ß é¸®qí0Ò¦DÒ4Ffóý×Þ Û½×vºSüãÿ¸»h5 4ôÚ9eí(]çJNÌ¸diÕ©¨*5ÊÖkZñÒäYP ÒùýðòXPûsÿöiQ^òSÖ £X  æ_®^¾òwI CTË¬·åµíÐæÓË}BSn -xü]-?JËú@§egHlò¹VUS2GY=bÐÄ 5Då7ÏÙá ÃNµÌDÈò+üùÖ<Cx¥Ãü'|¤Ã§ó  p¤·éêp×ÕxZæ5ÔÀaó2FÈ¶LrXºU èÍ¸A NêÎ].!ë¢;&G©äjUInºÉ ì B®ZÌìø+×XdZdø¹3º¸ û|  rfRßÌe6~T­´ûªð°¬Ëý  ÌÙ²,þÓí=²Y|:â>ÂÕ¡ÁâR°¨ºÝE£uËðçí øWQÂì%å0ûa·mp(xÇG7îêaE>ÞäuÃ¨*4·mJ¨Î»Î%±ü&õ¸Ü­²n.oça7L÷ ;X [RÝpÌi9*ñÓ÷1ÊÙstÊZµ¶gm¹ªQüìDú·()ÅÆ&°<µîg,l×mä&ä®½¦üÝ8ÕãN wÆVµ+ '\§yÜöåedêzDvÛ µnfí +g<§­àxú-¤åCÑ2ò½à.w®;1÷iU$nRê5^+½ú*tÆÞÓýtðJJÅÓ |£l@(#Î`GGQ§Ó§Ùá» |Ê}»g]g u{Lj½î1?RÓJ¬P2Xg½ceþ`L0Kiq0§è«´k­l¸,Á ÑÑTÍïuµTØ´ÄÀ?b\  êjÎvàå]MN Ù% xK6~¼D ,ïsÃT"Bßt²ýS×Tô¯7£·¼}OÕ= ÖÀæÌ*vÂ>"e°ånV* 6Â oI«/÷ÐÂ)ÇHzo Ña!5=öëdùòG×Ö7äe¤%Ö >»7<ö Sî?PÞf¹óÔ¨lEüx´­7Å ßÔÊ+A}ÇC@ï ;_ÝÓé½¦¤úÚÊ×¾Ý¡ÏòXnIë¨ J¦·1¥¡R#®#ï#;PV uùÆj,^gKue\âÁâeÜf¼osLû5`±6¶OdÙzÖ¤à3UþYý@­¡l7AÆ<ºðí <~â,¥ÇÎuêhh³F=¸Ô[*WÅÜÜ+©_ï Ôw6OÎÅ ,íAeí! (extracted from145.html)
äºÜ:c'r[þß ä¸M,Ñ¿³Ào¯µÑ2ÇaOU¿Õçdi ¼róZAÃNéDì[oåEó:@¶×¶ÛÈJuL»Mø`LâQó­-ÛtÞ¾z|GDÍZ}´JÈ ©0v&É#õnùüëWPCÚP~ádðG¶ Lí¯è:¤v XxfÍïñ¸ø {WàÛ´_=duàÃïÉí[<&í'ë ±| ¡'þz±:Ë_+¨OR> ûh.À¬MéÏ¸ {úMÝ&75vð_ëæáN­1·[½Ïï^|Ý÷sÅ"6´þN»ÔV.Yªûú©EZjNÌÛl-$;o¿ ¾ÌhKûÂÑË2¬?¼Ç©w«s<º´%ÁE·x\,©ñ²û tNq"'Ñ1=8lxþ%ô5U~ç4gwø5]á<Jqæ9i·s;KIâvímZ?RGÌmA|Gù¤?:Ðuä%:ÇÜÐÞ-é Íè ×atP|-prµñE-,.Ë¢eâÈìBæSÎ?ÉFAQb¢×çÐuÓ 5¢¹êHó¬KA(É¾û dÛÎm0ìRØÙýNQ-Ô ÝNÁnÚãÝ/ðëïdçÛ¶{Ûë¿ÕüüÛdÖfCDÙ o,kS Ë¢¯þù;XÎº¿ Ë¦mî MÍë ,u&ã²»Qö8dÉÊZâ­½p³Ìñ4ÕTvÆ9ßÕÀ !;²Éy2íj#Ö%îÌÂ÷/ßäÿÊôkß4 Ý¤ù Ì1(:Í ã¡8Þºýô ·Ã 'ClqtKäJ\ÓhÒk>æ±JxgH Kú"cºÕXjÏ .È­?¡½YEÁa¿÷»¶9ûô F¤êD¬sËzz°émÂõËª]{ øh0ãÔ+ÀN X»µÆ<u»ç`OvvÏõø¢kj÷Á Ò¦#$EMA`P_Ú½J±aèâK4°,év£ôß;wn¸ýðéúnqkjr 4÷íñ ük9%¬_ª'ÍxöaìÔ, ª^ÄAªÓS[è¬{-GÌEÇú Ü¬?Î£þ;Pdx±xµÑ£[ºv²è¾|qà½B"ÒÄ¶ã~ýsÇW9¥½B¶Åý3bE6üvðF:' ×E%ü)Ðk1e vN I "ãKäXD7è£)Nå!xèxÜí?è åMý cv2M ªïÓ':fÛïn´ûû ÓB ôp¨VxLOÏ¶zðü&Ðá«ÿúÚ üÿ¦x4±ü×ý+Ø¶é¢¤kÿ ðC' endstream endobj 313 0 obj <</Subtype/Link/Rect[ 68.65 735.06 526.35 746] /BS<</W 0>>/F 4/A<</Type/Action/S/URI/URI(https://www.theguardian.com/technology/2017/apr/13/ai-programs-exhibit-racist-and-sexist-biases-research-reveals) >>/StructParent 184>> endobj 314 0 obj <</Subtype/Link/Rect[ 68.65 718.13 180.88 735.06] /BS<</W 0>>/F 4/A<</Type/Action/S/URI/URI(https://www.theguardian.com/technology/2017/apr/13/ai-programs-exhibit-racist-and-sexist-biases-research-reveals) >>/StructParent 185>> endobj 315 0 obj <</Subtype/Link/Rect[ 68.65 690.25 489.1 707.19] /BS<</W 0>>/F 4/A<</Type/Action/S/URI/URI(https://www.theguardian.com/science/2016/sep/01/how-algorithms-rule-our-working-lives) >>/StructParent 186>> endobj 316 0 obj <</Subtype/Link/Rect[ 84.853 668.37 526.35 679.31] /BS<</W 0>>/F 4/A<</Type/Action/S/URI/URI(https://www.washingtonpost.com/opinions/big-data-may-be-reinforcing-racial-bias-in-the-criminal-justice-system/2017/02/10/d63de518-ee3a-11e6-9973-c5efb7ccfb0d_story.html?utm_term=.720084735d73) >>/StructParent 187>> endobj 317 0 obj <</Subtype/Link/Rect[ 68.65 657.44 526.35 668.37] /BS<</W 0>>/F 4/A<</Type/Action/S/URI/URI(https://www.washingtonpost.com/opinions/big-data-may-be-reinforcing-racial-bias-in-the-criminal-justice-system/2017/02/10/d63de518-ee3a-11e6-9973-c5efb7ccfb0d_story.html?utm_term=.720084735d73) >>/StructParent 188>> endobj 318 0 obj <</Subtype/Link/Rect[ 68.65 640.5 314.53 657.44] /BS<</W 0>>/F 4/A<</Type/Action/S/URI/URI(https://www.washingtonpost.com/opinions/big-data-may-be-reinforcing-racial-bias-in-the-criminal-justice-system/2017/02/10/d63de518-ee3a-11e6-9973-c5efb7ccfb0d_story.html?utm_term=.720084735d73) >>/StructParent 189>> endobj 319 0 obj <</Subtype/Link/Rect[ 254.37 629.56 526.35 640.5] /BS<</W 0>>/F 4/A<</Type/Action/S/URI/URI(https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing) >>/StructParent 190>> endobj 320 0 obj <</Subtype/Link/Rect[ 68.65 606.62 235.75 629.56] /BS<</W 0>>/F 4/A<</Type/Action/S/URI/URI(https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing) >>/StructParent 191>> endobj 321 0 obj <</Subtype/Link/Rect[ 84.853 572.75 481.65 595.68] /BS<</W 0>>/F 4/A<</Type/Action/S/URI/URI(https://motherboard.vice.com/en_us/article/mg9vvn/how-our-likes-helped-trump-win) >>/StructParent 192>> endobj 322 0 obj <</Subtype/Link/Rect[ 68.65 527.93 368.73 550.87] /BS<</W 0>>/F 4/A<</Type/Action/S/URI/URI(http://journals.uic.edu/ojs/index.php/fm/article/view/7090/5653) >>/StructParent 193>> endobj 323 0 obj <</Subtype/Link/Rect[ 68.65 506.06 526.35 516.99] /BS<</W 0>>/F 4/A<</Type/Action/S/URI/URI(https://www.theguardian.com/world/2016/oct/27/angela-merkel-internet-search-engines-are-distorting-our-perception) >>/StructParent 194>> endobj 324 0 obj <</Subtype/Link/Rect[ 68.65 483.12 186.9 506.06] /BS<</W 0>>/F 4/A<</Type/Action/S/URI/URI(https://www.theguardian.com/world/2016/oct/27/angela-merkel-internet-search-engines-are-distorting-our-perception) >>/StructParent 195>> endobj 325 0 obj <</Subtype/Link/Rect[ 127.94 426.3 369.24 449.24] /BS<</W 0>>/F 4/A<</Type/Action/S/URI/URI(http://www.unesco.org/new/en/world-media-trends) >>/StructParent 196>> endobj 326 0 obj <</Subtype/Link/Rect[ 322.87 404.43 526.35 415.37] /BS<</W 0>>/F 4/A<</Type/Action/S/URI/URI(http://isc.independent.gov.uk/committee-reports/special-reports) >>/StructParent 197>> endobj 327 0 obj <</Subtype/Link/Rect[ 68.65 381.49 176.71 404.43] /BS<</W 0>>/F 4/A<</Type/Action/S/URI/URI(http://isc.independent.gov.uk/committee-reports/special-reports) >>/StructParent 198>> endobj 328 0 obj <</Subtype/Link/Rect[ 375.67 348.68 526.35 359.61] /BS<</W 0>>/F 4/A<</Type/Action/S/URI/URI(https://ec.europa.eu/home-affairs/sites/homeaffairs/files/what-we-do/policies/european-agenda-security/legislative-documents/docs/20160420/communication_eas_progress_since_april_2015_en.pdf) >>/StructParent 199>> endobj 329 0 obj <</Subtype/Link/Rect[ 68.65 337.74 526.35 348.68] /BS<</W 0>>/F 4/A<</Type/Action/S/URI/URI(https://ec.europa.eu/home-affairs/sites/homeaffairs/files/what-we-do/policies/european-agenda-security/legislative-documents/docs/20160420/communication_eas_progress_since_april_2015_en.pdf) >>/StructParent 200>> endobj 330 0 obj <</Subtype/Link/Rect[ 68.65 314.8 453.1 337.74] /BS<</W 0>>/F 4/A<</Type/Action/S/URI/URI(https://ec.europa.eu/home-affairs/sites/homeaffairs/files/what-we-do/policies/european-agenda-security/legislative-documents/docs/20160420/communication_eas_progress_since_april_2015_en.pdf) >>/StructParent 201>> endobj 331 0 obj <</Subtype/Link/Rect[ 68.65 192.36 486.6 215.3] /BS<</W 0>>/F 4/A<</Type/Action/S/URI/URI(http://www.venice.coe.int/webforms/documents/default.aspx?pdffile=CDL-AD\(2017\)009-e) >>/StructParent 202>> endobj 332 0 obj <</Type/Page/Parent 2 0 R/Resources<</Font<</F1 5 0 R/F11 239 0 R>>/ExtGState<</GS41 41 0 R>>/ProcSet[/PDF/Text/ImageB/ImageC/ImageI] >>/MediaBox[ 0 0 594.96 842.04] /Contents 333 0 R/Group<</Type/Group/S/Transparency/CS/DeviceRGB>>/Tabs/S/StructParents 203>> endobj 333 0 obj <</Filter/FlateDecode/Length 199>> stream x}= Â0÷@þÃº$ii"´õ BÁlÅ!øQ;ÔfñßÛî]»çx|õÕÑí<& {ïv§Ã ·ÍeËíýrà¹+«³óUsNÌR$¾`Ñ°GJö¥bR#4#¦#Øºµ-7*@y£D ìdII1Àp »¦dÞa¥à=Iã}SÿãxWK²ñÊ0ù,þ.ùt¾ ¥`QôÃúäÏ;,]Í ¾Wÿ fB¨5k_Oä äSß endstream endobj 334 0 obj <</Type/Page/Parent 2 0 R/Resources<</Font<</F1 5 0 R/F11 239 0 R>>/ExtGState<</GS41 41 0 R>>/XObject<</Image336 336 0 R/Image338 338 0 R/Image340 340 0 R/Image342 342 0 R/Image344 344 0 R/Image346 346 0 R/Image348 348 0 R/Image350 350 0 R/Image352 352 0 R/Image354 354 0 R/Image356 356 0 R/Image358 358 0 R/Image360 360 0 R/Image362 362 0 R/Image364 364 0 R/Image366 366 0 R/Image368 368 0 R/Image370 370 0 R/Image372 372 0 R/Image374 374 0 R/Image376 376 0 R/Image378 378 0 R/Image380 380 0 R/Image382 382 0 R/Image384 384 0 R/Image386 386 0 R/Image388 388 0 R/Image390 390 0 R/Image392 392 0 R/Image394 394 0 R/Image396 396 0 R/Image398 398 0 R/Image400 400 0 R/Image402 402 0 R/Image404 404 0 R/Image406 406 0 R/Image408 408 0 R/Image410 410 0 R/Image412 412 0 R/Image414 414 0 R/Image416 416 0 R/Image418 418 0 R/Image420 420 0 R/Image422 422 0 R/Image424 424 0 R/Image426 426 0 R/Image428 428 0 R/Image430 430 0 R/Image13 13 0 R>>/ProcSet[/PDF/Text/ImageB/ImageC/ImageI] >>/MediaBox[ 0 0 594.96 842.04] /Contents 335 0 R/Group<</Type/Group/S/Transparency/CS/DeviceRGB>>/Tabs/S/StructParents 3>> endobj 335 0 obj <</Filter/FlateDecode/Length 1303>> stream xMk Gï û úèøÐÛU]Õ ` ä 6^ÈAè dID£,üûTÍîJÓ3S&mµx´5O¼õVu{óîiwwsyµsoÞlÞívW\sçíã÷Íößï×Ï·w»»Ç·oÝÉÙ©;Ù®W_ÁU_ÛÞ¬WàüÇH Ë5úTÜö^¾öá+»ý{½ îV|X¯Î_¹_.Üö·õê½¼È½ÿtê6Æ N w»Çû ãÆ=GGÕã n!ão Á²ðÕa<nóYGòéôã í¬¦¯D.§äåã¼¯!@$¡{3ù.ðÅqÊòë§ëõêæõáM ³Ïà"G_Ña©>Â<|÷÷×îA¾=Bây ÞÇÆ},Ïc#ù h?A@"_e¦ì¹¸+ÝÙ÷·×1&wöè¾,L2ö£lË(**¼,:Å§"Æà,"÷ ê¯'Ë*Èþ7D´©(ÿÆfÄ|nds'1k¼,*²O¼4Sº·±J"ÕÇ¼4S»÷1z¥GJ¨Ö2"²© ÝHÁ IÙiJ   síH¦R4µ½¾YjG í¥a%¦) èu ZÀó¤U iÊºGô#9>ArEd²õÓk=@¸0KÎÅKM#mýôzOëQ Z DÛ ×{¢8f²§¼4K[²Ýö£^0_ØµDö4ûíôù)ÝOl¦b±×~¤! ,3¤<® 2Å~ûaí#@|}iJ{ýG73Î\CñÜ"Mý`¯ýH^VyÉdkm¦~°×~&%:õÜ9ÑNLìu¢]@3j¶0K[²½ö©¨áLV:jnÅl¯ÿPÛÞE)[!/!mÅöÚÏÐå<Æ%¦-Ù^ÿIìkI´HûtÐÝþàüM_©3MÅ^Â5í§Ó"«YMÅ^ÿÉ~ävhÊ'öº´¬D³Nä [¤©ØÝü 4¾tk¶|zýGp>É çÜ míô4Ô2\PLñÈ§Rð%/«>³¸,y$ãx ÇF¥Á´Îû`£<käaE¤CÒ~)é(ÇÃÃÉðìÑ¬t¸RFô.aÁê<ôð¤W4bá yvÁ²vrjAiÒ¢ÞføP¤ ûÁdGv5F¬Póa'ÞÝpú@³;·s¥ùVHê%Y(¾ XÎ¤íØº¼gyt4r6 ÞÒ>t ¢ý Qg yÐÊs<°´ ®.@AÍUYÎ[¨cdÿItÙG3ÅøpCÑHÒä 3ãáÙ72Ø¬tLÔ , â¦áRì½< Q2a h.Q/(yHÏHñÄKHÓp»÷HÓc²¬rÀM-Òô[JýÒa½ ¥ë -ÒNî^X.zýÒ"¡3hk§ôÓóF zÐÓláÔî%ÅìÃ4Aü½©'hêC7QJð(I Ñ ÿãÔçû"o MÕp·áÚîGzç ç¼8­Ñ rd9zg=¼LZÎ´ä5·HRK¬¡Dj¨ûG?"Uý?w.UA]È«Ý7'Sc­¤£6÷ëãÃ.àþ\¯¾®WGÈ6V¦ endstream endobj 336 0 obj <</Type/XObject/Subtype/Image/Width 2/Height 2/ColorSpace[/Indexed/DeviceRGB 1 <FFFFFFFFFFFF>] /BitsPerComponent 1/Interpolate false/SMask 337 0 R/Length 2>> stream    endstream endobj 337 0 obj <</Type/XObject/Subtype/Image/Width 289/Height 120/ColorSpace/DeviceGray/BitsPerComponent 1/Filter/FlateDecode/Length 313>> stream xíÖÁ  PÿH BiP¶-a)Á£Ç|À¯®BHöæü%Î D Ã2LG uôgdýäðo¤9È8ª¿õ Ü ¡¦cÅ,ìî<ñ3ªÇAÃô¾¥4¸õ°àÒáF4ÄÀÐ`ÌHAQ ;ú2Ò ô 0gd¯hÉÈ6ÐOÐ°¢ Pî±"pÒah{w ¢YÈ ]cGñl½,òý LdI¸(PÈÇÎÌ¨µ» TV4ò 42PÌ=L¹¨érHÁD£G«¥6$¡HgRÒHãp ï»Ã¼RÈp¾f³ÔG#=È1)CîfÍ>` Áº¡øA +3!Á@nNßj_b(GÝÿ};êècÑ ,êëÑ endstream endobj 338 0 obj <</Type/XObject/Subtype/Image/Width 2/Height 2/ColorSpace[/Indexed/DeviceRGB 1 <FFFFFFFFFFFF>] /BitsPerComponent 1/Interpolate false/SMask 339 0 R/Length 2>> stream    endstream endobj 339 0 obj <</Type/XObject/Subtype/Image/Width 277/Height 120/ColorSpace/DeviceGray/BitsPerComponent 1/Filter/FlateDecode/Length 559>> stream xíÕM²¤  `x,XzÂ» Í£x,(2IÅNOÍb6ZÕ %_Ë_.+¼ä% âK¾&¿"±³HÂKþ ñsOLº YyÂÏéç °`_Ù Xð ¼0X¬'â/ß!:$S#á2Ðt´¸ ¸ Ig2»ø³ÔMo"¢Aº/ Ç4¥{²àXÑöDuf1Ú"&÷Dóº#íì3ÁShJ=¼1 Oût}Ù7$nä²">»:ë-jhëFDu$>ÕÈ2$aÖ(H áã­¤4@$ e@übvâ`} ¹ d@ÜzeLljW$ç%S:KiÝ {& `@òepF6»å âwb:â®Ä7ßÉL}¶¦ÒÅßI`b¾ 5IðÖÇ;Y86¢ \ÈÂÄî KD½púâÚ'"öªF#Íg¨jÐ^»VKÒåzè8,M#$C$Õ} *Ø©TìóµQ>7eT#9pÅ¤Æ=¾b ïRàºn$SäeÜ0X±okKû¢òÈÃJ ø@M+Ì5aé¸xàª³5-èècÃcRül!ok;ÈÓÕîñ_Y$A&ûWö_H+ -"²HLB ÈgbùÓðL Eò@J$E&I&«HLÉú;òæ} endstream endobj 340 0 obj <</Type/XObject/Subtype/Image/Width 2/Height 2/ColorSpace[/Indexed/DeviceRGB 1 <FFFFFFFFFFFF>] /BitsPerComponent 1/Interpolate false/SMask 341 0 R/Length 2>> stream    endstream endobj 341 0 obj <</Type/XObject/Subtype/Image/Width 1821/Height 120/ColorSpace/DeviceGray/BitsPerComponent 1/Filter/FlateDecode/Length 2730>> stream xíÜK¤È `$ÇÆr ß$®âcxa Z³ðÒG£Q/¼ôÖ,z9XÞPðÿàÿ¬¤²³Z :«xü ¯x@MQ>o À;¤ËxtÏ.ã9Òe<GºçHñ é2#]Æs¤ËxtÏ.#¦ùùAØþ©Õ/ÆÒç2øp¯o¶ýxGîl2Sæûð5ºç+ÿÍóæ[½ïÞÞ*¦×£¶~ î9Æ¿ÑgÒXã÷¾ßÞlqOÔõcÆ5¼¼Ï¤Ñµp­ø3EïL¯Gí 2ká¬1àÆ¦k¥ýøXïñzÔÍøÇ¹ãT­áeoñ3iÛy´Yî0 Ç[xo5ÚéùFyW{_ãÙ8¾±=;N{cçÈ«þ»5Öqc=<Ý¨þ Æ %{_ÅÆà¾ ÆêÉ÷Õ»ûêÞë ±1è ÁX>¹|Ð¸ë ÆÌ8»îÅ@âMéu£<O¾Â8'a,ÒBc UEùT=h\Óe¼¥ËX\ÆCé26Lf(vFK½ëdÙzêR¿×- ¸YTUÊÞºÆÈ¨  ÕÔjÕºVµª³[ÙØlùx,çfúh° OKµ÷ðC 3Åàq{éeûW;ËQ(¾-¡Â×-]oiÙ«ÂÂf0ÒXÃZê ÑâTFY=ÚLa¤¬ð¯+?È= ÕQxÐ4Lt'Ê õÐÚÖ¡ù*ð¸+Ãx9E°z9Ë­F$ËõÚ³XØ µ4b4±±Á ãíü×ÖË£ÕaxPÇ4ÒV2Zq ÈO¸ ¶-ñk¥r^íS.·±¹ë= ~ ÍèÚwFÏ;?Â©=ïÜE£S'VáAÓÐHYÉèÄá #^øwy"cMq:e¤ºÄ¯¼ #h©>÷É{¾ðì QÆ`àñ·m.fÅ]Ô¨S « áýâjbcÍFr¾Æ<ÑQ¸ºïÕåÉåV£Ãó ës¿Ò!à¦Í(÷ÅÀ#ãgÞ1ÁøØ1 é(W»« á½ ¶RC>>W!Ëü îçéRRÆf°|h£¼6¸ÜjÈ>R~Z.}ç¦Â¼H£ÃÀ÷F¿#ë</*3UWñÎÿ=Ë±è Ù@{`l:7ÇFßUhìjalàJF*·áôy2s¿x òvgá½Ù®ÆvÍÇ[t5xüN÷ëÑ_h+[§´Â£C]Øy] ï ð \9E%PÅ¸åa¥rÁÔø¡< {ì¼p ówyè¡ËS÷¸77£/ÜËù*¸ÕãÒÒ¿lÑCu yÙJwÑ¹øCdÄHhÔëyµQé|è ÇK^A·0r¹Å·¶W |-[ì% (!­íí`á à"ÂKË }!oÌX [Ì¶q*~PFÌ§]ÉG:ÐØ8cË-F¼5·¸<ìèJÇåÕíBàÚ8ó.Ffqgï ÙÁÒE0s¼ó²5E.<©ß ïaxæj\ uYÕna2lF.·± BE -ã¢o¼+b0f­/Æ·Ô0ÆÁ{E;úX·Rµ²W##õ¢d½MÃ«±~ÍØ+c»fhlZ#|2Ê§¢6±¥¯±ukÒhØß³±8ä½ÃXhãLçF -ãp>6Ò qotÚØ¹B0½ÑnF3NÔÇ" :÷ÆaoóPn1¶Òh±SëÑØneì ±äR"³n½i hÅÚEÆ qÂØGÆN±Z¸1jh g=®3jD3Â.Ú²BdÇ#ã6büÆvÁmÆ»ûª2r¹ÅÈËF(Ùì4'õín\oÆnéíqÒÓÍ`¬¹°µNèCÆý³­»ý70ª:¥±§¸Æ#R]>${»×Pû÷±ô ã3ªëþQ 6Üi_ 66Ç6büð4[Jqy%ÓÄã~#Öà#ã¡w÷qâ5ö±QÜuAVë#ÆúÑí ö 2:µcßÕØ 36âÞqÈX« ÷Fe 1ÊÛá!£U°«îí; 7V~Jõ Yc ­=Þm¬ôâÆxwÿø¸ÑàLûÑÃ!½U»à7e*{¯q êMF;'Ç Yc)Ç1£Wò2{'ãtÌ(ûcF9Å] ÞÇÈóûF«×Wï7Ò8 0Îyã}ãz8h¬Þj,ä y:4&Oå\Ã¸ Ë©ÃÍ;2Æz±4Â8 w<bì7ÝðÝ·Ã±«ûâ±ªý*ÆòNcsÓ8ò:2n±×:øQqÚ¸Ðî0b0ñ: üÈË[z~ZëÆÄFûºQ¬Ye6cçR®ÉÝ!c0VycX»e kSÞ¸ þÞb yí±ßy¥â=¡pÊ¸EW²{ìbãJ*cr 9¹re4Yã²N~Ã¸< ðÊ8HãR¯4N9£ÑF^\YhãîyÇj,µ±¿e Ï;c2Ò302±ÖÊèE[ýeÂûÆjãL]N=íô,)mDº2Â÷6k¬Âs«° |[üp¤Ì/õ`¦0vFJ+#=½¥UóÈHOîâgsÁÈOÚåËóÇÍøC<À¶f~ÔZv«qF<`U[uòÏàØÚ =c í=ÑHCåÂH«IYãO[W#\»­X(eãT¡±¡EÆ-ÖÍP][rI~v¿$0Åhñ ¡hÍgBÚH1ù\ÎY#NÀ¥±Ç"cóÏ 4«Þ¯Y¸bÔVêüí§½Ñz?kã¿©XI¯Hh#¯¥¡ra¤sFï¿³øm0©÷:FÏË{#FÓªtÚ8iã¿¸XÂX>g¬Õ ù¦ÂûU­Ö°­Á/!V£Ä7ÑË*~^Ç±_ÖÒÆ>¼ æ}d´¤OK®\®ç`9#Tôã,ÄhÔë9XAÏ ±RFz]nJ+|kH?ðûXno¤wÉFØ2VbÎmó\"µò °å}¹Ö/ï¤ XKeÄs§(õ rÎbH°Ã?|ÕÆÑ®ÕFz'0mc¡øÂ!\-#Ý5ÅQ *¹ôËïv6<OwËRk#pÐ°/·U¬ªK¡?5÷®Øld¤ ÷¤+F 1o¬;a¬è}£vËÅá2Z¬Û\óù"îÁ÷¯²pÎ1þ&¼u[O»¼û'ÓÙÊu8û$«¹ªQæÿóß]ñ{Jæ2"íouÛ¦w ã©É^ÆS$9 Òib?Nç0¶ z;Jç0öz0Jç0úÅª(ÂXúÖÊéDNbT+Mq:1¿ùÆÊëU(ÂXzý'|Q: ×üúüÖsí­Ëñ$Fõ°wNb¼.ã9Òÿ ñ|«ì endstream endobj 342 0 obj <</Type/XObject/Subtype/Image/Width 2/Height 2/ColorSpace[/Indexed/DeviceRGB 1 <FFFFFFFFFFFF>] /BitsPerComponent 1/Interpolate false/SMask 343 0 R/Length 2>> stream    endstream endobj 343 0 obj <</Type/XObject/Subtype/Image/Width 2357/Height 120/ColorSpace/DeviceGray/BitsPerComponent 1/Filter/FlateDecode/Length 4450>> stream xíM®ä¶Ç%ËgX^zD>FVÑUr,³@^xé#ä(ÖÀYÎF^Z£e1¬ú?¤VwSR÷³VoØO"U?~,±ßdÙ)÷EýÖ| rbJSäÄ$'¦$91%É)INLIrbJSäÄ$'¦$91%É)INLIrbJSäÄ$'¦$ù`ÊÇã:)7\Í©åðøÚL³RiSª¬úzÿ ôi)kÌi)+r.þ&Êæn-ò>A]ÑÝ¼½©¸Iµ< SÕ&dJÇT¤ô.Õß¼½IÝÃTv< SÝ%dJÇ´çn¡ ¬`*Mî/®>OÃ¤o)v©ÔU·Y®aJ¬bªqñ©ªyYL#9éÃÔGU\Ãôê Ü#é¦¿ÿ¾0 OÀä ÀTL/)~Ð\ÃT>Ó'Ý¦'=éîb²î<ÓuÓ¦'­îb²ë¦§b:° _Ãô¤UxÝÏ×VáOÅ$é !+t>_«òÄtbZSäÄ$'¦$y&&^)Ñ ¹! (extracted from145.html)
Establish a data protection framework with legal backing: The work being done by Justice Srikrishna Committee on data protection law is very opportune and timely. (extracted from479.pdf)
Department of Commerce Subcommittee on Networking and Information Technology Research and Development Co-Chair Co-Chair Bryan Biegel James Kurose Director, National Coordination Office for Assistant Director, Computer and Information Networking and Information Technology Science and Engineering Research and Development National Science Foundation Networking and Information Technology Research and Development Task Force on Artificial Intelligence Co-Chair Co-Chair Lynne Parker Jason Matheny Division Director Director Information and Intelligent Systems Intelligence Advanced Research Projects Activity National Science Foundation Members Milton Corn Nikunj Oza National Institutes of Health National Aeronautics and Space Administration William Ford Robinson Pino National Institute of Justice Department of Energy Michael Garris Gregory Shannon National Institute of Standards and Technology Office of Science and Technology Policy Steven Knox Scott Tousley National Security Agency Department of Homeland Security vii John Launchbury Faisal D’Souza Defense Advanced Research Projects Agency Technical Coordinator National Coordination Office for Networking and Richard Linderman Information Technology Research and Development Office of the Secretary of Defense viii NATIONAL ARTIFICIAL INTELLIGENCE RESEARCH AND DEVELOPMENT STRATEGIC PLAN Contents About the National Science and Technology Council .......................................................................................... (extracted from486.pdf)
The Federal government should therefore emphasize AI investments in areas of strong societal importance that are not aimed at consumer markets—areas such as AI for public health, urban systems and smart communities, social welfare, criminal justice, environmental sustainability, and national security, as well as long-term research that accelerates the production of AI knowledge and technologies. (extracted from486.pdf)
Scientists must also study to what extent justice and fairness considerations can be designed into the system, and how to accomplish this within the bounds of current engineering techniques. (extracted from486.pdf)
26 NATIONAL ARTIFICIAL INTELLIGENCE RESEARCH AND DEVELOPMENT STRATEGIC PLAN Building ethical AI Beyond fundamental assumptions of justice and fairness are other concerns about whether AI systems can exhibit behavior that abides by general ethical principles. (extracted from486.pdf)
23In considering the applications of AI in areas such as criminal justice and health care, organizations should design, build and deploy AI systems that leverage human judgment and responsibility where they are needed. (extracted from478.pdf)
Suggested lead: Department of Commerce, Department of State, Department of Justice. (extracted from478.pdf)
Suggested lead: Department of Commerce, Department of State, Department of Justice. (extracted from478.pdf)
In particular, in medicine, CNOM refers in its report to the applicability to digital technologies of the four principles of medical ethics: beneficence, non-maleficence, autonomy, and justice. (extracted from322.pdf)
Regulation and soft law (self-compliance and voluntary certification) The ruling on December 7 last by the European Court of Justice notably circumscribed national capacity for regulation on digital innovation in the healthcare sphere.41 This Community framework therefore opens the door to methods of regulation that do not entail enforceable law. (extracted from322.pdf)
41 The Court of Justice of the European Union (CJEU) was called upon to rule on a prejudicial question regarding whether a software program for prescription support meets the definition of a medical device, if that program provides at least one function that can be used to process data specific to the patient for the purpose of helping the doctor to establish a prescription, in particular by detecting contraindications, drug interactions, and overdoses, although it does not of itself act in or upon the human body (CE, June 8, 2016, n° 387156). (extracted from322.pdf)
Court of Appeals for the Second Circuit and Justice David H. (extracted from336.pdf)
Mariano-Florentino Cuéllar is a Justice on the Supreme Court of California, the Herman Phleger Visiting Professor of Law at Stanford University, and a faculty affiliate at the Stanford Center for AI Safety. (extracted from336.pdf)
We also appreciate superb editorial assistance and dedicated research support from three members of Justice Cuéllar’s staff: Ryan Azad, Alexandra Havrylyshyn, and Mikayla Hardisty. (extracted from336.pdf)
A large number of use cases fell under machine learning and were excluded.5 We attempted to health- and law-enforcement-focused subagencies such resolve boundary issues as well as we could through multiple as the Food and Drug Administration, the Office of Justice rounds of quality control.6 Programs, and the Transportation Safety Administration and Customs and Border Protection. (extracted from336.pdf)
As a result, the Department of Health and Human Services, the Department of Justice, Contrary to popular perceptions and the Department of Homeland Security account for a collective 51 use cases. (extracted from336.pdf)
TOP TEN AGENCIES AND SUBAGENCIES BY NUMBER OF USE CASES Number of Agency Name Use Cases Results Office of Justice Programs 12 The results of this survey shed significant light on the state of Securities and Exchange Commission 10 AI/ML in federal administrative agencies. (extracted from336.pdf)
For example, the Department of Health and Human Administration is exploring the use of image recognition to Services (19 use cases), the Department of Justice (16 use cases), screen passenger luggage for explosive devices. (extracted from336.pdf)
The Centers and the Department of Homeland Security (16 use cases) have been refactored into respective sub-agencies (e.g., the Food and for Medicare and Medicaid Services is developing AI-based Drug Administration, the Office of Justice Programs, and Customs tools to predict health care fraud. (extracted from336.pdf)
Others, such powered risk assessment scores for passengers75 and as the Federal Bureau of Investigation Terrorist Screening claims to instead compare each passenger’s personally Database and the Department of Justice National Crime identifiable information “against lookouts and patterns of Information Center, come from peer agencies. (extracted from336.pdf)
While the foregoing cannot possibly do justice to ongoing debate about the proper role of technology-enabled surveillance, it is a crucial debate to have. (extracted from336.pdf)
First, gender promotes accuracy that ultimately inures to the the emerging consensus within machine learning is that benefit of the justice system.”101 Due to the doctrinal “blinding” algorithms to protected characteristics is unlikely uncertainty, states and localities using criminal risk to be effective. (extracted from336.pdf)
Dep’t of Justice, Exec. (extracted from336.pdf)
public-justice-safety/gov-social-media-transforms-chicago-policing. (extracted from336.pdf)
sector-industry-solutions/justice-law-enforcement-and-border- Insider (Oct 12, 2019, 7:59 AM), https://www.businessinsider.com/ security-solutions/linesight (last visited Apr 7, 2019). (extracted from336.pdf)
Mashaw, Bureaucratic Justice: Managing Social Security the residual functional capacity determination, made using the grid Disability Claims (1985); David Ames, Cassandra Handan-Nader, Daniel E. (extracted from336.pdf)
Conference of the U.S., A Review in the Department of Justice and decide immigration cases. (extracted from336.pdf)
Immigration Judge, Dep’t of Justice (June 9, 2017), Rethinking Judicial Review of High Volume Agency Adjudication, 96 Tex. (extracted from336.pdf)
https://www.justice.gov/legal-careers/job/immigration-judge; 4 Executive L. (extracted from336.pdf)
Dep’t of Justice, Executive Off. (extracted from336.pdf)
One of the more interesting arguments MemorandaofUnderstandingMOUs/OtherMOUs/ucm622056.htm; for “internal” constraints on algorithmic decision-making is that, while Memorandum of Understanding Between the Health Information marquee uses of algorithmic decisions systems—e.g., the criminal justice Sharing & Analysis Center, Inc. (extracted from336.pdf)
Mashaw, Bureaucratic Justice: Algorithmic Support in High-Stakes Public Sector Decision-Making, CHI Managing Social Security (1985). (extracted from336.pdf)
101, 109 (2019); Rebecca Wexler, Life, Liberty, and Trade Secrets: Punish the Poor (2018); Cathy O’Neil, Weapons of Math Destruction Intellectual Property in the Criminal Justice System, 70 Stan. (extracted from336.pdf)
Tyler, What Is Procedural Justice?: Criteria Used by Citizens to 91 U.S. (extracted from336.pdf)
Mashaw, Bureaucratic Justice: Managing Social Security 138 Cf. (extracted from336.pdf)
124 Eugene Volokh, Chief Justice Robots, 68 Duke L.J. (extracted from336.pdf)
Accept additional cookies Reject additional cookies View cookies Hide cookie message Skip to main content GOV.UK Navigation menu Menu Menu Services and information Benefits Births, death, marriages and care Business and self-employed Childcare and parenting Citizenship and living in the UK Crime, justice and the law Disabled people Driving and transport Education and learning Employing people Environment and countryside Housing and local services Money and tax Passports, travel and living abroad Visas and immigration Working, jobs and pensions Government activity Departments Departments, agencies and public bodies News News stories, speeches, letters and notices Guidance and regulation Detailed guidance, regulations and rules Research and statistics Reports, analysis and official statistics Policy papers and consultations Consultations and strategy Transparency Data, Freedom of Information releases and corporate reports Search GOV.UK × Search GOV.UK Search Search GOV.UK Search Home Business and industry Industrial strategy Artificial Intelligence Sector Deal Department for Science, Innovation & Technology Department for Business & Trade Office for Artificial Intelligence Department for Digital, Culture, Media & Sport Department for Business, Energy & Industrial Strategy Policy paper AI Sector Deal Updated 21 May 2019 This was published under the 2016 to 2019 May Conservative government This policy paper was withdrawn on 25 June 2025 In June 2025, the government published its Modern Industrial Strategy - a new economic approach to back our strengths and realise Britain’s potential. (extracted from186.html)
Cancel Services and information Benefits Births, death, marriages and care Business and self-employed Childcare and parenting Citizenship and living in the UK Crime, justice and the law Disabled people Driving and transport Education and learning Employing people Environment and countryside Housing and local services Money and tax Passports, travel and living abroad Visas and immigration Working, jobs and pensions Government activity Departments News Guidance and regulation Research and statistics Policy papers and consultations Transparency How government works Get involved Support links Help Privacy Cookies Accessibility statement Contact Terms and conditions Rhestr o Wasanaethau Cymraeg Government Digital Service All content is available under the Open Government Licence v3.0 , except where otherwise stated © Crown copyright (extracted from186.html)
Such disparities are often sterilized by well-intentioned names (e.g., “disproportionate contact” in criminal justice or the “achievement gap” in education) that hide the social consequence of structural racism: that, as a group, Black, Indigenous, and people of color in America have worse outcomes in many human service system outcome measures regardless of socioeconomic status.6 And yet, many agency solutions and data initiatives are largely disconnected from this root cause, and the “hunt for more data is [often] a barrier for acting on what we already know.”7 3 Racial Equity Tools (n.d.), Core Concepts: Racism 4 We intentionally use the acronym BIPoC (Black, Indigenous, people of color) as a term that seeks to recognize the unique experience of Black and Indigenous People within the United States. (extracted from35.pdf)
The BDC consists of Broward County Public Schools, Broward County Human Services Department, Broward Behavioral Health Coalition, Florida Departments of Children and Families and Juvenile Justice, Early Learning Coalition of Broward, and the Children’s Services Council of Broward County (which acts as the BDC backbone organization). (extracted from35.pdf)
Working with the Kirwan Institute of Race and Social Justice at The Ohio State University, the City of Tacoma created an Equity Index to measure social mobility in the city. (extracted from35.pdf)
So when racial justice doesn’t have a critique of patriarchy and homophobia, the particular way that racism is experienced and exacerbated by heterosexism, classism, etc., falls outside of our political organizing. (extracted from35.pdf)
It means that significant numbers of people in our communities aren’t being served by social justice frames because they don’t address the particular ways that they’re experiencing discrimination.” — Kimberlé Williams Crenshaw13 13 Quoted in Guobadia, O. (extracted from35.pdf)
Members of the group were all previously involved in efforts to promote algorithmic justice and had been accumulating knowledge on NYC-specific ADSs since fall of 2018. (extracted from35.pdf)
Provide a plan to the group that defines harm, and outline processes for repair and restoring justice. (extracted from35.pdf)
Other useful resources for understanding accountability and repairing harm include: Tools for Addressing Chapter Conflict from Black Lives Matter37 Challenging Neutrality, Examining Privilege and Encouraging Practitioner Self-Reflexivity: A Social Justice Approach to ADR38 Celebrate community and the effort you are putting toward growth and change. (extracted from35.pdf)
(n.d.) 40 Future of Privacy Forum & AISP (2018) 41 King County Office of Equity and Social Justice (2015) 42 Gorski, P. (extracted from35.pdf)
Community Reconciliation Through Facilitated Dialogue & Restorative Justice. (extracted from35.pdf)
How schools are using restorative justice to remedy racial disparities in discipline. (extracted from35.pdf)
The Little Book of Race and Restorative Justice: Black Lives, Healing, and US Social Transformation. (extracted from35.pdf)
The numbers don’t speak for themselves: Racial Disparities and the persistence of inequality in the criminal justice system. (extracted from35.pdf)
https://dl.acm.org/doi/abs/10.1145/3351095.3372874 King County Office of Equity and Social Justice. (extracted from35.pdf)
No Small Matters: Reimagining the Use of Research Evidence From A Racial Justice Perspective. (extracted from35.pdf)
How Philanthropy Can Help Lead on Data Justice. (extracted from35.pdf)
Challenging Neutrality, Examining Privilege, and Encouraging Practitioner Self-Reflexivity: A Social Justice Approach to ADR. (extracted from35.pdf)
Just mercy: A story of justice and redemption. (extracted from35.pdf)
Racial Equity in Planning: WORK IN ACTION Broward Data Collaborative by Sue Gallagher Who: Broward Data Collaborative, Children’s Services Council of Broward County Where: Broward County, Florida Organization Type: Government Agency, Community-Based Organization Domains: Child welfare, behavioral health, juvenile justice, early learning, school, human services, prevention programs. (extracted from35.pdf)
The BDC seeks to improve the outcomes of residents in Broward County by integrating child-serving data from child welfare, behavioral health, juvenile justice, schools, early learning systems, county human services, and prevention programs. (extracted from35.pdf)
One of the BDC’s desired outcomes is for the people whose data are in the IDS (e.g., youth aging out of the child welfare system, youth in the juvenile justice system) to be integrally involved in the use, interpretation, and evaluation of their data. (extracted from35.pdf)
Recognizing that every child-serving system (e.g., child welfare, juvenile justice, education) produces racially disparate outcomes, the CPAR work is supported by training partners on the history and structures of racism (both nationally and locally) to build a common language and framework for system participants and system professionals. (extracted from35.pdf)
It centers on themes of equity, opportunity, partnerships, and accountability, and specifically reflects community members’ desire for racial fairness and social justice across all public programs. (extracted from35.pdf)
Allegheny County Department of Human Services, Office of Analytics, Technology and Planning by AISP with contributions from Samantha Loaney, Brian Bell, & Jamaal Davis Location: Allegheny County, Pennsylvania Organization Types Represented: Government Agency Domains Represented: Child welfare, family support, housing/income support, behavioral health, criminal justice, aging and intellectual disabilities Goal: To provide clients and providers access to the robust data held by the county’s Department of Human Services, allowing clients to understand what data is collected and providers to give more holistic care. (extracted from35.pdf)
Allegheny County’s Client Experience Analytics Unit is developing a new client portal that will allow Department of Human Services (DHS) staff, network providers, and clients to access data housed in the county’s data warehouse.53 The client portal will contain data from local school districts, family support programs, housing, income support programs, child welfare agencies, drug and alcohol treatment, the criminal justice system, mental health providers, and more. (extracted from35.pdf)
Racial Equity in Algorithms / Use of Statistical Tools: WORK IN ACTION Allegheny County Child Welfare Algorithm by AISP with contributions from Katy Collins Location: Allegheny County, Pennsylvania Organization Types Represented: Government Agency Domains Represented: Criminal justice system, behavioral health, public assistance, child welfare, education Goal: To use predictive analytics to allow caseworkers to engage in data-driven decision making that creates a more equitable, efficient, and successful child welfare system. (extracted from35.pdf)
Members of the group were all previously involved in efforts to promote algorithmic justice and had been accumulating knowledge on NYC-specific ADS since fall of 2018. (extracted from35.pdf)
In an effort to ensure that an analysis of these indicators felt useful to both participating agencies and to community members that are impacted by these systems, the Educational Services Division of Youth & Family Justice formed a community engagement committee. (extracted from35.pdf)
One of the co-chairs of the committee, 62 ELCYC EFIL ATAD EHT TUOHGUORHT NOITCA NI KROW :I XIDNEPPA Sarah Zeller-Berkman, Director of Youth Studies Programs at the City University of New York (CUNY), who is a critical participatory action researcher, proposed that sustained engagement with a group of young people who had been in ACS care (preventive or juvenile justice) or young people who had fallen behind in middle school could generate data from other young people about the lived experience of being enmeshed in these systems. (extracted from35.pdf)
Most youth team members had either fallen behind in school or had been involved with ACS through foster care or juvenile justice, while others were simply committed to making positive change using research. (extracted from35.pdf)
See http://racialequitytools.org/glossary#structural-racism Social Justice: The proactive reinforcement of policies, practices, attitudes, and actions that produce equitable power, access, opportunities, treatment, impacts, and outcomes for all. (extracted from35.pdf)
How Philanthropy Can Help Lead on Data Justice, Stanford Social Innovation Review. (extracted from35.pdf)
No Small Matters: Reimagining the Use of Research Evidence From A Racial Justice Perspective. (extracted from35.pdf)
http://wtgrantfoundation.org/digest/no-small- matters-reimagining-the-use-of-research-evidence-from-a-racial-justice-perspective Actionable Intelligence for Social Policy University of Pennsylvania 3701 Locust Walk, Philadelphia, PA 19104 215.573.5827 www.aisp.upenn.edu (extracted from35.pdf)
They don’t connect our ideas about what’s good or just to the practices that create goodness and justice. (extracted from493.html)
Such tasks concern low-skilled as well as highly-skilled personnel, for example in sectors such as banking, insurance or justice. (extracted from295.pdf)
For the deployment of ADS, certification can be on either a voluntary basis (as encouraged by the GDPR), or mandatory in certain areas such as justice and healthcare. (extracted from295.pdf)
Decision-making algorithms are increasingly used in areas such as access to information, e-commerce, recommendation systems, employment, health, justice, policing, banking and insurance. (extracted from295.pdf)
Users Individuals Private sector Public sector Objectives Improvement of N/A Drugs discovery Climate general Weather forecast Knowledge Environment Healthcare Quantified-self Risk scoring Predictive justice Finance Payment systems Predictive policing Digital services Note taking Targeting Hazard prediction Smart home Personalised services Infrastructure Recommendations development planning Autonomous Cars Autonomous robots Autonomous Home Robots weapons Physical systems Security Defence Personal assistants in Transport the home Smart cities Smart grids 6 Understanding algorithmic decision-making: Opportunities and challenges 3. (extracted from295.pdf)
For example, Directive 2000/78/EC9 lays down: 'a general framework for combating discrimination on the grounds of religion or belief, disability, age or sexual orientation as regards employment and occupation, with a view to putting into effect in the Member States the principle of equal treatment.' In a similar vein, the Convention for the Protection of Human Rights and Fundamental Freedoms10 provides that: 'the enjoyment of the rights and freedoms set forth in this Convention shall be secured without discrimination on any ground such as sex, race, colour, language, religion, political or other opinion, national or social origin, association with a national minority, property, birth or other status.' The fact that ADS can lead to discrimination has been documented in many areas, such as the justice system, targeted advertisements and employment. (extracted from295.pdf)
Discrimination in justice: Another area that has raised much concern is the increasing reliance on ADS in the criminal justice system. (extracted from295.pdf)
COMPAS scores can be used at different stages of the criminal justice system, e.g. (extracted from295.pdf)
Several occurrences of this process have already been observed, not only in the field of justice with COMPAS, but also in education with the public debate raised by an algorithm called APB32 in France. (extracted from295.pdf)
47 https://gobo.social 14 Understanding algorithmic decision-making: Opportunities and challenges authors, 'perhaps even more problematic is the theory of justice implicitly embedded in the algorithms'.48 The point is that most ADS used in this context are risk-assessment tools: based on a number of factors about the defendants' criminal history, sociological data or demographic features, they provide an estimation of their risk of recidivism. (extracted from295.pdf)
ADS are already in use in the medical sector and can potentially contribute to improve the decisions taken by practitioners and specialists in many ways: 48 Angèle Christin, Alex Rosenblat, Danah Boyd; Courts and predictive algorithms; Data & Civil Rights: A new era of policing and justice; 2015. (extracted from295.pdf)
Public services ADS are currently being used by government and public agencies to provide new services or improve existing ones in many areas, such as energy, education, healthcare, transportation, justice systems and security. (extracted from295.pdf)
These tasks concern low-skilled as well as highly- skilled personnel, for example in sectors like banking, insurance or justice. (extracted from295.pdf)
ADS Fairness As ADS replace or support human decision-makers in a number of sensitive domains such as justice, health or education, it is important to ensure that they do not result in decisions that are considered unfair or discriminatory. (extracted from295.pdf)
For example, it is well known that, in certain cities, there is a strong correlation between the religion or 141 Richard Berk, Hoda Heidari, Shahin Jabbari, Michael Kearns, Aaron Roth; Fairness in Criminal Justice Risk Assessments: The State of the Art; Sociological Methods & Research; 2018. (extracted from295.pdf)
186 Richard Berk, Hoda Heidari, Shahin Jabbari, Michael Kearns, Aaron Roth; Fairness in Criminal Justice Risk Assessments: The State of the Art; Sociological Methods & Research; 2018. (extracted from295.pdf)
187 Richard Berk, Hoda Heidari, Shahin Jabbari, Michael Kearns, Aaron Roth; Fairness in Criminal Justice Risk Assessments: The State of the Art; Sociological Methods & Research; 2018. (extracted from295.pdf)
Ethical and political debate As illustrated in Chapter 3, ADS raise far reaching issues in many areas such as justice, policing, healthcare, democratic life, etc. (extracted from295.pdf)
Another example of the systematic analysis of ethical issues that can be useful in this context is the EDPS Ethics Advisory Group Report,204 which proposes a list of 'foundational values to digital ethics': dignity, freedom, autonomy, solidarity, equality, democracy, justice and trust. (extracted from295.pdf)
As seen previously, different approaches can be taken to 212 Rebecca Wexler; Life, liberty, and trade secrets : intellectual property in the criminal justice system; (70); Standford Law Review; (1343); 2018; https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2920883. (extracted from295.pdf)
[…] The trend towards using risk instruments in all sectors of the criminal justice system, therefore, merits further theoretical deliberation and empirical study.'221 • In the same vein, Chelsea Barabas and her colleagues argue: 'for a shift away from predictive technologies, towards diagnostic methods that will help us to understand the criminogenic effects of the criminal justice system itself, as well as evaluate the effectiveness of interventions designed to interrupt cycles of crime. (extracted from295.pdf)
221 Kelly Hannah-Moffat; Actuarial sentencing: an 'unsettled' proposition; Justice Quarterly; (30,2); 2013. (extracted from295.pdf)
Dillon Reisman and his colleagues have already advocated AIA as a 'practical framework for public agency accountability' in a recent AINow Institute report.226 Beyond a 'self-assessment of existing and proposed automated decision systems, evaluating potential impacts on fairness, justice, bias, or other concerns across affected communities', they emphasise the need for 'researcher review processes before the system has been acquired'. (extracted from295.pdf)
For example, several studies have been conducted about the use of ADS in the area of justice, some of them focusing on the risks of discrimination,232 others on the benefits in improving judges' decisions.233 In addition, the benefit risk balance applies to both the primary functionalities of the ADS and to its transparency and explainability features. (extracted from295.pdf)
ADS certification can be either on a voluntary basis (as encouraged by the GDPR) or mandatory in certain areas such as justice and healthcare. (extracted from295.pdf)
Berk R., Heidari H., Jabbari S., Kearns M., Roth A.; Fairness in Criminal Justice Risk Assessments: The State of the Art; Sociological Methods & Research; 2018. (extracted from295.pdf)
Christin A., Rosenblat A., Boyd D.; Courts and predictive algorithms; Workshop on Data & Civil Rights: A new era of policing and justice; 2015. (extracted from295.pdf)
Hannah-Moffat K.; Actuarial sentencing: an 'unsettled' proposition; Justice Quarterly (30); 2013. (extracted from295.pdf)
Governing data and artificial intelligence for all Models for sustainable and just data governance STUDY Panel for the Future of Science and Technology EPRS | European Parliamentary Research Service Scientific Foresight Unit (STOA) EN PE 729.533 – July 2022 Governing data and artificial intelligence for all Models for sustainable and just data governance With a particular focus on artificial intelligence (AI), this study identifies and examines policy options for the EU's data governance framework that align with a data justice perspective. (extracted from281.pdf)
A data justice approach is one that centres on equity, recognition and representation of plural interests, and the creation and preservation of public goods as its principal goals. (extracted from281.pdf)
Four benchmarks for good data governance are proposed, in line with the principles of justice: preserving and strengthening public infrastructure and public goods, inclusiveness, contestability and accountability, and global responsibility. (extracted from281.pdf)
STOA | Panel for the Future of Science and Technology AUTHORS This study was written by Joan Lopez Solano, Aaron Martin, Siddharth de Souza and Linnet Taylor of the Global Data Justice project, Tilburg University, at the request of the Panel for the Future of Science and Technology (STOA) and managed by the Scientific Foresight Unit, within the Directorate-General for Parliamentary Research Services (EPRS) of the Secretariat of the European Parliament. (extracted from281.pdf)
The Global Data Justice project would like to acknowledge valuable contributions to the analysis in this report from: Maria Anagnostu, Shweta Degalahal, Paula Ferreira Vidal, Yash Kaushal, Andrew Key, Janne Joosten, Alexis Manus, Franklyn Ohai, Gargi Sharma and Zsuzsanna Véghné Ujj. (extracted from281.pdf)
PE 729.533 ISBN: 978-92-846-9623-9 doi: 10.2861/915401 QA-05-22-170-EN-N http://www.europarl.europa.eu/stoa (STOA website) http://www.eprs.ep.parl.union.eu (intranet) http://www.europarl.europa.eu/thinktank (internet) http://epthinktank.eu (blog) II Governing data and artificial intelligence for all Executive summary With particular regard to artificial intelligence (AI), this study aims to identify and examine policy options for Europe's data governance framework that align with a data justice perspective. (extracted from281.pdf)
A data justice approach is one that centres on equity, the recognition and representation of plural interests, and the creation and preservation of public goods as its principal goals. (extracted from281.pdf)
A data justice perspective is a particularly appropriate tool for this analysis, because AI is not a bottom-up class of technology, in terms of either development or use. (extracted from281.pdf)
Starting from research on data justice, we propose four benchmarks for good governance: preserving and strengthening public infrastructure and public goods, inclusiveness, contestability and accountability, and global responsibility. (extracted from281.pdf)
Policy options and alternatives: a data justice analysis ______________________________ 58 6.1. (extracted from281.pdf)
An important component of that representation can be framed as 'data justice' - the view that data governance should not only seek to do no harm, but should positively contribute to people's autonomy and to their ability to participate in society and make claims about their needs, on a more general level.1 In contrast, our current worldwide model for data governance represents a very specific set of interests: those of the largest players in the technology sphere, and the states in whose economies they are embedded. (extracted from281.pdf)
1 Linnet Taylor, 'What Is Data Justice? (extracted from281.pdf)
Technology, https://www.theguardian.com/technology/2017/dec/20/uber-european-court-of-justice-ruling-barcelona-taxi-drivers- ecj-eu. (extracted from281.pdf)
Lessons from Cybersecurity Vulnerability Disclosure for Algorithmic Harms Discovery, Disclosure, and Redress' (Algorithmic Justice League, January 2022), https://www.ajl.org/bugs. (extracted from281.pdf)
One such proposal from India is the creation of an Interoperable Criminal Justice Database. (extracted from281.pdf)
The stated objective of this initiative is that for the criminal justice system to work more effectively, data needs to be shared across different institutions in the system, and be made accessible and interoperable.59 The Supreme Court of India, in its vision document for a digitised judiciary, has called for the 'seamless exchange of live data' rather than data being shared on a need-to-know basis. (extracted from281.pdf)
59 High Court of Tripura, 'Interoperable Criminal Justice System,' accessed April 26, 2022, https://thc.nic.in/user%20manual/ICJS-manual.pdf. (extracted from281.pdf)
62 It was found for instance that the creation of registers of repeat offenders had an underlying caste bias, and rather than challenging the existence of such registers, they would now be part of a centralised data base where their use would be further cemented.The Indian Express, 'The Dangers of a Centralised Database for Justice System,' May 28, 2021, https://indianexpress.com/article/opinion/columns/the-dangers-of-a-centralised-database-for-justice-system-7333252/. (extracted from281.pdf)
For instance in the Dutch government's misuse of data to predict fraud among 63 Linnet Taylor, 'What Is Data Justice? (extracted from281.pdf)
These values relating to the economic growth and wellbeing of EU Member States potentially stand in tension with the rights- and justice-based orientation of the other core value statements to do with digital strategy, for instance in the statement that the innovation principle is legislatively as important as the precautionary principle, which underlies much thinking about digital rights and data protection. (extracted from281.pdf)
One way to arbitrate between these differing visions is to address them through a justice lens, which asserts that there are tests we can apply to statements about good governance and related models, to see whether they align with the core assumptions that make governance functional for people, and place it at the service of the public rather than in the interests of the most powerful and privileged. (extracted from281.pdf)
In 2017, the Global Data Justice project (the authors of this report, based in the Netherlands) asserted three fundamental pillars of 'good' data governance.159 These pillars offer a tool for understanding how public values can be incorporated in governance frameworks. (extracted from281.pdf)
If it has the effect of channelling power and profit toward the best-resourced and most powerful actors, whether governmental or corporate, a governance model is not in line with principles of social justice and requires reorienting. (extracted from281.pdf)
This justice-based reasoning leads to a set of core benchmarks for good governance of data: 159 Linnet Taylor, 'What Is Data Justice? (extracted from281.pdf)
In terms of the first of these two, these infrastructure-related goods include the social safety net, scientific knowledge,163 public education and healthcare, access to justice, electoral processes and law enforcement. (extracted from281.pdf)
See: Karen Maex: 'Protect independent and public knowledge.' Speech January 8th 2021 https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=&cad=rja&uact=8&ved=2ahUKEwiavsCj5_D4AhV L- aQKHQzEA64QFnoECAUQAQ&url=https%3A%2F%2Fwww.uva.nl%2Fbinaries%2Fcontent%2Fassets%2Fuva%2Fnl%2Fo ver-de-uva%2Fspeech-karen-maex---dies-2021.pdf&usg=AOvVaw2V5_UZK8444_8hFUU2XU9s 164 See the work of the Global Data Justice project on pandemic-related commercialisation of public goods: https://globaldatajustice.org/sphere-trans/ 165 For further explanation, see for example Timo Meynhardt, 'Public Value Inside: What Is Public Value Creation?,' International Journal of Public Administration 32, no. (extracted from281.pdf)
Inclusiveness An inclusive perspective on data governance demands that any model centre justice and take into account the interests of all who will be affected by it, rather than simply a body of citizens in a particular state or region. (extracted from281.pdf)
Data regulation tends to cite both human rights and market efficiency, the former of which aligns well with a social justice-oriented governance model. (extracted from281.pdf)
In contrast, a social justice perspective starts from the assumption that all societies are unequal playing fields where people's circumstances give them different degrees of agency and ability to claim their rights. (extracted from281.pdf)
A social justice-oriented data governance model suggests additions to our way of conceptualising vulnerability in relation to technology. (extracted from281.pdf)
Beyond the notion that people are more easily harmed if they have certain minority or sensitive attributes, a social justice view would add another facet to these considerations: that of unequally distributed power and agency. (extracted from281.pdf)
A data justice perspective demands that we broaden our conceptualisation of contestability beyond claims based on competition and consumer protection regulation. (extracted from281.pdf)
Taking a justice perspective, our analysis explores the extent to which these interests productively interact, where governance becomes skewed toward economic priorities at the expense of people, and how the plural interests of the EU's diverse populations can be recognised and represented through the process of governing technologies. (extracted from281.pdf)
'Abnormal justice.' Critical inquiry, 34(3), 39. (extracted from281.pdf)
In our discussion so far on thinking about data governance from a social justice standpoint, there are few considerations that have emerged that require further discussion. (extracted from281.pdf)
Policy options and alternatives: a data justice analysis In this section we will analyse policy options first in relation to the European Strategy for Data, and then in relation to the principal legislative files currently under development. (extracted from281.pdf)
233 Sohel Sarkar and Amay Korjan, eds., A Digital New Deal: Visions of Justice in a Post-Covid World (IT for Change and Just Net Coalition, 2021). (extracted from281.pdf)
Starting from a definition of governance as arbitration between different interests with regard to public and private goods, we have offered a justice-based analysis of the legislative context with regard to artificial intelligence and contributing data technologies and argue that along with considerations of its economic benefits, a strategy as to how European AI generates public value could be central to the EU's policy aims. (extracted from281.pdf)
We then conducted an analysis of the core set of legislative files relating to AI, and explored how they could be aligned with these justice-based benchmarks for good governance. (extracted from281.pdf)
67 With a particular focus on artificial intelligence (AI), this study identifies and examines policy options for the EU's data governance framework that align with a data justice perspective. (extracted from281.pdf)
A data justice approach is one that centres on equity, recognition and representation of plural interests, and the creation and preservation of public goods as its principal goals. (extracted from281.pdf)
We propose four benchmarks for good data governance according to principles of justice: preserving and strengthening public infrastructures and public goods, inclusiveness, contestability and accountability, and global responsibility. (extracted from281.pdf)
Accept additional cookies Reject additional cookies View cookies Hide cookie message Skip to main content GOV.UK Navigation menu Menu Menu Services and information Benefits Births, death, marriages and care Business and self-employed Childcare and parenting Citizenship and living in the UK Crime, justice and the law Disabled people Driving and transport Education and learning Employing people Environment and countryside Housing and local services Money and tax Passports, travel and living abroad Visas and immigration Working, jobs and pensions Government activity Departments Departments, agencies and public bodies News News stories, speeches, letters and notices Guidance and regulation Detailed guidance, regulations and rules Research and statistics Reports, analysis and official statistics Policy papers and consultations Consultations and strategy Transparency Data, Freedom of Information releases and corporate reports Search GOV.UK × Search GOV.UK Search Search GOV.UK Search Home Defence and armed forces Ambitious, safe, responsible: our approach to the delivery of AI-enabled capability in Defence Ministry of Defence Policy paper Ambitious, safe, responsible: our approach to the delivery of AI-enabled capability in Defence Published 15 June 2022 This was published under the 2019 to 2022 Johnson Conservative government Contents Executive Summary Ambitious delivery of capability Our approach and AI-enabled weapons Key challenges to Defence AI Adoption Using AI Safely Using AI Legally Using AI Ethically Partnerships and Consultation Governance Implementation – building justified trust Annex A: Ethical Principles for AI in Defence Annex B: The Ministry of Defence AI Ethics Advisory Panel ANNEX C: Lethal Autonomous Weapon Systems (LAWS) Print this page © Crown copyright 2022 This publication is licensed under the terms of the Open Government Licence v3.0 except where otherwise stated. (extracted from651.html)
Cancel Services and information Benefits Births, death, marriages and care Business and self-employed Childcare and parenting Citizenship and living in the UK Crime, justice and the law Disabled people Driving and transport Education and learning Employing people Environment and countryside Housing and local services Money and tax Passports, travel and living abroad Visas and immigration Working, jobs and pensions Government activity Departments News Guidance and regulation Research and statistics Policy papers and consultations Transparency How government works Get involved Support links Help Privacy Cookies Accessibility statement Contact Terms and conditions Rhestr o Wasanaethau Cymraeg Government Digital Service All content is available under the Open Government Licence v3.0 , except where otherwise stated © Crown copyright (extracted from651.html)
49 http://ec.europa.eu/justice/citizen/document/files/2015_public_consultation_booklet_en.pdf, p. (extracted from252.pdf)
149 of Consumer protection policies, strategies and statistics http://ec.europa.eu/justice/consumer-marketing/files/ucp_guidance_en.pdf. (extracted from252.pdf)
Article 6 of the Directive, http://ec.europa.eu/justice/consumer- marketing/files/ucp_guidance_en.pdf, p. (extracted from252.pdf)
122 http://ec.europa.eu/justice/consumer-marketing/files/ucp_guidance_en.pdf, p. (extracted from252.pdf)
http://ec.europa.eu/justice/dataprotection/files/factsheets/factsheet_data_protection_eurobarometer_240615_en.p df . (extracted from252.pdf)
The Court of Justice enshrined the right to effective remedy in its judgment of 15 May 1986 as a general principle of Union law (Case 222/84 Johnston [1986] ECR 1651; see also judgment of 15 October 1987, Case 222/86 Heylens [1987] ECR 4097 and judgment of 3 December 1992, Case C-97/91 Borelli [1992] ECR I-6313). (extracted from252.pdf)
Notably, in the Recommendation on common principles for injunctive and compensatory collective redress mechanisms in the Member States concerning violations of rights granted under Union Law, the Commission determined that the recourse to opt-out collective redress mechanisms may be justified “by reasons of sound administration of justice”, see Article 21, , Commission Recommendation of 11.06.2013 on common principles for injunctive and compensatory collective redress mechanisms in the Member States concerning violations of rights granted under Union Law (2013/396/EU) http://eur-lex.europa.eu/legal- content/EN/TXT/?uri=OJ:JOL_2013_201_R_NS0013. (extracted from252.pdf)
Peace, inclusiveness and justice, equity and interconnectedness should be promoted throughout the lifecycle of AI systems. (extracted from285.pdf)
These include freedom, dignity and autonomy, privacy and data protection, non- discrimination and equality, diversity, fairness, social justice, and internationally recognised labour rights'. (extracted from285.pdf)
States that in line with strict liability systems of the Member States, the proposed Regulation should cover violations of the important legally protected rights to life, health, physical integrity and property, and should set out the amounts and extent of compensation, as well as the limitation period; is of the opinion that the proposed Regulation should also incorporate significant immaterial harm that results in a verifiable economic loss above a threshold harmonised in Union liability law, that balances the access to justice of affected persons and the interests of other involved persons; urges the Commission to re-evaluate and to align the thresholds for damages in Union law; is of the opinion that the Commission should analyse in depth the legal traditions in all Member States and their existing national laws that grant compensation for immaterial harm, in order to evaluate if the inclusion of immaterial harm in AI- specific legislative acts is necessary and if it contradicts the existing Union legal framework or undermines the national law of the Member States; 20. (extracted from291.pdf)
This follows from general and widely accepted liability concepts of justice, according to which the person that creates or maintains a risk for the public is liable if that risk causes harm or damage, and thus should ex-ante minimise or ex-post compensate that risk. (extracted from291.pdf)
(16) This Regulation should cover harm or damage to life, health, physical integrity, property and significant immaterial harm that results in a verifiable economic loss above a threshold, harmonised in Union liability law, that balances the access to justice of affected persons with the interests of other involved persons. (extracted from291.pdf)
How to use the Principles for Action The first version of these principles has been co-drafted through a multistakeholder process, while paying careful As stated, this first version of the Principles for Action will attention to the EU GDPR11 and the police and criminal be reviewed based on the practical findings of the policy justice directive,12 and has drawn inspiration from some of pilot. (extracted from722.pdf)
“EU Data Protection Rules”, EU website, https://ec.europa.eu/commission/priorities/justice-and-fundamental-rights/ data-protection/2018- reform-eu-data-protection-rules/eu-data-protection-rules_en (link as of 28/1/20). (extracted from722.pdf)
Koenig, Jacob Metcalf, Arvind Narayanan, Alondra Nelson, Frank Pasquale Ten simple rules for responsible big data research www.ploscompbiol.org www.ploscompbiol.org endstream endobj 3 0 obj < >/XObject< >>>/CropBox[0 0 612 792]/MediaBox[0 0 612 792]/Parent 9 0 R/Annots 10 0 R/Contents 11 0 R/TrimBox[0 0 612 792]>> endobj 10 0 obj [12 0 R 13 0 R 14 0 R 15 0 R 16 0 R 17 0 R 18 0 R 19 0 R 20 0 R 21 0 R 22 0 R 23 0 R 24 0 R 25 0 R 26 0 R] endobj 12 0 obj < >/Border[0 0 0]/A 27 0 R>> endobj 27 0 obj < > endobj 13 0 obj < >/Border[0 0 0]/Dest(Rpcbi.1005399.ref001)>> endobj 14 0 obj < >/Border[0 0 0]/Dest(Rpcbi.1005399.ref002)>> endobj 15 0 obj < >/Border[0 0 0]/Dest(Rpcbi.1005399.ref003)>> endobj 16 0 obj < >/Border[0 0 0]/Dest(Rpcbi.1005399.ref004)>> endobj 17 0 obj < >/Border[0 0 0]/Dest(Rpcbi.1005399.ref005)>> endobj 18 0 obj < >/Border[0 0 0]/Dest(Rpcbi.1005399.ref006)>> endobj 19 0 obj < >/Border[0 0 0]/Dest(Rpcbi.1005399.ref007)>> endobj 20 0 obj < >/Border[0 0 0]/Dest(Rpcbi.1005399.ref008)>> endobj 21 0 obj < >/Border[0 0 0]/Dest(Rpcbi.1005399.ref009)>> endobj 22 0 obj < >/Border[0 0 0]/Dest(Rpcbi.1005399.ref009)>> endobj 23 0 obj < >/Border[0 0 0]/Dest(Rpcbi.1005399.ref010)>> endobj 24 0 obj < >/Border[0 0 0]/Dest(Rpcbi.1005399.ref011)>> endobj 25 0 obj < >/Border[0 0 0]/Dest(Rpcbi.1005399.ref012)>> endobj 26 0 obj < >/Border[0 0 0]/A 28 0 R>> endobj 28 0 obj < > endobj 11 0 obj [29 0 R 30 0 R 31 0 R 32 0 R 33 0 R 34 0 R 35 0 R 36 0 R 37 0 R 38 0 R 39 0 R 40 0 R 41 0 R 42 0 R 43 0 R] endobj 29 0 obj < >stream q 0.83 0.64 0.02 0 k 407.5654 537.3354 m 514.148 537.3354 l h f* 560.7496 511.3134 m 565.3984 511.3134 l h f* 231.0236 387.6094 m 235.7858 387.6094 l h f* 203.4142 309.6 m 208.1764 309.6 l h f* 210.3874 309.6 m 215.1496 309.6 l h f* 233.4614 296.6173 m 238.2236 296.6173 l h f* 237.0331 283.6346 m 241.7953 283.6346 l h f* 488.3528 270.5953 m 493.115 270.5953 l h f* 390.7843 218.6079 m 395.4898 218.6079 l h f* 482.9669 205.6252 m 487.7291 205.6252 l h f* 271.5024 166.6205 m 276.2646 166.6205 l h f* 278.5323 166.6205 m 288.0567 166.6205 l h f* 355.9181 81.9213 m 365.3858 81.9213 l h f* 435.7984 81.9213 m 445.3228 81.9213 l h f* 0 g 1 j 1 J 0 w 10 0 0 10 211.9748 707.4141 cm BT /F6 1 Tf 1 TL -0.004 Tc 0 0 Td (The)Tj 1.7858 0 Td (variety)Tj 2.9253 0 Td (in)Tj 1.0318 0 Td (data)Tj 1.9162 0 Td (sources,)Tj 3.4186 0 Td (research)Tj 3.5546 0 Td (topics,)Tj 2.8233 0 Td (and)Tj 1.7291 0 Td (methodological)Tj 6.4176 0 Td (approaches)Tj 4.7395 0 Td (in)Tj 1.0318 0 Td (big)Tj 1.4627 0 Td (data)Tj -34.0325 -1.3039 Td (belies)Tj 2.4491 0 Td (a)Tj 0.6519 0 Td (one-size-fits-all)Tj 6.3553 0 Td (checklist;)Tj 3.9117 0 Td (as)Tj 1.0148 0 Td (a)Tj 0.6577 0 Td (result,)Tj 2.6702 0 Td (these)Tj 2.2564 0 Td (rules)Tj 2.1486 0 Td (are)Tj 1.4457 0 Td (less)Tj 1.6157 0 Td (specific)Tj 3.1918 0 Td (than)Tj 2.0239 0 Td (some)Tj 2.3357 0 Td (might)Tj 2.5966 0 Td (hope.)Tj -35.3251 -1.2982 Td (Rather,)Tj 3.118 0 Td (we)Tj 1.3323 0 Td (exhort)Tj 2.812 0 Td (researchers)Tj 4.7111 0 Td (to)Tj 1.0261 0 Td (recognize)Tj 4.0592 0 Td (the)Tj 1.474 0 Td (human)Tj 3.0728 0 Td (participants)Tj 4.9606 0 Td (and)Tj 1.7291 0 Td (complex)Tj 3.6226 0 Td (systems)Tj 3.2996 0 Td (con-)Tj -35.2174 -1.2983 Td (tained)Tj 2.7098 0 Td (within)Tj 2.8063 0 Td (their)Tj 2.1033 0 Td (data)Tj 1.9106 0 Td (and)Tj 1.7234 0 Td (make)Tj 2.3868 0 Td (grappling)Tj 4.0421 0 Td (with)Tj 1.9956 0 Td (ethical)Tj 2.829 0 Td (questions)Tj 4.0195 0 Td (part)Tj 1.8312 0 Td (of)Tj 1.0091 0 Td (their)Tj 2.1033 0 Td (standard)Tj 3.7133 0 Td (work-)Tj -35.1833 -1.2983 Td (flow.)Tj 2.1713 0 Td (Towards)Tj 3.7133 0 Td (this)Tj 1.6725 0 Td (end,)Tj 1.9502 0 Td (we)Tj 1.3266 0 Td (structure)Tj 3.8211 0 Td (the)Tj 1.4683 0 Td (first)Tj 1.8085 0 Td (five)Tj 1.6497 0 Td (rules)Tj 2.1544 0 Td (around)Tj 3.1351 0 Td (how)Tj 1.9388 0 Td (to)Tj 1.0262 0 Td (reduce)Tj 2.914 0 Td (the)Tj 1.4683 0 Td (chance)Tj 3.0047 0 Td (of)Tj -35.223 -1.3039 Td (harm)Tj 2.364 0 Td (resulting)Tj 3.719 0 Td (from)Tj 2.1941 0 Td (big)Tj 1.4513 0 Td (data)Tj 1.8992 0 Td (research)Tj 3.5489 0 Td (practices;)Tj 3.9572 0 Td (the)Tj 1.457 0 Td (second)Tj 3.0047 0 Td (five)Tj 1.6384 0 Td (rules)Tj 2.143 0 Td (focus)Tj 2.3187 0 Td (on)Tj 1.2642 0 Td (ways)Tj 2.143 0 Td (researchers)Tj -33.1027 -1.2982 Td (can)Tj 1.627 0 Td (contribute)Tj 4.388 0 Td (to)Tj 1.0262 0 Td (building)Tj 3.5773 0 Td (best)Tj 1.8084 0 Td (practices)Tj 3.7418 0 Td (that)Tj 1.7801 0 Td (fit)Tj 1.0715 0 Td (their)Tj 2.109 0 Td (disciplinary)Tj 4.9039 0 Td (and)Tj 1.7291 0 Td (methodological)Tj -27.7623 -1.2983 Td (approaches.)Tj 4.9605 0 Td (At)Tj 1.2019 0 Td (the)Tj 1.4684 0 Td (core)Tj 1.9445 0 Td (of)Tj 1.0148 0 Td (these)Tj 2.2564 0 Td (rules,)Tj 2.3754 0 Td (we)Tj 1.3266 0 Td (challenge)Tj 3.9571 0 Td (big)Tj 1.4627 0 Td (data)Tj 1.9105 0 Td (researchers)Tj 4.7112 0 Td (who)Tj 1.9389 0 Td (consider)Tj 3.651 0 Td (their)Tj -34.1799 -1.3039 Td (data)Tj 1.9105 0 Td (disentangled)Tj 5.2951 0 Td (from)Tj 2.2053 0 Td (the)Tj 1.4683 0 Td (ability)Tj 2.6929 0 Td (to)Tj 1.0262 0 Td (harm)Tj 2.3754 0 Td (to)Tj 1.0261 0 Td (reexamine)Tj 4.3937 0 Td (their)Tj 2.109 0 Td (assumptions.)Tj 5.4481 0 Td (The)Tj 1.7802 0 Td (examples)Tj 3.9117 0 Td (in)Tj -35.6425 -1.2983 Td (this)Tj 1.6724 0 Td (paper)Tj 2.4831 0 Td (show)Tj 2.3017 0 Td (how)Tj 1.9389 0 Td (often)Tj 2.2847 0 Td (even)Tj 2.0693 0 Td (seemingly)Tj 4.2293 0 Td (innocuous)Tj 4.439 0 Td (and)Tj 1.7291 0 Td (anonymized)Tj 5.1647 0 Td (data)Tj 1.9162 0 Td (have)Tj 2.0579 0 Td (produced)Tj -32.2863 -1.2983 Td (unanticipated)Tj 5.7372 0 Td (ethical)Tj 2.8403 0 Td (questions)Tj 4.0195 0 Td (and)Tj 1.7291 0 Td (detrimental)Tj 4.8756 0 Td (impacts.)Tj -18.0055 -1.3039 Td (This)Tj -0.01 Tc 1.9899 0 Td (paper)Tj 2.4547 0 Td (is)Tj 0.8334 0 Td (a)Tj 0.6463 0 Td (result)Tj 2.4151 0 Td (of)Tj 1.0035 0 Td (a)Tj 0.6463 0 Td (two-year)Tj 3.6963 0 Td (National)Tj 3.6454 0 Td (Science)Tj 3.1634 0 Td (Foundation)Tj 4.8529 0 Td (\(NSF\)-funded)Tj 5.76 0 Td (project)Tj 2.9763 0 Td (that)Tj -35.2797 -1.2982 Td (established)Tj 4.5297 0 Td (the)Tj 1.4513 0 Td (Council)Tj 3.3619 0 Td (for)Tj 1.3663 0 Td (Big)Tj 1.5137 0 Td (Data,)Tj 2.3187 0 Td (Ethics,)Tj 2.8516 0 Td (and)Tj 1.7121 0 Td (Society,)Tj 3.2485 0 Td (a)Tj 0.6463 0 Td (group)Tj 2.5795 0 Td (of)Tj 1.0035 0 Td (20)Tj 1.1565 0 Td (scholars)Tj 3.4072 0 Td (from)Tj 2.1827 0 Td (a)Tj 0.6463 0 Td (wide)Tj -33.9758 -1.2983 Td (range)Tj 2.432 0 Td (of)Tj 1.0035 0 Td (social,)Tj 2.6419 0 Td (natural,)Tj 3.2541 0 Td (and)Tj 1.7121 0 Td (computational)Tj 5.9754 0 Td (sciences)Tj 3.4016 0 Td (\()Tj 0.83 0.64 0.02 0 k (http://bdes.datasociety.n)Tj 9.9609 0 Td (et/)Tj 0 g (\).)Tj 1.8028 0 Td (The)Tj 1.7688 0 Td (Council)Tj -33.9531 -1.3039 Td (was)Tj 1.678 0 Td (charged)Tj 3.3506 0 Td (with)Tj 1.9729 0 Td (providing)Tj 4.0932 0 Td (guidance)Tj 3.7927 0 Td (to)Tj 1.0091 0 Td (the)Tj 1.457 0 Td (NSF)Tj 1.9389 0 Td (on)Tj 1.2643 0 Td (how)Tj 1.9218 0 Td (to)Tj 1.0092 0 Td (best)Tj 1.7858 0 Td (encourage)Tj 4.2916 0 Td (ethical)Tj 2.8006 0 Td (practices)Tj 3.685 0 Td (in)Tj -36.0507 -1.2983 Td (scientific)Tj 3.7246 0 Td (and)Tj 1.7122 0 Td (engineering)Tj 4.9095 0 Td (research,)Tj 3.7247 0 Td (utilizing)Tj 3.4696 0 Td (big)Tj 1.44 0 Td (data)Tj 1.8879 0 Td (research)Tj 3.5092 0 Td (methods)Tj 3.6454 0 Td (and)Tj 1.7121 0 Td (infrastructures)Tj 6.0037 0 Td ([)Tj 0.83 0.64 0.02 0 k (1)Tj 0 g (].)Tj ET Q q 1 j 1 J 0 w 11.9999 0 0 11.9999 200.0125 483.7039 cm BT /F2 1 Tf 1 TL -0.0033 Tc 0 0 Td (1.)Tj 1.0441 0 Td (Acknowledge)Tj 6.6284 0 Td (that)Tj 2.0362 0 Td (data)Tj 2.2583 0 Td (are)Tj 1.696 0 Td (people)Tj 3.4253 0 Td (and)Tj 1.9937 0 Td (can)Tj 1.9276 0 Td (do)Tj 1.4409 0 Td (harm)Tj ET Q q 1 j 1 J 0 w 10 0 0 10 200.0125 466.696 cm BT /F6 1 Tf 1 TL -0.004 Tc 0 0 Td (One)Tj 1.9388 0 Td (of)Tj 1.0148 0 Td (the)Tj 1.4684 0 Td (most)Tj 2.2053 0 Td (fundamental)Tj 5.3178 0 Td (rules)Tj 2.1486 0 Td (of)Tj 1.0148 0 Td (responsible)Tj 4.7509 0 Td (big)Tj 1.4626 0 Td (data)Tj 1.9106 0 Td (research)Tj 3.5603 0 Td (is)Tj 0.8447 0 Td (the)Tj 1.4683 0 Td (steadfast)Tj 3.651 0 Td (recognition)Tj -32.7569 -1.2982 Td (that)Tj 1.7744 0 Td (most)Tj 2.211 0 Td (data)Tj 1.9106 0 Td (represent)Tj 3.9571 0 Td (or)Tj 1.0999 0 Td (impact)Tj 2.982 0 Td (people.)Tj 3.0784 0 Td (Simply)Tj 2.999 0 Td (starting)Tj 3.2655 0 Td (with)Tj 1.9956 0 Td (the)Tj 1.474 0 Td (assumption)Tj 4.8586 0 Td (that)Tj 1.7801 0 Td (all)Tj 1.1452 0 Td (data)Tj -34.5314 -1.3039 Td (are)Tj 1.4456 0 Td (people)Tj 2.8573 0 Td (until)Tj 2.109 0 Td (proven)Tj 3.0387 0 Td (otherwise)Tj 4.0875 0 Td (places)Tj 2.6249 0 Td (the)Tj 1.4683 0 Td (difficulty)Tj 3.8098 0 Td (of)Tj 1.0148 0 Td (disassociating)Tj 5.7429 0 Td (data)Tj 1.9106 0 Td (from)Tj 2.2053 0 Td (specific)Tj 3.1918 0 Td (indi-)Tj -35.5065 -1.2983 Td (viduals)Tj 3.0387 0 Td (front)Tj 2.2337 0 Td (and)Tj 1.7291 0 Td (center.)Tj 2.931 0 Td (This)Tj 1.9899 0 Td (logic)Tj 2.126 0 Td (is)Tj 0.8504 0 Td (readily)Tj 2.9423 0 Td (evident)Tj 3.1634 0 Td (for)Tj 1.389 0 Td (ªriskyº)Tj 2.9537 0 Td (datasets,)Tj 3.583 0 Td (e.g.,)Tj 1.7858 0 Td (social)Tj 2.4604 0 Td (media)Tj -33.1764 -1.2983 Td (with)Tj 1.9955 0 Td (inflammatory)Tj 5.709 0 Td (language,)Tj 3.9911 0 Td (but)Tj 1.5534 0 Td (even)Tj 2.0693 0 Td (seemingly)Tj 4.2292 0 Td (benign)Tj 2.9821 0 Td (data)Tj 1.9105 0 Td (can)Tj 1.6271 0 Td (contain)Tj 3.2485 0 Td (sensitive)Tj 3.6226 0 Td (and)Tj 1.7291 0 Td (private)Tj -34.6674 -1.2982 Td (information,)Tj 5.3007 0 Td (e.g.,)Tj 1.7858 0 Td (it)Tj 0.7824 0 Td (is)Tj 0.8504 0 Td (possible)Tj 3.4072 0 Td (to)Tj 1.0261 0 Td (extract)Tj 2.9367 0 Td (data)Tj 1.9105 0 Td (on)Tj 1.2756 0 Td (the)Tj 1.474 0 Td (exact)Tj 2.2621 0 Td (heart)Tj 2.2733 0 Td (rates)Tj 2.109 0 Td (of)Tj 1.0148 0 Td (people)Tj 2.8573 0 Td (from)Tj 2.2053 0 Td (YouTube)Tj -33.4712 -1.3039 Td (videos)Tj 2.7609 0 Td ([)Tj 0.83 0.64 0.02 0 k (2)Tj 0 g (].)Tj 1.5987 0 Td (Even)Tj 2.2167 0 Td (data)Tj 1.9162 0 Td (that)Tj 1.7745 0 Td (seemingly)Tj 4.2292 0 Td (have)Tj 2.0636 0 Td (nothing)Tj 3.3789 0 Td (to)Tj 1.0262 0 Td (do)Tj 1.2585 0 Td (with)Tj 1.9956 0 Td (people)Tj 2.8516 0 Td (might)Tj 2.6022 0 Td (impact)Tj 2.9764 0 Td (individuals')Tj -32.6492 -1.2983 Td (lives)Tj 1.9672 0 Td (in)Tj 1.0318 0 Td (unexpected)Tj 4.7905 0 Td (ways,)Tj 2.3754 0 Td (e.g.,)Tj 1.7802 0 Td (oceanographic)Tj 6.0661 0 Td (data)Tj 1.9105 0 Td (that)Tj 1.7745 0 Td (change)Tj 3.0387 0 Td (the)Tj 1.4627 0 Td (risk)Tj 1.7121 0 Td (profiles)Tj 3.1975 0 Td (of)Tj 1.0091 0 Td (communities')Tj -32.1163 -1.2983 Td (and)Tj 1.7177 0 Td (properties')Tj 4.4844 0 Td (values)Tj 2.6589 0 Td (or)Tj 1.0885 0 Td (Exchangeable)Tj 5.6806 0 Td (Image)Tj 2.6929 0 Td (Format)Tj 3.1691 0 Td (\(EXIF\))Tj 2.982 0 Td (records)Tj 3.1975 0 Td (from)Tj 2.1997 0 Td (photos)Tj 2.9253 0 Td (that)Tj 1.7688 0 Td (contain)Tj -34.5654 -1.3039 Td (location)Tj 3.4525 0 Td (coordinates)Tj 4.8869 0 Td (and)Tj 1.7291 0 Td (reveal)Tj 2.5739 0 Td (the)Tj 1.4683 0 Td (photographer's)Tj 6.2646 0 Td (movement)Tj 4.5127 0 Td (or)Tj 1.0998 0 Td (even)Tj 2.0693 0 Td (home)Tj 2.5001 0 Td (location.)Tj -29.361 -1.2983 Td (Harm)Tj 2.6135 0 Td (can)Tj 1.6271 0 Td (also)Tj 1.7688 0 Td (result)Tj 2.4491 0 Td (when)Tj 2.4038 0 Td (seemingly)Tj 4.2292 0 Td (innocuous)Tj 4.439 0 Td (datasets)Tj 3.3562 0 Td (about)Tj 2.4945 0 Td (population-wide)Tj 6.8428 0 Td (effects)Tj 2.7269 0 Td (are)Tj -36.1471 -1.2982 Td (used)Tj 2.0579 0 Td (to)Tj 1.0261 0 Td (shape)Tj 2.4832 0 Td (the)Tj 1.4683 0 Td (lives)Tj 1.9729 0 Td (of)Tj 1.0148 0 Td (individuals)Tj 4.6488 0 Td (or)Tj 1.0998 0 Td (stigmatize)Tj 4.269 0 Td (groups,)Tj 3.1918 0 Td (often)Tj 2.2847 0 Td (without)Tj 3.3335 0 Td (procedural)Tj 4.5694 0 Td (recourse)Tj -33.4202 -1.3039 Td ([)Tj 0.83 0.64 0.02 0 k (3)Tj 0 g (,)Tj 0.83 0.64 0.02 0 k (4)Tj 0 g (].)Tj 2.3017 0 Td (For)Tj 1.627 0 Td (example,)Tj 3.7701 0 Td (social)Tj 2.4605 0 Td (network)Tj 3.5489 0 Td (maps)Tj 2.3528 0 Td (for)Tj 1.3889 0 Td (services)Tj 3.3052 0 Td (such)Tj 2.0579 0 Td (as)Tj 1.0148 0 Td (Twitter)Tj 3.1748 0 Td (can)Tj 1.6271 0 Td (determine)Tj 4.32 0 Td (credit-wor-)Tj -32.9497 -1.2983 Td (thiness)Tj 3.0047 0 Td ([)Tj 0.83 0.64 0.02 0 k (5)Tj 0 g (],)Tj 1.5987 0 Td (opaque)Tj 3.1351 0 Td (recidivism)Tj 4.3993 0 Td (scores)Tj 2.6646 0 Td (can)Tj 1.6214 0 Td (shape)Tj 2.4831 0 Td (criminal)Tj 3.5943 0 Td (justice)Tj 2.778 0 Td (decisions)Tj 3.9004 0 Td (in)Tj 1.0318 0 Td (a)Tj 0.652 0 Td (racially)Tj 3.0954 0 Td (disparate)Tj -33.9588 -1.2983 Td (manner)Tj 3.3618 0 Td ([)Tj 0.83 0.64 0.02 0 k (6)Tj 0 g (],)Tj 1.6044 0 Td (and)Tj 1.7291 0 Td (categorization)Tj 5.8791 0 Td (based)Tj 2.4718 0 Td (on)Tj 1.2755 0 Td (zip)Tj 1.4174 0 Td (codes)Tj 2.4604 0 Td (resulted)Tj 3.4016 0 Td (in)Tj 1.0374 0 Td (less)Tj 1.6101 0 Td (access)Tj 2.6475 0 Td (to)Tj 1.0262 0 Td (Amazon)Tj 3.6226 0 Td (Prime)Tj -33.5449 -1.3039 Td (same-day)Tj 4.0251 0 Td (delivery)Tj 3.3902 0 Td (service)Tj 2.9424 0 Td (for)Tj 1.3833 0 Td (African-Americans)Tj 7.9199 0 Td (in)Tj 1.0375 0 Td (United)Tj 3.0217 0 Td (States)Tj 2.5115 0 Td (cities)Tj 2.262 0 Td ([)Tj 0.83 0.64 0.02 0 k (7)Tj 0 g (].)Tj 1.5988 0 Td (These)Tj 2.5681 0 Td (high-profile)Tj -32.6605 -1.2982 Td (cases)Tj 2.2223 0 Td (show)Tj 2.296 0 Td (that)Tj 1.7802 0 Td (apparently)Tj 4.4674 0 Td (neutral)Tj 3.067 0 Td (data)Tj 1.9162 0 Td (can)Tj 1.6271 0 Td (yield)Tj 2.1373 0 Td (discriminatory)Tj 6.1398 0 Td (outcomes,)Tj 4.3143 0 Td (thereby)Tj 3.2258 0 Td (com-)Tj -33.1934 -1.2983 Td (pounding)Tj 4.1272 0 Td (social)Tj 2.4604 0 Td (inequities.)Tj -5.3914 -1.2983 Td (Other)Tj 2.5908 0 Td (cases)Tj 2.2224 0 Td (show)Tj 2.3017 0 Td (that)Tj 1.7745 0 Td (ªpublicº)Tj 3.4922 0 Td (datasets)Tj 3.3619 0 Td (are)Tj 1.4456 0 Td (easily)Tj 2.4095 0 Td (adapted)Tj 3.3789 0 Td (for)Tj 1.3889 0 Td (highly)Tj 2.7099 0 Td (invasive)Tj 3.4299 0 Td (research)Tj 3.5547 0 Td (by)Tj -35.2571 -1.3039 Td (incorporating)Tj 5.7372 0 Td (other)Tj 2.3414 0 Td (data,)Tj 2.1317 0 Td (such)Tj 2.0522 0 Td (as)Tj 1.0092 0 Td (Hague)Tj 2.8346 0 Td (et)Tj 0.9354 0 Td (al.'s)Tj 1.6951 0 Td ([)Tj 0.83 0.64 0.02 0 k (8)Tj 0 g (])Tj 1.3663 0 Td (use)Tj 1.5251 0 Td (of)Tj 1.0091 0 Td (property)Tj 3.6737 0 Td (records)Tj 3.1974 0 Td (and)Tj 1.7235 0 Td (geographic)Tj 4.6148 0 Td (pro-)Tj -35.8467 -1.2983 Td (filing)Tj 2.3017 0 Td (techniques)Tj 4.5297 0 Td (to)Tj 1.0205 0 Td (allegedly)Tj 3.6907 0 Td (identify)Tj 3.2938 0 Td (the)Tj 1.4683 0 Td (pseudonymous)Tj 6.3042 0 Td (artist)Tj 2.2507 0 Td (Banksy)Tj 3.0954 0 Td ([)Tj 0.83 0.64 0.02 0 k (9)Tj 0 g (].)Tj 1.5988 0 Td (In)Tj 1.1112 0 Td (particular,)Tj 4.3369 0 Td (data)Tj -35.0019 -1.2982 Td (ungoverned)Tj 5.0172 0 Td (by)Tj 1.1792 0 Td (substantive)Tj 4.7055 0 Td (consent)Tj 3.3279 0 Td (practices,)Tj 3.9628 0 Td (whether)Tj 3.4809 0 Td (social)Tj 2.4548 0 Td (media)Tj 2.6929 0 Td (or)Tj 1.0998 0 Td (the)Tj 1.4684 0 Td (residual)Tj 3.3788 0 Td (DNA)Tj 2.3755 0 Td (we)Tj -35.1437 -1.3039 Td (continually)Tj 4.7224 0 Td (leave)Tj 2.1997 0 Td (behind)Tj 3.0217 0 Td (us,)Tj 1.3323 0 Td (may)Tj 1.9332 0 Td (seem)Tj 2.245 0 Td (public)Tj 2.7099 0 Td (but)Tj 1.5534 0 Td (can)Tj 1.6271 0 Td (cause)Tj 2.3868 0 Td (unintentional)Tj 5.6862 0 Td (breaches)Tj 3.6907 0 Td (of)Tj 1.0148 0 Td (privacy)Tj -34.1232 -1.2983 Td (and)Tj 1.7291 0 Td (other)Tj 2.347 0 Td (harms)Tj 2.7326 0 Td ([)Tj 0.83 0.64 0.02 0 k (9)Tj 0 g (,)Tj 0.83 0.64 0.02 0 k (10)Tj 0 g (].)Tj -5.6125 -1.2983 Td (Start)Tj 2.0976 0 Td (with)Tj 2.0012 0 Td (the)Tj 1.4684 0 Td (assumption)Tj 4.8642 0 Td (that)Tj 1.7745 0 Td (data)Tj 1.9162 0 Td (are)Tj 1.4456 0 Td (people)Tj 2.8573 0 Td (\(until)Tj 2.4492 0 Td (proven)Tj 3.0387 0 Td (otherwise\),)Tj 4.6488 0 Td (and)Tj 1.7348 0 Td (use)Tj 1.5307 0 Td (it)Tj 0.7823 0 Td (to)Tj 1.0262 0 Td (guide)Tj -34.8319 -1.3039 Td (your)Tj 2.0806 0 Td (analysis.)Tj 3.5489 0 Td (No)Tj 1.4627 0 Td (one)Tj 1.7008 0 Td (gets)Tj 1.7688 0 Td (an)Tj 1.2019 0 Td (automatic)Tj 4.2292 0 Td (pass)Tj 1.8936 0 Td (on)Tj 1.2755 0 Td (ethics.)Tj ET Q q 1 j 1 J 0 w 11.9999 0 0 11.9999 200.0125 112.9889 cm BT /F2 1 Tf 1 TL -0.0033 Tc 0 0 Td (2.)Tj 1.0441 0 Td (Recognize)Tj 5.1874 0 Td (that)Tj 2.0362 0 Td (privacy)Tj 3.6851 0 Td (is)Tj 1.0441 0 Td (more)Tj 2.641 0 Td (than)Tj 2.3197 0 Td (a)Tj 0.7701 0 Td (binary)Tj 3.1984 0 Td (value)Tj ET Q q 1 j 1 J 0 w 10 0 0 10 200.0125 95.9811 cm BT /F6 1 Tf 1 TL -0.004 Tc 0 0 Td (Breaches)Tj 3.7643 0 Td (of)Tj 1.0148 0 Td (privacy)Tj 3.1408 0 Td (are)Tj 1.4513 0 Td (key)Tj 1.5931 0 Td (means)Tj 2.8063 0 Td (by)Tj 1.1792 0 Td (which)Tj 2.6532 0 Td (big)Tj 1.457 0 Td (data)Tj 1.9162 0 Td (research)Tj 3.5546 0 Td (can)Tj 1.6271 0 Td (do)Tj 1.2529 0 Td (harm,)Tj 2.6022 0 Td (and)Tj 1.7291 0 Td (it)Tj 0.7824 0 Td (is)Tj 0.8504 0 Td (important)Tj -33.3749 -1.2982 Td (to)Tj 1.0261 0 Td (recognize)Tj 4.0592 0 Td (that)Tj 1.7801 0 Td (privacy)Tj 3.1408 0 Td (is)Tj 0.8504 0 Td (contextual)Tj 4.3936 0 Td ([)Tj 0.83 0.64 0.02 0 k (11)Tj 0 g (])Tj 1.8482 0 Td (and)Tj 1.7291 0 Td (situational)Tj 4.4107 0 Td ([)Tj 0.83 0.64 0.02 0 k (12)Tj 0 g (],)Tj 2.075 0 Td (not)Tj 1.576 0 Td (reducible)Tj 3.9402 0 Td (to)Tj 1.0204 0 Td (a)Tj 0.6577 0 Td (simple)Tj ET Q q 1 j 1 J 0 w 576 737.1 m 36 737.1 l 36 737.6 l 576 737.6 l f* 36 741.2598 169.285 23.6976 re W* n q 169.2283 0 0 23.6409 36 741.3165 cm q /I0 Do Q Q Q q 0 0 612 792 re W* n 1 j 1 J 0 w 7.9999 0 0 7.9999 391.9748 745.7952 cm BT /F0 1 Tf 1 TL -0.005 Tc 0 0 Td (Ten)Tj 1.9204 0 Td (Simple)Tj 3.2245 0 Td (Rules)Tj 2.7283 0 Td (for)Tj 1.3678 0 Td (Respons)Tj 3.8764 0 Td (ible)Tj 1.7362 0 Td (Big)Tj 1.637 0 Td (Data)Tj 2.3032 0 Td (Researc)Tj 3.6567 0 Td (h)Tj ET Q q 1 j 1 J 0 w 36 48.0002 m 576 48.0002 l 576 47.5002 l 36 47.5002 l f* 0.83 0.64 0.02 0 k 142.5827 34.9228 m 295.8236 34.9228 l h f* 0 g 7.9999 0 0 7.9999 36 36 cm BT /F0 1 Tf 1 TL -0.005 Tc 0 0 Td (PLOS)Tj 2.8559 0 Td (Computationa)Tj 6.1229 0 Td (l)Tj 0.4322 0 Td (Biology)Tj 3.4371 0 Td (|)Tj 0.83 0.64 0.02 0 k 0.4748 0 Td (https:/)Tj 2.6788 0 Td (/doi.org/10.13)Tj 6.0094 0 Td (71/journal.p)Tj 5.1166 0 Td (cbi.1005399)Tj 0 g 6.3497 0 Td (March)Tj 2.9551 0 Td (30,)Tj 1.5874 0 Td (2017)Tj 27.1207 0 Td (2)Tj 0.7654 0 Td (/)Tj ET endstream endobj 30 0 obj < >stream endstream endobj 31 0 obj < >stream endstream endobj 32 0 obj < >stream endstream endobj 33 0 obj < >stream endstream endobj 34 0 obj < >stream endstream endobj 35 0 obj < >stream endstream endobj 36 0 obj < >stream endstream endobj 37 0 obj < >stream endstream endobj 38 0 obj < >stream endstream endobj 39 0 obj < >stream endstream endobj 40 0 obj < >stream endstream endobj 41 0 obj < >stream endstream endobj 42 0 obj < >stream endstream endobj 43 0 obj < >stream BT 66.402 0 Td (10)Tj ET Q endstream endobj 44 0 obj < >/XObject< >>>/CropBox[0 0 612 792]/MediaBox[0 0 612 792]/Parent 9 0 R/Annots 45 0 R/Contents 46 0 R/TrimBox[0 0 612 792]>> endobj 45 0 obj [47 0 R 48 0 R 49 0 R 50 0 R 51 0 R 52 0 R 53 0 R 54 0 R 55 0 R 56 0 R 57 0 R 58 0 R 59 0 R 60 0 R 61 0 R 62 0 R 63 0 R] endobj 47 0 obj < >/Border[0 0 0]/Dest(Rpcbi.1005399.ref013)>> endobj 48 0 obj < >/Border[0 0 0]/Dest(Rpcbi.1005399.ref010)>> endobj 49 0 obj < >/Border[0 0 0]/Dest(Rpcbi.1005399.ref014)>> endobj 50 0 obj < >/Border[0 0 0]/Dest(Rpcbi.1005399.ref015)>> endobj 51 0 obj < >/Border[0 0 0]/Dest(Rpcbi.1005399.ref016)>> endobj 52 0 obj < >/Border[0 0 0]/Dest(Rpcbi.1005399.ref017)>> endobj 53 0 obj < >/Border[0 0 0]/Dest(Rpcbi.1005399.ref018)>> endobj 54 0 obj < >/Border[0 0 0]/Dest(Rpcbi.1005399.ref019)>> endobj 55 0 obj < >/Border[0 0 0]/Dest(Rpcbi.1005399.ref020)>> endobj 56 0 obj < >/Border[0 0 0]/Dest(Rpcbi.1005399.ref021)>> endobj 57 0 obj < >/Border[0 0 0]/Dest(Rpcbi.1005399.ref022)>> endobj 58 0 obj < >/Border[0 0 0]/Dest(Rpcbi.1005399.ref023)>> endobj 59 0 obj < >/Border[0 0 0]/Dest(Rpcbi.1005399.ref024)>> endobj 60 0 obj < >/Border[0 0 0]/Dest(Rpcbi.1005399.ref025)>> endobj 61 0 obj < >/Border[0 0 0]/Dest(Rpcbi.1005399.ref026)>> endobj 62 0 obj < >/Border[0 0 0]/Dest(Rpcbi.1005399.ref027)>> endobj 63 0 obj < >/Border[0 0 0]/A 64 0 R>> endobj 64 0 obj < > endobj 46 0 obj [65 0 R 66 0 R 67 0 R 68 0 R 69 0 R 70 0 R 71 0 R 72 0 R 73 0 R 74 0 R 75 0 R 76 0 R 77 0 R 78 0 R 79 0 R 80 0 R 81 0 R 82 0 R] endobj 65 0 obj < >stream q 0.83 0.64 0.02 0 k 355.1811 615.3449 m 364.7055 615.3449 l h f* 478.3181 511.3134 m 487.8425 511.3134 l h f* 412.4409 485.348 m 421.9654 485.348 l h f* 426.4441 485.348 m 435.9118 485.348 l h f* 507.2882 459.326 m 516.7559 459.326 l h f* 454.1102 315.9496 m 463.578 315.9496 l h f* 337.0961 302.9102 m 346.6205 302.9102 l h f* 351.0992 302.9102 m 360.6236 302.9102 l h f* 532.2331 263.9055 m 541.7575 263.9055 l h f* 449.2913 237.9402 m 458.8157 237.9402 l h f* 272.863 198.9354 m 282.3874 198.9354 l h f* 203.4142 172.9134 m 212.9386 172.9134 l h f* 529.3984 159.9307 m 538.8661 159.9307 l h f* 277.9087 133.9087 m 287.4331 133.9087 l h f* 538.3559 120.926 m 547.8803 120.926 l h f* 250.7528 81.9213 m 260.2772 81.9213 l h f* 0 g 1 j 1 J 0 w 10 0 0 10 200.0125 707.4141 cm BT /F6 1 Tf 1 TL -0.004 Tc 0 0 Td (public/private)Tj 5.7939 0 Td (binary.)Tj 3.0161 0 Td (Just)Tj 1.7291 0 Td (because)Tj 3.3052 0 Td (something)Tj 4.4277 0 Td (has)Tj 1.5363 0 Td (been)Tj 2.109 0 Td (shared)Tj 2.8573 0 Td (publicly)Tj 3.4016 0 Td (does)Tj 2.0239 0 Td (not)Tj 1.5647 0 Td (mean)Tj 2.4378 0 Td (any)Tj 1.6441 0 Td (sub-)Tj -35.8467 -1.3039 Td (sequent)Tj 3.3108 0 Td (use)Tj 1.5307 0 Td (would)Tj 2.7099 0 Td (be)Tj 1.1509 0 Td (unproblematic.)Tj 6.3382 0 Td (Looking)Tj 3.5489 0 Td (at)Tj 0.9525 0 Td (a)Tj 0.6519 0 Td (single)Tj 2.5342 0 Td (Instagram)Tj 4.2916 0 Td (photo)Tj 2.5795 0 Td (by)Tj 1.1792 0 Td (an)Tj 1.2019 0 Td (individual)Tj 4.286 0 Td (has)Tj -36.2662 -1.2982 Td (different)Tj 3.6566 0 Td (ethical)Tj 2.8403 0 Td (implications)Tj 5.176 0 Td (than)Tj 2.0296 0 Td (looking)Tj 3.2542 0 Td (at)Tj 0.9524 0 Td (someone's)Tj 4.388 0 Td (full)Tj 1.5251 0 Td (history)Tj 3.0103 0 Td (of)Tj 1.0148 0 Td (all)Tj 1.1452 0 Td (social)Tj 2.4605 0 Td (media)Tj 2.6929 0 Td (posts.)Tj -34.1459 -1.2983 Td (Privacy)Tj 3.1804 0 Td (depends)Tj 3.5546 0 Td (on)Tj 1.2756 0 Td (the)Tj 1.4683 0 Td (nature)Tj 2.8233 0 Td (of)Tj 1.0148 0 Td (the)Tj 1.4684 0 Td (data,)Tj 2.1373 0 Td (the)Tj 1.474 0 Td (context)Tj 3.1804 0 Td (in)Tj 1.0375 0 Td (which)Tj 2.6476 0 Td (they)Tj 1.9275 0 Td (were)Tj 2.1203 0 Td (created)Tj 3.1238 0 Td (and)Tj -32.4338 -1.2983 Td (obtained,)Tj 3.9514 0 Td (and)Tj 1.7235 0 Td (the)Tj 1.4626 0 Td (expectations)Tj 5.1761 0 Td (and)Tj 1.7234 0 Td (norms)Tj 2.8233 0 Td (of)Tj 1.0035 0 Td (those)Tj 2.33 0 Td (who)Tj 1.9333 0 Td (are)Tj 1.4399 0 Td (affected.)Tj 3.5433 0 Td (Understand)Tj 4.989 0 Td (that)Tj 1.7745 0 Td (your)Tj 2.0749 0 Td (atti-)Tj -35.9487 -1.3039 Td (tude)Tj 1.9955 0 Td (towards)Tj 3.4016 0 Td (acceptable)Tj 4.354 0 Td (use)Tj 1.5307 0 Td (and)Tj 1.7291 0 Td (privacy)Tj 3.1464 0 Td (may)Tj 1.9276 0 Td (not)Tj 1.5704 0 Td (correspond)Tj 4.7791 0 Td (with)Tj 2.0013 0 Td (those)Tj 2.3357 0 Td (whose)Tj 2.7213 0 Td (data)Tj 1.9162 0 Td (you)Tj 1.7064 0 Td (are)Tj -35.1153 -1.2982 Td (using,)Tj 2.6135 0 Td (as)Tj 1.0148 0 Td (privacy)Tj 3.1407 0 Td (preferences)Tj 4.7962 0 Td (differ)Tj 2.3868 0 Td (across)Tj 2.6759 0 Td (and)Tj 1.7291 0 Td (within)Tj 2.8176 0 Td (societies.)Tj -19.9784 -1.2983 Td (For)Tj 1.627 0 Td (example,)Tj 3.7758 0 Td (Tene)Tj 2.228 0 Td (and)Tj 1.7291 0 Td (Polonetsky)Tj 4.6204 0 Td ([)Tj 0.83 0.64 0.02 0 k (13)Tj 0 g (])Tj 1.8539 0 Td (explore)Tj 3.1748 0 Td (how)Tj 1.9389 0 Td (pushing)Tj 3.4355 0 Td (past)Tj 1.8255 0 Td (social)Tj 2.4605 0 Td (norms,)Tj 3.0557 0 Td (particularly)Tj -32.9213 -1.3039 Td (in)Tj 1.0318 0 Td (novel)Tj 2.4037 0 Td (situations)Tj 4.0876 0 Td (created)Tj 3.1237 0 Td (by)Tj 1.1792 0 Td (new)Tj 1.8765 0 Td (technologies,)Tj 5.4482 0 Td (is)Tj 0.8447 0 Td (perceived)Tj 4.0535 0 Td (by)Tj 1.1792 0 Td (individuals)Tj 4.6488 0 Td (as)Tj 1.0148 0 Td (ªcreepyº)Tj 3.6227 0 Td (even)Tj -34.5144 -1.2983 Td (when)Tj 2.4037 0 Td (they)Tj 1.9219 0 Td (do)Tj 1.2586 0 Td (not)Tj 1.5704 0 Td (violate)Tj 2.8516 0 Td (data)Tj 1.9162 0 Td (protection)Tj 4.3823 0 Td (regulations)Tj 4.6658 0 Td (or)Tj 1.0999 0 Td (privacy)Tj 3.1407 0 Td (laws.)Tj 2.1714 0 Td (Social)Tj 2.5738 0 Td (media)Tj 2.6929 0 Td (apps)Tj 2.0466 0 Td (that)Tj -34.6958 -1.2983 Td (utilize)Tj 2.6645 0 Td (users')Tj 2.4775 0 Td (locations)Tj 3.8097 0 Td (to)Tj 1.0261 0 Td (push)Tj 2.1544 0 Td (information,)Tj 5.3007 0 Td (corporate)Tj 4.0762 0 Td (tracking)Tj 3.5206 0 Td (of)Tj 1.0148 0 Td (individuals')Tj 4.8642 0 Td (social)Tj 2.4605 0 Td (media)Tj 2.6929 0 Td (and)Tj -36.0621 -1.3039 Td (private)Tj 2.9876 0 Td (communications)Tj 6.9676 0 Td (to)Tj 1.0261 0 Td (gain)Tj 1.9389 0 Td (customer)Tj 3.9515 0 Td (intelligence,)Tj 5.0229 0 Td (and)Tj 1.7348 0 Td (marketing)Tj 4.3427 0 Td (based)Tj 2.4718 0 Td (on)Tj 1.2755 0 Td (search)Tj 2.761 0 Td (pat-)Tj -34.4804 -1.2982 Td (terns)Tj 2.2223 0 Td (have)Tj 2.0579 0 Td (been)Tj 2.126 0 Td (perceived)Tj 4.0479 0 Td (by)Tj 1.1848 0 Td (some)Tj 2.3301 0 Td (to)Tj 1.0261 0 Td (be)Tj 1.1509 0 Td (ªcreepyº)Tj 3.617 0 Td (or)Tj 1.0998 0 Td (even)Tj 2.0693 0 Td (outright)Tj 3.4809 0 Td (breaches)Tj 3.6907 0 Td (of)Tj 1.0148 0 Td (privacy.)Tj 3.3675 0 Td (Like-)Tj -34.486 -1.2983 Td (wise,)Tj 2.1769 0 Td (distributing)Tj 4.9266 0 Td (health)Tj 2.6816 0 Td (records)Tj 3.2031 0 Td (is)Tj 0.8504 0 Td (a)Tj 0.652 0 Td (necessary)Tj 4.0195 0 Td (part)Tj 1.8425 0 Td (of)Tj 1.0148 0 Td (receiving)Tj 3.8664 0 Td (health)Tj 2.6759 0 Td (care,)Tj 2.0976 0 Td (but)Tj 1.5534 0 Td (this)Tj 1.6724 0 Td (same)Tj -33.2331 -1.3039 Td (sharing)Tj 3.1917 0 Td (brings)Tj 2.744 0 Td (new)Tj 1.8765 0 Td (ethical)Tj 2.8403 0 Td (concerns)Tj 3.8267 0 Td (when)Tj 2.4038 0 Td (it)Tj 0.7824 0 Td (goes)Tj 1.9785 0 Td (beyond)Tj 3.1861 0 Td (providers)Tj 4.0195 0 Td (to)Tj 1.0262 0 Td (marketers.)Tj -26.6795 -1.2983 Td (Privacy)Tj 3.1861 0 Td (also)Tj 1.7688 0 Td (goes)Tj 1.9786 0 Td (beyond)Tj 3.1861 0 Td (single)Tj 2.5341 0 Td (individuals)Tj 4.6488 0 Td (and)Tj 1.7292 0 Td (extends)Tj 3.2654 0 Td (to)Tj 1.0262 0 Td (groups)Tj 2.9707 0 Td ([)Tj 0.83 0.64 0.02 0 k (10)Tj 0 g (].)Tj 2.0749 0 Td (This)Tj 1.9899 0 Td (is)Tj 0.8447 0 Td (particularly)Tj -32.3997 -1.2982 Td (resonant)Tj 3.7133 0 Td (for)Tj 1.3833 0 Td (communities)Tj 5.4822 0 Td (who)Tj 1.9389 0 Td (have)Tj 2.0636 0 Td (been)Tj 2.1203 0 Td (on)Tj 1.2756 0 Td (the)Tj 1.4683 0 Td (receiving)Tj 3.8664 0 Td (end)Tj 1.7178 0 Td (of)Tj 1.0148 0 Td (discriminatory)Tj 6.1398 0 Td (data-driven)Tj -32.1843 -1.2983 Td (policies)Tj 3.2314 0 Td (historically,)Tj 4.8472 0 Td (such)Tj 2.0636 0 Td (as)Tj 1.0148 0 Td (the)Tj 1.4684 0 Td (practice)Tj 3.3789 0 Td (of)Tj 1.0148 0 Td (redlining)Tj 3.8834 0 Td ([)Tj 0.83 0.64 0.02 0 k (14)Tj 0 g (,)Tj 0.83 0.64 0.02 0 k 1.7405 0 Td (15)Tj 0 g (].)Tj 1.7347 0 Td (Other)Tj 2.5852 0 Td (examples)Tj 3.9118 0 Td (include)Tj 3.1861 0 Td (commu-)Tj -34.0608 -1.3039 Td (nity)Tj 1.7858 0 Td (mapsÐmade)Tj 5.4821 0 Td (to)Tj 1.0262 0 Td (identify)Tj 3.2938 0 Td (problematic)Tj 5.0343 0 Td (properties)Tj 4.2803 0 Td (or)Tj 1.0942 0 Td (an)Tj 1.2075 0 Td (assertion)Tj 3.7871 0 Td (of)Tj 1.0148 0 Td (land)Tj 1.9785 0 Td (rightsÐbeing)Tj -29.9846 -1.2983 Td (reused)Tj 2.8516 0 Td (by)Tj 1.1849 0 Td (others)Tj 2.7099 0 Td (to)Tj 1.0204 0 Td (identify)Tj 3.2995 0 Td (opportunities)Tj 5.6296 0 Td (for)Tj 1.3833 0 Td (redevelopment)Tj 6.2078 0 Td (or)Tj 1.0999 0 Td (exploitation)Tj 5.0003 0 Td ([)Tj 0.83 0.64 0.02 0 k (16)Tj 0 g (].)Tj 2.0749 0 Td (Thus,)Tj 2.4718 0 Td (reus-)Tj -34.9339 -1.2983 Td (ing)Tj 1.5023 0 Td (a)Tj 0.652 0 Td (seemingly)Tj 4.2292 0 Td (public)Tj 2.7099 0 Td (dataset)Tj 2.9934 0 Td (could)Tj 2.4548 0 Td (run)Tj 1.6668 0 Td (counter)Tj 3.3221 0 Td (to)Tj 1.0205 0 Td (the)Tj 1.474 0 Td (original)Tj 3.3222 0 Td (privacy)Tj 3.1464 0 Td (intents)Tj 2.965 0 Td (of)Tj 1.0148 0 Td (those)Tj 2.3358 0 Td (who)Tj -34.8092 -1.3039 Td (created)Tj 3.118 0 Td (it)Tj 0.7881 0 Td (and)Tj 1.7291 0 Td (raise)Tj 2.0806 0 Td (questions)Tj 4.0195 0 Td (about)Tj 2.4945 0 Td (whether)Tj 3.4752 0 Td (it)Tj 0.7824 0 Td (represents)Tj 4.32 0 Td (responsible)Tj 4.7565 0 Td (big)Tj 1.457 0 Td (data)Tj 1.9162 0 Td (research.)Tj -29.7409 -1.2982 Td (Situate)Tj 2.948 0 Td (and)Tj 1.7291 0 Td (contextualize)Tj 5.4935 0 Td (your)Tj 2.0863 0 Td (data)Tj 1.9105 0 Td (to)Tj 1.0262 0 Td (anticipate)Tj 4.1329 0 Td (privacy)Tj 3.1407 0 Td (breaches)Tj 3.6907 0 Td (and)Tj 1.7291 0 Td (minimize)Tj 4.0479 0 Td (harm.)Tj 2.5965 0 Td (The)Tj -35.7276 -1.2983 Td (availability)Tj 4.5297 0 Td (or)Tj 1.0998 0 Td (perceived)Tj 4.0479 0 Td (publicness)Tj 4.405 0 Td (of)Tj 1.0148 0 Td (data)Tj 1.9162 0 Td (does)Tj 2.0352 0 Td (not)Tj 1.5761 0 Td (guarantee)Tj 4.1442 0 Td (lack)Tj 1.8199 0 Td (of)Tj 1.0148 0 Td (harm,)Tj 2.6021 0 Td (nor)Tj 1.6441 0 Td (does)Tj 2.041 0 Td (it)Tj 0.7823 0 Td (mean)Tj -34.6731 -1.3039 Td (that)Tj 1.7744 0 Td (data)Tj 1.9162 0 Td (creators)Tj 3.4129 0 Td (consent)Tj 3.3279 0 Td (to)Tj 1.0261 0 Td (researchers)Tj 4.7112 0 Td (using)Tj 2.3867 0 Td (their)Tj 2.109 0 Td (data.)Tj ET Q q 1 j 1 J 0 w 11.9999 0 0 11.9999 200.0125 360 cm BT /F2 1 Tf 1 TL -0.0033 Tc 0 0 Td (3.)Tj 1.0441 0 Td (Guard)Tj 3.1512 0 Td (against)Tj 3.6898 0 Td (the)Tj 1.7102 0 Td (reidentification)Tj 7.3371 0 Td (of)Tj 1.1622 0 Td (your)Tj 2.3717 0 Td (data)Tj ET Q q 1 j 1 J 0 w 10 0 0 10 200.0125 342.9921 cm BT /F6 1 Tf 1 TL -0.004 Tc 0 0 Td (It)Tj 0.856 0 Td (is)Tj 0.8504 0 Td (problematic)Tj 5.0343 0 Td (to)Tj 1.0261 0 Td (assume)Tj 3.1465 0 Td (that)Tj 1.7744 0 Td (data)Tj 1.9163 0 Td (cannot)Tj 2.982 0 Td (be)Tj 1.1508 0 Td (reidentified.)Tj 5.074 0 Td (There)Tj 2.5795 0 Td (are)Tj 1.4514 0 Td (numerous)Tj 4.3029 0 Td (examples)Tj 3.9118 0 Td (of)Tj -36.0564 -1.2982 Td (researchers)Tj 4.7111 0 Td (with)Tj 1.9956 0 Td (good)Tj 2.228 0 Td (intentions)Tj 4.286 0 Td (and)Tj 1.7347 0 Td (seemingly)Tj 4.2293 0 Td (good)Tj 2.2224 0 Td (methods)Tj 3.685 0 Td (failing)Tj 2.7382 0 Td (to)Tj 1.0262 0 Td (anonymize)Tj 4.6374 0 Td (data)Tj 1.9106 0 Td (suffi-)Tj -35.4045 -1.2983 Td (ciently)Tj 2.8799 0 Td (to)Tj 1.0262 0 Td (prevent)Tj 3.2598 0 Td (the)Tj 1.4683 0 Td (later)Tj 1.9899 0 Td (identification)Tj 5.5899 0 Td (of)Tj 1.0148 0 Td (specific)Tj 3.1918 0 Td (individuals)Tj 4.6488 0 Td ([)Tj 0.83 0.64 0.02 0 k (17)Tj 0 g (];)Tj 2.0749 0 Td (in)Tj 1.0375 0 Td (other)Tj 2.3471 0 Td (cases,)Tj 2.4491 0 Td (these)Tj -32.978 -1.3039 Td (efforts)Tj 2.7552 0 Td (were)Tj 2.1203 0 Td (extremely)Tj 4.1499 0 Td (superficial)Tj 4.3427 0 Td ([)Tj 0.83 0.64 0.02 0 k (18)Tj 0 g (,)Tj 0.83 0.64 0.02 0 k 1.7404 0 Td (19)Tj 0 g (].)Tj 1.7348 0 Td (When)Tj 2.6872 0 Td (datasets)Tj 3.3562 0 Td (thought)Tj 3.3732 0 Td (to)Tj 1.0262 0 Td (be)Tj 1.1508 0 Td (anonymized)Tj 5.1647 0 Td (are)Tj 1.4457 0 Td (com-)Tj -35.0473 -1.2983 Td (bined)Tj 2.4944 0 Td (with)Tj 1.9956 0 Td (other)Tj 2.3471 0 Td (variables,)Tj 3.9458 0 Td (it)Tj 0.7823 0 Td (may)Tj 1.9276 0 Td (result)Tj 2.4491 0 Td (in)Tj 1.0375 0 Td (unexpected)Tj 4.7962 0 Td (reidentification,)Tj 6.6103 0 Td (much)Tj 2.5172 0 Td (like)Tj 1.6554 0 Td (a)Tj 0.6519 0 Td (chemical)Tj -33.2104 -1.2982 Td (reaction)Tj 3.4922 0 Td (resulting)Tj 3.7304 0 Td (from)Tj 2.2053 0 Td (the)Tj 1.4684 0 Td (addition)Tj 3.5999 0 Td (of)Tj 1.0148 0 Td (a)Tj 0.652 0 Td (final)Tj 2.0012 0 Td (ingredient.)Tj -16.968 -1.3039 Td (While)Tj 2.6532 0 Td (the)Tj 1.4683 0 Td (identificatory)Tj 5.5956 0 Td (power)Tj 2.7269 0 Td (of)Tj 1.0148 0 Td (birthdate,)Tj 4.0988 0 Td (gender,)Tj 3.2032 0 Td (and)Tj 1.7291 0 Td (zip)Tj 1.4173 0 Td (code)Tj 2.1033 0 Td (is)Tj 0.8504 0 Td (well)Tj 1.8198 0 Td (known)Tj 3.0047 0 Td ([)Tj 0.83 0.64 0.02 0 k (20)Tj 0 g (],)Tj 2.075 0 Td (there)Tj -34.9566 -1.2983 Td (are)Tj 1.4456 0 Td (a)Tj 0.6576 0 Td (number)Tj 3.4186 0 Td (of)Tj 1.0148 0 Td (other)Tj 2.3471 0 Td (parametersÐparticularly)Tj 10.1876 0 Td (the)Tj 1.4684 0 Td (metadata)Tj 3.8891 0 Td (associated)Tj 4.2576 0 Td (with)Tj 1.9956 0 Td (digital)Tj 2.7269 0 Td (activityÐ)Tj -33.4089 -1.2983 Td (that)Tj 1.7744 0 Td (may)Tj 1.9332 0 Td (be)Tj 1.1509 0 Td (as)Tj 1.0148 0 Td (or)Tj 1.0942 0 Td (even)Tj 2.0692 0 Td (more)Tj 2.3414 0 Td (useful)Tj 2.5966 0 Td (for)Tj 1.3889 0 Td (identifying)Tj 4.5751 0 Td (individuals)Tj 4.6488 0 Td ([)Tj 0.83 0.64 0.02 0 k (21)Tj 0 g (].)Tj 2.075 0 Td (Surprising)Tj 4.3936 0 Td (to)Tj 1.0262 0 Td (many,)Tj 2.6985 0 Td (unla-)Tj -34.7808 -1.3039 Td (beled)Tj 2.347 0 Td (network)Tj 3.549 0 Td (graphsÐsuch)Tj 5.6579 0 Td (as)Tj 1.0148 0 Td (location)Tj 3.4526 0 Td (and)Tj 1.7291 0 Td (movement,)Tj 4.7452 0 Td (DNA)Tj 2.3754 0 Td (profiles,)Tj 3.4242 0 Td (call)Tj 1.5704 0 Td (records)Tj 3.2088 0 Td (from)Tj -33.0744 -1.2983 Td (mobile)Tj 2.9876 0 Td (phone)Tj 2.744 0 Td (data,)Tj 2.1429 0 Td (and)Tj 1.7292 0 Td (even)Tj 2.0692 0 Td (high-resolution)Tj 6.4176 0 Td (satellite)Tj 3.2145 0 Td (images)Tj 2.9934 0 Td (of)Tj 1.0148 0 Td (the)Tj 1.4683 0 Td (earthÐcan)Tj 4.6034 0 Td (be)Tj 1.1509 0 Td (used)Tj 2.0636 0 Td (to)Tj -34.5994 -1.2982 Td (reidentify)Tj 4.0875 0 Td (people)Tj 2.8573 0 Td ([)Tj 0.83 0.64 0.02 0 k (22)Tj 0 g (].)Tj 2.0749 0 Td (More)Tj 2.4151 0 Td (important)Tj 4.2803 0 Td (than)Tj 2.0296 0 Td (specifying)Tj 4.235 0 Td (the)Tj 1.4683 0 Td (variables)Tj 3.719 0 Td (that)Tj 1.7802 0 Td (allow)Tj 2.3357 0 Td (for)Tj 1.389 0 Td (reidentifica-)Tj -32.6719 -1.3039 Td (tion,)Tj 2.0636 0 Td (however,)Tj 3.838 0 Td (is)Tj 0.8504 0 Td (the)Tj 1.4684 0 Td (realization)Tj 4.4277 0 Td (that)Tj 1.7801 0 Td (it)Tj 0.788 0 Td (is)Tj 0.8448 0 Td (difficult)Tj 3.3562 0 Td (to)Tj 1.0261 0 Td (recognize)Tj 4.0592 0 Td (these)Tj 2.2563 0 Td (vulnerable)Tj 4.3994 0 Td (points)Tj 2.7212 0 Td (a)Tj 0.652 0 Td (priori)Tj -34.5314 -1.2983 Td ([)Tj 0.83 0.64 0.02 0 k (23)Tj 0 g (].)Tj 2.0749 0 Td (Factors)Tj 3.1408 0 Td (discounted)Tj 4.6374 0 Td (today)Tj 2.4378 0 Td (as)Tj 1.0148 0 Td (irrelevant)Tj 4.0592 0 Td (or)Tj 1.0998 0 Td (inherently)Tj 4.3313 0 Td (harmlessÐsuch)Tj 6.5253 0 Td (as)Tj 1.0148 0 Td (battery)Tj 3.0104 0 Td (usageÐ)Tj -33.3465 -1.2983 Td (may)Tj 1.9275 0 Td (very)Tj 1.9219 0 Td (well)Tj 1.8198 0 Td (prove)Tj 2.4945 0 Td (to)Tj 1.0261 0 Td (be)Tj 1.1509 0 Td (a)Tj 0.6519 0 Td (significant)Tj 4.388 0 Td (vector)Tj 2.6986 0 Td (of)Tj 1.0148 0 Td (personal)Tj 3.634 0 Td (identification)Tj 5.5842 0 Td (tomorrow)Tj 4.286 0 Td ([)Tj 0.83 0.64 0.02 0 k (24)Tj 0 g (].)Tj 2.0749 0 Td (For)Tj -34.6731 -1.2982 Td (example,)Tj 3.7757 0 Td (the)Tj 1.4683 0 Td (addition)Tj 3.5943 0 Td (of)Tj 1.0148 0 Td (spatial)Tj 2.778 0 Td (location)Tj 3.4525 0 Td (can)Tj 1.6271 0 Td (turn)Tj 1.9672 0 Td (social)Tj 2.4548 0 Td (media)Tj 2.6929 0 Td (posts)Tj 2.2677 0 Td (into)Tj 1.8369 0 Td (a)Tj 0.6576 0 Td (means)Tj 2.8063 0 Td (of)Tj 1.0148 0 Td (identify-)Tj -33.4089 -1.3039 Td (ing)Tj 1.5023 0 Td (home)Tj 2.4945 0 Td (location)Tj 3.4526 0 Td ([)Tj 0.83 0.64 0.02 0 k (25)Tj 0 g (],)Tj 2.0749 0 Td (and)Tj 1.7291 0 Td (Google's)Tj 3.6624 0 Td (reverse)Tj 3.05 0 Td (image)Tj 2.6306 0 Td (search)Tj 2.7609 0 Td (can)Tj 1.6214 0 Td (connect)Tj 3.3959 0 Td (previously)Tj 4.3483 0 Td (separate)Tj -32.7229 -1.2983 Td (personal)Tj 3.6283 0 Td (activitiesÐsuch)Tj 6.4799 0 Td (as)Tj 1.0148 0 Td (dating)Tj 2.761 0 Td (and)Tj 1.7291 0 Td (professional)Tj 5.0513 0 Td (profilesÐin)Tj 4.9379 0 Td (unanticipated)Tj 5.7373 0 Td (ways)Tj 2.1543 0 Td ([)Tj 0.83 0.64 0.02 0 k (26)Tj 0 g (].)Tj 2.075 0 Td (Even)Tj -35.5689 -1.2983 Td (data)Tj 1.9048 0 Td (about)Tj 2.4888 0 Td (groupsÐªaggregate)Tj 8.056 0 Td (statisticsºÐcan)Tj 6.3042 0 Td (have)Tj 2.0523 0 Td (serious)Tj 3.0274 0 Td (implications)Tj 5.1704 0 Td (if)Tj 0.771 0 Td (they)Tj 1.9162 0 Td (reveal)Tj 2.5625 0 Td (that)Tj 1.7745 0 Td (cer-)Tj -36.0281 -1.3039 Td (tain)Tj 1.7688 0 Td (communities,)Tj 5.7089 0 Td (for)Tj 1.3833 0 Td (example,)Tj 3.7757 0 Td (suffer)Tj 2.4775 0 Td (from)Tj 2.2053 0 Td (stigmatized)Tj 4.7962 0 Td (diseases)Tj 3.3789 0 Td (or)Tj 1.0998 0 Td (social)Tj 2.4548 0 Td (behavior)Tj 3.7134 0 Td (much)Tj 2.5171 0 Td (more)Tj -35.2797 -1.2983 Td (than)Tj 2.0295 0 Td (others)Tj 2.7043 0 Td ([)Tj 0.83 0.64 0.02 0 k (27)Tj 0 g (].)Tj ET Q q 1 j 1 J 0 w 576 737.1 m 36 737.1 l 36 737.6 l 576 737.6 l f* 36 741.2598 169.285 23.6976 re W* n q 169.2283 0 0 23.6409 36 741.3165 cm q /I0 Do Q Q Q q 0 0 612 792 re W* n 1 j 1 J 0 w 7.9999 0 0 7.9999 391.9748 745.7952 cm BT /F0 1 Tf 1 TL -0.005 Tc 0 0 Td (Ten)Tj 1.9204 0 Td (Simple)Tj 3.2245 0 Td (Rules)Tj 2.7283 0 Td (for)Tj 1.3678 0 Td (Respons)Tj 3.8764 0 Td (ible)Tj 1.7362 0 Td (Big)Tj 1.637 0 Td (Data)Tj 2.3032 0 Td (Researc)Tj 3.6567 0 Td (h)Tj ET Q q 1 j 1 J 0 w 36 48.0002 m 576 48.0002 l 576 47.5002 l 36 47.5002 l f* 0.83 0.64 0.02 0 k 142.5827 34.9228 m 295.8236 34.9228 l h f* 0 g 7.9999 0 0 7.9999 36 36 cm BT /F0 1 Tf 1 TL -0.005 Tc 0 0 Td (PLOS)Tj 2.8559 0 Td (Computationa)Tj 6.1229 0 Td (l)Tj 0.4322 0 Td (Biology)Tj 3.4371 0 Td (|)Tj 0.83 0.64 0.02 0 k 0.4748 0 Td (https:/)Tj 2.6788 0 Td (/doi.org/10.13)Tj 6.0094 0 Td (71/journal.p)Tj 5.1166 0 Td (cbi.1005399)Tj 0 g 6.3497 0 Td (March)Tj 2.9551 0 Td (30,)Tj 1.5874 0 Td (2017)Tj 27.1207 0 Td (3)Tj 0.7654 0 Td (/)Tj ET endstream endobj 66 0 obj < >stream endstream endobj 67 0 obj < >stream endstream endobj 68 0 obj < >stream endstream endobj 69 0 obj < >stream endstream endobj 70 0 obj < >stream endstream endobj 71 0 obj < >stream endstream endobj 72 0 obj < >stream endstream endobj 73 0 obj < >stream endstream endobj 74 0 obj < >stream endstream endobj 75 0 obj < >stream endstream endobj 76 0 obj < >stream endstream endobj 77 0 obj < >stream endstream endobj 78 0 obj < >stream endstream endobj 79 0 obj < >stream endstream endobj 80 0 obj < >stream endstream endobj 81 0 obj < >stream endstream endobj 82 0 obj < >stream BT 66.402 0 Td (10)Tj ET Q endstream endobj 83 0 obj < >/XObject< >>>/CropBox[0 0 612 792]/MediaBox[0 0 612 792]/Parent 9 0 R/Annots 84 0 R/Contents 85 0 R/TrimBox[0 0 612 792]>> endobj 84 0 obj [86 0 R 87 0 R 88 0 R] endobj 86 0 obj < >/Border[0 0 0]/Dest(Rpcbi.1005399.ref028)>> endobj 87 0 obj < >/Border[0 0 0]/Dest(Rpcbi.1005399.ref029)>> endobj 88 0 obj < >/Border[0 0 0]/A 89 0 R>> endobj 89 0 obj < > endobj 85 0 obj [90 0 R 91 0 R 92 0 R 93 0 R 94 0 R] endobj 90 0 obj < >stream q 0.83 0.64 0.02 0 k 217.1339 544.1386 m 226.6583 544.1386 l h f* 291.8551 232.1008 m 301.3228 232.1008 l h f* 0 g 1 j 1 J 0 w 10 0 0 10 211.9748 707.4141 cm BT /F6 1 Tf 1 TL -0.004 Tc 0 0 Td (Identify)Tj 3.3732 0 Td (possible)Tj 3.4072 0 Td (vectors)Tj 3.0614 0 Td (of)Tj 1.0148 0 Td (reidentification)Tj 6.3779 0 Td (in)Tj 1.0375 0 Td (your)Tj 2.0806 0 Td (data.)Tj 2.1373 0 Td (Work)Tj 2.5625 0 Td (to)Tj 1.0261 0 Td (minimize)Tj 4.0422 0 Td (them)Tj 2.2904 0 Td (in)Tj 1.0375 0 Td (your)Tj -34.6448 -1.3039 Td (published)Tj 4.1215 0 Td (results)Tj 2.8119 0 Td (to)Tj 1.0262 0 Td (the)Tj 1.4683 0 Td (greatest)Tj 3.2938 0 Td (extent)Tj 2.6816 0 Td (possible.)Tj ET Q q 1 j 1 J 0 w 11.9999 0 0 11.9999 200.0125 666.1984 cm BT /F2 1 Tf 1 TL -0.0033 Tc 0 0 Td (4.)Tj 1.0441 0 Td (Practice)Tj 4.0677 0 Td (ethical)Tj 3.3591 0 Td (data)Tj 2.2583 0 Td (sharing)Tj ET Q q 1 j 1 J 0 w 10 0 0 10 200.0125 649.1905 cm BT /F6 1 Tf 1 TL -0.004 Tc 0 0 Td (For)Tj 1.627 0 Td (some)Tj 2.3301 0 Td (projects,)Tj 3.6 0 Td (sharing)Tj 3.1974 0 Td (data)Tj 1.9162 0 Td (is)Tj 0.8448 0 Td (an)Tj 1.2018 0 Td (expectation)Tj 4.8302 0 Td (of)Tj 1.0148 0 Td (the)Tj 1.4684 0 Td (human)Tj 3.0784 0 Td (participants)Tj 4.9549 0 Td (involved)Tj 3.651 0 Td (and)Tj 1.7291 0 Td (thus)Tj -35.4441 -1.2982 Td (a)Tj 0.6519 0 Td (key)Tj 1.5931 0 Td (part)Tj 1.8425 0 Td (of)Tj 1.0148 0 Td (ethical)Tj 2.8403 0 Td (research.)Tj 3.7814 0 Td (For)Tj 1.627 0 Td (example,)Tj 3.7758 0 Td (in)Tj 1.0318 0 Td (rare)Tj 1.8255 0 Td (genetic)Tj 3.067 0 Td (disease)Tj 3.0161 0 Td (research,)Tj 3.7814 0 Td (biological)Tj 4.0932 0 Td (samples)Tj -33.9418 -1.2983 Td (are)Tj 1.4456 0 Td (shared)Tj 2.8687 0 Td (in)Tj 1.0318 0 Td (the)Tj 1.474 0 Td (hope)Tj 2.194 0 Td (of)Tj 1.0148 0 Td (finding)Tj 3.1294 0 Td (cures,)Tj 2.5568 0 Td (making)Tj 3.2485 0 Td (dissemination)Tj 5.8507 0 Td (a)Tj 0.6519 0 Td (condition)Tj 4.1159 0 Td (of)Tj 1.0148 0 Td (participation.)Tj 5.5956 0 Td (In)Tj -36.1925 -1.3039 Td (other)Tj 2.347 0 Td (projects,)Tj 3.6057 0 Td (questions)Tj 4.0195 0 Td (of)Tj 1.0148 0 Td (the)Tj 1.4683 0 Td (larger)Tj 2.5342 0 Td (public)Tj 2.7042 0 Td (goodÐan)Tj 4.1329 0 Td (admittedly)Tj 4.5184 0 Td (difficult)Tj 3.3562 0 Td (to)Tj 1.0261 0 Td (define)Tj 2.6986 0 Td (category)Tj -33.4259 -1.2983 Td (Ðprovide)Tj 4.2122 0 Td (compelling)Tj 4.6828 0 Td (arguments)Tj 4.4674 0 Td (for)Tj 1.389 0 Td (sharing)Tj 3.1917 0 Td (data,)Tj 2.143 0 Td (e.g.,)Tj 1.7858 0 Td (the)Tj 1.4684 0 Td (NIH-sponsored)Tj 6.5366 0 Td (database)Tj 3.6397 0 Td (of)Tj 1.0148 0 Td (Geno-)Tj -34.5314 -1.2982 Td (types)Tj 2.2733 0 Td (and)Tj 1.7291 0 Td (Phenotypes)Tj 4.8416 0 Td (\(dbGaP\),)Tj 3.8664 0 Td (which)Tj 2.6532 0 Td (makes)Tj 2.7553 0 Td (deidentified)Tj 5.0059 0 Td (genomic)Tj 3.6794 0 Td (data)Tj 1.9105 0 Td (widely)Tj 2.8233 0 Td (available)Tj 3.668 0 Td (to)Tj -35.206 -1.3039 Td (researchers,)Tj 4.9379 0 Td (democratizing)Tj 5.981 0 Td (access,)Tj 2.8743 0 Td (or)Tj 1.0999 0 Td (the)Tj 1.4683 0 Td (justice)Tj 2.778 0 Td (claim)Tj 2.4094 0 Td (made)Tj 2.4264 0 Td (by)Tj 1.1792 0 Td (the)Tj 1.4684 0 Td (Institute)Tj 3.5829 0 Td (of)Tj 1.0205 0 Td (Medicine)Tj 3.9912 0 Td (about)Tj -35.2174 -1.2983 Td (the)Tj 1.4626 0 Td (value)Tj 2.3017 0 Td (of)Tj 1.0092 0 Td (mandating)Tj 4.558 0 Td (that)Tj 1.7745 0 Td (individual-level)Tj 6.429 0 Td (data)Tj 1.9105 0 Td (from)Tj 2.1997 0 Td (clinical)Tj 3.0727 0 Td (trials)Tj 2.194 0 Td (be)Tj 1.1452 0 Td (shared)Tj 2.863 0 Td (among)Tj 2.9877 0 Td (research-)Tj -33.9078 -1.2983 Td (ers)Tj 1.3719 0 Td ([)Tj 0.83 0.64 0.02 0 k (28)Tj 0 g (].)Tj 2.0806 0 Td (Asking)Tj 3.0387 0 Td (participants)Tj 4.9606 0 Td (for)Tj 1.3833 0 Td (broad,)Tj 2.795 0 Td (as)Tj 1.0148 0 Td (opposed)Tj 3.583 0 Td (to)Tj 1.0204 0 Td (narrowly)Tj 3.8381 0 Td (structured)Tj 4.354 0 Td (consent)Tj 3.3278 0 Td (for)Tj 1.389 0 Td (down-)Tj -34.1572 -1.3039 Td (stream)Tj 2.9253 0 Td (data)Tj 1.9162 0 Td (management)Tj 5.4311 0 Td (makes)Tj 2.7553 0 Td (it)Tj 0.788 0 Td (easier)Tj 2.5002 0 Td (to)Tj 1.0261 0 Td (share)Tj 2.3357 0 Td (data.)Tj 2.1373 0 Td (Careful)Tj 3.1748 0 Td (research)Tj 3.5603 0 Td (design)Tj 2.812 0 Td (and)Tj 1.7291 0 Td (guidance)Tj -33.0914 -1.2983 Td (from)Tj 2.2053 0 Td (IRBs)Tj 2.1203 0 Td (can)Tj 1.6271 0 Td (help)Tj 1.9332 0 Td (clarify)Tj 2.7042 0 Td (consent)Tj 3.3335 0 Td (processes.)Tj 4.1953 0 Td (However,)Tj 4.0762 0 Td (we)Tj 1.3266 0 Td (caution)Tj 3.2258 0 Td (that)Tj 1.7801 0 Td (even)Tj 2.0693 0 Td (when)Tj 2.4038 0 Td (broad)Tj 2.5682 0 Td (con-)Tj -35.5689 -1.2982 Td (sent)Tj 1.8481 0 Td (was)Tj 1.7008 0 Td (obtained)Tj 3.7304 0 Td (upfront,)Tj 3.5092 0 Td (researchers)Tj 4.7112 0 Td (should)Tj 2.914 0 Td (consider)Tj 3.651 0 Td (the)Tj 1.4683 0 Td (best)Tj 1.8085 0 Td (interests)Tj 3.5773 0 Td (of)Tj 1.0148 0 Td (the)Tj 1.4684 0 Td (human)Tj 3.0784 0 Td (partici-)Tj -34.4804 -1.3039 Td (pant,)Tj 2.245 0 Td (proactively)Tj 4.6147 0 Td (considering)Tj 4.9323 0 Td (the)Tj 1.4683 0 Td (likelihood)Tj 4.235 0 Td (of)Tj 1.0148 0 Td (privacy)Tj 3.1407 0 Td (breaches)Tj 3.6964 0 Td (and)Tj 1.7291 0 Td (reidentification)Tj 6.3779 0 Td (issues.)Tj -33.4542 -1.2983 Td (This)Tj 1.9842 0 Td (is)Tj 0.8504 0 Td (of)Tj 1.0148 0 Td (particular)Tj 4.1102 0 Td (concern)Tj 3.4639 0 Td (for)Tj 1.389 0 Td (human)Tj 3.0784 0 Td (DNA)Tj 2.3754 0 Td (data,)Tj 2.1373 0 Td (which)Tj 2.6532 0 Td (is)Tj 0.8447 0 Td (uniquely)Tj 3.7247 0 Td (identifiable.)Tj -26.43 -1.2983 Td (These)Tj 2.5681 0 Td (types)Tj 2.2734 0 Td (of)Tj 1.0148 0 Td (projects,)Tj 3.6057 0 Td (howeverÐin)Tj 5.3517 0 Td (which)Tj 2.6532 0 Td (rules)Tj 2.1487 0 Td (of)Tj 1.0148 0 Td (use)Tj 1.5307 0 Td (and)Tj 1.7291 0 Td (sharing)Tj 3.1975 0 Td (are)Tj 1.4456 0 Td (well)Tj 1.8199 0 Td (governed)Tj 3.9458 0 Td (by)Tj -35.4952 -1.2982 Td (informed)Tj 3.9741 0 Td (consent)Tj 3.3279 0 Td (and)Tj 1.7291 0 Td (right)Tj 2.1486 0 Td (of)Tj 1.0148 0 Td (withdrawalÐare)Tj 6.8485 0 Td (increasingly)Tj 5.0343 0 Td (the)Tj 1.4683 0 Td (exception)Tj 4.0932 0 Td (rather)Tj 2.6476 0 Td (than)Tj 2.0296 0 Td (the)Tj 1.4683 0 Td (rule)Tj -35.7843 -1.3039 Td (for)Tj 1.3832 0 Td (big)Tj 1.4627 0 Td (data.)Tj 2.1373 0 Td (In)Tj 1.1112 0 Td (our)Tj 1.6214 0 Td (digital)Tj 2.7269 0 Td (society,)Tj 3.1805 0 Td (we)Tj 1.3266 0 Td (are)Tj 1.4513 0 Td (followed)Tj 3.651 0 Td (by)Tj 1.1792 0 Td (data)Tj 1.9162 0 Td (clouds)Tj 2.812 0 Td (composed)Tj 4.3029 0 Td (of)Tj 1.0148 0 Td (the)Tj 1.474 0 Td (trace)Tj 2.1714 0 Td (ele-)Tj -34.9226 -1.2983 Td (ments)Tj 2.6702 0 Td (of)Tj 1.0148 0 Td (daily)Tj 2.1486 0 Td (lifeÐcredit)Tj 4.6828 0 Td (card)Tj 1.9729 0 Td (transactions,)Tj 5.3008 0 Td (medical)Tj 3.3618 0 Td (test)Tj 1.5988 0 Td (results,)Tj 3.033 0 Td (closed-circuit)Tj 5.6409 0 Td (television)Tj -31.4246 -1.2983 Td (\(CCTV\))Tj 3.5319 0 Td (images)Tj 2.9877 0 Td (and)Tj 1.7348 0 Td (video,)Tj 2.6249 0 Td (smart)Tj 2.5001 0 Td (phone)Tj 2.7439 0 Td (apps,)Tj 2.2791 0 Td (etc.Ðcollected)Tj 6.032 0 Td (under)Tj 2.6192 0 Td (mandatory)Tj 4.6148 0 Td (terms)Tj 2.4945 0 Td (of)Tj 1.0148 0 Td (ser-)Tj -35.1777 -1.3039 Td (vice)Tj 1.7858 0 Td (rather)Tj 2.6475 0 Td (than)Tj 2.0296 0 Td (responsible)Tj 4.7508 0 Td (research)Tj 3.5603 0 Td (design)Tj 2.812 0 Td (overseen)Tj 3.7304 0 Td (by)Tj 1.1848 0 Td (university)Tj 4.1896 0 Td (compliance)Tj 4.8302 0 Td (officers.)Tj 3.3789 0 Td (While)Tj -34.8999 -1.2983 Td (we)Tj 1.3266 0 Td (might)Tj 2.5965 0 Td (wish)Tj 2.0636 0 Td (to)Tj 1.0204 0 Td (have)Tj 2.0637 0 Td (the)Tj 1.4683 0 Td (standards)Tj 4.0819 0 Td (of)Tj 1.0148 0 Td (informed)Tj 3.9741 0 Td (consent)Tj 3.3335 0 Td (and)Tj 1.7292 0 Td (right)Tj 2.1486 0 Td (of)Tj 1.0148 0 Td (withdrawal,)Tj 4.9209 0 Td (these)Tj 2.2507 0 Td (in-)Tj -35.0076 -1.2982 Td (formal)Tj 2.8856 0 Td (big)Tj 1.457 0 Td (data)Tj 1.9162 0 Td (sources)Tj 3.1918 0 Td (are)Tj 1.4513 0 Td (gathered)Tj 3.6907 0 Td (by)Tj 1.1792 0 Td (agents)Tj 2.7553 0 Td (other)Tj 2.347 0 Td (than)Tj 2.0296 0 Td (the)Tj 1.4684 0 Td (researcherÐprivate)Tj 8.039 0 Td (software)Tj -32.4111 -1.3039 Td (companies,)Tj 4.7451 0 Td (state)Tj 2.0353 0 Td (agencies,)Tj 3.7927 0 Td (and)Tj 1.7291 0 Td (telecommunications)Tj 8.3622 0 Td (firms.)Tj 2.5568 0 Td (These)Tj 2.5625 0 Td (data)Tj 1.9162 0 Td (are)Tj 1.4514 0 Td (only)Tj 1.9729 0 Td (accessible)Tj 4.0932 0 Td (to)Tj -35.2174 -1.2983 Td (researchers)Tj 4.7111 0 Td (after)Tj 2.0353 0 Td (their)Tj 2.1089 0 Td (creation,)Tj 3.7191 0 Td (making)Tj 3.2484 0 Td (it)Tj 0.7881 0 Td (impossible)Tj 4.4957 0 Td (to)Tj 1.0261 0 Td (gain)Tj 1.9332 0 Td (informed)Tj 3.9742 0 Td (consent)Tj 3.3278 0 Td (a)Tj 0.652 0 Td (priori,)Tj 2.7496 0 Td (and)Tj -34.7695 -1.2983 Td (contacting)Tj 4.4333 0 Td (the)Tj 1.4683 0 Td (human)Tj 3.0785 0 Td (participants)Tj 4.9605 0 Td (retroactively)Tj 5.1931 0 Td (for)Tj 1.3833 0 Td (permission)Tj 4.6658 0 Td (is)Tj 0.8447 0 Td (often)Tj 2.2847 0 Td (forbidden)Tj 4.1896 0 Td (by)Tj 1.1792 0 Td (the)Tj -33.681 -1.2982 Td (owner)Tj 2.7552 0 Td (of)Tj 1.0148 0 Td (the)Tj 1.4683 0 Td (data)Tj 1.9106 0 Td (or)Tj 1.0998 0 Td (is)Tj 0.8504 0 Td (impossible)Tj 4.4957 0 Td (to)Tj 1.0262 0 Td (do)Tj 1.2529 0 Td (at)Tj 0.9524 0 Td (scale.)Tj -15.6301 -1.304 Td (Of)Tj 1.2585 0 Td (course,)Tj 3.0558 0 Td (researchers)Tj 4.7111 0 Td (within)Tj 2.8176 0 Td (software)Tj 3.583 0 Td (companies)Tj 4.5241 0 Td (and)Tj 1.7291 0 Td (state)Tj 2.0353 0 Td (institutions)Tj 4.7678 0 Td (collecting)Tj 4.0705 0 Td (these)Tj -33.749 -1.2982 Td (data)Tj 1.9105 0 Td (have)Tj 2.0636 0 Td (a)Tj 0.652 0 Td (special)Tj 2.8913 0 Td (responsibility)Tj 5.6182 0 Td (to)Tj 1.0262 0 Td (address)Tj 3.2258 0 Td (the)Tj 1.4683 0 Td (terms)Tj 2.4945 0 Td (under)Tj 2.6192 0 Td (which)Tj 2.6532 0 Td (data)Tj 1.9105 0 Td (are)Tj 1.4513 0 Td (collected;)Tj 3.9629 0 Td (but)Tj 1.5533 0 Td (that)Tj -35.5008 -1.2983 Td (does)Tj 2.0352 0 Td (not)Tj 1.5761 0 Td (exempt)Tj 3.1691 0 Td (the)Tj 1.4683 0 Td (end-user)Tj 3.7531 0 Td (of)Tj 1.0148 0 Td (shared)Tj 2.8686 0 Td (data.)Tj 2.1373 0 Td (In)Tj 1.1055 0 Td (short,)Tj 2.5115 0 Td (the)Tj 1.474 0 Td (burden)Tj 3.1238 0 Td (of)Tj 1.0148 0 Td (ethical)Tj 2.8403 0 Td (use)Tj 1.5307 0 Td (\(see)Tj 1.7688 0 Td (Rules)Tj 2.3981 0 Td (1)Tj 0.6973 0 Td (to)Tj -36.4873 -1.3039 Td (3\))Tj 1.0318 0 Td (and)Tj 1.7347 0 Td (sharing)Tj 3.1918 0 Td (is)Tj 0.8504 0 Td (placed)Tj 2.7893 0 Td (on)Tj 1.2756 0 Td (the)Tj 1.474 0 Td (researcher,)Tj 4.5751 0 Td (since)Tj 2.2393 0 Td (the)Tj 1.474 0 Td (terms)Tj 2.4945 0 Td (of)Tj 1.0148 0 Td (service)Tj 2.9424 0 Td (under)Tj 2.6191 0 Td (which)Tj 2.6476 0 Td (the)Tj 1.474 0 Td (human)Tj -33.8284 -1.2983 Td (subjects')Tj 3.5943 0 Td (data)Tj 1.9105 0 Td (were)Tj 2.126 0 Td (produced)Tj 4.0421 0 Td (can)Tj 1.6271 0 Td (often)Tj 2.2847 0 Td (be)Tj 1.1509 0 Td (extremely)Tj 4.1442 0 Td (broad)Tj 2.5682 0 Td (with)Tj 1.9956 0 Td (little)Tj 2.0012 0 Td (protection)Tj 4.3824 0 Td (for)Tj 1.3889 0 Td (breaches)Tj -33.2161 -1.2982 Td (of)Tj 1.0147 0 Td (privacy.)Tj 3.3676 0 Td (In)Tj 1.1055 0 Td (these)Tj 2.2564 0 Td (circumstances,)Tj 6.1284 0 Td (researchers)Tj 4.7112 0 Td (must)Tj 2.228 0 Td (balance)Tj 3.2371 0 Td (the)Tj 1.4684 0 Td (requirements)Tj 5.5615 0 Td (from)Tj 2.2053 0 Td (funding)Tj -33.2841 -1.3039 Td (agencies)Tj 3.5659 0 Td (to)Tj 1.0261 0 Td (share)Tj 2.3358 0 Td (data)Tj 1.9162 0 Td ([)Tj 0.83 0.64 0.02 0 k (29)Tj 0 g (])Tj 1.8482 0 Td (with)Tj 1.9955 0 Td (their)Tj 2.109 0 Td (responsibilities)Tj 6.2135 0 Td (to)Tj 1.0261 0 Td (the)Tj 1.4684 0 Td (human)Tj 3.0784 0 Td (beings)Tj 2.7949 0 Td (behind)Tj 3.0217 0 Td (the)Tj 1.4684 0 Td (data)Tj 1.9162 0 Td (they)Tj -35.7843 -1.2983 Td (acquired.)Tj 3.9231 0 Td (A)Tj 0.9071 0 Td (researcher)Tj 4.3483 0 Td (needs)Tj 2.5058 0 Td (to)Tj 1.0261 0 Td (inform)Tj 3.0217 0 Td (funding)Tj 3.3902 0 Td (agencies)Tj 3.566 0 Td (about)Tj 2.4945 0 Td (possible)Tj 3.4129 0 Td (ethical)Tj 2.8403 0 Td (concerns)Tj -31.436 -1.2983 Td (before)Tj 2.7382 0 Td (the)Tj 1.4683 0 Td (research)Tj 3.5603 0 Td (begins)Tj 2.7893 0 Td (and)Tj 1.7291 0 Td (guard)Tj 2.5455 0 Td (against)Tj 3.0274 0 Td (reidentification)Tj 6.3836 0 Td (before)Tj 2.7382 0 Td (sharing.)Tj -25.7837 -1.3039 Td (Share)Tj 2.4548 0 Td (data)Tj 1.9162 0 Td (as)Tj 1.0148 0 Td (specified)Tj 3.719 0 Td (in)Tj 1.0318 0 Td (research)Tj 3.5546 0 Td (protocols,)Tj 4.1839 0 Td (but)Tj 1.5534 0 Td (proactively)Tj 4.6204 0 Td (address)Tj 3.2259 0 Td (concerns)Tj 3.8267 0 Td (of)Tj 1.0148 0 Td (potential)Tj -33.3125 -1.2983 Td (harm)Tj 2.3697 0 Td (from)Tj 2.2053 0 Td (informally)Tj 4.4051 0 Td (collected)Tj 3.7417 0 Td (big)Tj 1.457 0 Td (data.)Tj ET Q q 1 j 1 J 0 w 11.9999 0 0 11.9999 200.0125 153.0141 cm BT /F2 1 Tf 1 TL -0.0033 Tc 0 0 Td (5.)Tj 1.0441 0 Td (Consider)Tj 4.526 0 Td (the)Tj 1.7055 0 Td (strengths)Tj 4.7386 0 Td (and)Tj 1.989 0 Td (limitations)Tj 5.1922 0 Td (of)Tj 1.1575 0 Td (your)Tj 2.3716 0 Td (data;)Tj 2.589 0 Td (big)Tj 1.7197 0 Td (does)Tj 2.5371 0 Td (not)Tj -29.5703 -1.1669 Td (automatically)Tj 6.5575 0 Td (mean)Tj 2.8063 0 Td (better)Tj ET Q q 1 j 1 J 0 w 10 0 0 10 200.0125 122.0031 cm BT /F6 1 Tf 1 TL -0.004 Tc 0 0 Td (In)Tj 1.1055 0 Td (order)Tj 2.4207 0 Td (to)Tj 1.0262 0 Td (do)Tj 1.2585 0 Td (both)Tj 2.058 0 Td (accurate)Tj 3.5546 0 Td (and)Tj 1.7291 0 Td (responsible)Tj 4.7565 0 Td (big)Tj 1.457 0 Td (data)Tj 1.9162 0 Td (research,)Tj 3.7814 0 Td (it)Tj 0.7824 0 Td (is)Tj 0.8504 0 Td (important)Tj 4.2803 0 Td (to)Tj 1.0261 0 Td (ground)Tj 3.1691 0 Td (data-)Tj -35.172 -1.2982 Td (sets)Tj 1.661 0 Td (in)Tj 1.0375 0 Td (their)Tj 2.109 0 Td (proper)Tj 2.9253 0 Td (context)Tj 3.1861 0 Td (including)Tj 4.0422 0 Td (conflicts)Tj 3.583 0 Td (of)Tj 1.0148 0 Td (interests.)Tj 3.7984 0 Td (Context)Tj 3.4242 0 Td (also)Tj 1.7688 0 Td (affects)Tj 2.7326 0 Td (every)Tj 2.3471 0 Td (stage)Tj 2.2053 0 Td (of)Tj -35.8353 -1.3039 Td (research:)Tj 3.7813 0 Td (from)Tj 2.2054 0 Td (data)Tj 1.9105 0 Td (acquisition,)Tj 4.8586 0 Td (to)Tj 1.0204 0 Td (cleaning,)Tj 3.8041 0 Td (to)Tj 1.0261 0 Td (interpretation)Tj 5.794 0 Td (of)Tj 1.0148 0 Td (findings,)Tj 3.7191 0 Td (and)Tj 1.7291 0 Td (dissemination)Tj 5.8506 0 Td (of)Tj -36.714 -1.2983 Td (the)Tj 1.4683 0 Td (results.)Tj 3.033 0 Td (During)Tj 3.1351 0 Td (the)Tj 1.474 0 Td (step)Tj 1.8199 0 Td (of)Tj 1.0148 0 Td (data)Tj 1.9105 0 Td (acquisition,)Tj 4.8529 0 Td (it)Tj 0.788 0 Td (is)Tj 0.8504 0 Td (crucial)Tj 2.9083 0 Td (to)Tj 1.0262 0 Td (understand)Tj 4.7905 0 Td (both)Tj 2.0579 0 Td (the)Tj 1.474 0 Td (source)Tj 2.829 0 Td (of)Tj ET Q q 1 j 1 J 0 w 576 737.1 m 36 737.1 l 36 737.6 l 576 737.6 l f* 36 741.2598 169.285 23.6976 re W* n q 169.2283 0 0 23.6409 36 741.3165 cm q /I0 Do Q Q Q q 0 0 612 792 re W* n 1 j 1 J 0 w 7.9999 0 0 7.9999 391.9748 745.7952 cm BT /F0 1 Tf 1 TL -0.005 Tc 0 0 Td (Ten)Tj 1.9204 0 Td (Simple)Tj 3.2245 0 Td (Rules)Tj 2.7283 0 Td (for)Tj 1.3678 0 Td (Respons)Tj 3.8764 0 Td (ible)Tj 1.7362 0 Td (Big)Tj 1.637 0 Td (Data)Tj 2.3032 0 Td (Researc)Tj 3.6567 0 Td (h)Tj ET Q q 1 j 1 J 0 w 36 48.0002 m 576 48.0002 l 576 47.5002 l 36 47.5002 l f* 0.83 0.64 0.02 0 k 142.5827 34.9228 m 295.8236 34.9228 l h f* 0 g 7.9999 0 0 7.9999 36 36 cm BT /F0 1 Tf 1 TL -0.005 Tc 0 0 Td (PLOS)Tj 2.8559 0 Td (Computationa)Tj 6.1229 0 Td (l)Tj 0.4322 0 Td (Biology)Tj 3.4371 0 Td (|)Tj 0.83 0.64 0.02 0 k 0.4748 0 Td (https:/)Tj 2.6788 0 Td (/doi.org/10.13)Tj 6.0094 0 Td (71/journal.p)Tj 5.1166 0 Td (cbi.1005399)Tj 0 g 6.3497 0 Td (March)Tj 2.9551 0 Td (30,)Tj 1.5874 0 Td (2017)Tj 27.1207 0 Td (4)Tj 0.7654 0 Td (/)Tj ET endstream endobj 91 0 obj < >stream endstream endobj 92 0 obj < >stream endstream endobj 93 0 obj < >stream endstream endobj 94 0 obj < >stream BT 66.402 0 Td (10)Tj ET Q endstream endobj 95 0 obj < >/XObject< >>>/CropBox[0 0 612 792]/MediaBox[0 0 612 792]/Parent 9 0 R/Annots 97 0 R/Contents 98 0 R/TrimBox[0 0 612 792]>> endobj 96 0 obj < > endobj 99 0 obj < > endobj 100 0 obj < >stream %!PS-AdobeFont-1.0 %%This font is a conversion from MinionPro-Bold with the following copyright notice: %\050c\051 2000, 2002 Adobe Systems Incorporated. (extracted from312.html)
uÁOÕ*Ý®ÅÓÙ_pÙ§½Û øäÖZDÀ#Ä3û UõàÚ^&eèXÖÔñ÷ X¨Þ Æ×Ì `0RâÚB¢Òï{§:M¾(¤í5óö fÄ¬ë=æ¨t% (*\YwÜÐ&(PP³ÂûD¨dÏ·ôqû9µD9ª7Òèì² éùEX<Ü:×;²0( ´0vh´ß´~ ½käF8Õ¯ÜÏ¨·L a6bé¨ns&Ù ¿åE9u{ÞÇeÂ*Vw'¯(5#¦@ì åÂw^^mµ£HqêõMKM^Ëí£ÕBÀgÎÏÇàAE&I÷.®ïG1bm®ÇùÏMyr ±Þú`Ð¿¼X=ø®üÑø5*µärºi¤Í/ã¿òu$ÙæµU8¹é&rgeJ\³Uy>]¿ÙûqØËUÆ×E>{F0é è ò'àÅ å`ÒQ§fj¬Ü°Ù3?º *U@*U(lóÒdÆ¶,0+¹_+Gð±v!^² üZ?øÃ$x¿Ì» VÐzè@ ?N¹?}|¼jpvÖ_NÜ¥­ÙzKZi ZkPR{Q§wð)=&ß'»T¶P×uô "YbnfBèæÎ Þ Ú!oXöghVØë6Ó¶(×dÝRéðE Õ¹äË0 tFµ R?É©äWÂ`ÝXöf}ïï­ùñEaHt:KwøÚ` ê.*ë<¹{ç¢á°vV/d¼q                                0000000000000000000000000000000000000000000000000000000000000000 0000000000000000000000000000000000000000000000000000000000000000 0000000000000000000000000000000000000000000000000000000000000000 0000000000000000000000000000000000000000000000000000000000000000 0000000000000000000000000000000000000000000000000000000000000000 0000000000000000000000000000000000000000000000000000000000000000 0000000000000000000000000000000000000000000000000000000000000000 cleartomark endstream endobj 97 0 obj [101 0 R 102 0 R] endobj 101 0 obj < >/Border[0 0 0]/Dest(Rpcbi.1005399.ref030)>> endobj 102 0 obj < >/Border[0 0 0]/A 103 0 R>> endobj 103 0 obj < > endobj 98 0 obj [104 0 R 105 0 R 106 0 R 107 0 R] endobj 104 0 obj < >stream q 0.83 0.64 0.02 0 k 488.0693 146.948 m 497.5937 146.948 l h f* 0 g 1 j 1 J 0 w 10 0 0 10 200.0125 707.4141 cm BT /F6 1 Tf 1 TL -0.004 Tc 0 0 Td (the)Tj 1.4683 0 Td (data)Tj 1.9162 0 Td (and)Tj 1.7291 0 Td (the)Tj 1.4684 0 Td (rules)Tj 2.1486 0 Td (and)Tj 1.7291 0 Td (regulations)Tj 4.6715 0 Td (with)Tj 1.9956 0 Td (which)Tj 2.6532 0 Td (they)Tj 1.9219 0 Td (were)Tj 2.1259 0 Td (gathered.)Tj 3.9175 0 Td (This)Tj 1.9842 0 Td (is)Tj 0.8504 0 Td (especially)Tj 4.0138 0 Td (impor-)Tj -34.5937 -1.3039 Td (tant)Tj 1.7971 0 Td (in)Tj 1.0375 0 Td (cases)Tj 2.2223 0 Td (of)Tj 1.0148 0 Td (research)Tj 3.5546 0 Td (conducted)Tj 4.4277 0 Td (in)Tj 1.0375 0 Td (relatively)Tj 3.8381 0 Td (loose)Tj 2.2677 0 Td (regulatory)Tj 4.3143 0 Td (environments,)Tj 6.0207 0 Td (in)Tj 1.0319 0 Td (which)Tj 2.6532 0 Td (use)Tj 1.5307 0 Td (of)Tj -36.7481 -1.2982 Td (answers)Tj 3.4015 0 Td (to)Tj 1.0261 0 Td (research)Tj 3.5547 0 Td (questions)Tj 4.0195 0 Td (may)Tj 1.9332 0 Td (conflict)Tj 3.2201 0 Td (with)Tj 1.9956 0 Td (the)Tj 1.4683 0 Td (expectations)Tj 5.1874 0 Td (of)Tj 1.0148 0 Td (those)Tj 2.3414 0 Td (who)Tj 1.9389 0 Td (provided)Tj 3.8154 0 Td (the)Tj -34.9169 -1.2983 Td (data.)Tj 2.1373 0 Td (One)Tj 1.9388 0 Td (possible)Tj 3.4129 0 Td (approach)Tj 3.9515 0 Td (might)Tj 2.5965 0 Td (be)Tj 1.1509 0 Td (the)Tj 1.4683 0 Td (ethical)Tj 2.8403 0 Td (norms)Tj 2.829 0 Td (employed)Tj 4.1385 0 Td (to)Tj 1.0262 0 Td (track)Tj 2.245 0 Td (the)Tj 1.4683 0 Td (provenance)Tj 4.8699 0 Td (of)Tj -36.0734 -1.2983 Td (artifacts,)Tj 3.6169 0 Td (often)Tj 2.2904 0 Td (in)Tj 1.0318 0 Td (cooperation)Tj 5.023 0 Td (and)Tj 1.7348 0 Td (collaboration)Tj 5.5162 0 Td (with)Tj 1.9955 0 Td (the)Tj 1.4684 0 Td (communities)Tj 5.4821 0 Td (from)Tj 2.2111 0 Td (which)Tj 2.6475 0 Td (they)Tj 1.9219 0 Td (come)Tj -34.9396 -1.3039 Td (\(e.g.,)Tj 2.1259 0 Td (archaeologists)Tj 5.845 0 Td (working)Tj 3.5603 0 Td (in)Tj 1.0318 0 Td (indigenous)Tj 4.6658 0 Td (communities)Tj 5.4822 0 Td (to)Tj 1.0261 0 Td (determine)Tj 4.3257 0 Td (the)Tj 1.4683 0 Td (disposition)Tj 4.6488 0 Td (of)Tj 1.0148 0 Td (mate-)Tj -35.1947 -1.2982 Td (rial)Tj 1.5363 0 Td (culture\).)Tj 3.6057 0 Td (In)Tj 1.1055 0 Td (a)Tj 0.6576 0 Td (similar)Tj 2.9877 0 Td (manner,)Tj 3.5886 0 Td (computer)Tj 4.1103 0 Td (scientists)Tj 3.8267 0 Td (use)Tj 1.5307 0 Td (data)Tj 1.9162 0 Td (lineage)Tj 3.0274 0 Td (techniques)Tj 4.5297 0 Td (to)Tj 1.0262 0 Td (track)Tj 2.2393 0 Td (the)Tj -35.6879 -1.2983 Td (evolution)Tj 3.9968 0 Td (of)Tj 1.0148 0 Td (a)Tj 0.6576 0 Td (dataset)Tj 2.9934 0 Td (and)Tj 1.7348 0 Td (often)Tj 2.2847 0 Td (to)Tj 1.0204 0 Td (trace)Tj 2.1714 0 Td (bugs)Tj 2.0806 0 Td (in)Tj 1.0375 0 Td (the)Tj 1.4683 0 Td (data.)Tj -19.2641 -1.3039 Td (Being)Tj 2.5058 0 Td (mindful)Tj 3.4469 0 Td (of)Tj 1.0148 0 Td (the)Tj 1.4683 0 Td (data's)Tj 2.4888 0 Td (context)Tj 3.1861 0 Td (provides)Tj 3.651 0 Td (the)Tj 1.4684 0 Td (foundation)Tj 4.6714 0 Td (for)Tj 1.389 0 Td (clarifying)Tj 3.9855 0 Td (when)Tj 2.4038 0 Td (your)Tj 2.0806 0 Td (data)Tj -34.9566 -1.2983 Td (and)Tj 1.7291 0 Td (analysis)Tj 3.3278 0 Td (are)Tj 1.4457 0 Td (working)Tj 3.5603 0 Td (and)Tj 1.7291 0 Td (when)Tj 2.4038 0 Td (they)Tj 1.9275 0 Td (are)Tj 1.4457 0 Td (not.)Tj 1.8028 0 Td (While)Tj 2.6475 0 Td (it)Tj 0.7881 0 Td (is)Tj 0.8447 0 Td (tempting)Tj 3.8607 0 Td (to)Tj 1.0262 0 Td (interpret)Tj 3.736 0 Td (findings)Tj -32.275 -1.2983 Td (based)Tj 2.4717 0 Td (on)Tj 1.2756 0 Td (big)Tj 1.457 0 Td (data)Tj 1.9162 0 Td (as)Tj 1.0148 0 Td (a)Tj 0.652 0 Td (clear)Tj 2.1203 0 Td (outcome,)Tj 3.9515 0 Td (a)Tj 0.6519 0 Td (key)Tj 1.5988 0 Td (step)Tj 1.8198 0 Td (within)Tj 2.8119 0 Td (scientific)Tj 3.7871 0 Td (research)Tj 3.5546 0 Td (is)Tj 0.8504 0 Td (clearly)Tj 2.8176 0 Td (articulating)Tj -32.7512 -1.3039 Td (what)Tj 2.1656 0 Td (data)Tj 1.9105 0 Td (or)Tj 1.0999 0 Td (an)Tj 1.2019 0 Td (indicator)Tj 3.8664 0 Td (represent)Tj 3.9571 0 Td (and)Tj 1.7292 0 Td (what)Tj 2.1656 0 Td (they)Tj 1.9219 0 Td (do)Tj 1.2586 0 Td (not.)Tj 1.7971 0 Td (Are)Tj 1.7008 0 Td (your)Tj 2.0806 0 Td (findings)Tj 3.4923 0 Td (as)Tj 1.0148 0 Td (clear-cut)Tj 3.719 0 Td (if)Tj -35.0813 -1.2982 Td (your)Tj 2.0806 0 Td (interpretation)Tj 5.7939 0 Td (of)Tj 1.0148 0 Td (a)Tj 0.652 0 Td (social)Tj 2.4605 0 Td (media)Tj 2.6929 0 Td (posting)Tj 3.1804 0 Td (switches)Tj 3.566 0 Td (from)Tj 2.211 0 Td (a)Tj 0.6519 0 Td (recording)Tj 4.1273 0 Td (of)Tj 1.0148 0 Td (fact)Tj 1.6611 0 Td (to)Tj 1.0261 0 Td (the)Tj 1.4683 0 Td (perfor-)Tj -33.6016 -1.2983 Td (mance)Tj 2.8686 0 Td (of)Tj 1.0148 0 Td (a)Tj 0.652 0 Td (social)Tj 2.4604 0 Td (identity?)Tj 3.6737 0 Td (Given)Tj 2.6305 0 Td (the)Tj 1.4684 0 Td (messy,)Tj 2.8686 0 Td (almost)Tj 2.8857 0 Td (organic)Tj 3.2371 0 Td (nature)Tj 2.8233 0 Td (of)Tj 1.0148 0 Td (many)Tj 2.4775 0 Td (datasets)Tj 3.3562 0 Td (derived)Tj -33.4316 -1.3039 Td (from)Tj 2.2053 0 Td (social)Tj 2.4604 0 Td (actions,)Tj 3.2825 0 Td (it)Tj 0.7824 0 Td (is)Tj 0.8504 0 Td (fundamental)Tj 5.3178 0 Td (that)Tj 1.7744 0 Td (researchers)Tj 4.7169 0 Td (be)Tj 1.1451 0 Td (sensitive)Tj 3.6284 0 Td (to)Tj 1.0261 0 Td (the)Tj 1.4683 0 Td (potential)Tj 3.7588 0 Td (multiple)Tj -32.4168 -1.2983 Td (meanings)Tj 4.0875 0 Td (of)Tj 1.0148 0 Td (data.)Tj -3.9061 -1.2982 Td (For)Tj 1.627 0 Td (example,)Tj 3.7758 0 Td (is)Tj 0.8504 0 Td (a)Tj 0.6519 0 Td (Facebook)Tj 4.0479 0 Td (post)Tj 1.8992 0 Td (or)Tj 1.0998 0 Td (an)Tj 1.2019 0 Td (Instagram)Tj 4.2916 0 Td (photo)Tj 2.5795 0 Td (best)Tj 1.8085 0 Td (interpreted)Tj 4.6942 0 Td (as)Tj 1.0148 0 Td (an)Tj 1.2018 0 Td (approval/dis-)Tj -31.9405 -1.2983 Td (approval)Tj 3.702 0 Td (of)Tj 1.0148 0 Td (a)Tj 0.6519 0 Td (phenomenon,)Tj 5.8167 0 Td (a)Tj 0.652 0 Td (simple)Tj 2.8573 0 Td (observation,)Tj 5.125 0 Td (or)Tj 1.0998 0 Td (an)Tj 1.2019 0 Td (effort)Tj 2.3924 0 Td (to)Tj 1.0261 0 Td (improve)Tj 3.583 0 Td (status)Tj 2.5002 0 Td (within)Tj 2.8119 0 Td (a)Tj 0.652 0 Td (friend)Tj -35.087 -1.3039 Td (network?)Tj 3.9174 0 Td (While)Tj 2.6532 0 Td (any)Tj 1.6554 0 Td (of)Tj 1.0148 0 Td (these)Tj 2.2564 0 Td (interpretations)Tj 6.1511 0 Td (are)Tj 1.4514 0 Td (potentially)Tj 4.4617 0 Td (valid,)Tj 2.3754 0 Td (the)Tj 1.4683 0 Td (lack)Tj 1.8199 0 Td (of)Tj 1.0148 0 Td (context)Tj 3.1861 0 Td (makes)Tj 2.7552 0 Td (it)Tj -36.1811 -1.2983 Td (even)Tj 2.0692 0 Td (more)Tj 2.3414 0 Td (difficult)Tj 3.3562 0 Td (to)Tj 1.0205 0 Td (justify)Tj 2.6759 0 Td (the)Tj 1.4683 0 Td (choice)Tj 2.795 0 Td (of)Tj 1.0148 0 Td (one)Tj 1.6951 0 Td (understanding)Tj 6.0717 0 Td (over)Tj 1.9786 0 Td (another.)Tj 3.5546 0 Td (Reflecting)Tj 4.2236 0 Td (on)Tj 1.2756 0 Td (the)Tj -35.5405 -1.2983 Td (potential)Tj 3.7587 0 Td (multiple)Tj 3.5659 0 Td (meanings)Tj 4.0876 0 Td (of)Tj 1.0148 0 Td (data)Tj 1.9162 0 Td (fosters)Tj 2.8289 0 Td (greater)Tj 3.0047 0 Td (clarity)Tj 2.7156 0 Td (in)Tj 1.0375 0 Td (research)Tj 3.5546 0 Td (hypotheses)Tj 4.6204 0 Td (and)Tj 1.7348 0 Td (also)Tj -33.8397 -1.3039 Td (makes)Tj 2.7552 0 Td (researchers)Tj 4.7112 0 Td (aware)Tj 2.5625 0 Td (of)Tj 1.0148 0 Td (the)Tj 1.474 0 Td (other)Tj 2.347 0 Td (potential)Tj 3.7588 0 Td (uses)Tj 1.8935 0 Td (of)Tj 1.0148 0 Td (their)Tj 2.1089 0 Td (data.)Tj 2.1374 0 Td (Again,)Tj 2.8459 0 Td (the)Tj 1.4684 0 Td (act)Tj 1.3776 0 Td (of)Tj 1.0148 0 Td (interpreta-)Tj -32.4848 -1.2982 Td (tion)Tj 1.8425 0 Td (is)Tj 0.8447 0 Td (a)Tj 0.6519 0 Td (human)Tj 3.0784 0 Td (process,)Tj 3.4129 0 Td (and)Tj 1.7292 0 Td (because)Tj 3.3221 0 Td (the)Tj 1.4684 0 Td (judgments)Tj 4.4447 0 Td (of)Tj 1.0148 0 Td (those)Tj 2.3414 0 Td (\(re\)using)Tj 3.8607 0 Td (your)Tj 2.0807 0 Td (data)Tj 1.9162 0 Td (may)Tj 1.9275 0 Td (differ)Tj -33.9361 -1.2983 Td (from)Tj 2.2053 0 Td (your)Tj 2.0806 0 Td (own,)Tj 2.1827 0 Td (it)Tj 0.788 0 Td (is)Tj 0.8447 0 Td (essential)Tj 3.583 0 Td (to)Tj 1.0261 0 Td (clarify)Tj 2.7043 0 Td (both)Tj 2.0636 0 Td (the)Tj 1.4683 0 Td (strengths)Tj 3.8721 0 Td (and)Tj 1.7348 0 Td (shortcomings)Tj 5.6749 0 Td (of)Tj 1.0148 0 Td (the)Tj 1.4684 0 Td (data.)Tj -31.5154 -1.3039 Td (Document)Tj 4.507 0 Td (the)Tj 1.4684 0 Td (provenance)Tj 4.8755 0 Td (and)Tj 1.7291 0 Td (evolution)Tj 3.9969 0 Td (of)Tj 1.0148 0 Td (your)Tj 2.0806 0 Td (data.)Tj 2.143 0 Td (Do)Tj 1.4626 0 Td (not)Tj 1.5704 0 Td (overstate)Tj 3.7927 0 Td (clarity;)Tj 2.9424 0 Td (acknowl-)Tj -32.7796 -1.2983 Td (edge)Tj 2.0579 0 Td (messiness)Tj 4.1442 0 Td (and)Tj 1.7348 0 Td (multiple)Tj 3.566 0 Td (meanings.)Tj ET Q q 1 j 1 J 0 w 11.9999 0 0 11.9999 200.0125 347.0173 cm BT /F2 1 Tf 1 TL -0.0033 Tc 0 0 Td (6.)Tj 1.0441 0 Td (Debate)Tj 3.5197 0 Td (the)Tj 1.7102 0 Td (tough,)Tj 3.2693 0 Td (ethical)Tj 3.3591 0 Td (choices)Tj ET Q q 1 j 1 J 0 w 10 0 0 10 200.0125 330.0094 cm BT /F6 1 Tf 1 TL -0.004 Tc 0 0 Td (Research)Tj 3.804 0 Td (involving)Tj 3.9798 0 Td (human)Tj 3.0784 0 Td (participants)Tj 4.955 0 Td (at)Tj 0.9524 0 Td (federally)Tj 3.6397 0 Td (funded)Tj 3.0614 0 Td (institutions)Tj 4.7678 0 Td (is)Tj 0.8504 0 Td (governed)Tj 3.9401 0 Td (by)Tj 1.1849 0 Td (IRBs)Tj -34.2139 -1.2982 Td (charged)Tj 3.3902 0 Td (with)Tj 2.0012 0 Td (preventing)Tj 4.5354 0 Td (harm)Tj 2.3754 0 Td (through)Tj 3.4413 0 Td (well-established)Tj 6.548 0 Td (procedures)Tj 4.6771 0 Td (and)Tj 1.7291 0 Td (are)Tj 1.4457 0 Td (familiar)Tj 3.3505 0 Td (to)Tj 1.0261 0 Td (many)Tj -34.52 -1.3039 Td (researchers.)Tj 4.9379 0 Td (IRBs,)Tj 2.347 0 Td (however,)Tj 3.8381 0 Td (are)Tj 1.4457 0 Td (not)Tj 1.576 0 Td (the)Tj 1.4684 0 Td (sole)Tj 1.7574 0 Td (arbiter)Tj 2.8914 0 Td (of)Tj 1.0148 0 Td (ethics;)Tj 2.7496 0 Td (many)Tj 2.4774 0 Td (ethical)Tj 2.8403 0 Td (issues)Tj 2.5172 0 Td (involving)Tj 3.9798 0 Td (big)Tj -35.841 -1.2983 Td (data)Tj 1.9105 0 Td (are)Tj 1.4513 0 Td (outside)Tj 3.1294 0 Td (of)Tj 1.0148 0 Td (their)Tj 2.109 0 Td (governance)Tj 4.8189 0 Td (mandate.)Tj 3.9344 0 Td (Precisely)Tj 3.7474 0 Td (because)Tj 3.3222 0 Td (big)Tj 1.457 0 Td (data)Tj 1.9162 0 Td (researchers)Tj 4.7112 0 Td (often)Tj -33.5223 -1.2983 Td (encounter)Tj 4.2916 0 Td (situations)Tj 4.0875 0 Td (that)Tj 1.7802 0 Td (are)Tj 1.4513 0 Td (foreign)Tj 3.0897 0 Td (to)Tj 1.0262 0 Td (or)Tj 1.0941 0 Td (outside)Tj 3.1351 0 Td (of)Tj 1.0148 0 Td (the)Tj 1.4684 0 Td (mandate)Tj 3.7077 0 Td (of)Tj 1.0148 0 Td (IRBs,)Tj 2.347 0 Td (we)Tj 1.3266 0 Td (emphasize)Tj 4.405 0 Td (the)Tj -35.24 -1.3039 Td (importance)Tj 4.8302 0 Td (of)Tj 1.0148 0 Td (debating)Tj 3.6906 0 Td (the)Tj 1.4684 0 Td (issues)Tj 2.5228 0 Td (within)Tj 2.8119 0 Td (groups)Tj 2.9707 0 Td (of)Tj 1.0148 0 Td (peers.)Tj -19.128 -1.2982 Td (Rather)Tj 2.897 0 Td (than)Tj 2.0295 0 Td (a)Tj 0.6577 0 Td (bug,)Tj 1.9445 0 Td (the)Tj 1.4684 0 Td (lack)Tj 1.8198 0 Td (of)Tj 1.0148 0 Td (clear-cut)Tj 3.719 0 Td (solutions)Tj 3.8438 0 Td (and)Tj 1.7291 0 Td (governance)Tj 4.8189 0 Td (protocols)Tj 3.9571 0 Td (should)Tj 2.9197 0 Td (be)Tj 1.1508 0 Td (more)Tj -35.1663 -1.2983 Td (appropriately)Tj 5.5785 0 Td (understood)Tj 4.8189 0 Td (as)Tj 1.0148 0 Td (a)Tj 0.6519 0 Td (feature)Tj 2.9877 0 Td (that)Tj 1.7802 0 Td (researchers)Tj 4.7111 0 Td (should)Tj 2.9197 0 Td (embrace)Tj 3.6226 0 Td (within)Tj 2.812 0 Td (their)Tj 2.109 0 Td (own)Tj 1.9558 0 Td (work.)Tj -34.9622 -1.3039 Td (Discussion)Tj 4.5807 0 Td (and)Tj 1.7291 0 Td (debate)Tj 2.8347 0 Td (of)Tj 1.0148 0 Td (ethical)Tj 2.8403 0 Td (issues)Tj 2.5171 0 Td (is)Tj 0.8504 0 Td (an)Tj 1.2019 0 Td (essential)Tj 3.5829 0 Td (part)Tj 1.8426 0 Td (of)Tj 1.0147 0 Td (professional)Tj 5.0514 0 Td (developmentÐboth)Tj -29.0606 -1.2983 Td (within)Tj 2.8119 0 Td (and)Tj 1.7291 0 Td (between)Tj 3.532 0 Td (disciplinesÐas)Tj 6.1454 0 Td (it)Tj 0.7881 0 Td (can)Tj 1.627 0 Td (establish)Tj 3.6454 0 Td (a)Tj 0.6519 0 Td (mature)Tj 3.0955 0 Td (community)Tj 4.8869 0 Td (of)Tj 1.0148 0 Td (responsible)Tj 4.7508 0 Td (practi-)Tj -34.6788 -1.2983 Td (tioners.)Tj 3.2201 0 Td (Bringing)Tj 3.736 0 Td (these)Tj 2.2507 0 Td (debates)Tj 3.1975 0 Td (into)Tj 1.8368 0 Td (coursework)Tj 4.8926 0 Td (and)Tj 1.7348 0 Td (training)Tj 3.4185 0 Td (can)Tj 1.6271 0 Td (produce)Tj 3.515 0 Td (peer)Tj 1.9558 0 Td (reviewers)Tj 4.0025 0 Td (who)Tj -35.3874 -1.3039 Td (are)Tj 1.4456 0 Td (particularly)Tj 4.8132 0 Td (well)Tj 1.8198 0 Td (placed)Tj 2.795 0 Td (to)Tj 1.0261 0 Td (raise)Tj 2.075 0 Td (these)Tj 2.2507 0 Td (ethical)Tj 2.8403 0 Td (questions)Tj 4.0251 0 Td (and)Tj 1.7292 0 Td (spur)Tj 1.9955 0 Td (recognition)Tj 4.8472 0 Td (of)Tj 1.0148 0 Td (the)Tj 1.4684 0 Td (need)Tj 2.143 0 Td (for)Tj -36.2889 -1.2982 Td (these)Tj 2.2506 0 Td (conversations.)Tj -1.0544 -1.2983 Td (A)Tj 0.907 0 Td (precondition)Tj 5.4255 0 Td (of)Tj 1.0148 0 Td (any)Tj 1.6611 0 Td (formal)Tj 2.8857 0 Td (ethics)Tj 2.5171 0 Td (rules)Tj 2.1487 0 Td (or)Tj 1.0998 0 Td (regulations)Tj 4.6715 0 Td (is)Tj 0.8503 0 Td (the)Tj /F11 1 Tf 1.4684 0 Td (capacity)Tj /F6 1 Tf 3.6396 0 Td (to)Tj 1.0205 0 Td (have)Tj 2.0636 0 Td (such)Tj 2.058 0 Td (open-)Tj -34.6278 -1.2983 Td (ended)Tj 2.6702 0 Td (debates.)Tj 3.4185 0 Td (As)Tj 1.2643 0 Td (digital)Tj 2.7269 0 Td (social)Tj 2.4604 0 Td (scientist)Tj 3.4696 0 Td (and)Tj 1.7292 0 Td (ethicist)Tj 3.084 0 Td (Annette)Tj 3.4469 0 Td (Markham)Tj 4.1953 0 Td ([)Tj 0.83 0.64 0.02 0 k (30)Tj 0 g (])Tj 1.8538 0 Td (writes,)Tj 2.846 0 Td (ªwe)Tj 1.7234 0 Td (can)Tj -34.8885 -1.3039 Td (make)Tj 2.3924 0 Td ([data)Tj 2.2563 0 Td (ethics])Tj 2.8573 0 Td (an)Tj 1.2019 0 Td (easier)Tj 2.5002 0 Td (topic)Tj 2.2337 0 Td (to)Tj 1.0261 0 Td (broach)Tj 2.9934 0 Td (by)Tj 1.1792 0 Td (addressing)Tj 4.507 0 Td (ethics)Tj 2.5228 0 Td (as)Tj 1.0148 0 Td (being)Tj 2.4321 0 Td (about)Tj 2.4889 0 Td (choices)Tj 3.1577 0 Td (we)Tj -34.7638 -1.2982 Td (make)Tj 2.3924 0 Td (at)Tj 0.9524 0 Td (critical)Tj 2.948 0 Td (junctures;)Tj 4.1839 0 Td (choices)Tj 3.1578 0 Td (that)Tj 1.7745 0 Td (will)Tj 1.6667 0 Td (invariably)Tj 4.201 0 Td (have)Tj 2.0636 0 Td (impact.º)Tj 3.5943 0 Td (Given)Tj 2.6305 0 Td (the)Tj 1.4684 0 Td (nature)Tj 2.8232 0 Td (of)Tj 1.0148 0 Td (big)Tj -34.8715 -1.2983 Td (data,)Tj 2.1373 0 Td (bringing)Tj 3.6623 0 Td (technical,)Tj 4.0365 0 Td (scientific,)Tj 4.0138 0 Td (social,)Tj 2.6816 0 Td (and)Tj 1.7291 0 Td (humanistic)Tj 4.6998 0 Td (researchers)Tj 4.7112 0 Td (together)Tj 3.5319 0 Td (on)Tj 1.2756 0 Td (projects)Tj -32.4791 -1.3039 Td (enables)Tj 3.1634 0 Td (this)Tj 1.6724 0 Td (debate)Tj 2.8347 0 Td (to)Tj 1.0261 0 Td (emerge)Tj 3.1464 0 Td (as)Tj 1.0148 0 Td (a)Tj 0.652 0 Td (strength)Tj 3.5149 0 Td (because,)Tj 3.5433 0 Td (if)Tj 0.7767 0 Td (done)Tj 2.228 0 Td (well,)Tj 2.0466 0 Td (it)Tj 0.7824 0 Td (provides)Tj 3.651 0 Td (the)Tj 1.4683 0 Td (means)Tj 2.8063 0 Td (to)Tj -34.3273 -1.2983 Td (understand)Tj 4.7848 0 Td (the)Tj 1.474 0 Td (ethical)Tj 2.8403 0 Td (issues)Tj 2.5172 0 Td (from)Tj 2.2053 0 Td (a)Tj 0.6576 0 Td (range)Tj 2.4605 0 Td (of)Tj 1.0148 0 Td (perspectives)Tj 5.0626 0 Td (and)Tj 1.7292 0 Td (disrupt)Tj 3.0897 0 Td (the)Tj 1.474 0 Td (silos)Tj 1.9616 0 Td (of)Tj 1.0148 0 Td (disciplines)Tj ET Q q 1 j 1 J 0 w 576 737.1 m 36 737.1 l 36 737.6 l 576 737.6 l f* 36 741.2598 169.285 23.6976 re W* n q 169.2283 0 0 23.6409 36 741.3165 cm q /I0 Do Q Q Q q 0 0 612 792 re W* n 1 j 1 J 0 w 7.9999 0 0 7.9999 391.9748 745.7952 cm BT /F0 1 Tf 1 TL -0.005 Tc 0 0 Td (Ten)Tj 1.9204 0 Td (Simple)Tj 3.2245 0 Td (Rules)Tj 2.7283 0 Td (for)Tj 1.3678 0 Td (Respons)Tj 3.8764 0 Td (ible)Tj 1.7362 0 Td (Big)Tj 1.637 0 Td (Data)Tj 2.3032 0 Td (Researc)Tj 3.6567 0 Td (h)Tj ET Q q 1 j 1 J 0 w 36 48.0002 m 576 48.0002 l 576 47.5002 l 36 47.5002 l f* 0.83 0.64 0.02 0 k 142.5827 34.9228 m 295.8236 34.9228 l h f* 0 g 7.9999 0 0 7.9999 36 36 cm BT /F0 1 Tf 1 TL -0.005 Tc 0 0 Td (PLOS)Tj 2.8559 0 Td (Computationa)Tj 6.1229 0 Td (l)Tj 0.4322 0 Td (Biology)Tj 3.4371 0 Td (|)Tj 0.83 0.64 0.02 0 k 0.4748 0 Td (https:/)Tj 2.6788 0 Td (/doi.org/10.13)Tj 6.0094 0 Td (71/journal.p)Tj 5.1166 0 Td (cbi.1005399)Tj 0 g 6.3497 0 Td (March)Tj 2.9551 0 Td (30,)Tj 1.5874 0 Td (2017)Tj 27.1207 0 Td (5)Tj 0.7654 0 Td (/)Tj ET endstream endobj 105 0 obj < >stream endstream endobj 106 0 obj < >stream endstream endobj 107 0 obj < >stream BT 66.402 0 Td (10)Tj ET Q endstream endobj 108 0 obj < >/XObject< >>>/CropBox[0 0 612 792]/MediaBox[0 0 612 792]/Parent 9 0 R/Annots 109 0 R/Contents 110 0 R/TrimBox[0 0 612 792]>> endobj 109 0 obj [111 0 R 112 0 R 113 0 R 114 0 R 115 0 R 116 0 R 117 0 R 118 0 R 119 0 R] endobj 111 0 obj < >/Border[0 0 0]/Dest(Rpcbi.1005399.ref031)>> endobj 112 0 obj < >/Border[0 0 0]/Dest(Rpcbi.1005399.ref032)>> endobj 113 0 obj < >/Border[0 0 0]/Dest(Rpcbi.1005399.ref033)>> endobj 114 0 obj < >/Border[0 0 0]/Dest(Rpcbi.1005399.ref034)>> endobj 115 0 obj < >/Border[0 0 0]/Dest(Rpcbi.1005399.ref035)>> endobj 116 0 obj < >/Border[0 0 0]/A 120 0 R>> endobj 120 0 obj < > endobj 117 0 obj < >/Border[0 0 0]/Dest(Rpcbi.1005399.ref013)>> endobj 118 0 obj < >/Border[0 0 0]/Dest(Rpcbi.1005399.ref001)>> endobj 119 0 obj < >/Border[0 0 0]/A 121 0 R>> endobj 121 0 obj < > endobj 110 0 obj [122 0 R 123 0 R 124 0 R 125 0 R 126 0 R 127 0 R 128 0 R 129 0 R 130 0 R] endobj 122 0 obj < >stream q 0.83 0.64 0.02 0 k 203.4142 706.337 m 212.9386 706.337 l h f* 249.4488 680.315 m 258.9732 680.315 l h f* 393.052 680.315 m 402.5764 680.315 l h f* 217.1339 641.3102 m 226.6583 641.3102 l h f* 227.2819 615.3449 m 236.8063 615.3449 l h f* 203.4142 459.5528 m 341.7449 459.5528 l 341.7449 459.0992 l 203.4142 459.0992 l f* 295.1433 185.8961 m 304.611 185.8961 l h f* 555.1937 107.9433 m 559.9559 107.9433 l h f* 0 g 1 j 1 J 0 w 10 0 0 10 200.0125 707.4141 cm BT /F6 1 Tf 1 TL -0.004 Tc 0 0 Td ([)Tj 0.83 0.64 0.02 0 k (31)Tj 0 g (].)Tj 2.0749 0 Td (There)Tj 2.5795 0 Td (are)Tj 1.4457 0 Td (a)Tj 0.6576 0 Td (number)Tj 3.4129 0 Td (of)Tj 1.0148 0 Td (good)Tj 2.228 0 Td (models)Tj 3.1068 0 Td (for)Tj 1.3833 0 Td (interdisciplinary)Tj 6.8144 0 Td (ethics)Tj 2.5228 0 Td (research,)Tj 3.7814 0 Td (such)Tj 2.058 0 Td (as)Tj 1.0148 0 Td (the)Tj -34.0949 -1.3039 Td (trainings)Tj 3.7813 0 Td (offered)Tj 3.0501 0 Td (by)Tj 1.1792 0 Td (the)Tj 1.4683 0 Td (Science)Tj 3.2088 0 Td (and)Tj 1.7292 0 Td (Justice)Tj 2.8516 0 Td (research)Tj 3.5546 0 Td (center)Tj 2.7099 0 Td (at)Tj 0.9525 0 Td (the)Tj 1.4683 0 Td (University)Tj 4.3993 0 Td (of)Tj 1.0148 0 Td (California,)Tj -31.3679 -1.2982 Td (Santa)Tj 2.4094 0 Td (Cruz)Tj 2.194 0 Td ([)Tj 0.83 0.64 0.02 0 k (32)Tj 0 g (])Tj 1.8538 0 Td (and)Tj 1.7292 0 Td (Values)Tj 2.9083 0 Td (in)Tj 1.0318 0 Td (Design)Tj 3.0217 0 Td (curricula)Tj 3.8154 0 Td ([)Tj 0.83 0.64 0.02 0 k (33)Tj 0 g (].)Tj 2.075 0 Td (Research)Tj 3.8097 0 Td (ethics)Tj 2.5171 0 Td (consultation)Tj 5.1874 0 Td (services,)Tj -32.5528 -1.2983 Td (available)Tj 3.668 0 Td (at)Tj 0.9467 0 Td (some)Tj 2.3358 0 Td (universities)Tj 4.7905 0 Td (as)Tj 1.0148 0 Td (a)Tj 0.6519 0 Td (result)Tj 2.4492 0 Td (of)Tj 1.0148 0 Td (the)Tj 1.4683 0 Td (Clinical)Tj 3.3165 0 Td (and)Tj 1.7291 0 Td (Translational)Tj 5.5276 0 Td (Science)Tj 3.2088 0 Td (Award)Tj -32.122 -1.2983 Td (\(CTSA\))Tj 3.3278 0 Td (program)Tj 3.702 0 Td (of)Tj 1.0092 0 Td (the)Tj 1.4626 0 Td (National)Tj 3.685 0 Td (Institutes)Tj 3.9402 0 Td (of)Tj 1.0091 0 Td (Health)Tj 2.914 0 Td (\(NIH\),)Tj 2.965 0 Td (can)Tj 1.6214 0 Td (also)Tj 1.7632 0 Td (be)Tj 1.1452 0 Td (resources)Tj 3.9798 0 Td (for)Tj 1.3833 0 Td (research-)Tj -33.9078 -1.3039 Td (ers)Tj 1.3719 0 Td ([)Tj 0.83 0.64 0.02 0 k (34)Tj 0 g (].)Tj -0.1757 -1.2982 Td (Some)Tj 2.4491 0 Td (of)Tj 1.0148 0 Td (the)Tj 1.4683 0 Td (better-known)Tj 5.6749 0 Td (ªbig)Tj 1.8539 0 Td (dataº)Tj 2.3074 0 Td (ethical)Tj 2.8403 0 Td (casesÐi.e.,)Tj 4.5127 0 Td (the)Tj 1.4683 0 Td (Facebook)Tj 4.0479 0 Td (emotional)Tj 4.2689 0 Td (contagion)Tj -33.1027 -1.2983 Td (study)Tj 2.3867 0 Td ([)Tj 0.83 0.64 0.02 0 k (35)Tj 0 g (]Ðprovide)Tj 5.845 0 Td (extremely)Tj 4.1442 0 Td (productive)Tj 4.5411 0 Td (venues)Tj 2.9594 0 Td (for)Tj 1.3833 0 Td (cross-disciplinary)Tj 7.2793 0 Td (discussions.)Tj 4.9606 0 Td (Why)Tj -33.4996 -1.3039 Td (might)Tj 2.5965 0 Td (one)Tj 1.7007 0 Td (set)Tj 1.304 0 Td (of)Tj 1.0148 0 Td (scholars)Tj 3.4469 0 Td (see)Tj 1.4286 0 Td (this)Tj 1.6781 0 Td (as)Tj 1.0148 0 Td (a)Tj 0.652 0 Td (relatively)Tj 3.8381 0 Td (benign)Tj 2.982 0 Td (approach)Tj 3.9515 0 Td (while)Tj 2.3697 0 Td (other)Tj 2.3471 0 Td (groups)Tj 2.9707 0 Td (see)Tj 1.423 0 Td (signifi-)Tj -34.7185 -1.2983 Td (cant)Tj 1.9218 0 Td (ethical)Tj 2.8403 0 Td (shortcomings?)Tj 6.0491 0 Td (Where)Tj 2.931 0 Td (do)Tj 1.2586 0 Td (researchers)Tj 4.7112 0 Td (differ)Tj 2.381 0 Td (in)Tj 1.0375 0 Td (drawing)Tj 3.5206 0 Td (the)Tj 1.4684 0 Td (line)Tj 1.7064 0 Td (between)Tj 3.5263 0 Td (responsi-)Tj -33.3522 -1.2983 Td (ble)Tj 1.3946 0 Td (and)Tj 1.7291 0 Td (irresponsible)Tj 5.3915 0 Td (research)Tj 3.5603 0 Td (and)Tj 1.7291 0 Td (why?)Tj 2.2564 0 Td (Understanding)Tj 6.2758 0 Td (the)Tj 1.474 0 Td (different)Tj 3.6567 0 Td (ways)Tj 2.1486 0 Td (people)Tj 2.8573 0 Td (discuss)Tj 3.0501 0 Td (these)Tj -35.5235 -1.3039 Td (challenges)Tj 4.3199 0 Td (and)Tj 1.7291 0 Td (processes)Tj 3.9685 0 Td (provides)Tj 3.651 0 Td (an)Tj 1.2019 0 Td (important)Tj 4.2803 0 Td (check)Tj 2.5171 0 Td (for)Tj 1.3833 0 Td (researchers,)Tj 4.938 0 Td (especially)Tj 4.0195 0 Td (if)Tj 0.771 0 Td (they)Tj 1.9275 0 Td (come)Tj -34.7071 -1.2982 Td (from)Tj 2.2053 0 Td (disciplines)Tj 4.4277 0 Td (not)Tj 1.576 0 Td (focused)Tj 3.2769 0 Td (on)Tj 1.2755 0 Td (human)Tj 3.0785 0 Td (subject)Tj 3.016 0 Td (concerns.)Tj -17.6597 -1.2983 Td (Moreover,)Tj 4.3993 0 Td (the)Tj 1.4683 0 Td (high)Tj 2.007 0 Td (visibility)Tj 3.6113 0 Td (surrounding)Tj 5.244 0 Td (these)Tj 2.2564 0 Td (events)Tj 2.7269 0 Td (means)Tj 2.8063 0 Td (that)Tj 1.7801 0 Td (\(for)Tj 1.7235 0 Td (better)Tj 2.5455 0 Td (or)Tj 1.0942 0 Td (worse\))Tj 2.9083 0 Td (they)Tj -35.7673 -1.3039 Td (represent)Tj 3.9571 0 Td (the)Tj 1.4683 0 Td (ªpublicº)Tj 3.4923 0 Td (view)Tj 2.0523 0 Td (of)Tj 1.0148 0 Td (big)Tj 1.457 0 Td (data)Tj 1.9162 0 Td (research,)Tj 3.7814 0 Td (and)Tj 1.7291 0 Td (becoming)Tj 4.1839 0 Td (an)Tj 1.2019 0 Td (active)Tj 2.5171 0 Td (member)Tj 3.5887 0 Td (of)Tj 1.0148 0 Td (this)Tj 1.6724 0 Td (con-)Tj -35.0473 -1.2983 Td (versation)Tj 3.8834 0 Td (ensures)Tj 3.2371 0 Td (that)Tj 1.7802 0 Td (researchers)Tj 4.7111 0 Td (can)Tj 1.6271 0 Td (give)Tj 1.8312 0 Td (voice)Tj 2.2904 0 Td (to)Tj 1.0261 0 Td (their)Tj 2.109 0 Td (insights)Tj 3.3165 0 Td (rather)Tj 2.6475 0 Td (than)Tj 2.0296 0 Td (simply)Tj 2.8857 0 Td (being)Tj 2.4321 0 Td (at)Tj -35.807 -1.2982 Td (the)Tj 1.4683 0 Td (receiving)Tj 3.8664 0 Td (end)Tj 1.7178 0 Td (of)Tj 1.0148 0 Td (policy)Tj 2.6362 0 Td (decisions.)Tj 4.1329 0 Td (In)Tj 1.1055 0 Td (an)Tj 1.2019 0 Td (effort)Tj 2.3981 0 Td (to)Tj 1.0261 0 Td (help)Tj 1.9332 0 Td (these)Tj 2.2564 0 Td (debates)Tj 3.1918 0 Td (along,)Tj 2.6475 0 Td (the)Tj 1.4684 0 Td (Council)Tj 3.4072 0 Td (for)Tj -35.4725 -1.2983 Td (Big)Tj 1.5306 0 Td (Data,)Tj 2.3471 0 Td (Ethics,)Tj 2.8913 0 Td (and)Tj 1.7348 0 Td (Society)Tj 3.0671 0 Td (has)Tj 1.542 0 Td (produced)Tj 4.0479 0 Td (a)Tj 0.652 0 Td (number)Tj 3.4185 0 Td (of)Tj 1.0148 0 Td (case)Tj 1.8595 0 Td (studies)Tj 2.9877 0 Td (focused)Tj 3.2769 0 Td (specifically)Tj 4.575 0 Td (on)Tj 1.2756 0 Td (big)Tj -36.2208 -1.3039 Td (data)Tj 1.9105 0 Td (research)Tj 3.5603 0 Td (and)Tj 1.7291 0 Td (a)Tj 0.652 0 Td (white)Tj 2.4207 0 Td (paper)Tj 2.4832 0 Td (with)Tj 2.0012 0 Td (recommendations)Tj 7.4948 0 Td (to)Tj 1.0261 0 Td (start)Tj 1.9843 0 Td (these)Tj 2.2563 0 Td (important)Tj 4.2803 0 Td (conversations)Tj -31.7988 -1.2983 Td (\()Tj 0.83 0.64 0.02 0 k (http://bdes.datasociety.net/o)Tj 11.6729 0 Td (utput/)Tj 0 g (\).)Tj -10.4767 -1.2983 Td (Engage)Tj 3.1294 0 Td (your)Tj 2.0806 0 Td (colleagues)Tj 4.2746 0 Td (and)Tj 1.7348 0 Td (students)Tj 3.566 0 Td (about)Tj 2.4945 0 Td (ethical)Tj 2.8403 0 Td (practice)Tj 3.3732 0 Td (for)Tj 1.3889 0 Td (big)Tj 1.457 0 Td (data)Tj 1.9162 0 Td (research.)Tj ET Q q 1 j 1 J 0 w 11.9999 0 0 11.9999 200.0125 413.0078 cm BT /F2 1 Tf 1 TL -0.0033 Tc 0 0 Td (7.)Tj 1.0441 0 Td (Develop)Tj 4.0772 0 Td (a)Tj 0.77 0 Td (code)Tj 2.5371 0 Td (of)Tj 1.1622 0 Td (conduct)Tj 4.0866 0 Td (for)Tj 1.5449 0 Td (your)Tj 2.3717 0 Td (organization,)Tj 6.4111 0 Td (research)Tj -24.0049 -1.1669 Td (community,)Tj 5.7968 0 Td (or)Tj 1.2095 0 Td (industry)Tj ET Q q 1 j 1 J 0 w 10 0 0 10 200.0125 381.9968 cm BT /F6 1 Tf 1 TL -0.004 Tc 0 0 Td (The)Tj 1.7801 0 Td (process)Tj 3.1861 0 Td (of)Tj 1.0148 0 Td (debating)Tj 3.6907 0 Td (tough)Tj 2.5455 0 Td (choices)Tj 3.1521 0 Td (inserts)Tj 2.8516 0 Td (ethics)Tj 2.5229 0 Td (directly)Tj 3.2314 0 Td (into)Tj 1.8425 0 Td (the)Tj 1.4684 0 Td (workflow)Tj 4.0025 0 Td (of)Tj 1.0148 0 Td (research,)Tj -32.3034 -1.2982 Td (making)Tj 3.2484 0 Td (ªfaking)Tj 3.1125 0 Td (ethicsº)Tj 2.914 0 Td (as)Tj 1.0147 0 Td (unacceptable)Tj 5.4255 0 Td (as)Tj 1.0148 0 Td (faking)Tj 2.7213 0 Td (data)Tj 1.9105 0 Td (or)Tj 1.0998 0 Td (results.)Tj 3.0331 0 Td (Internalizing)Tj 5.3915 0 Td (these)Tj 2.2563 0 Td (debates,)Tj -33.1424 -1.3039 Td (rather)Tj 2.6475 0 Td (than)Tj 2.0239 0 Td (treating)Tj 3.3279 0 Td (them)Tj 2.2903 0 Td (as)Tj 1.0148 0 Td (an)Tj 1.2019 0 Td (afterthought)Tj 5.1874 0 Td (or)Tj 1.0998 0 Td (a)Tj 0.652 0 Td (problem)Tj 3.6113 0 Td (to)Tj 1.0261 0 Td (outsource,)Tj 4.3937 0 Td (is)Tj 0.8447 0 Td (key)Tj 1.5988 0 Td (for)Tj 1.3833 0 Td (successful)Tj -32.3034 -1.2983 Td (research,)Tj 3.7813 0 Td (particularly)Tj 4.8132 0 Td (when)Tj 2.4038 0 Td (using)Tj 2.3868 0 Td (trace)Tj 2.1713 0 Td (data)Tj 1.9162 0 Td (produced)Tj 4.0422 0 Td (by)Tj 1.1848 0 Td (people.)Tj 3.0785 0 Td (This)Tj 1.9842 0 Td (is)Tj 0.8504 0 Td (relevant)Tj 3.4185 0 Td (for)Tj 1.389 0 Td (all)Tj 1.1452 0 Td (re-)Tj -34.5654 -1.2983 Td (search)Tj 2.7609 0 Td (including)Tj 4.0422 0 Td (those)Tj 2.3357 0 Td (within)Tj 2.8119 0 Td (industry)Tj 3.5773 0 Td (who)Tj 1.9389 0 Td (have)Tj 2.058 0 Td (privileged)Tj 4.1839 0 Td (access)Tj 2.6475 0 Td (to)Tj 1.0262 0 Td (the)Tj 1.4683 0 Td (data)Tj 1.9162 0 Td (streams)Tj 3.2882 0 Td (of)Tj 1.0148 0 Td (digital)Tj -35.07 -1.2982 Td (daily)Tj 2.1486 0 Td (life.)Tj 1.6724 0 Td (Public)Tj 2.7496 0 Td (attention)Tj 3.8438 0 Td (to)Tj 1.0261 0 Td (the)Tj 1.4683 0 Td (ethical)Tj 2.8403 0 Td (use)Tj 1.5307 0 Td (of)Tj 1.0148 0 Td (these)Tj 2.2564 0 Td (data)Tj 1.9105 0 Td (should)Tj 2.9197 0 Td (not)Tj 1.5704 0 Td (be)Tj 1.1509 0 Td (avoided;)Tj 3.5886 0 Td (after)Tj 2.0353 0 Td (all,)Tj 1.3719 0 Td (these)Tj -35.0983 -1.3039 Td (datasets)Tj 3.3561 0 Td (are)Tj 1.4514 0 Td (based)Tj 2.4718 0 Td (on)Tj 1.2756 0 Td (an)Tj 1.2018 0 Td (infrastructure)Tj 5.7317 0 Td (that)Tj 1.7801 0 Td (billions)Tj 3.1691 0 Td (of)Tj 1.0148 0 Td (people)Tj 2.8516 0 Td (are)Tj 1.4514 0 Td (using)Tj 2.3867 0 Td (to)Tj 1.0262 0 Td (live)Tj 1.61 0 Td (their)Tj 2.109 0 Td (lives,)Tj 2.1997 0 Td (and)Tj -35.087 -1.2983 Td (there)Tj 2.262 0 Td (is)Tj 0.8504 0 Td (a)Tj 0.6519 0 Td (compelling)Tj 4.6828 0 Td (public)Tj 2.7099 0 Td (interest)Tj 3.2145 0 Td (that)Tj 1.7745 0 Td (research)Tj 3.5603 0 Td (is)Tj 0.8447 0 Td (done)Tj 2.228 0 Td (responsibly.)Tj -21.5828 -1.2983 Td (One)Tj 1.9388 0 Td (of)Tj 1.0148 0 Td (the)Tj 1.4741 0 Td (best)Tj 1.8084 0 Td (ways)Tj 2.1544 0 Td (to)Tj 1.0261 0 Td (cement)Tj 3.1578 0 Td (this)Tj 1.6724 0 Td (in)Tj 1.0318 0 Td (daily)Tj 2.1543 0 Td (practice)Tj 3.3789 0 Td (is)Tj 0.8447 0 Td (to)Tj 1.0262 0 Td (develop)Tj 3.3221 0 Td (codes)Tj 2.4605 0 Td (of)Tj 1.0148 0 Td (conduct)Tj 3.4752 0 Td (for)Tj 1.389 0 Td (use)Tj -35.5405 -1.3039 Td (in)Tj 1.0318 0 Td (your)Tj 2.0806 0 Td (organization)Tj 5.2837 0 Td (or)Tj 1.0999 0 Td (research)Tj 3.5546 0 Td (community)Tj 4.8869 0 Td (and)Tj 1.7291 0 Td (for)Tj 1.389 0 Td (inclusion)Tj 3.9118 0 Td (in)Tj 1.0374 0 Td (formal)Tj 2.8857 0 Td (education)Tj 4.1782 0 Td (and)Tj 1.7291 0 Td (ongo-)Tj -34.7978 -1.2983 Td (ing)Tj 1.5023 0 Td (training.)Tj 3.6453 0 Td (The)Tj 1.7802 0 Td (codes)Tj 2.4604 0 Td (can)Tj 1.6271 0 Td (provide)Tj 3.2882 0 Td (guidance)Tj 3.8381 0 Td (in)Tj 1.0374 0 Td (peer)Tj 1.9559 0 Td (review)Tj 2.8403 0 Td (of)Tj 1.0148 0 Td (publications)Tj 5.125 0 Td (and)Tj 1.7348 0 Td (in)Tj 1.0318 0 Td (funding)Tj -32.8816 -1.2982 Td (consideration.)Tj 5.93 0 Td (In)Tj 1.1055 0 Td (practice,)Tj 3.6056 0 Td (a)Tj 0.652 0 Td (highly)Tj 2.7099 0 Td (visible)Tj 2.7496 0 Td (case)Tj 1.8595 0 Td (of)Tj 1.0148 0 Td (unethical)Tj 3.9175 0 Td (research)Tj 3.5546 0 Td (brings)Tj 2.7382 0 Td (problems)Tj 3.9742 0 Td (to)Tj 1.0261 0 Td (an)Tj -34.8375 -1.3039 Td (entire)Tj 2.5511 0 Td (field,)Tj 2.1997 0 Td (not)Tj 1.5704 0 Td (just)Tj 1.6667 0 Td (to)Tj 1.0262 0 Td (those)Tj 2.3357 0 Td (directly)Tj 3.2315 0 Td (involved.)Tj 3.8777 0 Td (Moreover,)Tj 4.3937 0 Td (designing)Tj 4.0932 0 Td (codes)Tj 2.4661 0 Td (of)Tj 1.0148 0 Td (conduct)Tj 3.4753 0 Td (makes)Tj -33.9021 -1.2983 Td (researchers)Tj 4.7111 0 Td (more)Tj 2.3414 0 Td (successful.)Tj 4.388 0 Td (Issues)Tj 2.5909 0 Td (that)Tj 1.7801 0 Td (might)Tj 2.5965 0 Td (otherwise)Tj 4.0876 0 Td (be)Tj 1.1508 0 Td (ignored)Tj 3.3279 0 Td (until)Tj 2.1089 0 Td (they)Tj 1.9219 0 Td (blow)Tj 2.1657 0 Td (upÐe.g.,)Tj -33.1708 -1.2983 Td (Are)Tj 1.6951 0 Td (we)Tj 1.3322 0 Td (abiding)Tj 3.2372 0 Td (by)Tj 1.1792 0 Td (the)Tj 1.4683 0 Td (terms)Tj 2.4945 0 Td (of)Tj 1.0148 0 Td (service)Tj 2.9423 0 Td (or)Tj 1.0999 0 Td (users')Tj 2.4831 0 Td (expectations?)Tj 5.5559 0 Td (Does)Tj 2.245 0 Td (the)Tj 1.474 0 Td (general)Tj 3.1294 0 Td (public)Tj 2.7099 0 Td (consider)Tj -34.0608 -1.3039 Td (our)Tj 1.627 0 Td (research)Tj 3.5546 0 Td (ªcreepyº?)Tj 3.9912 0 Td ([)Tj 0.83 0.64 0.02 0 k (13)Tj 0 g (]Ðcan)Tj 4.1782 0 Td (be)Tj 1.1509 0 Td (addressed)Tj 4.1782 0 Td (thoughtfully)Tj 5.1364 0 Td (rather)Tj 2.6419 0 Td (than)Tj 2.0296 0 Td (in)Tj 1.0374 0 Td (a)Tj 0.652 0 Td (scramble)Tj 3.8097 0 Td (for)Tj 1.3833 0 Td (dam-)Tj -35.3704 -1.2982 Td (age)Tj 1.542 0 Td (control.)Tj 3.3448 0 Td (This)Tj 1.99 0 Td (is)Tj 0.8447 0 Td (particularly)Tj 4.8132 0 Td (relevant)Tj 3.4185 0 Td (to)Tj 1.0262 0 Td (public-facing)Tj 5.4878 0 Td (private)Tj 2.9877 0 Td (businesses)Tj 4.3597 0 Td (interested)Tj 4.1612 0 Td (in)Tj 1.0375 0 Td (avoid-)Tj -35.0133 -1.2983 Td (ing)Tj 1.5023 0 Td (potentially)Tj 4.456 0 Td (unfavorable)Tj 4.9663 0 Td (attention.)Tj -9.7284 -1.2983 Td (An)Tj 1.457 0 Td (additional)Tj 4.2746 0 Td (and)Tj 1.7291 0 Td (longer-term)Tj 5.0456 0 Td (advantage)Tj 4.2406 0 Td (of)Tj 1.0148 0 Td (developing)Tj 4.6035 0 Td (codes)Tj 2.4604 0 Td (of)Tj 1.0148 0 Td (conduct)Tj 3.4753 0 Td (is)Tj 0.8504 0 Td (that)Tj 1.7801 0 Td (it)Tj 0.7824 0 Td (is)Tj 0.8504 0 Td (clear)Tj -34.7752 -1.3039 Td (that)Tj 1.7744 0 Td (change)Tj 3.0444 0 Td (is)Tj 0.8504 0 Td (coming)Tj 3.2485 0 Td (to)Tj 1.0261 0 Td (big)Tj 1.457 0 Td (data)Tj 1.9162 0 Td (research.)Tj 3.7814 0 Td (The)Tj 1.7802 0 Td (NSF)Tj 1.9615 0 Td (funded)Tj 3.0614 0 Td (the)Tj 1.474 0 Td (Council)Tj 3.4016 0 Td (for)Tj 1.3833 0 Td (Big)Tj 1.5364 0 Td (Data,)Tj 2.347 0 Td (Ethics,)Tj -34.0438 -1.2983 Td (and)Tj 1.7234 0 Td (Society)Tj 3.0614 0 Td (as)Tj 1.0035 0 Td (a)Tj 0.6463 0 Td (means)Tj 2.8006 0 Td (of)Tj 1.0034 0 Td (getting)Tj 2.9821 0 Td (in)Tj 1.0261 0 Td (front)Tj 2.2223 0 Td (of)Tj 1.0092 0 Td (a)Tj 0.6463 0 Td (developing)Tj 4.5921 0 Td (issue)Tj 2.1486 0 Td (and)Tj 1.7235 0 Td (pending)Tj 3.5092 0 Td (regulatory)Tj 4.3143 0 Td (changes)Tj -34.4123 -1.2982 Td (within)Tj 2.8119 0 Td (federal)Tj 2.9367 0 Td (rules)Tj 2.1486 0 Td (for)Tj 1.3833 0 Td (the)Tj 1.4684 0 Td (protection)Tj 4.3823 0 Td (of)Tj 1.0148 0 Td (human)Tj 3.0784 0 Td (subjects)Tj 3.3789 0 Td (that)Tj 1.7801 0 Td (are)Tj 1.4457 0 Td (currently)Tj 3.8891 0 Td (under)Tj 2.6192 0 Td (review)Tj 2.8403 0 Td ([)Tj 0.83 0.64 0.02 0 k (1)Tj 0 g (].)Tj -35.1777 -1.3039 Td (Actively)Tj 3.4695 0 Td (developing)Tj 4.6035 0 Td (rules)Tj 2.1486 0 Td (for)Tj 1.389 0 Td (responsible)Tj 4.7508 0 Td (big)Tj 1.457 0 Td (data)Tj 1.9162 0 Td (research)Tj 3.5547 0 Td (within)Tj 2.8119 0 Td (a)Tj 0.6576 0 Td (research)Tj 3.5547 0 Td (community)Tj 4.8869 0 Td (is)Tj 0.8447 0 Td (a)Tj -36.0451 -1.2983 Td (key)Tj 1.593 0 Td (way)Tj 1.7915 0 Td (researchers)Tj 4.7111 0 Td (can)Tj 1.6271 0 Td (join)Tj 1.8028 0 Td (this)Tj 1.6725 0 Td (ongoing)Tj 3.5263 0 Td (process.)Tj ET Q q 1 j 1 J 0 w 576 737.1 m 36 737.1 l 36 737.6 l 576 737.6 l f* 36 741.2598 169.285 23.6976 re W* n q 169.2283 0 0 23.6409 36 741.3165 cm q /I0 Do Q Q Q q 0 0 612 792 re W* n 1 j 1 J 0 w 7.9999 0 0 7.9999 391.9748 745.7952 cm BT /F0 1 Tf 1 TL -0.005 Tc 0 0 Td (Ten)Tj 1.9204 0 Td (Simple)Tj 3.2245 0 Td (Rules)Tj 2.7283 0 Td (for)Tj 1.3678 0 Td (Respons)Tj 3.8764 0 Td (ible)Tj 1.7362 0 Td (Big)Tj 1.637 0 Td (Data)Tj 2.3032 0 Td (Researc)Tj 3.6567 0 Td (h)Tj ET Q q 1 j 1 J 0 w 36 48.0002 m 576 48.0002 l 576 47.5002 l 36 47.5002 l f* 0.83 0.64 0.02 0 k 142.5827 34.9228 m 295.8236 34.9228 l h f* 0 g 7.9999 0 0 7.9999 36 36 cm BT /F0 1 Tf 1 TL -0.005 Tc 0 0 Td (PLOS)Tj 2.8559 0 Td (Computationa)Tj 6.1229 0 Td (l)Tj 0.4322 0 Td (Biology)Tj 3.4371 0 Td (|)Tj 0.83 0.64 0.02 0 k 0.4748 0 Td (https:/)Tj 2.6788 0 Td (/doi.org/10.13)Tj 6.0094 0 Td (71/journal.p)Tj 5.1166 0 Td (cbi.1005399)Tj 0 g 6.3497 0 Td (March)Tj 2.9551 0 Td (30,)Tj 1.5874 0 Td (2017)Tj 27.1207 0 Td (6)Tj 0.7654 0 Td (/)Tj ET endstream endobj 123 0 obj < >stream endstream endobj 124 0 obj < >stream endstream endobj 125 0 obj < >stream endstream endobj 126 0 obj < >stream endstream endobj 127 0 obj < >stream endstream endobj 128 0 obj < >stream endstream endobj 129 0 obj < >stream endstream endobj 130 0 obj < >stream BT 66.402 0 Td (10)Tj ET Q endstream endobj 131 0 obj < >/XObject< >>>/CropBox[0 0 612 792]/MediaBox[0 0 612 792]/Parent 9 0 R/Annots 132 0 R/Contents 133 0 R/TrimBox[0 0 612 792]>> endobj 132 0 obj [134 0 R 135 0 R 136 0 R] endobj 134 0 obj < >/Border[0 0 0]/Dest(Rpcbi.1005399.ref036)>> endobj 135 0 obj < >/Border[0 0 0]/Dest(Rpcbi.1005399.ref037)>> endobj 136 0 obj < >/Border[0 0 0]/A 137 0 R>> endobj 137 0 obj < > endobj 133 0 obj [138 0 R 139 0 R 140 0 R 141 0 R 142 0 R 143 0 R] endobj 138 0 obj < >stream q 0.83 0.64 0.02 0 k 203.4142 350.3622 m 212.9386 350.3622 l h f* 203.4142 220.3654 m 212.9386 220.3654 l h f* 0 g 1 j 1 J 0 w 10 0 0 10 211.9748 707.4141 cm BT /F6 1 Tf 1 TL -0.004 Tc 0 0 Td (Establish)Tj 3.7927 0 Td (appropriate)Tj 4.8812 0 Td (codes)Tj 2.4605 0 Td (of)Tj 1.0148 0 Td (ethical)Tj 2.8403 0 Td (conduct)Tj 3.4752 0 Td (within)Tj 2.8176 0 Td (your)Tj 2.0807 0 Td (community.)Tj 5.108 0 Td (Make)Tj 2.4661 0 Td (industry)Tj -32.1333 -1.3039 Td (researchers)Tj 4.7111 0 Td (and)Tj 1.7291 0 Td (representatives)Tj 6.1965 0 Td (of)Tj 1.0148 0 Td (affected)Tj 3.3279 0 Td (communities)Tj 5.4821 0 Td (active)Tj 2.5228 0 Td (contributors)Tj 5.2044 0 Td (to)Tj 1.0262 0 Td (this)Tj 1.6781 0 Td (process.)Tj ET Q q 1 j 1 J 0 w 11.9999 0 0 11.9999 200.0125 663.9307 cm BT /F2 1 Tf 1 TL -0.0033 Tc 0 0 Td (8.)Tj 1.0441 0 Td (Design)Tj 3.5291 0 Td (your)Tj 2.3717 0 Td (data)Tj 2.2583 0 Td (and)Tj 1.989 0 Td (systems)Tj 4.1717 0 Td (for)Tj 1.5448 0 Td (auditability)Tj ET Q q 1 j 1 J 0 w 10 0 0 10 200.0125 646.9228 cm BT /F6 1 Tf 1 TL -0.004 Tc 0 0 Td (Although)Tj 3.9968 0 Td (codes)Tj 2.4491 0 Td (of)Tj 1.0091 0 Td (conduct)Tj 3.4639 0 Td (will)Tj 1.6555 0 Td (vary)Tj 1.9275 0 Td (depending)Tj 4.4617 0 Td (on)Tj 1.2643 0 Td (the)Tj 1.4626 0 Td (topic)Tj 2.2224 0 Td (and)Tj 1.7234 0 Td (research)Tj 3.549 0 Td (community,)Tj 5.1023 0 Td (a)Tj 0.6463 0 Td (partic-)Tj -34.9339 -1.2982 Td (ularly)Tj 2.5001 0 Td (important)Tj 4.2803 0 Td (element)Tj 3.4015 0 Td (is)Tj 0.8504 0 Td (designing)Tj 4.0932 0 Td (data)Tj 1.9162 0 Td (and)Tj 1.7292 0 Td (systems)Tj 3.2995 0 Td (for)Tj 1.3833 0 Td (auditability.)Tj 4.9776 0 Td (Responsible)Tj 5.0003 0 Td (internal)Tj -33.4316 -1.2983 Td (auditing)Tj 3.5546 0 Td (processes)Tj 3.9685 0 Td (flow)Tj 1.9445 0 Td (easily)Tj 2.4094 0 Td (into)Tj 1.8369 0 Td (audit)Tj 2.279 0 Td (systems)Tj 3.2995 0 Td (and)Tj 1.7291 0 Td (also)Tj 1.7689 0 Td (keep)Tj 2.0806 0 Td (track)Tj 2.2393 0 Td (of)Tj 1.0148 0 Td (factors)Tj 2.9027 0 Td (that)Tj 1.7801 0 Td (might)Tj 2.5966 0 Td (con-)Tj -35.4045 -1.3039 Td (tribute)Tj 2.9139 0 Td (to)Tj 1.0262 0 Td (problematic)Tj 5.0343 0 Td (outcomes.)Tj 4.3086 0 Td (Developing)Tj 4.8132 0 Td (automated)Tj 4.4901 0 Td (testing)Tj 2.8799 0 Td (processes)Tj 3.9685 0 Td (for)Tj 1.389 0 Td (assessing)Tj 3.7984 0 Td (prob-)Tj -34.6221 -1.2983 Td (lematic)Tj 3.1294 0 Td (outcomes)Tj 4.0875 0 Td (and)Tj 1.7291 0 Td (mechanisms)Tj 5.2101 0 Td (for)Tj 1.3833 0 Td (auditing)Tj 3.5546 0 Td (other's)Tj 2.9253 0 Td (work)Tj 2.2791 0 Td (during)Tj 2.931 0 Td (review)Tj 2.8403 0 Td (processes)Tj 3.9685 0 Td (can)Tj 1.627 0 Td (help)Tj -35.6652 -1.2982 Td (strengthen)Tj 4.4843 0 Td (research)Tj 3.5547 0 Td (as)Tj 1.0148 0 Td (a)Tj 0.6576 0 Td (whole.)Tj 2.8346 0 Td (The)Tj 1.7802 0 Td (goal)Tj 1.8708 0 Td (of)Tj 1.0148 0 Td (auditability)Tj 4.7508 0 Td (is)Tj 0.8448 0 Td (to)Tj 1.0261 0 Td (clearly)Tj 2.8233 0 Td (document)Tj 4.2916 0 Td (when)Tj 2.4038 0 Td (decisions)Tj -33.3522 -1.3039 Td (are)Tj 1.4456 0 Td (made)Tj 2.4265 0 Td (and,)Tj 1.9558 0 Td (if)Tj 0.7767 0 Td (necessary,)Tj 4.2463 0 Td (backtrack)Tj 4.1045 0 Td (to)Tj 1.0262 0 Td (an)Tj 1.2019 0 Td (earlier)Tj 2.7552 0 Td (dataset)Tj 2.9991 0 Td (and)Tj 1.7291 0 Td (address)Tj 3.2258 0 Td (the)Tj 1.4683 0 Td (issue)Tj 2.16 0 Td (at)Tj 0.9524 0 Td (the)Tj 1.4684 0 Td (root)Tj -33.9418 -1.2983 Td (\(e.g.,)Tj 2.1259 0 Td (if)Tj 0.771 0 Td (strategies)Tj 3.9232 0 Td (for)Tj 1.3889 0 Td (anonymizing)Tj 5.4935 0 Td (data)Tj 1.9106 0 Td (are)Tj 1.4513 0 Td (compromised\).)Tj -15.8682 -1.2983 Td (Designing)Tj 4.3029 0 Td (for)Tj 1.389 0 Td (auditability)Tj 4.7508 0 Td (also)Tj 1.7632 0 Td (brings)Tj 2.7439 0 Td (direct)Tj 2.5285 0 Td (benefits)Tj 3.3392 0 Td (to)Tj 1.0261 0 Td (researchers)Tj 4.7112 0 Td (by)Tj 1.1792 0 Td (providing)Tj 4.1499 0 Td (a)Tj 0.6519 0 Td (mecha-)Tj -33.732 -1.3039 Td (nism)Tj 2.2166 0 Td (for)Tj 1.3833 0 Td (double-checking)Tj 6.8882 0 Td (work)Tj 2.279 0 Td (and)Tj 1.7291 0 Td (forcing)Tj 3.0898 0 Td (oneself)Tj 3.016 0 Td (to)Tj 1.0262 0 Td (be)Tj 1.1508 0 Td (explicit)Tj 3.1294 0 Td (about)Tj 2.4945 0 Td (decisions,)Tj 4.1329 0 Td (increasing)Tj -32.5358 -1.2983 Td (understandability)Tj 7.2623 0 Td (and)Tj 1.7291 0 Td (replicability.)Tj 5.1704 0 Td (For)Tj 1.627 0 Td (example,)Tj 3.7701 0 Td (many)Tj 2.4774 0 Td (types)Tj 2.2791 0 Td (of)Tj 1.0148 0 Td (social)Tj 2.4548 0 Td (media)Tj 2.6929 0 Td (and)Tj 1.7291 0 Td (other)Tj 2.3471 0 Td (trace)Tj -34.5541 -1.2982 Td (data)Tj 1.9105 0 Td (are)Tj 1.4513 0 Td (unstructured,)Tj 5.6523 0 Td (and)Tj 1.7291 0 Td (answers)Tj 3.4015 0 Td (to)Tj 1.0262 0 Td (even)Tj 2.0749 0 Td (basic)Tj 2.211 0 Td (questions)Tj 4.0195 0 Td (such)Tj 2.058 0 Td (as)Tj 1.0148 0 Td (network)Tj 3.5489 0 Td (ties,)Tj 1.7972 0 Td (location,)Tj 3.6793 0 Td (and)Tj -35.5745 -1.2983 Td (randomness)Tj 5.1193 0 Td (depend)Tj 3.1918 0 Td (on)Tj 1.2756 0 Td (the)Tj 1.4683 0 Td (steps)Tj 2.1827 0 Td (taken)Tj 2.4207 0 Td (to)Tj 1.0205 0 Td (collect)Tj 2.7893 0 Td (and)Tj 1.7291 0 Td (collate)Tj 2.8006 0 Td (data.)Tj 2.143 0 Td (Systems)Tj 3.4129 0 Td (of)Tj 1.0148 0 Td (auditability)Tj 4.7451 0 Td (clar-)Tj -35.3137 -1.3039 Td (ify)Tj 1.2302 0 Td (how)Tj 1.9389 0 Td (different)Tj 3.6566 0 Td (datasets)Tj 3.3562 0 Td (\(and)Tj 2.0693 0 Td (the)Tj 1.4683 0 Td (subsequent)Tj 4.7055 0 Td (analysis\))Tj 3.668 0 Td (differ)Tj 2.3868 0 Td (from)Tj 2.2053 0 Td (each)Tj 2.0296 0 Td (other,)Tj 2.5739 0 Td (aiding)Tj 2.7269 0 Td (under-)Tj -34.0155 -1.2983 Td (standing)Tj 3.668 0 Td (and)Tj 1.7291 0 Td (creating)Tj 3.4526 0 Td (better)Tj 2.5455 0 Td (research.)Tj -10.199 -1.2982 Td (Plan)Tj 2.0125 0 Td (for)Tj 1.3833 0 Td (and)Tj 1.7292 0 Td (welcome)Tj 3.7473 0 Td (audits)Tj 2.6363 0 Td (of)Tj 1.0147 0 Td (your)Tj 2.0807 0 Td (big)Tj 1.4626 0 Td (data)Tj 1.9106 0 Td (practices.)Tj ET Q q 1 j 1 J 0 w 11.9999 0 0 11.9999 200.0125 421.4551 cm BT /F2 1 Tf 1 TL -0.0033 Tc 0 0 Td (9.)Tj 1.0441 0 Td (Engage)Tj 3.8126 0 Td (with)Tj 2.2157 0 Td (the)Tj 1.7056 0 Td (broader)Tj 3.9165 0 Td (consequences)Tj 7.1103 0 Td (of)Tj 1.1575 0 Td (data)Tj 2.2583 0 Td (and)Tj 1.989 0 Td (analysis)Tj -25.2096 -1.1669 Td (practices)Tj ET Q q 1 j 1 J 0 w 10 0 0 10 200.0125 390.444 cm BT /F6 1 Tf 1 TL -0.004 Tc 0 0 Td (It)Tj 0.856 0 Td (is)Tj 0.8504 0 Td (also)Tj 1.7688 0 Td (important)Tj 4.2803 0 Td (for)Tj 1.3833 0 Td (responsible)Tj 4.7565 0 Td (big)Tj 1.457 0 Td (data)Tj 1.9162 0 Td (researchers)Tj 4.7112 0 Td (to)Tj 1.0261 0 Td (think)Tj 2.3584 0 Td (beyond)Tj 3.1861 0 Td (the)Tj 1.4684 0 Td (traditional)Tj 4.422 0 Td (metrics)Tj -34.4407 -1.2982 Td (of)Tj 1.0147 0 Td (success)Tj 3.1011 0 Td (in)Tj 1.0318 0 Td (business)Tj 3.5773 0 Td (and)Tj 1.7348 0 Td (the)Tj 1.4684 0 Td (academy.)Tj 3.9628 0 Td (For)Tj 1.627 0 Td (example,)Tj 3.7701 0 Td (the)Tj 1.474 0 Td (energy)Tj 2.9027 0 Td (demands)Tj 3.8664 0 Td (for)Tj 1.3833 0 Td (digital)Tj 2.7269 0 Td (daily)Tj 2.1487 0 Td (life,)Tj -35.79 -1.2983 Td (a)Tj 0.6519 0 Td (key)Tj 1.5931 0 Td (source)Tj 2.8346 0 Td (of)Tj 1.0148 0 Td (big)Tj 1.4627 0 Td (data)Tj 1.9105 0 Td (for)Tj 1.389 0 Td (social)Tj 2.4548 0 Td (science)Tj 3.0954 0 Td (research,)Tj 3.7814 0 Td (are)Tj 1.4456 0 Td (significant)Tj 4.388 0 Td (in)Tj 1.0375 0 Td (this)Tj 1.6724 0 Td (era)Tj 1.4514 0 Td (of)Tj 1.0148 0 Td (climate)Tj 3.1294 0 Td (change)Tj -34.3273 -1.3039 Td ([)Tj 0.83 0.64 0.02 0 k (36)Tj 0 g (].)Tj 2.0749 0 Td (How)Tj 2.177 0 Td (might)Tj 2.6022 0 Td (big)Tj 1.457 0 Td (data)Tj 1.9162 0 Td (research)Tj 3.5546 0 Td (lessen)Tj 2.5852 0 Td (the)Tj 1.4683 0 Td (environmental)Tj 6.1115 0 Td (impact)Tj 2.9763 0 Td (of)Tj 1.0148 0 Td (data)Tj 1.9163 0 Td (analytics)Tj 3.685 0 Td (work?)Tj -33.5393 -1.2983 Td (For)Tj 1.627 0 Td (example,)Tj 3.7701 0 Td (should)Tj 2.9196 0 Td (researchers)Tj 4.7112 0 Td (take)Tj 1.8709 0 Td (the)Tj 1.474 0 Td (lead)Tj 1.8481 0 Td (in)Tj 1.0375 0 Td (asking)Tj 2.7893 0 Td (cloud)Tj 2.4548 0 Td (storage)Tj 3.0784 0 Td (providers)Tj 4.0195 0 Td (and)Tj 1.7348 0 Td (data)Tj 1.9105 0 Td (pro-)Tj -35.2457 -1.2982 Td (cessing)Tj 3.067 0 Td (centers)Tj 3.0728 0 Td (to)Tj 1.0261 0 Td (shift)Tj 1.9616 0 Td (to)Tj 1.0261 0 Td (sustainable)Tj 4.6261 0 Td (and)Tj 1.7291 0 Td (renewable)Tj 4.2803 0 Td (energy)Tj 2.9083 0 Td (sources?)Tj 3.566 0 Td (As)Tj 1.2642 0 Td (important)Tj 4.2803 0 Td (and)Tj 1.7348 0 Td (pub-)Tj -34.5427 -1.2983 Td (licly)Tj 1.8538 0 Td (visible)Tj 2.7496 0 Td (users)Tj 2.262 0 Td (of)Tj 1.0148 0 Td (the)Tj 1.474 0 Td (cloud,)Tj 2.6759 0 Td (big)Tj 1.457 0 Td (data)Tj 1.9162 0 Td (researchers)Tj 4.7112 0 Td (collectively)Tj 4.6318 0 Td (represent)Tj 3.9628 0 Td (an)Tj 1.2018 0 Td (interest)Tj 3.2088 0 Td (group)Tj 2.6079 0 Td (that)Tj -35.7276 -1.3039 Td (could)Tj 2.4491 0 Td (rally)Tj 1.9729 0 Td (behind)Tj 3.0217 0 Td (such)Tj 2.0636 0 Td (a)Tj 0.652 0 Td (call)Tj 1.5703 0 Td (for)Tj 1.3833 0 Td (change.)Tj -11.9167 -1.2983 Td (The)Tj 1.7858 0 Td (pursuit)Tj 3.0897 0 Td (of)Tj 1.0148 0 Td (citations,)Tj 3.8494 0 Td (reputation,)Tj 4.6375 0 Td (or)Tj 1.0998 0 Td (money)Tj 2.9764 0 Td (is)Tj 0.8447 0 Td (a)Tj 0.652 0 Td (key)Tj 1.5987 0 Td (incentive)Tj 3.8721 0 Td (for)Tj 1.389 0 Td (pushing)Tj 3.4355 0 Td (research)Tj 3.5547 0 Td (for-)Tj -34.9963 -1.2982 Td (ward,)Tj 2.4604 0 Td (but)Tj 1.5534 0 Td (it)Tj 0.788 0 Td (can)Tj 1.6214 0 Td (also)Tj 1.7688 0 Td (result)Tj 2.4491 0 Td (in)Tj 1.0375 0 Td (unintended)Tj 4.8586 0 Td (and)Tj 1.7348 0 Td (undesirable)Tj 4.8528 0 Td (outcomes.)Tj 4.3143 0 Td (In)Tj 1.1112 0 Td (contrast,)Tj 3.685 0 Td (we)Tj 1.3266 0 Td (might)Tj 2.6022 0 Td (ask)Tj -36.1641 -1.304 Td (to)Tj 1.0261 0 Td (what)Tj 2.16 0 Td (extent)Tj 2.6815 0 Td (is)Tj 0.8447 0 Td (a)Tj 0.6577 0 Td (research)Tj 3.5546 0 Td (project)Tj 3.016 0 Td (focused)Tj 3.2769 0 Td (on)Tj 1.2756 0 Td (enhancing)Tj 4.4106 0 Td (the)Tj 1.4684 0 Td (public)Tj 2.7099 0 Td (good)Tj 2.2223 0 Td (or)Tj 1.0999 0 Td (the)Tj 1.4683 0 Td (underserved)Tj -31.8725 -1.2982 Td (of)Tj 1.0147 0 Td (society?)Tj 3.3279 0 Td (Are)Tj 1.6951 0 Td (questions)Tj 4.0195 0 Td (about)Tj 2.4945 0 Td (equity)Tj 2.6985 0 Td (or)Tj 1.0999 0 Td (promoting)Tj 4.524 0 Td (other)Tj 2.3471 0 Td (public)Tj 2.7043 0 Td (values)Tj 2.6702 0 Td (being)Tj 2.4321 0 Td (addressed)Tj 4.1782 0 Td (in)Tj -35.206 -1.2983 Td (one's)Tj 2.2733 0 Td (data)Tj 1.9162 0 Td (streams,)Tj 3.5093 0 Td (or)Tj 1.0998 0 Td (is)Tj 0.8504 0 Td (a)Tj 0.652 0 Td (big)Tj 1.4627 0 Td (data)Tj 1.9105 0 Td (focus)Tj 2.3301 0 Td (rendering)Tj 4.1669 0 Td (them)Tj 2.2847 0 Td (invisible)Tj 3.5659 0 Td (or)Tj 1.0942 0 Td (irrelevant)Tj 4.0592 0 Td (to)Tj 1.0261 0 Td (your)Tj 2.0806 0 Td (analysis)Tj -34.2819 -1.3039 Td ([)Tj 0.83 0.64 0.02 0 k (37)Tj 0 g (]?)Tj 2.2223 0 Td (How)Tj 2.177 0 Td (can)Tj 1.6271 0 Td (increasingly)Tj 5.0286 0 Td (vulnerable)Tj 4.405 0 Td (yet)Tj 1.3946 0 Td (fundamentally)Tj 6.0208 0 Td (important)Tj 4.2803 0 Td (public)Tj 2.7099 0 Td (resourcesÐsuch)Tj 6.7521 0 Td (as)Tj -36.6177 -1.2983 Td (state-mandated)Tj 6.4005 0 Td (cancer)Tj 2.8403 0 Td (registriesÐbe)Tj 5.6863 0 Td (protected?)Tj 4.3823 0 Td (How)Tj 2.177 0 Td (might)Tj 2.5965 0 Td (research)Tj 3.5547 0 Td (aid)Tj 1.4513 0 Td (or)Tj 1.0998 0 Td (inhibit)Tj 2.9027 0 Td (different)Tj -33.0914 -1.2982 Td (business)Tj 3.5773 0 Td (and)Tj 1.7291 0 Td (political)Tj 3.4242 0 Td (actors?)Tj 2.9877 0 Td (While)Tj 2.6532 0 Td (all)Tj 1.1452 0 Td (big)Tj 1.457 0 Td (data)Tj 1.9162 0 Td (research)Tj 3.5546 0 Td (need)Tj 2.143 0 Td (not)Tj 1.5761 0 Td (take)Tj 1.8708 0 Td (up)Tj 1.2643 0 Td (social)Tj 2.4604 0 Td (and)Tj 1.7291 0 Td (cultural)Tj -33.4882 -1.2983 Td (questions,)Tj 4.2462 0 Td (a)Tj 0.652 0 Td (fundamental)Tj 5.3177 0 Td (aim)Tj 1.7405 0 Td (of)Tj 1.0148 0 Td (research)Tj 3.5546 0 Td (goes)Tj 1.9786 0 Td (beyond)Tj 3.1861 0 Td (understanding)Tj 6.0718 0 Td (the)Tj 1.4683 0 Td (world)Tj 2.5569 0 Td (to)Tj 1.0261 0 Td (considering)Tj -32.8136 -1.3039 Td (ways)Tj 2.1543 0 Td (to)Tj 1.0261 0 Td (improve)Tj 3.5773 0 Td (it.)Tj -5.5615 -1.2983 Td (Recognize)Tj 4.3143 0 Td (that)Tj 1.7801 0 Td (doing)Tj 2.5342 0 Td (big)Tj 1.4626 0 Td (data)Tj 1.9106 0 Td (research)Tj 3.5546 0 Td (has)Tj 1.542 0 Td (societal-wide)Tj 5.4369 0 Td (effects.)Tj ET Q q 1 j 1 J 0 w 11.9999 0 0 11.9999 200.0125 125.9716 cm BT /F2 1 Tf 1 TL -0.0033 Tc 0 0 Td (10.)Tj 1.5921 0 Td (Know)Tj 2.9339 0 Td (when)Tj 2.7685 0 Td (to)Tj 1.1575 0 Td (break)Tj 2.8583 0 Td (these)Tj 2.8063 0 Td (rules)Tj ET Q q 1 j 1 J 0 w 10 0 0 10 200.0125 109.0204 cm BT /F6 1 Tf 1 TL -0.004 Tc 0 0 Td (The)Tj 1.7801 0 Td (final)Tj 2.0069 0 Td (\(and)Tj 2.0693 0 Td (counterintuitive\))Tj 7.0129 0 Td (rule)Tj 1.7858 0 Td (is)Tj 0.8504 0 Td (the)Tj 1.4683 0 Td (charge)Tj 2.8686 0 Td (to)Tj 1.0205 0 Td (recognize)Tj 4.0649 0 Td (when)Tj 2.4037 0 Td (it)Tj 0.7881 0 Td (is)Tj 0.8447 0 Td (appropriate)Tj 4.8812 0 Td (to)Tj 1.0205 0 Td (stray)Tj -34.8659 -1.3039 Td (from)Tj 2.2053 0 Td (these)Tj 2.2507 0 Td (rules.)Tj 2.3754 0 Td (For)Tj 1.6271 0 Td (example,)Tj 3.7757 0 Td (in)Tj 1.0375 0 Td (times)Tj 2.3867 0 Td (of)Tj 1.0148 0 Td (natural)Tj 3.0841 0 Td (disaster)Tj 3.2598 0 Td (or)Tj 1.0999 0 Td (a)Tj 0.6519 0 Td (public)Tj 2.7099 0 Td (health)Tj 2.6759 0 Td (emergency,)Tj 4.8019 0 Td (it)Tj 0.7823 0 Td (may)Tj -35.7389 -1.2982 Td (be)Tj 1.1508 0 Td (important)Tj 4.2803 0 Td (to)Tj 1.0261 0 Td (temporarily)Tj 4.9266 0 Td (put)Tj 1.5647 0 Td (aside)Tj 2.2337 0 Td (questions)Tj 4.0195 0 Td (of)Tj 1.0148 0 Td (individual)Tj 4.2916 0 Td (privacy)Tj 3.1408 0 Td (in)Tj 1.0375 0 Td (order)Tj 2.4208 0 Td (to)Tj 1.0261 0 Td (serve)Tj 2.2507 0 Td (a)Tj 0.6576 0 Td (larger)Tj ET Q q 1 j 1 J 0 w 576 737.1 m 36 737.1 l 36 737.6 l 576 737.6 l f* 36 741.2598 169.285 23.6976 re W* n q 169.2283 0 0 23.6409 36 741.3165 cm q /I0 Do Q Q Q q 0 0 612 792 re W* n 1 j 1 J 0 w 7.9999 0 0 7.9999 391.9748 745.7952 cm BT /F0 1 Tf 1 TL -0.005 Tc 0 0 Td (Ten)Tj 1.9204 0 Td (Simple)Tj 3.2245 0 Td (Rules)Tj 2.7283 0 Td (for)Tj 1.3678 0 Td (Respons)Tj 3.8764 0 Td (ible)Tj 1.7362 0 Td (Big)Tj 1.637 0 Td (Data)Tj 2.3032 0 Td (Researc)Tj 3.6567 0 Td (h)Tj ET Q q 1 j 1 J 0 w 36 48.0002 m 576 48.0002 l 576 47.5002 l 36 47.5002 l f* 0.83 0.64 0.02 0 k 142.5827 34.9228 m 295.8236 34.9228 l h f* 0 g 7.9999 0 0 7.9999 36 36 cm BT /F0 1 Tf 1 TL -0.005 Tc 0 0 Td (PLOS)Tj 2.8559 0 Td (Computationa)Tj 6.1229 0 Td (l)Tj 0.4322 0 Td (Biology)Tj 3.4371 0 Td (|)Tj 0.83 0.64 0.02 0 k 0.4748 0 Td (https:/)Tj 2.6788 0 Td (/doi.org/10.13)Tj 6.0094 0 Td (71/journal.p)Tj 5.1166 0 Td (cbi.1005399)Tj 0 g 6.3497 0 Td (March)Tj 2.9551 0 Td (30,)Tj 1.5874 0 Td (2017)Tj 27.1207 0 Td (7)Tj 0.7654 0 Td (/)Tj ET endstream endobj 139 0 obj < >stream endstream endobj 140 0 obj < >stream endstream endobj 141 0 obj < >stream endstream endobj 142 0 obj < >stream endstream endobj 143 0 obj < >stream BT 66.402 0 Td (10)Tj ET Q endstream endobj 144 0 obj < >/XObject< >>>/CropBox[0 0 612 792]/MediaBox[0 0 612 792]/Parent 9 0 R/Annots 146 0 R/Contents 147 0 R/TrimBox[0 0 612 792]>> endobj 145 0 obj < > endobj 148 0 obj < > endobj 151 0 obj < >stream   MinionPro-Regular 6ø  à á âøûµûü ú)   ±í   H?   HDÎ   H¹\        ' - 3 > H N T Z ` j q t {    ¢ ¦ ± · ¹ À Ç Í Ø ä ê ð ü   +2=CIU[cnz~£ª±¶¼ÂÍÖÜçíóý "(.9CIOU[elov|¡¬²´»ÂÈÓßåë÷û&-8>DPV^iuy¥¬±·½ÈÑ×âèîø &+/49@DGLPU[]_ahjmru|  ¨±½ÉÕäîõü  &-4;BIPW^elsz¤«²¹ÀÇÎÕÜãêñøÿ ")07>ELSZahov} §®µ¼ÃÊÑØßæíôû  %,3:AHOV]dkry¡¨¬¯´¸½ÃÅÇÎÐÓØÛâåèëðú#/;JTey£ª±¸¿ÆÍÔÛâéð÷þ    ! (extracted from312.html)
o÷§ ¿´÷÷½ ­ q÷k ÷& ¥ø³ ¦ø)¦ M q ÷÷aÙ÷,§¦ ltthc] Y`o ~|{nOÃfÐ ¯­} vea ø ¸·µûû ¨÷t£  ÃWwîw Òû¸ q÷j¥ ÷% ¤¢ÐÄÑ ÷Å6eÚvû4 »Ë ®ûÌ ¶bØ ð  Ò÷ Ó TD==Ã]Ñm ¢ÿ ÿ   cs³j B\^Xa«p©| ~I 2©÷Î÷\ ÷M ÷I û¥ùBw  ¤û]r ÷¹÷P ¦÷Ü / ¾Ù÷ ®x¸ ÷»±}ûB ÷°}û. v«vø»Ô øvøtndR ÷¹Ç©÷cÞ v}yop ^ªø%÷B ÷Ü endstream endobj 149 0 obj < > endobj 150 0 obj < >stream /CIDInit/ProcSet findresource begin 12 dict begin begincmap /CMapName/Adobe-Identity-UCS def /CIDSystemInfo< >def /CMapType 2 def 1 begincodespacerange <00> endcodespacerange 1 beginbfchar <01><00EF> endbfchar endcmap CMapName currentdict /CMap defineresource pop end end endstream endobj 146 0 obj [152 0 R 153 0 R 154 0 R 155 0 R 156 0 R 157 0 R 158 0 R 159 0 R] endobj 152 0 obj < >/Border[0 0 0]/Dest(Rpcbi.1005399.ref038)>> endobj 153 0 obj < >/Border[0 0 0]/Dest(Rpcbi.1005399.ref018)>> endobj 154 0 obj < >/Border[0 0 0]/A 160 0 R>> endobj 160 0 obj < > endobj 155 0 obj < >/Border[0 0 0]/Dest(Lpcbi.1005399.ref001)>> endobj 156 0 obj < >/Border[0 0 0]/A 161 0 R>> endobj 161 0 obj < > endobj 157 0 obj < >/Border[0 0 0]/A 162 0 R>> endobj 162 0 obj < > endobj 158 0 obj < >/Border[0 0 0]/Dest(Lpcbi.1005399.ref002)>> endobj 159 0 obj < >/Border[0 0 0]/A 163 0 R>> endobj 163 0 obj < > endobj 147 0 obj [164 0 R 165 0 R 166 0 R 167 0 R 168 0 R 169 0 R] endobj 164 0 obj < >stream q 0.83 0.64 0.02 0 k 265.5496 423.0425 m 275.0173 423.0425 l h f* 417.1465 397.0205 m 426.6709 397.0205 l h f* 384.8315 267.2504 m 493.115 267.2504 l 493.115 266.7969 l 384.8315 266.7969 l f* 287.5465 123.4205 m 560.6362 123.4205 l h f* 221.4992 113.8961 m 248.2016 113.8961 l h f* 0 g 1 j 1 J 0 w 10 0 0 10 200.0125 707.4141 cm BT /F6 1 Tf 1 TL -0.004 Tc 0 0 Td (public)Tj 2.7099 0 Td (good.)Tj 2.4491 0 Td (Likewise,)Tj 3.9061 0 Td (the)Tj 1.4683 0 Td (use)Tj 1.5364 0 Td (of)Tj 1.0148 0 Td (genetic)Tj 3.0671 0 Td (or)Tj 1.0998 0 Td (other)Tj 2.3471 0 Td (biological)Tj 4.0875 0 Td (data)Tj 1.9162 0 Td (collected)Tj 3.7417 0 Td (without)Tj 3.3279 0 Td (informed)Tj -32.6719 -1.3039 Td (consent)Tj 3.3278 0 Td (might)Tj 2.6022 0 Td (be)Tj 1.1452 0 Td (vital)Tj 1.9219 0 Td (in)Tj 1.0374 0 Td (managing)Tj 4.201 0 Td (an)Tj 1.2018 0 Td (emerging)Tj 4.0025 0 Td (disease)Tj 3.0217 0 Td (epidemic.)Tj -21.2653 -1.2982 Td (Moreover,)Tj 4.3993 0 Td (be)Tj 1.1509 0 Td (sure)Tj 1.9048 0 Td (to)Tj 1.0205 0 Td (review)Tj 2.846 0 Td (the)Tj 1.4683 0 Td (regulatory)Tj 4.32 0 Td (expectations)Tj 5.1873 0 Td (and)Tj 1.7292 0 Td (legal)Tj 2.0352 0 Td (demands)Tj 3.8665 0 Td (associated)Tj 4.2519 0 Td (with)Tj -35.3761 -1.2983 Td (protection)Tj 4.3823 0 Td (of)Tj 1.0148 0 Td (privacy)Tj 3.1407 0 Td (within)Tj 2.812 0 Td (your)Tj 2.0806 0 Td (dataset.)Tj 3.2258 0 Td (After)Tj 2.2847 0 Td (all,)Tj 1.372 0 Td (this)Tj 1.6724 0 Td (is)Tj 0.8504 0 Td (an)Tj 1.2019 0 Td (exceedingly)Tj 4.8869 0 Td (slippery)Tj 3.3789 0 Td (slope,)Tj 2.5001 0 Td (so)Tj -34.8035 -1.2983 Td (before)Tj 2.7382 0 Td (following)Tj 3.9798 0 Td (this)Tj 1.6724 0 Td (rule)Tj 1.7915 0 Td (\(to)Tj 1.3663 0 Td (break)Tj 2.4491 0 Td (others\),)Tj 3.2769 0 Td (be)Tj 1.1508 0 Td (cautious)Tj 3.566 0 Td (that)Tj 1.7745 0 Td (the)Tj 1.474 0 Td (ªemergencyº)Tj 5.3574 0 Td (is)Tj 0.8447 0 Td (not)Tj 1.5761 0 Td (simply)Tj 2.8856 0 Td (a)Tj -35.9033 -1.3039 Td (convenient)Tj 4.6601 0 Td (justification.)Tj 5.193 0 Td (The)Tj 1.7802 0 Td (best)Tj 1.8085 0 Td (way)Tj 1.7971 0 Td (to)Tj 1.0205 0 Td (ensure)Tj 2.88 0 Td (this)Tj 1.6724 0 Td (is)Tj 0.8504 0 Td (to)Tj 1.0204 0 Td (build)Tj 2.2961 0 Td (experience)Tj 4.507 0 Td (in)Tj 1.0375 0 Td (engaging)Tj 3.8381 0 Td (in)Tj 1.0375 0 Td (the)Tj -35.3988 -1.2982 Td (tough)Tj 2.5454 0 Td (debates)Tj 3.1918 0 Td (\(Rule)Tj 2.3811 0 Td (6\),)Tj 1.2586 0 Td (constructing)Tj 5.2611 0 Td (codes)Tj 2.4661 0 Td (of)Tj 1.0148 0 Td (conduct)Tj 3.4752 0 Td (\(Rule)Tj 2.3811 0 Td (7\),)Tj 1.2586 0 Td (and)Tj 1.7291 0 Td (developing)Tj 4.6035 0 Td (systems)Tj 3.2995 0 Td (for)Tj -34.8659 -1.2983 Td (auditing)Tj 3.5546 0 Td (\(Rule)Tj 2.3811 0 Td (8\).)Tj 1.2585 0 Td (The)Tj 1.7802 0 Td (more)Tj 2.3414 0 Td (mature)Tj 3.0954 0 Td (the)Tj 1.474 0 Td (community)Tj 4.8812 0 Td (of)Tj 1.0148 0 Td (researchers)Tj 4.7169 0 Td (is)Tj 0.8447 0 Td (about)Tj 2.4944 0 Td (their)Tj 2.109 0 Td (processes,)Tj -31.9462 -1.3039 Td (checks,)Tj 3.101 0 Td (and)Tj 1.7292 0 Td (balances,)Tj 3.821 0 Td (the)Tj 1.474 0 Td (better)Tj 2.5399 0 Td (equipped)Tj 3.9288 0 Td (it)Tj 0.788 0 Td (is)Tj 0.8447 0 Td (to)Tj 1.0261 0 Td (assess)Tj 2.5229 0 Td (when)Tj 2.4037 0 Td (breaking)Tj 3.7304 0 Td (the)Tj 1.474 0 Td (rules)Tj 2.1486 0 Td (is)Tj 0.8448 0 Td (acceptable.)Tj 4.5807 0 Td (It)Tj -36.9578 -1.2983 Td (may)Tj 1.9275 0 Td (very)Tj 1.9219 0 Td (well)Tj 1.8198 0 Td (be)Tj 1.1509 0 Td (that)Tj 1.7801 0 Td (you)Tj 1.7121 0 Td (do)Tj 1.2529 0 Td (not)Tj 1.5704 0 Td (come)Tj 2.3981 0 Td (to)Tj 1.0261 0 Td (a)Tj 0.652 0 Td (final)Tj 2.0013 0 Td (clear)Tj 2.1203 0 Td (set)Tj 1.3039 0 Td (of)Tj 1.0148 0 Td (practices.)Tj 3.9628 0 Td (After)Tj 2.2847 0 Td (all,)Tj 1.372 0 Td (just)Tj 1.6667 0 Td (as)Tj 1.0148 0 Td (privacy)Tj -33.9531 -1.2983 Td (is)Tj 0.8447 0 Td (not)Tj 1.576 0 Td (binary)Tj 2.8006 0 Td (\(Rule)Tj 2.3811 0 Td (2\),)Tj 1.2586 0 Td (neither)Tj 3.0841 0 Td (is)Tj 0.8447 0 Td (responsible)Tj 4.7565 0 Td (research.)Tj 3.7814 0 Td (Ethics)Tj 2.6645 0 Td (is)Tj 0.8504 0 Td (often)Tj 2.2847 0 Td (about)Tj 2.4945 0 Td (finding)Tj 3.1294 0 Td (a)Tj 0.6577 0 Td (good)Tj 2.2223 0 Td (or)Tj -35.6312 -1.3039 Td (better,)Tj 2.7665 0 Td (but)Tj 1.5534 0 Td (not)Tj 1.5704 0 Td (perfect,)Tj 3.1918 0 Td (answer,)Tj 3.2712 0 Td (and)Tj 1.7291 0 Td (it)Tj 0.7823 0 Td (is)Tj 0.8504 0 Td (important)Tj 4.2803 0 Td (to)Tj 1.0261 0 Td (ask)Tj 1.5137 0 Td (\(and)Tj 2.0693 0 Td (try)Tj 1.3436 0 Td (to)Tj 1.0262 0 Td (answer\))Tj 3.3845 0 Td (the)Tj 1.4684 0 Td (challenging)Tj -31.8272 -1.2982 Td (questions.)Tj 4.2462 0 Td (Only)Tj 2.2167 0 Td (through)Tj 3.4412 0 Td (this)Tj 1.6781 0 Td (engagement)Tj 5.0684 0 Td (can)Tj 1.627 0 Td (a)Tj 0.652 0 Td (culture)Tj 3.0387 0 Td (of)Tj 1.0148 0 Td (responsible)Tj 4.7565 0 Td (big)Tj 1.457 0 Td (data)Tj 1.9162 0 Td (research)Tj -31.1128 -1.2983 Td (emerge.)Tj 1.1962 -1.3039 Td (Understand)Tj 5.0002 0 Td (that)Tj 1.7802 0 Td (responsible)Tj 4.7508 0 Td (big)Tj 1.457 0 Td (data)Tj 1.9162 0 Td (research)Tj 3.5547 0 Td (depends)Tj 3.5489 0 Td (on)Tj 1.2756 0 Td (more)Tj 2.3414 0 Td (than)Tj 2.0296 0 Td (meeting)Tj 3.4639 0 Td (checklists.)Tj ET Q q 1 j 1 J 0 w 11.9999 0 0 11.9999 200.0125 493.1149 cm BT /F2 1 Tf 1 TL -0.0033 Tc 0 0 Td (Conclusion)Tj ET Q q 1 j 1 J 0 w 10 0 0 10 200.0125 476.107 cm BT /F6 1 Tf 1 TL -0.004 Tc 0 0 Td (The)Tj 1.7801 0 Td (goal)Tj 1.8709 0 Td (of)Tj 1.0147 0 Td (this)Tj 1.6781 0 Td (set)Tj 1.304 0 Td (of)Tj 1.0148 0 Td (ten)Tj 1.4853 0 Td (rules)Tj 2.1543 0 Td (is)Tj 0.8448 0 Td (to)Tj 1.0261 0 Td (help)Tj 1.9332 0 Td (researchers)Tj 4.7112 0 Td (do)Tj 1.2585 0 Td (better)Tj 2.5399 0 Td (work)Tj 2.279 0 Td (and)Tj 1.7291 0 Td (ultimately)Tj 4.235 0 Td (become)Tj -32.859 -1.2982 Td (more)Tj 2.3414 0 Td (successful)Tj 4.1612 0 Td (while)Tj 2.3697 0 Td (avoiding)Tj 3.6907 0 Td (larger)Tj 2.5342 0 Td (complications,)Tj 6.0604 0 Td (including)Tj 4.0422 0 Td (public)Tj 2.7099 0 Td (mistrust.)Tj 3.753 0 Td (To)Tj 1.338 0 Td (achieve)Tj -33.0007 -1.3039 Td (this,)Tj 1.8992 0 Td (however,)Tj 3.838 0 Td (scholars)Tj 3.4526 0 Td (must)Tj 2.2224 0 Td (shift)Tj 1.9615 0 Td (from)Tj 2.2054 0 Td (a)Tj 0.6576 0 Td (mindset)Tj 3.4639 0 Td (that)Tj 1.7802 0 Td (is)Tj 0.8447 0 Td (rigorous)Tj 3.6 0 Td (when)Tj 2.4037 0 Td (focused)Tj 3.2769 0 Td (on)Tj 1.2755 0 Td (techniques)Tj -32.8816 -1.2983 Td (and)Tj 1.7291 0 Td (methodology)Tj 5.5048 0 Td (and)Tj 1.7291 0 Td (na)Tj /F12 1 Tf [()]TJ /F6 1 Tf (ve)Tj 2.3358 0 Td (when)Tj 2.4037 0 Td (it)Tj 0.7881 0 Td (comes)Tj 2.7552 0 Td (to)Tj 1.0262 0 Td (ethics.)Tj 2.7439 0 Td (Statements)Tj 4.6034 0 Td (to)Tj 1.0205 0 Td (the)Tj 1.474 0 Td (effect)Tj 2.3641 0 Td (that)Tj 1.7801 0 Td (ªData)Tj 2.5115 0 Td (is)Tj 0.8504 0 Td ([sic])Tj -35.6199 -1.2983 Td (already)Tj 3.1124 0 Td (publicº)Tj 3.1011 0 Td ([)Tj 0.83 0.64 0.02 0 k (38)Tj 0 g (])Tj 1.8481 0 Td (are)Tj 1.4514 0 Td (unjustified)Tj 4.5127 0 Td (simplifications)Tj 6.0944 0 Td (of)Tj 1.0148 0 Td (much)Tj 2.5172 0 Td (more)Tj 2.3414 0 Td (complex)Tj 3.6226 0 Td (data)Tj 1.9106 0 Td (ecosystems)Tj -31.5267 -1.3039 Td (embedded)Tj 4.3993 0 Td (in)Tj 1.0375 0 Td (even)Tj 2.0692 0 Td (more)Tj 2.3414 0 Td (complex)Tj 3.6227 0 Td (and)Tj 1.7291 0 Td (contingent)Tj 4.5468 0 Td (social)Tj 2.4604 0 Td (practices.)Tj 3.9628 0 Td (Data)Tj 2.126 0 Td (are)Tj 1.4457 0 Td (people,)Tj 3.084 0 Td (and)Tj 1.7292 0 Td (to)Tj -34.5541 -1.2982 Td (maintain)Tj 3.8324 0 Td (a)Tj 0.6463 0 Td (rigorously)Tj 4.2973 0 Td (na)Tj /F12 1 Tf [()]TJ /F6 1 Tf (ve)Tj 2.33 0 Td (definition)Tj 4.1613 0 Td (to)Tj 1.0204 0 Td (the)Tj 1.4627 0 Td (contrary)Tj 3.6226 0 Td ([)Tj 0.83 0.64 0.02 0 k (18)Tj 0 g (])Tj 1.8482 0 Td (will)Tj 1.6611 0 Td (end)Tj 1.7121 0 Td (up)Tj 1.2643 0 Td (harming)Tj 3.6453 0 Td (research)Tj 3.5546 0 Td (efforts)Tj -35.0586 -1.2983 Td (in)Tj 1.0318 0 Td (the)Tj 1.474 0 Td (long)Tj 1.9842 0 Td (run)Tj 1.6668 0 Td (as)Tj 1.0148 0 Td (pushback)Tj 4.0138 0 Td (comes)Tj 2.7552 0 Td (from)Tj 2.2054 0 Td (the)Tj 1.474 0 Td (people)Tj 2.8516 0 Td (whose)Tj 2.7269 0 Td (actions)Tj 3.0558 0 Td (and)Tj 1.7291 0 Td (utterances)Tj 4.3313 0 Td (are)Tj 1.4513 0 Td (subject)Tj 3.0161 0 Td (to)Tj -36.7821 -1.3039 Td (analysis.)Tj 1.1962 -1.2983 Td (In)Tj 1.1111 0 Td (short,)Tj 2.5115 0 Td (responsible)Tj 4.7509 0 Td (big)Tj 1.4626 0 Td (data)Tj 1.9106 0 Td (research)Tj 3.5546 0 Td (is)Tj 0.8504 0 Td (not)Tj 1.576 0 Td (about)Tj 2.4945 0 Td (preventing)Tj 4.5354 0 Td (research)Tj 3.5546 0 Td (but)Tj 1.5534 0 Td (making)Tj 3.2542 0 Td (sure)Tj -34.316 -1.2983 Td (that)Tj 1.7744 0 Td (the)Tj 1.474 0 Td (work)Tj 2.2791 0 Td (is)Tj 0.8447 0 Td (sound,)Tj 2.9197 0 Td (accurate,)Tj 3.7814 0 Td (and)Tj 1.7291 0 Td (maximizes)Tj 4.49 0 Td (the)Tj 1.4684 0 Td (good)Tj 2.228 0 Td (while)Tj 2.3697 0 Td (minimizing)Tj 4.9039 0 Td (harm.)Tj 2.5966 0 Td (The)Tj 1.7801 0 Td (prob-)Tj -34.6391 -1.2982 Td (lems)Tj 2.0692 0 Td (and)Tj 1.7292 0 Td (choices)Tj 3.1521 0 Td (researchers)Tj 4.7111 0 Td (face)Tj 1.7915 0 Td (are)Tj 1.4513 0 Td (real,)Tj 1.9162 0 Td (complex,)Tj 3.8495 0 Td (and)Tj 1.7291 0 Td (challenging)Tj 4.8188 0 Td (and)Tj 1.7292 0 Td (so)Tj 1.0885 0 Td (too)Tj 1.5307 0 Td (must)Tj 2.228 0 Td (be)Tj 1.1508 0 Td (our)Tj -34.9452 -1.3039 Td (response.)Tj 3.9571 0 Td (We)Tj 1.6101 0 Td (must)Tj 2.228 0 Td (treat)Tj 2.0409 0 Td (big)Tj 1.4627 0 Td (data)Tj 1.9105 0 Td (research)Tj 3.5603 0 Td (with)Tj 1.9956 0 Td (the)Tj 1.4683 0 Td (respect)Tj 3.0387 0 Td (that)Tj 1.7802 0 Td (it)Tj 0.7823 0 Td (deserves)Tj 3.566 0 Td (and)Tj 1.7348 0 Td (recognize)Tj 4.0592 0 Td (that)Tj -35.1947 -1.2983 Td (unethical)Tj 3.9174 0 Td (research)Tj 3.5546 0 Td (undermines)Tj 5.04 0 Td (the)Tj 1.4683 0 Td (production)Tj 4.7169 0 Td (of)Tj 1.0148 0 Td (knowledge.)Tj 4.7678 0 Td (Fantastic)Tj 3.8097 0 Td (opportunities)Tj 5.6296 0 Td (to)Tj 1.0261 0 Td (better)Tj -34.9452 -1.2983 Td (understand)Tj 4.7791 0 Td (society)Tj 2.948 0 Td (and)Tj 1.7178 0 Td (our)Tj 1.6158 0 Td (world)Tj 2.5511 0 Td (exist,)Tj 2.2507 0 Td (but)Tj 1.5421 0 Td (with)Tj 1.9899 0 Td (these)Tj 2.245 0 Td (opportunities)Tj 5.6239 0 Td (also)Tj 1.7575 0 Td (come)Tj 2.3867 0 Td (the)Tj 1.457 0 Td (responsibil-)Tj -32.8646 -1.3039 Td (ity)Tj 1.2415 0 Td (to)Tj 1.0205 0 Td (consider)Tj 3.651 0 Td (the)Tj 1.4683 0 Td (ethics)Tj 2.5228 0 Td (of)Tj 1.0148 0 Td (our)Tj 1.6271 0 Td (choices)Tj 3.1521 0 Td (in)Tj 1.0375 0 Td (the)Tj 1.4683 0 Td (everyday)Tj 3.7644 0 Td (practices)Tj 3.7417 0 Td (and)Tj 1.7291 0 Td (actions)Tj 3.0558 0 Td (of)Tj 1.0148 0 Td (our)Tj 1.627 0 Td (research.)Tj -33.1367 -1.2982 Td (The)Tj 1.7801 0 Td (Council)Tj 3.4015 0 Td (for)Tj 1.389 0 Td (Big)Tj 1.5307 0 Td (Data,)Tj 2.3471 0 Td (Ethics,)Tj 2.897 0 Td (and)Tj 1.7291 0 Td (Society)Tj 3.0671 0 Td (\()Tj 0.83 0.64 0.02 0 k (http://bdes.datasociet)Tj 8.8894 0 Td (y.net/)Tj 0 g (\))Tj 2.8403 0 Td (provides)Tj 3.6453 0 Td (an)Tj 1.2019 0 Td (initial)Tj -34.7185 -1.2983 Td (set)Tj 1.3039 0 Td (of)Tj 1.0148 0 Td (case)Tj 1.8595 0 Td (studies,)Tj 3.2145 0 Td (papers,)Tj 3.067 0 Td (and)Tj 1.7291 0 Td (even)Tj 2.0693 0 Td (ten)Tj 1.491 0 Td (simple)Tj 2.8573 0 Td (rules)Tj 2.1487 0 Td (for)Tj 1.389 0 Td (guiding)Tj 3.2881 0 Td (this)Tj 1.6725 0 Td (process;)Tj 3.4072 0 Td (it)Tj 0.788 0 Td (is)Tj 0.8447 0 Td (now)Tj 1.9616 0 Td (incum-)Tj -34.1062 -1.3039 Td (bent)Tj 1.9955 0 Td (on)Tj 1.2756 0 Td (you)Tj 1.7121 0 Td (to)Tj 1.0205 0 Td (use)Tj 1.5364 0 Td (and)Tj 1.7291 0 Td (improve)Tj 3.5773 0 Td (these)Tj 2.2563 0 Td (in)Tj 1.0375 0 Td (your)Tj 2.0806 0 Td (research.)Tj ET Q q 1 j 1 J 0 w 11.9999 0 0 11.9999 200.0125 196.7811 cm BT /F2 1 Tf 1 TL -0.0033 Tc 0 0 Td (Acknowledgmen)Tj 7.8945 0 Td (ts)Tj ET Q q 1 j 1 J 0 w 10 0 0 10 200.0125 179.7732 cm BT /F6 1 Tf 1 TL -0.004 Tc 0 0 Td (This)Tj 1.9842 0 Td (article)Tj 2.6872 0 Td (also)Tj 1.7688 0 Td (benefitted)Tj 4.2236 0 Td (from)Tj 2.2054 0 Td (the)Tj 1.4683 0 Td (input)Tj 2.3811 0 Td (of)Tj 1.0148 0 Td (Geoff)Tj 2.4434 0 Td (Bowker)Tj 3.2769 0 Td (and)Tj 1.7348 0 Td (Helen)Tj 2.6248 0 Td (Nissenbaum.)Tj ET Q q 1 j 1 J 0 w 11.9999 0 0 11.9999 200.0125 147.5149 cm BT /F2 1 Tf 1 TL -0.0033 Tc 0 0 Td (References)Tj ET Q q 1 j 1 J 0 w 7.9999 0 0 7.9999 204.8881 134.022 cm BT /F2 1 Tf 1 TL -0.005 Tc 0 0 Td (1.)Tj /F0 1 Tf 2.0763 0 Td (Metcalf)Tj 3.3946 0 Td (J,)Tj 0.985 0 Td (boyd)Tj 2.3528 0 Td (d,)Tj 1.0417 0 Td (Keller)Tj 2.7213 0 Td (E.)Tj 1.1551 0 Td (Perspecti)Tj 4.0961 0 Td (ves)Tj 1.7575 0 Td (on)Tj 1.3111 0 Td (Big)Tj 1.637 0 Td (Data,)Tj 2.5724 0 Td (Ethics,)Tj 3.1749 0 Td (and)Tj 1.8567 0 Td (Society.)Tj 3.7205 0 Td (Council)Tj 3.4866 0 Td (for)Tj 1.3677 0 Td (Big)Tj 1.6371 0 Td (Data,)Tj 2.5724 0 Td (Ethics,)Tj -40.8405 -1.1905 Td (and)Tj 1.8568 0 Td (Society.)Tj 3.7205 0 Td (2016.)Tj 0.83 0.64 0.02 0 k 2.6787 0 Td (http://bdes)Tj 4.5922 0 Td (.datasociety.n)Tj 6.0662 0 Td (et/council-ou)Tj 5.563 0 Td (tput/perspec)Tj 5.4142 0 Td (tives-on-big-)Tj 5.4001 0 Td (data-eth)Tj 3.6 0 Td (ics-and-)Tj -38.8917 -1.1906 Td (society/)Tj 0 g (.)Tj 3.8268 0 Td (Accessed)Tj 4.493 0 Td (31)Tj 1.311 0 Td (May)Tj 2.0835 0 Td (2016.)Tj /F2 1 Tf -13.7906 -1.6228 Td (2.)Tj /F0 1 Tf 2.0763 0 Td (Wu)Tj 1.7079 0 Td (HY,)Tj 1.8638 0 Td (Rubinstein)Tj 4.8544 0 Td (M,)Tj 1.3181 0 Td (Shih)Tj 2.1827 0 Td (E,)Tj 1.1551 0 Td (Guttag)Tj 3.182 0 Td (JV,)Tj 1.6511 0 Td (Durand)Tj 3.4371 0 Td (F,)Tj 1.1055 0 Td (Freeman)Tj 4.1599 0 Td (WT.)Tj 2.0409 0 Td (Eulerian)Tj 3.8198 0 Td (video)Tj 2.5582 0 Td (magnificatio)Tj 5.2371 0 Td (n)Tj 0.7654 0 Td (for)Tj -41.039 -1.1835 Td (revealing)Tj 4.1954 0 Td (subtle)Tj 2.8346 0 Td (changes)Tj 3.9331 0 Td (in)Tj 0.978 0 Td (the)Tj 1.5874 0 Td (world.)Tj 2.8418 0 Td (Eulerian)Tj 3.8126 0 Td (Video)Tj 2.7283 0 Td (Magnificatio)Tj 5.2371 0 Td (n)Tj 0.7654 0 Td (for)Tj 1.3677 0 Td (Revealing)Tj 4.578 0 Td (Subtle)Tj 3.0047 0 Td (Changes)Tj 4.1457 0 Td (in)Tj -42.0098 -1.1906 Td (the)Tj 1.5875 0 Td (World.)Tj 3.0685 0 Td (ACM)Tj 2.4165 0 Td (Transact)Tj 3.8268 0 Td (ions)Tj 2.0197 0 Td (on)Tj 1.3111 0 Td (Graphics.)Tj 4.4291 0 Td (2012;)Tj 2.6788 0 Td (31\(4\).)Tj ET Q q 1 j 1 J 0 w 576 737.1 m 36 737.1 l 36 737.6 l 576 737.6 l f* 36 741.2598 169.285 23.6976 re W* n q 169.2283 0 0 23.6409 36 741.3165 cm q /I0 Do Q Q Q q 0 0 612 792 re W* n 1 j 1 J 0 w 7.9999 0 0 7.9999 391.9748 745.7952 cm BT /F0 1 Tf 1 TL -0.005 Tc 0 0 Td (Ten)Tj 1.9204 0 Td (Simple)Tj 3.2245 0 Td (Rules)Tj 2.7283 0 Td (for)Tj 1.3678 0 Td (Respons)Tj 3.8764 0 Td (ible)Tj 1.7362 0 Td (Big)Tj 1.637 0 Td (Data)Tj 2.3032 0 Td (Researc)Tj 3.6567 0 Td (h)Tj ET Q q 1 j 1 J 0 w 36 48.0002 m 576 48.0002 l 576 47.5002 l 36 47.5002 l f* 0.83 0.64 0.02 0 k 142.5827 34.9228 m 295.8236 34.9228 l h f* 0 g 7.9999 0 0 7.9999 36 36 cm BT /F0 1 Tf 1 TL -0.005 Tc 0 0 Td (PLOS)Tj 2.8559 0 Td (Computationa)Tj 6.1229 0 Td (l)Tj 0.4322 0 Td (Biology)Tj 3.4371 0 Td (|)Tj 0.83 0.64 0.02 0 k 0.4748 0 Td (https:/)Tj 2.6788 0 Td (/doi.org/10.13)Tj 6.0094 0 Td (71/journal.p)Tj 5.1166 0 Td (cbi.1005399)Tj 0 g 6.3497 0 Td (March)Tj 2.9551 0 Td (30,)Tj 1.5874 0 Td (2017)Tj 27.1207 0 Td (8)Tj 0.7654 0 Td (/)Tj ET endstream endobj 165 0 obj < >stream endstream endobj 166 0 obj < >stream endstream endobj 167 0 obj < >stream endstream endobj 168 0 obj < >stream endstream endobj 169 0 obj < >stream BT 66.402 0 Td (10)Tj ET Q endstream endobj 170 0 obj < >/XObject< >>>/CropBox[0 0 612 792]/MediaBox[0 0 612 792]/Parent 9 0 R/Annots 171 0 R/Contents 172 0 R/TrimBox[0 0 612 792]>> endobj 171 0 obj [173 0 R 174 0 R 175 0 R 176 0 R 177 0 R 178 0 R 179 0 R 180 0 R 181 0 R 182 0 R 183 0 R 184 0 R 185 0 R 186 0 R 187 0 R 188 0 R 189 0 R 190 0 R 191 0 R 192 0 R 193 0 R 194 0 R 195 0 R 196 0 R 197 0 R 198 0 R 199 0 R 200 0 R 201 0 R 202 0 R 203 0 R 204 0 R 205 0 R 206 0 R 207 0 R 208 0 R 209 0 R 210 0 R 211 0 R 212 0 R] endobj 173 0 obj < >/Border[0 0 0]/Dest(Lpcbi.1005399.ref003)>> endobj 174 0 obj < >/Border[0 0 0]/Dest(Lpcbi.1005399.ref004)>> endobj 175 0 obj < >/Border[0 0 0]/Dest(Lpcbi.1005399.ref005)>> endobj 176 0 obj < >/Border[0 0 0]/Dest(Lpcbi.1005399.ref006)>> endobj 177 0 obj < >/Border[0 0 0]/A 213 0 R>> endobj 213 0 obj < > endobj 178 0 obj < >/Border[0 0 0]/A 214 0 R>> endobj 214 0 obj < > endobj 179 0 obj < >/Border[0 0 0]/Dest(Lpcbi.1005399.ref007)>> endobj 180 0 obj < >/Border[0 0 0]/A 215 0 R>> endobj 215 0 obj < > endobj 181 0 obj < >/Border[0 0 0]/Dest(Lpcbi.1005399.ref008)>> endobj 182 0 obj < >/Border[0 0 0]/Dest(Lpcbi.1005399.ref009)>> endobj 183 0 obj < >/Border[0 0 0]/Dest(Lpcbi.1005399.ref010)>> endobj 184 0 obj < >/Border[0 0 0]/Dest(Lpcbi.1005399.ref011)>> endobj 185 0 obj < >/Border[0 0 0]/Dest(Lpcbi.1005399.ref012)>> endobj 186 0 obj < >/Border[0 0 0]/Dest(Lpcbi.1005399.ref013)>> endobj 187 0 obj < >/Border[0 0 0]/Dest(Lpcbi.1005399.ref014)>> endobj 188 0 obj < >/Border[0 0 0]/Dest(Lpcbi.1005399.ref015)>> endobj 189 0 obj < >/Border[0 0 0]/A 216 0 R>> endobj 216 0 obj < > endobj 190 0 obj < >/Border[0 0 0]/A 217 0 R>> endobj 217 0 obj < > endobj 191 0 obj < >/Border[0 0 0]/Dest(Lpcbi.1005399.ref016)>> endobj 192 0 obj < >/Border[0 0 0]/Dest(Lpcbi.1005399.ref017)>> endobj 193 0 obj < >/Border[0 0 0]/Dest(Lpcbi.1005399.ref018)>> endobj 194 0 obj < >/Border[0 0 0]/A 218 0 R>> endobj 218 0 obj < > endobj 195 0 obj < >/Border[0 0 0]/A 219 0 R>> endobj 219 0 obj < > endobj 196 0 obj < >/Border[0 0 0]/Dest(Lpcbi.1005399.ref019)>> endobj 197 0 obj < >/Border[0 0 0]/A 220 0 R>> endobj 220 0 obj < > endobj 198 0 obj < >/Border[0 0 0]/Dest(Lpcbi.1005399.ref020)>> endobj 199 0 obj < >/Border[0 0 0]/Dest(Lpcbi.1005399.ref021)>> endobj 200 0 obj < >/Border[0 0 0]/Dest(Lpcbi.1005399.ref022)>> endobj 201 0 obj < >/Border[0 0 0]/Dest(Lpcbi.1005399.ref023)>> endobj 202 0 obj < >/Border[0 0 0]/Dest(Lpcbi.1005399.ref024)>> endobj 203 0 obj < >/Border[0 0 0]/Dest(Lpcbi.1005399.ref025)>> endobj 204 0 obj < >/Border[0 0 0]/Dest(Lpcbi.1005399.ref026)>> endobj 205 0 obj < >/Border[0 0 0]/Dest(Lpcbi.1005399.ref027)>> endobj 206 0 obj < >/Border[0 0 0]/A 221 0 R>> endobj 221 0 obj < > endobj 207 0 obj < >/Border[0 0 0]/A 222 0 R>> endobj 222 0 obj < > endobj 208 0 obj < >/Border[0 0 0]/Dest(Lpcbi.1005399.ref028)>> endobj 209 0 obj < >/Border[0 0 0]/A 223 0 R>> endobj 223 0 obj < > endobj 210 0 obj < >/Border[0 0 0]/A 224 0 R>> endobj 224 0 obj < > endobj 211 0 obj < >/Border[0 0 0]/A 225 0 R>> endobj 225 0 obj < > endobj 212 0 obj < >/Border[0 0 0]/A 226 0 R>> endobj 226 0 obj < > endobj 172 0 obj [227 0 R 228 0 R 229 0 R 230 0 R 231 0 R 232 0 R 233 0 R 234 0 R 235 0 R 236 0 R 237 0 R 238 0 R 239 0 R 240 0 R 241 0 R 242 0 R 243 0 R 244 0 R 245 0 R 246 0 R 247 0 R 248 0 R 249 0 R 250 0 R 251 0 R 252 0 R 253 0 R] endobj 227 0 obj < >stream q 0.83 0.64 0.02 0 k 502.7528 640.1197 m 545.7827 640.1197 l h f* 221.4992 630.652 m 485.5748 630.652 l h f* 270.822 598.6205 m 485.6315 598.6205 l h f* 465.052 437.6693 m 559.2189 437.6693 l h f* 221.4992 428.1449 m 444.189 428.1449 l h f* 531.6094 370.148 m 551.3953 370.148 l h f* 221.4992 360.6236 m 495.2693 360.6236 l h f* 274.9039 328.6488 m 503.6031 328.6488 l h f* 331.3701 120.1323 m 487.7291 120.1323 l h f* 513.1843 120.1323 m 548.1638 120.1323 l h f* 551.6787 107.1496 m 575.4331 107.1496 l h f* 221.4992 97.6252 m 330.2929 97.6252 l h f* 355.748 97.6252 m 390.6709 97.6252 l h f* 0 g 1 j 1 J 0 w 7.9999 0 0 7.9999 204.8881 708.7181 cm BT /F2 1 Tf 1 TL -0.005 Tc 0 0 Td (3.)Tj /F0 1 Tf 2.0763 0 Td (Crawford)Tj 4.2095 0 Td (K,)Tj 1.1552 0 Td (Schultz)Tj 3.4441 0 Td (J.)Tj 0.9921 0 Td (Big)Tj 1.637 0 Td (data)Tj 2.1331 0 Td (and)Tj 1.8496 0 Td (due)Tj 1.8567 0 Td (process:)Tj 3.9402 0 Td (Toward)Tj 3.5008 0 Td (a)Tj 0.7654 0 Td (framework)Tj 4.8189 0 Td (to)Tj 1.0418 0 Td (redress)Tj 3.4937 0 Td (predictive)Tj 4.4221 0 Td (privacy)Tj -39.2602 -1.1905 Td (harms.)Tj 3.2316 0 Td (BCL)Tj 2.1401 0 Td (Rev.)Tj 2.2394 0 Td (2014;)Tj 2.6788 0 Td (55:)Tj 1.5874 0 Td (93±128.)Tj /F2 1 Tf -13.9536 -1.6229 Td (4.)Tj /F0 1 Tf 2.0763 0 Td (Barocas)Tj 3.8268 0 Td (S,)Tj 1.1552 0 Td (Selbst)Tj 2.9551 0 Td (AD.)Tj 1.8638 0 Td (Big)Tj 1.637 0 Td (data's)Tj 2.8418 0 Td (disparat)Tj 3.4866 0 Td (e)Tj 0.7654 0 Td (impact.)Tj 3.3945 0 Td (California)Tj 4.3583 0 Td (Law)Tj 2.0197 0 Td (Review.)Tj 3.7134 0 Td (2016;)Tj 2.6788 0 Td (104\(3\):)Tj 3.3307 0 Td (671±732.)Tj /F2 1 Tf -40.1034 -1.6228 Td (5.)Tj /F0 1 Tf 2.0763 0 Td (Danyllo)Tj 3.4867 0 Td (WA,)Tj 2.0976 0 Td (Alisson)Tj 3.3804 0 Td (VB,)Tj 1.8142 0 Td (Alexandre)Tj 4.6417 0 Td (ND,)Tj 1.9205 0 Td (Moacir)Tj 3.1678 0 Td (LM,)Tj 1.8638 0 Td (Jansepetr)Tj 4.3157 0 Td (us)Tj 1.2615 0 Td (BP,)Tj 1.8142 0 Td (Oliveira)Tj 3.5433 0 Td (RF.)Tj 1.8142 0 Td (Identifying)Tj 4.6914 0 Td (relevant)Tj -39.813 -1.1906 Td (users)Tj 2.6221 0 Td (and)Tj 1.8567 0 Td (groups)Tj 3.2245 0 Td (in)Tj 0.9779 0 Td (the)Tj 1.5874 0 Td (context)Tj 3.3945 0 Td (of)Tj 1.0418 0 Td (credit)Tj 2.615 0 Td (analysis)Tj 3.763 0 Td (based)Tj 2.8984 0 Td (on)Tj 1.3111 0 Td (data)Tj 2.126 0 Td (from)Tj 2.1968 0 Td (Twitter.)Tj 3.4512 0 Td (InCloud)Tj 3.6001 0 Td (and)Tj 1.8567 0 Td (Green)Tj 2.9551 0 Td (Com-)Tj -41.4783 -1.1834 Td (puting)Tj 2.8914 0 Td (\(CGC\),)Tj 3.3449 0 Td (2013)Tj 2.4024 0 Td (Third)Tj 2.4591 0 Td (Internat)Tj 3.3307 0 Td (ional)Tj 2.2819 0 Td (Conference)Tj 5.3008 0 Td (on)Tj 1.3111 0 Td (2013)Tj 2.4024 0 Td (Sep)Tj 1.9701 0 Td (30)Tj 1.311 0 Td (\(pp.)Tj 1.9134 0 Td (587±592\).)Tj 4.6418 0 Td (IEEE.)Tj /F2 1 Tf -37.6373 -1.63 Td (6.)Tj /F0 1 Tf 2.0763 0 Td (Angwin)Tj 3.4371 0 Td (J,)Tj 0.9921 0 Td (Larson)Tj 3.2244 0 Td (J,)Tj 0.9851 0 Td (Mattu)Tj 2.6858 0 Td (S,)Tj 1.1552 0 Td (Kirchner)Tj 3.8764 0 Td (L.)Tj 1.0417 0 Td (Machine)Tj 3.9331 0 Td (bias.)Tj 2.2961 0 Td (Pro)Tj 1.7504 0 Td (Publica.)Tj 3.7134 0 Td (23)Tj 1.3111 0 Td (May)Tj 2.0834 0 Td (2016.)Tj 0.83 0.64 0.02 0 k 2.6717 0 Td (https://www)Tj 5.0953 0 Td (.)Tj -40.2523 -1.1834 Td (propublica.o)Tj 5.3363 0 Td (rg/article/mac)Tj 5.9032 0 Td (hine-bias-r)Tj 4.6276 0 Td (isk-assessme)Tj 5.967 0 Td (nts-in-)Tj 2.7213 0 Td (criminal-sente)Tj 6.1087 0 Td (ncing)Tj 0 g (.)Tj 2.8417 0 Td (Accesse)Tj 3.7205 0 Td (d)Tj 0.7654 0 Td (4)Tj 0.7653 0 Td (September)Tj -38.757 -1.1906 Td (2016.)Tj /F2 1 Tf -2.0763 -1.6229 Td (7.)Tj /F0 1 Tf 2.0763 0 Td (Ingold)Tj 2.8914 0 Td (D,)Tj 1.2047 0 Td (Spencer)Tj 3.8835 0 Td (S.)Tj 1.1552 0 Td (Amazon)Tj 3.8339 0 Td (Doesn't)Tj 3.5504 0 Td (Consider)Tj 4.1457 0 Td (the)Tj 1.5874 0 Td (Race)Tj 2.5158 0 Td (of)Tj 1.0417 0 Td (Its)Tj 1.2614 0 Td (Custome)Tj 3.9402 0 Td (rs.)Tj 1.3181 0 Td (Should)Tj 3.2741 0 Td (It?)Tj 1.3181 0 Td (Bloomb)Tj 3.3307 0 Td (erg.com)Tj -40.2523 -1.1905 Td (21)Tj 1.3111 0 Td (April)Tj 2.1756 0 Td (2016.)Tj 0.83 0.64 0.02 0 k 2.6787 0 Td (http://www)Tj 4.5993 0 Td (.bloomberg.c)Tj 5.6835 0 Td (om/graphic)Tj 4.8615 0 Td (s/2016-am)Tj 4.6488 0 Td (azon-same-)Tj 5.1945 0 Td (day/)Tj 0 g (.)Tj 2.3528 0 Td (Accessed)Tj 4.493 0 Td (12)Tj 1.311 0 Td (June)Tj 2.3457 0 Td (2016.)Tj /F2 1 Tf -43.7318 -1.6229 Td (8.)Tj /F0 1 Tf 2.0763 0 Td (Hauge)Tj 3.1111 0 Td (MV,)Tj 1.9843 0 Td (Stevenson)Tj 4.8685 0 Td (MD,)Tj 2.0339 0 Td (Rossmo)Tj 3.8339 0 Td (DK,)Tj 1.8638 0 Td (Le)Tj 1.311 0 Td (Comber)Tj 3.7205 0 Td (SC.)Tj 1.8638 0 Td (Tagging)Tj 3.7701 0 Td (Banksy:)Tj 3.7276 0 Td (using)Tj 2.5583 0 Td (geographic)Tj 5.074 0 Td (profiling)Tj 3.6426 0 Td (to)Tj -43.3634 -1.1834 Td (investigate)Tj 4.9111 0 Td (a)Tj 0.7654 0 Td (modern)Tj 3.5504 0 Td (art)Tj 1.3748 0 Td (myster)Tj 2.9551 0 Td (y.)Tj 0.9922 0 Td (Journal)Tj 3.437 0 Td (of)Tj 1.0347 0 Td (Spatial)Tj 3.2173 0 Td (Science.)Tj 3.9969 0 Td (2016;)Tj 2.6717 0 Td (61\(1\):185±)Tj 4.748 0 Td (90.)Tj /F2 1 Tf -35.7309 -1.63 Td (9.)Tj /F0 1 Tf 2.0763 0 Td (Metcalf)Tj 3.3946 0 Td (J,)Tj 0.985 0 Td (Crawford)Tj 4.2095 0 Td (K.)Tj 1.1551 0 Td (Where)Tj 3.1252 0 Td (are)Tj 1.6371 0 Td (Human)Tj 3.3945 0 Td (Subjects)Tj 3.9898 0 Td (in)Tj 0.9779 0 Td (Big)Tj 1.637 0 Td (Data)Tj 2.3032 0 Td (Resea)Tj 2.8347 0 Td (rch?)Tj 2.1331 0 Td (The)Tj 1.9205 0 Td (Emerging)Tj 4.4291 0 Td (Ethics)Tj 2.8985 0 Td (Divide.)Tj -41.0248 -1.1834 Td (The)Tj 1.9205 0 Td (Emergi)Tj 3.1111 0 Td (ng)Tj 1.311 0 Td (Ethics)Tj 2.9055 0 Td (Divide.)Tj 3.2174 0 Td (Big)Tj 1.637 0 Td (Data)Tj 2.2961 0 Td (and)Tj 1.8567 0 Td (Society,)Tj 3.7205 0 Td (2016.)Tj /F2 1 Tf -24.5977 -1.63 Td (10.)Tj /F0 1 Tf 2.6219 0 Td (Zwitter)Tj 3.1749 0 Td (A.)Tj 1.1551 0 Td (Big)Tj 1.637 0 Td (data)Tj 2.1331 0 Td (ethics.)Tj 3.0614 0 Td (Big)Tj 1.6371 0 Td (Data)Tj 2.2961 0 Td (&)Tj 0.8787 0 Td (Society.)Tj 3.7205 0 Td (2014;)Tj 2.6788 0 Td (1\(2\).)Tj /F2 1 Tf -24.9946 -1.6228 Td (11.)Tj /F0 1 Tf 2.6219 0 Td (Nissenbaum)Tj 5.6836 0 Td (H.)Tj 1.2047 0 Td (Privacy)Tj 3.4512 0 Td (in)Tj 0.978 0 Td (context)Tj 3.1677 0 Td (:)Tj 0.4961 0 Td (Technology)Tj 5.0811 0 Td (,)Tj 0.4961 0 Td (policy,)Tj 2.9976 0 Td (and)Tj 1.8567 0 Td (the)Tj 1.5875 0 Td (integrity)Tj 3.6567 0 Td (of)Tj 1.0346 0 Td (social)Tj 2.7284 0 Td (life.)Tj 1.963 0 Td (Stanfor)Tj 3.1677 0 Td (d)Tj 0.7654 0 Td (Univer-)Tj -40.3161 -1.1906 Td (sity)Tj 1.6938 0 Td (Press;)Tj 3.0189 0 Td (2009.)Tj /F2 1 Tf -7.3346 -1.6228 Td (12.)Tj /F0 1 Tf 2.6219 0 Td (Marwick)Tj 3.8268 0 Td (AE.)Tj 1.8142 0 Td (boyd)Tj 2.3528 0 Td (d.)Tj 1.0417 0 Td (Networke)Tj 4.1528 0 Td (d)Tj 0.7654 0 Td (privacy:)Tj 3.6071 0 Td (How)Tj 2.1898 0 Td (teenagers)Tj 4.5851 0 Td (negotiate)Tj 4.2591 0 Td (context)Tj 3.3945 0 Td (in)Tj 0.9779 0 Td (social)Tj 2.7213 0 Td (media.)Tj 3.1748 0 Td (New)Tj 2.1827 0 Td (Media)Tj -41.046 -1.1835 Td (&)Tj 0.8788 0 Td (Society.)Tj 3.7205 0 Td (2014:1461444)Tj 6.2788 0 Td (814543)Tj 3.2669 0 Td (995.)Tj /F2 1 Tf -16.7669 -1.6299 Td (13.)Tj /F0 1 Tf 2.6219 0 Td (Tene)Tj 2.4662 0 Td (O,)Tj 1.2685 0 Td (Polonets)Tj 3.8197 0 Td (ky)Tj 1.2048 0 Td (J.)Tj 0.9921 0 Td (Theory)Tj 3.2811 0 Td (of)Tj 1.0418 0 Td (Creepy:)Tj 3.6638 0 Td (Technolog)Tj 4.5851 0 Td (y,)Tj 0.9921 0 Td (Privacy)Tj 3.4441 0 Td (and)Tj 1.8567 0 Td (Shifting)Tj 3.4937 0 Td (Social)Tj 2.8914 0 Td (Norms,)Tj 3.3945 0 Td (A.)Tj 1.1551 0 Td (Yale)Tj 2.1827 0 Td (JL)Tj 1.2615 0 Td (&)Tj -42.9949 -1.1835 Td (Tech.)Tj 2.6859 0 Td (2013;)Tj 2.6788 0 Td (16:59.)Tj /F2 1 Tf -7.9866 -1.63 Td (14.)Tj /F0 1 Tf 2.6219 0 Td (Massey)Tj 3.6142 0 Td (DS,)Tj 1.8709 0 Td (Denton)Tj 3.3875 0 Td (NA.)Tj 1.8637 0 Td (American)Tj 4.3796 0 Td (aparth)Tj 2.778 0 Td (eid:)Tj 1.8 0 Td (Segregatio)Tj 4.7481 0 Td (n)Tj 0.7653 0 Td (and)Tj 1.8567 0 Td (the)Tj 1.5875 0 Td (making)Tj 3.3874 0 Td (of)Tj 1.0417 0 Td (the)Tj 1.5874 0 Td (undercl)Tj 3.2103 0 Td (ass.)Tj 2.2536 0 Td (Harvard)Tj -40.1319 -1.1834 Td (University)Tj 4.5284 0 Td (Press;)Tj 3.0189 0 Td (1993.)Tj /F2 1 Tf -10.1692 -1.6229 Td (15.)Tj /F0 1 Tf 2.6219 0 Td (Davidow)Tj 3.9828 0 Td (B.)Tj 1.1551 0 Td (Redlining)Tj 4.3016 0 Td (for)Tj 1.3677 0 Td (the)Tj 1.5874 0 Td (21st)Tj 2.0764 0 Td (Century.)Tj 3.9402 0 Td (The)Tj 1.9205 0 Td (Atlanti)Tj 2.7213 0 Td (c.)Tj 0.9921 0 Td (5)Tj 0.7654 0 Td (March)Tj 2.9551 0 Td (2014.)Tj 0.83 0.64 0.02 0 k 2.6788 0 Td (http://www)Tj 4.5992 0 Td (.theatlantic)Tj 4.7481 0 Td (.com/)Tj -39.7917 -1.1905 Td (business/ar)Tj 5.0245 0 Td (chive/2014)Tj 4.7481 0 Td (/03/red)Tj 3.0543 0 Td (lining-for-the-2)Tj 6.3213 0 Td (1st-centur)Tj 4.3725 0 Td (y/284235)Tj 4.0394 0 Td (/)Tj 0 g (.)Tj 0.7725 0 Td (Accesse)Tj 3.7205 0 Td (d)Tj 0.7653 0 Td (31)Tj 1.311 0 Td (May)Tj 2.0835 0 Td (2016.)Tj /F2 1 Tf -38.8348 -1.6229 Td (16.)Tj /F0 1 Tf 2.6219 0 Td (Young)Tj 3.0615 0 Td (JC,)Tj 1.7008 0 Td (Gilmore)Tj 3.6638 0 Td (MP.)Tj 1.9772 0 Td (Subaltern)Tj 4.4221 0 Td (empowermen)Tj 5.9669 0 Td (t)Tj 0.4961 0 Td (in)Tj 0.978 0 Td (the)Tj 1.5803 0 Td (Geoweb:)Tj 4.167 0 Td (Tensions)Tj 4.2094 0 Td (between)Tj 3.9331 0 Td (publicity)Tj 3.756 0 Td (and)Tj 1.8567 0 Td (pri-)Tj -41.7689 -1.1905 Td (vacy.)Tj 2.5229 0 Td (Antipode.)Tj 4.3654 0 Td (2014;)Tj 2.6787 0 Td (46\(2\):574±)Tj 4.7481 0 Td (91.)Tj /F2 1 Tf -16.937 -1.6229 Td (17.)Tj /F0 1 Tf 2.6219 0 Td (Barbaro)Tj 3.7135 0 Td (M,)Tj 1.3252 0 Td (Zeller)Tj 2.6716 0 Td (T,)Tj 1.0985 0 Td (Hansell)Tj 3.4866 0 Td (S.)Tj 1.1551 0 Td (A)Tj 0.8859 0 Td (face)Tj 2.0764 0 Td (is)Tj 0.9283 0 Td (exposed)Tj 3.9331 0 Td (for)Tj 1.3678 0 Td (AOL)Tj 2.1968 0 Td (searcher)Tj 4.0465 0 Td (no.)Tj 1.5803 0 Td (4417749.)Tj 4.3158 0 Td (New)Tj 2.1898 0 Td (York)Tj 2.2465 0 Td (Times.)Tj -39.2177 -1.1905 Td (2006)Tj 2.4024 0 Td (Aug)Tj 1.9701 0 Td (9;9.)Tj /F2 1 Tf -6.9944 -1.6229 Td (18.)Tj /F0 1 Tf 2.6219 0 Td (Cox)Tj 1.9701 0 Td (J.)Tj 0.9922 0 Td (70,000)Tj 3.2173 0 Td (OkCupid)Tj 4.0465 0 Td (Users)Tj 2.7922 0 Td (Just)Tj 2.0267 0 Td (Had)Tj 2.0268 0 Td (Their)Tj 2.4591 0 Td (Data)Tj 2.2961 0 Td (Published)Tj 4.3016 0 Td (.)Tj 0.4961 0 Td (Motherboard.)Tj 6.0732 0 Td (12)Tj 1.3111 0 Td (May)Tj 2.0835 0 Td (2016.)Tj 0.83 0.64 0.02 0 k 2.6716 0 Td (http://)Tj -38.7641 -1.1906 Td (motherbo)Tj 4.1528 0 Td (ard.vice.com/)Tj 5.8536 0 Td (read/70000-)Tj 5.2938 0 Td (okcupid-us)Tj 4.748 0 Td (ers-just-had-)Tj 5.5135 0 Td (their-data-)Tj 4.4717 0 Td (publishe)Tj 3.6425 0 Td (d)Tj 0 g (.)Tj 1.0417 0 Td (Accessed)Tj 4.493 0 Td (12)Tj 1.311 0 Td (June)Tj -40.5216 -1.1834 Td (2016.)Tj /F2 1 Tf -2.6219 -1.63 Td (19.)Tj /F0 1 Tf 2.6219 0 Td (Pandurang)Tj 4.8048 0 Td (an)Tj 1.311 0 Td (V.)Tj 1.1552 0 Td (On)Tj 1.5378 0 Td (Taxis)Tj 2.5724 0 Td (and)Tj 1.8567 0 Td (Rainbows)Tj 4.3087 0 Td (:)Tj 0.4961 0 Td (Lessons)Tj 3.8835 0 Td (from)Tj 2.1969 0 Td (NYC's)Tj 3.0047 0 Td (improperly)Tj 4.8048 0 Td (anonymiz)Tj 4.2024 0 Td (ed)Tj 1.311 0 Td (taxi)Tj 1.7504 0 Td (logs.)Tj -39.1964 -1.1834 Td (Medium.)Tj 3.9969 0 Td (2014.)Tj 0.83 0.64 0.02 0 k 2.6788 0 Td (https://)Tj 2.9551 0 Td (medium.com)Tj 5.641 0 Td (/@vijayp/of-tax)Tj 6.5198 0 Td (is-and-rain)Tj 4.6275 0 Td (bows-f6bc289)Tj 6.1229 0 Td (679a1)Tj 0 g (.)Tj 3.2174 0 Td (Accessed)Tj 4.4929 0 Td (10)Tj -40.2523 -1.1906 Td (Novemb)Tj 3.6639 0 Td (er)Tj 1.0984 0 Td (2015.)Tj /F2 1 Tf -7.3842 -1.6228 Td (20.)Tj /F0 1 Tf 2.6219 0 Td (Sweeney)Tj 4.2591 0 Td (L.)Tj 1.0347 0 Td (k-anonymity:)Tj 5.7969 0 Td (A)Tj 0.8716 0 Td (model)Tj 2.8914 0 Td (for)Tj 1.3606 0 Td (protectin)Tj 3.7631 0 Td (g)Tj 0.7653 0 Td (privacy.)Tj 3.6001 0 Td (Interna)Tj 3.0543 0 Td (tional)Tj 2.5583 0 Td (Journal)Tj 3.4299 0 Td (of)Tj 1.0347 0 Td (Uncertaint)Tj 4.4717 0 Td (y,)Tj 0.985 0 Td (Fuzziness)Tj -39.8767 -1.1906 Td (and)Tj 1.8568 0 Td (Knowled)Tj 3.763 0 Td (ge-Based)Tj 4.4291 0 Td (Systems.)Tj 4.2804 0 Td (2002;)Tj 2.6788 0 Td (10\(05\):557)Tj 4.748 0 Td (±70.)Tj /F2 1 Tf -24.378 -1.6229 Td (21.)Tj /F0 1 Tf 2.6219 0 Td (Zimmer)Tj 3.5646 0 Td (M.)Tj 1.3182 0 Td (ªBut)Tj 2.0338 0 Td (the)Tj 1.5804 0 Td (data)Tj 2.133 0 Td (is)Tj 0.9284 0 Td (already)Tj 3.437 0 Td (publicº)Tj 2.8772 0 Td (:)Tj 0.4961 0 Td (on)Tj 1.311 0 Td (the)Tj 1.5874 0 Td (ethics)Tj 2.7851 0 Td (of)Tj 1.0417 0 Td (research)Tj 4.0465 0 Td (in)Tj 0.978 0 Td (Facebook)Tj 4.3158 0 Td (.)Tj 0.496 0 Td (Ethics)Tj 2.8985 0 Td (and)Tj 1.8567 0 Td (informa-)Tj -39.6854 -1.1834 Td (tion)Tj 1.8001 0 Td (technolog)Tj 4.252 0 Td (y.)Tj 0.9921 0 Td (2010;)Tj 2.6717 0 Td (12\(4\):313±)Tj 4.748 0 Td (25.)Tj /F2 1 Tf -17.0858 -1.63 Td (22.)Tj /F0 1 Tf 2.6219 0 Td (Klouman)Tj 3.8765 0 Td (n)Tj 0.7653 0 Td (IM,)Tj 1.6016 0 Td (Kleinberg)Tj 4.3583 0 Td (JM.)Tj 1.8142 0 Td (Commun)Tj 3.9969 0 Td (ity)Tj 1.2047 0 Td (membership)Tj 5.6339 0 Td (identifi)Tj 2.8205 0 Td (cation)Tj 2.8418 0 Td (from)Tj 2.1897 0 Td (small)Tj 2.5158 0 Td (seed)Tj 2.3457 0 Td (sets.)Tj 2.3032 0 Td (InProceed-)Tj -38.2681 -1.1834 Td (ings)Tj 2.0197 0 Td (of)Tj 1.0418 0 Td (the)Tj 1.5803 0 Td (20th)Tj 2.1331 0 Td (ACM)Tj 2.4166 0 Td (SIGKDD)Tj 4.011 0 Td (internationa)Tj 5.1237 0 Td (l)Tj 0.4323 0 Td (conferen)Tj 3.8197 0 Td (ce)Tj 1.2614 0 Td (on)Tj 1.311 0 Td (Knowled)Tj 3.7631 0 Td (ge)Tj 1.311 0 Td (discovery)Tj 4.3725 0 Td (and)Tj 1.8567 0 Td (data)Tj 2.1331 0 Td (mining)Tj 3.1039 0 Td (2014)Tj -41.6909 -1.1906 Td (Aug)Tj 1.9701 0 Td (24)Tj 1.3111 0 Td (\(pp.)Tj 1.9134 0 Td (1366±1375\))Tj 5.237 0 Td (.)Tj 0.4961 0 Td (ACM.)Tj /F2 1 Tf -13.5496 -1.6228 Td (23.)Tj /F0 1 Tf 2.6219 0 Td (Narayanan)Tj 5.0245 0 Td (A,)Tj 1.1551 0 Td (Huey)Tj 2.5158 0 Td (J,)Tj 0.985 0 Td (Felten)Tj 2.9552 0 Td (EW.)Tj 2.0976 0 Td (A)Tj 0.8788 0 Td (precauti)Tj 3.4866 0 Td (onary)Tj 2.6788 0 Td (approach)Tj 4.3158 0 Td (to)Tj 1.0346 0 Td (big)Tj 1.5237 0 Td (data)Tj 2.133 0 Td (privacy.)Tj 3.6072 0 Td (In)Tj 1.0417 0 Td (Data)Tj 2.2961 0 Td (protection)Tj 4.5354 0 Td (on)Tj -42.2649 -1.1906 Td (the)Tj 1.5875 0 Td (move)Tj 2.6291 0 Td (2016)Tj 2.4024 0 Td (\(pp.)Tj 1.9134 0 Td (357±38)Tj 3.2669 0 Td (5\).)Tj 1.5946 0 Td (Springer)Tj 4.1457 0 Td (Netherlan)Tj 4.252 0 Td (ds.)Tj /F2 1 Tf -24.4135 -1.6229 Td (24.)Tj /F0 1 Tf 2.6219 0 Td (Michalevs)Tj 4.3654 0 Td (ky)Tj 1.2119 0 Td (Y,)Tj 1.1551 0 Td (Schulman)Tj 4.5921 0 Td (A,)Tj 1.1552 0 Td (Veerapand)Tj 4.8047 0 Td (ian)Tj 1.5237 0 Td (GA,)Tj 1.9275 0 Td (Boneh)Tj 3.0615 0 Td (D,)Tj 1.2047 0 Td (Nakibly)Tj 3.4371 0 Td (G.)Tj 1.2685 0 Td (Powerspy:)Tj 4.8189 0 Td (Location)Tj 3.926 0 Td (tracking)Tj -38.4523 -1.1905 Td (using)Tj 2.5654 0 Td (mobile)Tj 3.104 0 Td (device)Tj 3.0543 0 Td (power)Tj 2.8985 0 Td (analysis.)Tj 4.0394 0 Td (In24th)Tj 2.948 0 Td (USENIX)Tj 3.8977 0 Td (Security)Tj 3.7772 0 Td (Sympos)Tj 3.5575 0 Td (ium)Tj 1.8071 0 Td (\(USENIX)Tj 4.2237 0 Td (Security)Tj 3.7701 0 Td (15\))Tj 1.637 0 Td (2015)Tj -41.2799 -1.1835 Td (\(pp.)Tj 1.9134 0 Td (785±800\).)Tj /F2 1 Tf -4.5353 -1.6228 Td (25.)Tj /F0 1 Tf 2.6219 0 Td (Shelton)Tj 3.5505 0 Td (T,)Tj 1.1055 0 Td (Poorth)Tj 2.8913 0 Td (uis)Tj 1.4741 0 Td (A,)Tj 1.1551 0 Td (Zook)Tj 2.4095 0 Td (M.)Tj 1.3252 0 Td (Social)Tj 2.8913 0 Td (media)Tj 2.8914 0 Td (and)Tj 1.8567 0 Td (the)Tj 1.5874 0 Td (city:)Tj 1.9701 0 Td (Rethinking)Tj 4.8544 0 Td (urban)Tj 2.7284 0 Td (socio-spatia)Tj 5.237 0 Td (l)Tj 0.4323 0 Td (inequality)Tj -38.3602 -1.1906 Td (using)Tj 2.5654 0 Td (user-g)Tj 2.778 0 Td (enerated)Tj 4.0961 0 Td (geographic)Tj 5.074 0 Td (informati)Tj 3.7631 0 Td (on.)Tj 1.5874 0 Td (Landscape)Tj 5.0244 0 Td (and)Tj 1.8567 0 Td (Urban)Tj 2.8914 0 Td (Planning.)Tj 4.3087 0 Td (2015;)Tj 2.6788 0 Td (142:198±211)Tj 5.726 0 Td (.)Tj /F2 1 Tf -44.9719 -1.6228 Td (26.)Tj /F0 1 Tf 2.6219 0 Td (Acquisti)Tj 3.6568 0 Td (A,)Tj 1.1551 0 Td (Gross)Tj 2.8559 0 Td (R,)Tj 1.2048 0 Td (Stutzman)Tj 4.3866 0 Td (F.)Tj 1.1055 0 Td (Face)Tj 2.4095 0 Td (recognition)Tj 5.0174 0 Td (and)Tj 1.8496 0 Td (privacy)Tj 3.3378 0 Td (in)Tj 0.978 0 Td (the)Tj 1.5803 0 Td (age)Tj 1.8567 0 Td (of)Tj 1.0417 0 Td (augmente)Tj 4.3725 0 Td (d)Tj 0.7654 0 Td (reality.)Tj 3.111 0 Td (Journal)Tj -40.6846 -1.1906 Td (of)Tj 1.0418 0 Td (Privacy)Tj 3.4441 0 Td (and)Tj 1.8567 0 Td (Confidenti)Tj 4.415 0 Td (ality.)Tj 2.2323 0 Td (2014;)Tj 2.6788 0 Td (6\(2\):)Tj 2.2394 0 Td (1±20.)Tj /F2 1 Tf -20.53 -1.6229 Td (27.)Tj /F0 1 Tf 2.6219 0 Td (Homer)Tj 3.1749 0 Td (N,)Tj 1.2047 0 Td (Merriman)Tj 4.3796 0 Td (B,)Tj 1.1551 0 Td (Nelson)Tj 3.274 0 Td (SF.)Tj 1.7575 0 Td (BFAST:)Tj 3.6922 0 Td (an)Tj 1.311 0 Td (alignme)Tj 3.43 0 Td (nt)Tj 1.0417 0 Td (tool)Tj 1.8 0 Td (for)Tj 1.3678 0 Td (large)Tj 2.3952 0 Td (scale)Tj 2.5158 0 Td (genome)Tj 3.7701 0 Td (resequen)Tj 4.089 0 Td (cing.)Tj -40.3586 -1.1905 Td (PLoS)Tj 2.6292 0 Td (ONE.)Tj 2.6433 0 Td (2009;)Tj 2.6717 0 Td (4\(11\):e776)Tj 4.7481 0 Td (7.)Tj 0.83 0.64 0.02 0 k 1.0417 0 Td (https://doi.or)Tj 5.4142 0 Td (g/10.137)Tj 3.8197 0 Td (1/journal.po)Tj 5.1237 0 Td (ne.0007767)Tj 0 g 5.4071 0 Td (PMID:)Tj 0.83 0.64 0.02 0 k 2.9623 0 Td (19907642)Tj 0 g /F2 1 Tf -39.0829 -1.6229 Td (28.)Tj /F0 1 Tf 2.6219 0 Td (Lo)Tj 1.3111 0 Td (B.)Tj 1.1551 0 Td (Sharing)Tj 3.6 0 Td (clinical)Tj 3.1536 0 Td (trial)Tj 1.7929 0 Td (data:)Tj 2.4024 0 Td (maximizing)Tj 5.1378 0 Td (benefits,)Tj 3.9331 0 Td (minimizing)Tj 4.8544 0 Td (risk.)Tj 2.0197 0 Td (Jama.)Tj 2.9056 0 Td (2015;)Tj 2.6787 0 Td (313\(8\):793)Tj 4.7481 0 Td (±4.)Tj 0.83 0.64 0.02 0 k 1.5803 0 Td (https://)Tj -41.2728 -1.1905 Td (doi.org/10.10)Tj 5.7261 0 Td (01/jama.20)Tj 4.8614 0 Td (15.292)Tj 0 g 3.2245 0 Td (PMID:)Tj 0.83 0.64 0.02 0 k 2.9693 0 Td (25594500)Tj ET Q q 1 j 1 J 0 w 576 737.1 m 36 737.1 l 36 737.6 l 576 737.6 l f* 36 741.2598 169.285 23.6976 re W* n q 169.2283 0 0 23.6409 36 741.3165 cm q /I0 Do Q Q Q q 0 0 612 792 re W* n 1 j 1 J 0 w 7.9999 0 0 7.9999 391.9748 745.7952 cm BT /F0 1 Tf 1 TL -0.005 Tc 0 0 Td (Ten)Tj 1.9204 0 Td (Simple)Tj 3.2245 0 Td (Rules)Tj 2.7283 0 Td (for)Tj 1.3678 0 Td (Respons)Tj 3.8764 0 Td (ible)Tj 1.7362 0 Td (Big)Tj 1.637 0 Td (Data)Tj 2.3032 0 Td (Researc)Tj 3.6567 0 Td (h)Tj ET Q q 1 j 1 J 0 w 36 48.0002 m 576 48.0002 l 576 47.5002 l 36 47.5002 l f* 0.83 0.64 0.02 0 k 142.5827 34.9228 m 295.8236 34.9228 l h f* 0 g 7.9999 0 0 7.9999 36 36 cm BT /F0 1 Tf 1 TL -0.005 Tc 0 0 Td (PLOS)Tj 2.8559 0 Td (Computationa)Tj 6.1229 0 Td (l)Tj 0.4322 0 Td (Biology)Tj 3.4371 0 Td (|)Tj 0.83 0.64 0.02 0 k 0.4748 0 Td (https:/)Tj 2.6788 0 Td (/doi.org/10.13)Tj 6.0094 0 Td (71/journal.p)Tj 5.1166 0 Td (cbi.1005399)Tj 0 g 6.3497 0 Td (March)Tj 2.9551 0 Td (30,)Tj 1.5874 0 Td (2017)Tj 27.1207 0 Td (9)Tj 0.7654 0 Td (/)Tj ET endstream endobj 228 0 obj < >stream endstream endobj 229 0 obj < >stream endstream endobj 230 0 obj < >stream endstream endobj 231 0 obj < >stream endstream endobj 232 0 obj < >stream endstream endobj 233 0 obj < >stream endstream endobj 234 0 obj < >stream endstream endobj 235 0 obj < >stream endstream endobj 236 0 obj < >stream endstream endobj 237 0 obj < >stream endstream endobj 238 0 obj < >stream endstream endobj 239 0 obj < >stream endstream endobj 240 0 obj < >stream endstream endobj 241 0 obj < >stream endstream endobj 242 0 obj < >stream endstream endobj 243 0 obj < >stream endstream endobj 244 0 obj < >stream endstream endobj 245 0 obj < >stream endstream endobj 246 0 obj < >stream endstream endobj 247 0 obj < >stream endstream endobj 248 0 obj < >stream endstream endobj 249 0 obj < >stream endstream endobj 250 0 obj < >stream endstream endobj 251 0 obj < >stream endstream endobj 252 0 obj < >stream endstream endobj 253 0 obj < >stream BT 66.402 0 Td (10)Tj ET Q endstream endobj 254 0 obj < >/XObject< >>>/CropBox[0 0 612 792]/MediaBox[0 0 612 792]/Parent 9 0 R/Annots 255 0 R/Contents 256 0 R/TrimBox[0 0 612 792]>> endobj 255 0 obj [257 0 R 258 0 R 259 0 R 260 0 R 261 0 R 262 0 R 263 0 R 264 0 R 265 0 R 266 0 R 267 0 R 268 0 R 269 0 R 270 0 R 271 0 R 272 0 R 273 0 R] endobj 257 0 obj < >/Border[0 0 0]/Dest(Lpcbi.1005399.ref029)>> endobj 258 0 obj < >/Border[0 0 0]/Dest(Lpcbi.1005399.ref030)>> endobj 259 0 obj < >/Border[0 0 0]/A 274 0 R>> endobj 274 0 obj < > endobj 260 0 obj < >/Border[0 0 0]/Dest(Lpcbi.1005399.ref031)>> endobj 261 0 obj < >/Border[0 0 0]/Dest(Lpcbi.1005399.ref032)>> endobj 262 0 obj < >/Border[0 0 0]/Dest(Lpcbi.1005399.ref033)>> endobj 263 0 obj < >/Border[0 0 0]/Dest(Lpcbi.1005399.ref034)>> endobj 264 0 obj < >/Border[0 0 0]/A 275 0 R>> endobj 275 0 obj < > endobj 265 0 obj < >/Border[0 0 0]/Dest(Lpcbi.1005399.ref035)>> endobj 266 0 obj < >/Border[0 0 0]/Dest(Lpcbi.1005399.ref036)>> endobj 267 0 obj < >/Border[0 0 0]/A 276 0 R>> endobj 276 0 obj < > endobj 268 0 obj < >/Border[0 0 0]/A 277 0 R>> endobj 277 0 obj < > endobj 269 0 obj < >/Border[0 0 0]/Dest(Lpcbi.1005399.ref037)>> endobj 270 0 obj < >/Border[0 0 0]/Dest(Lpcbi.1005399.ref038)>> endobj 271 0 obj < >/Border[0 0 0]/A 278 0 R>> endobj 278 0 obj < > endobj 272 0 obj < >/Border[0 0 0]/A 279 0 R>> endobj 279 0 obj < > endobj 273 0 obj < >/Border[0 0 0]/A 280 0 R>> endobj 280 0 obj < > endobj 256 0 obj [281 0 R 282 0 R 283 0 R 284 0 R 285 0 R 286 0 R 287 0 R 288 0 R 289 0 R 290 0 R 291 0 R] endobj 281 0 obj < >stream q 0.83 0.64 0.02 0 k 242.9291 675.6661 m 534.7276 675.6661 l h f* 384.7748 585.6378 m 419.6976 585.6378 l h f* 380.4094 540.6236 m 561.8835 540.6236 l h f* 221.4992 531.1559 m 393.6756 531.1559 l h f* 524.8063 495.6661 m 567.7795 495.6661 l h f* 221.4992 486.1417 m 450.3118 486.1417 l h f* 0 g 1 j 1 J 0 w 7.9999 0 0 7.9999 200.5228 708.7181 cm BT /F2 1 Tf 1 TL -0.005 Tc 0 0 Td (29.)Tj /F0 1 Tf 2.622 0 Td (Goodman)Tj 4.5497 0 Td (A,)Tj 1.148 0 Td (Pepe)Tj 2.5158 0 Td (A,)Tj 1.1551 0 Td (Blocker)Tj 3.5008 0 Td (AW,)Tj 2.0977 0 Td (Borgma)Tj 3.4441 0 Td (n)Tj 0.7653 0 Td (CL,)Tj 1.7505 0 Td (Cranmer)Tj 4.0535 0 Td (K,)Tj 1.1552 0 Td (Crosas)Tj 3.3307 0 Td (M,)Tj 1.3252 0 Td (et)Tj 1.0417 0 Td (al.)Tj 1.2544 0 Td (Ten)Tj 1.9134 0 Td (simple)Tj 3.0543 0 Td (rules)Tj 2.3457 0 Td (for)Tj 1.3678 0 Td (the)Tj -41.7689 -1.1905 Td (care)Tj 2.1331 0 Td (and)Tj 1.8567 0 Td (feeding)Tj 3.43 0 Td (of)Tj 1.0417 0 Td (scientific)Tj 3.9827 0 Td (data.)Tj 2.4095 0 Td (PLoS)Tj 2.6291 0 Td (Compu)Tj 3.1678 0 Td (t)Tj 0.496 0 Td (Biol.;)Tj 2.4024 0 Td (10\(4\).)Tj /F2 1 Tf -26.171 -1.6229 Td (30.)Tj /F0 1 Tf 2.622 0 Td (Markham)Tj 4.33 0 Td (A.)Tj 1.1551 0 Td (OKCup)Tj 3.2315 0 Td (id)Tj 0.978 0 Td (data)Tj 2.1331 0 Td (release)Tj 3.437 0 Td (fiasco:)Tj 3.0614 0 Td (It's)Tj 1.4741 0 Td (time)Tj 2.0835 0 Td (to)Tj 1.0417 0 Td (rethink)Tj 3.1607 0 Td (ethics)Tj 2.785 0 Td (education.)Tj 4.7552 0 Td (Medium)Tj 3.4937 0 Td (.Points)Tj 3.2315 0 Td (18)Tj 1.3111 0 Td (May)Tj -41.6626 -1.1834 Td (2016.)Tj 0.83 0.64 0.02 0 k 2.6788 0 Td (https://p)Tj 3.5008 0 Td (oints.dataso)Tj 5.3008 0 Td (ciety.net/ok)Tj 4.9749 0 Td (cupid-data-r)Tj 5.237 0 Td (elease-fias)Tj 4.741 0 Td (co-ba0388348)Tj 6.2788 0 Td (cd#.g4ofb)Tj 4.3158 0 Td (pnc6)Tj 0 g (.)Tj 2.622 0 Td (Accessed)Tj -39.6499 -1.1906 Td (12)Tj 1.3111 0 Td (June)Tj 2.3456 0 Td (2016.)Tj /F2 1 Tf -6.2787 -1.6228 Td (31.)Tj /F0 1 Tf 2.622 0 Td (Ford)Tj 2.2465 0 Td (H.)Tj 1.2047 0 Td (Big)Tj 1.637 0 Td (Data)Tj 2.3032 0 Td (and)Tj 1.8496 0 Td (Small:)Tj 2.9552 0 Td (Collabora)Tj 4.1882 0 Td (tions)Tj 2.2961 0 Td (between)Tj 3.9331 0 Td (ethnographe)Tj 5.5134 0 Td (rs)Tj 1.0418 0 Td (and)Tj 1.8496 0 Td (data)Tj 2.1331 0 Td (scientists)Tj 4.0394 0 Td (.)Tj 0.496 0 Td (Big)Tj 1.6441 0 Td (Data)Tj 2.2961 0 Td (&)Tj -41.6271 -1.1906 Td (Society.)Tj 3.7205 0 Td (2014;)Tj 2.6788 0 Td (1\(2\):)Tj /F2 1 Tf -9.0213 -1.6228 Td (32.)Tj /F0 1 Tf 2.622 0 Td (Science)Tj 3.7134 0 Td (&)Tj 0.8788 0 Td (Justice)Tj 3.2811 0 Td (Research)Tj 4.4292 0 Td (Center)Tj 3.1677 0 Td (\(Collaboratio)Tj 5.556 0 Td (ns)Tj 1.2543 0 Td (Group.)Tj 3.2386 0 Td (Experi)Tj 2.778 0 Td (ments)Tj 2.9055 0 Td (in)Tj 0.978 0 Td (collaboration)Tj 5.5559 0 Td (:)Tj 0.4961 0 Td (interdisci)Tj 3.8622 0 Td (plin-)Tj -42.0948 -1.1906 Td (ary)Tj 1.5874 0 Td (graduate)Tj 4.0961 0 Td (education)Tj 4.4717 0 Td (in)Tj 0.978 0 Td (science)Tj 3.5504 0 Td (and)Tj 1.8567 0 Td (justice.)Tj 3.274 0 Td (PLoS)Tj 2.6292 0 Td (Biol.)Tj 2.126 0 Td (2013)Tj 2.4024 0 Td (Jul)Tj 1.474 0 Td (30;)Tj 1.5803 0 Td (11\(7\):e100)Tj 4.7481 0 Td (1619.)Tj /F2 1 Tf -37.3963 -1.6229 Td (33.)Tj /F0 1 Tf 2.622 0 Td (Knobel)Tj 3.2741 0 Td (C,)Tj 1.2047 0 Td (Bowker)Tj 3.5079 0 Td (GC.)Tj 1.9772 0 Td (Values)Tj 3.2244 0 Td (in)Tj 0.978 0 Td (design.)Tj 3.3803 0 Td (Commun)Tj 3.9969 0 Td (ications)Tj 3.5504 0 Td (of)Tj 1.0347 0 Td (the)Tj 1.5874 0 Td (ACM.)Tj 2.6929 0 Td (2011;)Tj 2.6788 0 Td (54\(7\):26±8)Tj 4.748 0 Td (.)Tj /F2 1 Tf -40.4577 -1.6228 Td (34.)Tj /F0 1 Tf 2.622 0 Td (Cho)Tj 2.0197 0 Td (MK,)Tj 1.9843 0 Td (Tobin)Tj 2.6787 0 Td (SL,)Tj 1.6938 0 Td (Greely)Tj 3.1181 0 Td (HT,)Tj 1.8213 0 Td (McCormi)Tj 3.9402 0 Td (ck)Tj 1.2047 0 Td (J,)Tj 0.9921 0 Td (Boyce)Tj 2.9552 0 Td (A,)Tj 1.1551 0 Td (Magnus)Tj 3.7205 0 Td (D.)Tj 1.2047 0 Td (Research)Tj 4.4292 0 Td (ethics)Tj 2.7851 0 Td (consultation)Tj 5.237 0 Td (:)Tj 0.4961 0 Td (The)Tj -41.4358 -1.1906 Td (Stanford)Tj 3.9402 0 Td (experien)Tj 3.7559 0 Td (ce.)Tj 1.5378 0 Td (IRB.)Tj 2.1402 0 Td (2008;\(6\):1±)Tj 5.0245 0 Td (6.)Tj 1.0417 0 Td (PMID:)Tj 0.83 0.64 0.02 0 k 2.9693 0 Td (191197)Tj 3.267 0 Td (57)Tj 0 g /F2 1 Tf -26.2986 -1.6228 Td (35.)Tj /F0 1 Tf 2.622 0 Td (boyd)Tj 2.3528 0 Td (d.)Tj 1.0346 0 Td (Untangling)Tj 4.9111 0 Td (resear)Tj 2.778 0 Td (ch)Tj 1.2614 0 Td (and)Tj 1.8567 0 Td (practice:)Tj 3.9331 0 Td (What)Tj 2.53 0 Td (Facebook)Tj 4.3157 0 Td ('s)Tj 0.9284 0 Td (ªemotion)Tj 3.8197 0 Td (al)Tj 0.9851 0 Td (contagion)Tj 4.252 0 Td (º)Tj 0.5456 0 Td (study)Tj 2.5725 0 Td (teaches)Tj 3.6638 0 Td (us.)Tj -41.7405 -1.1906 Td (Research)Tj 4.4292 0 Td (Ethics.)Tj 3.1748 0 Td (2016;)Tj 2.6788 0 Td (12\(1\):4±1)Tj 4.1953 0 Td (3.)Tj /F2 1 Tf -17.1001 -1.6228 Td (36.)Tj /F0 1 Tf 2.622 0 Td (Cook)Tj 2.5158 0 Td (G,)Tj 1.2685 0 Td (Dowdall)Tj 3.7063 0 Td (T,)Tj 1.1055 0 Td (Pomera)Tj 3.4442 0 Td (ntz)Tj 1.5378 0 Td (D,)Tj 1.2047 0 Td (Wang)Tj 2.7992 0 Td (Y.)Tj 1.1481 0 Td (Clicking)Tj 3.6496 0 Td (clean:)Tj 2.8418 0 Td (how)Tj 2.0197 0 Td (companies)Tj 4.9748 0 Td (are)Tj 1.637 0 Td (creating)Tj 3.7064 0 Td (the)Tj 1.5874 0 Td (green)Tj 2.7284 0 Td (inter-)Tj -41.8752 -1.1906 Td (net.)Tj 1.8638 0 Td (Greenpea)Tj 4.3725 0 Td (ce)Tj 1.2543 0 Td (Inc.,)Tj 2.0906 0 Td (Washingto)Tj 4.6489 0 Td (n,)Tj 1.0346 0 Td (DC.)Tj 1.9205 0 Td (2014.)Tj 0.83 0.64 0.02 0 k 2.6788 0 Td (http://w)Tj 3.1748 0 Td (ww.greenpea)Tj 5.8465 0 Td (ce.org/usa/w)Tj 5.5772 0 Td (p-content)Tj 4.0961 0 Td (/uploads/)Tj -38.5586 -1.1835 Td (legacy/Gl)Tj 4.0961 0 Td (obal/usa/p)Tj 4.5284 0 Td (lanet3/PDF)Tj 4.9252 0 Td (s/clickingc)Tj 4.4717 0 Td (lean.pdf)Tj 0 g /F2 1 Tf -20.6434 -1.6299 Td (37.)Tj /F0 1 Tf 2.622 0 Td (Zook)Tj 2.4095 0 Td (MA,)Tj 1.9701 0 Td (Graham)Tj 3.7772 0 Td (M.)Tj 1.3181 0 Td (Mapping)Tj 3.9756 0 Td (DigiPlace:)Tj 4.6347 0 Td (geocode)Tj 3.763 0 Td (d)Tj 0.7583 0 Td (Internet)Tj 3.5504 0 Td (data)Tj 2.126 0 Td (and)Tj 1.8496 0 Td (the)Tj 1.5804 0 Td (representat)Tj 4.9677 0 Td (ion)Tj 1.5237 0 Td (of)Tj 1.0346 0 Td (place.)Tj 2.8276 0 Td (Envi-)Tj -42.0665 -1.1835 Td (ronment)Tj 3.8268 0 Td (and)Tj 1.8567 0 Td (Planning)Tj 4.0323 0 Td (B:)Tj 1.1552 0 Td (Planning)Tj 4.0323 0 Td (and)Tj 1.8567 0 Td (Design.)Tj 3.5504 0 Td (2007)Tj 2.4024 0 Td (Jun)Tj 1.8071 0 Td (1;)Tj 1.0346 0 Td (34\(3\):466±)Tj 4.7481 0 Td (82.)Tj /F2 1 Tf -32.9246 -1.6228 Td (38.)Tj /F0 1 Tf 2.622 0 Td (Zimmer)Tj 3.5646 0 Td (M.)Tj 1.3181 0 Td (OkCupid)Tj 4.0536 0 Td (Study)Tj 2.7355 0 Td (Reveals)Tj 3.7701 0 Td (the)Tj 1.5874 0 Td (Perils)Tj 2.6717 0 Td (of)Tj 1.0346 0 Td (Big-Data)Tj 4.0465 0 Td (Science.)Tj 3.9898 0 Td (Wired.)Tj 3.0685 0 Td (14)Tj 1.3111 0 Td (May)Tj 2.0834 0 Td (2016.)Tj 0.83 0.64 0.02 0 k 2.6788 0 Td (https://)Tj 2.9551 0 Td (www.)Tj -40.8688 -1.1906 Td (wired.com/)Tj 4.7552 0 Td (2016/05/ok)Tj 4.8614 0 Td (cupid-study)Tj 5.0245 0 Td (-reveals-per)Tj 5.237 0 Td (ils-big-data-)Tj 5.1166 0 Td (science/)Tj 0 g (.)Tj 4.1032 0 Td (Accesse)Tj 3.7205 0 Td (d)Tj 0.7653 0 Td (12)Tj 1.3111 0 Td (June)Tj 2.3528 0 Td (2016.)Tj ET Q q 1 j 1 J 0 w 576 737.1 m 36 737.1 l 36 737.6 l 576 737.6 l f* 36 741.2598 169.285 23.6976 re W* n q 169.2283 0 0 23.6409 36 741.3165 cm q /I0 Do Q Q Q q 0 0 612 792 re W* n 1 j 1 J 0 w 7.9999 0 0 7.9999 391.9748 745.7952 cm BT /F0 1 Tf 1 TL -0.005 Tc 0 0 Td (Ten)Tj 1.9204 0 Td (Simple)Tj 3.2245 0 Td (Rules)Tj 2.7283 0 Td (for)Tj 1.3678 0 Td (Respons)Tj 3.8764 0 Td (ible)Tj 1.7362 0 Td (Big)Tj 1.637 0 Td (Data)Tj 2.3032 0 Td (Researc)Tj 3.6567 0 Td (h)Tj ET Q q 1 j 1 J 0 w 36 48.0002 m 576 48.0002 l 576 47.5002 l 36 47.5002 l f* 0.83 0.64 0.02 0 k 142.5827 34.9228 m 295.8236 34.9228 l h f* 0 g 7.9999 0 0 7.9999 36 36 cm BT /F0 1 Tf 1 TL -0.005 Tc 0 0 Td (PLOS)Tj 2.8559 0 Td (Computationa)Tj 6.1229 0 Td (l)Tj 0.4322 0 Td (Biology)Tj 3.4371 0 Td (|)Tj 0.83 0.64 0.02 0 k 0.4748 0 Td (https:/)Tj 2.6788 0 Td (/doi.org/10.13)Tj 6.0094 0 Td (71/journal.p)Tj 5.1166 0 Td (cbi.1005399)Tj 0 g 6.3497 0 Td (March)Tj 2.9551 0 Td (30,)Tj 1.5874 0 Td (2017)Tj 26.5821 0 Td (10)Tj 1.304 0 Td (/)Tj ET endstream endobj 282 0 obj < >stream endstream endobj 283 0 obj < >stream endstream endobj 284 0 obj < >stream endstream endobj 285 0 obj < >stream endstream endobj 286 0 obj < >stream endstream endobj 287 0 obj < >stream endstream endobj 288 0 obj < >stream endstream endobj 289 0 obj < >stream endstream endobj 290 0 obj < >stream endstream endobj 291 0 obj < >stream BT 66.402 0 Td (10)Tj ET Q endstream endobj 9 0 obj < > endobj 293 0 obj < > endobj 294 0 obj < > endobj 297 0 obj < > endobj 298 0 obj < > endobj 299 0 obj < > endobj 300 0 obj < > endobj 301 0 obj < > endobj 302 0 obj < > endobj 303 0 obj < > endobj 304 0 obj < > endobj 305 0 obj < > endobj 306 0 obj < > endobj 307 0 obj < > endobj 308 0 obj < > endobj 309 0 obj < > endobj 310 0 obj < > endobj 311 0 obj < > endobj 312 0 obj < > endobj 313 0 obj < > endobj 314 0 obj < > endobj 315 0 obj < > endobj 316 0 obj < > endobj 317 0 obj < > endobj 318 0 obj < > endobj 319 0 obj < > endobj 320 0 obj < > endobj 321 0 obj < > endobj 322 0 obj < > endobj 323 0 obj < > endobj 324 0 obj < > endobj 325 0 obj < > endobj 326 0 obj < > endobj 327 0 obj < > endobj 328 0 obj < > endobj 295 0 obj < > endobj 329 0 obj < > endobj 330 0 obj < > endobj 331 0 obj < > endobj 332 0 obj < > endobj 333 0 obj < > endobj 334 0 obj < > endobj 335 0 obj < > endobj 336 0 obj < > endobj 337 0 obj < > endobj 338 0 obj < > endobj 339 0 obj < > endobj 340 0 obj < > endobj 341 0 obj < > endobj 342 0 obj < > endobj 343 0 obj < > endobj 344 0 obj < > endobj 345 0 obj < > endobj 346 0 obj < > endobj 347 0 obj < > endobj 348 0 obj < > endobj 349 0 obj < > endobj 350 0 obj < > endobj 351 0 obj < > endobj 352 0 obj < > endobj 353 0 obj < > endobj 354 0 obj < > endobj 355 0 obj < > endobj 356 0 obj < > endobj 357 0 obj < > endobj 358 0 obj < > endobj 359 0 obj < > endobj 360 0 obj < > endobj 296 0 obj < > endobj 361 0 obj < > endobj 362 0 obj < > endobj 363 0 obj < > endobj 364 0 obj < > endobj 365 0 obj < > endobj 366 0 obj < > endobj 367 0 obj < > endobj 368 0 obj < > endobj 369 0 obj < > endobj 370 0 obj < > endobj 371 0 obj < > endobj 372 0 obj < > endobj 373 0 obj < > endobj 374 0 obj < > endobj 375 0 obj < > endobj 376 0 obj < > endobj 377 0 obj < > endobj 378 0 obj < > endobj 379 0 obj < > endobj 380 0 obj < > endobj 381 0 obj < > endobj 382 0 obj < > endobj 383 0 obj < > endobj 384 0 obj < > endobj 385 0 obj < >/Metadata 2 0 R>> endobj 292 0 obj < >/XObject< >>>/CropBox[0 0 612 792]/MediaBox[0 0 612 792]/Parent 9 0 R/Annots 396 0 R/Contents 397 0 R/TrimBox[0 0 612 792]>> endobj 4 0 obj [/PDF/ImageB/ImageC/ImageI/Text] endobj 5 0 obj < > endobj 398 0 obj < > endobj 399 0 obj < >stream %!PS-AdobeFont-1.0 %%This font is a conversion from Helvetica with the following copyright notice: %Copyright (c) 1985, 1987, 1989, 1990, 1997, 1998, 1999 Adobe Systems Incorporated. (extracted from312.html)
Accept additional cookies Reject additional cookies View cookies Hide cookie message Skip to main content GOV.UK Navigation menu Menu Menu Services and information Benefits Births, death, marriages and care Business and self-employed Childcare and parenting Citizenship and living in the UK Crime, justice and the law Disabled people Driving and transport Education and learning Employing people Environment and countryside Housing and local services Money and tax Passports, travel and living abroad Visas and immigration Working, jobs and pensions Government activity Departments Departments, agencies and public bodies News News stories, speeches, letters and notices Guidance and regulation Detailed guidance, regulations and rules Research and statistics Reports, analysis and official statistics Policy papers and consultations Consultations and strategy Transparency Data, Freedom of Information releases and corporate reports Search GOV.UK × Search GOV.UK Search Search GOV.UK Search Home Business and industry Industrial strategy Independent report Growing the artificial intelligence industry in the UK This independent review, carried out by Professor Dame Wendy Hall and Jérôme Pesenti reports on how the Artificial Intelligence industry can be grown in the UK. (extracted from187.html)
Cancel Services and information Benefits Births, death, marriages and care Business and self-employed Childcare and parenting Citizenship and living in the UK Crime, justice and the law Disabled people Driving and transport Education and learning Employing people Environment and countryside Housing and local services Money and tax Passports, travel and living abroad Visas and immigration Working, jobs and pensions Government activity Departments News Guidance and regulation Research and statistics Policy papers and consultations Transparency How government works Get involved Support links Help Privacy Cookies Accessibility statement Contact Terms and conditions Rhestr o Wasanaethau Cymraeg Government Digital Service All content is available under the Open Government Licence v3.0 , except where otherwise stated © Crown copyright (extracted from187.html)
Core public agencies, such as those responsible for criminal justice, healthcare, ​​ ​​ ​​ ​​ ​​ ​​ ​​ ​​ ​​ ​​ welfare, and education (e.g “high stakes” domains) should no longer use “black box” ​​ ​​ ​​ ​​ ​​ ​​ ​​ ​​ ​​ ​​ ​​ ​​ AI and algorithmic systems. (extracted from25.pdf)
Within ​​ ​​ ​​ ​​ ​​ ​​ ​​ ​​ ​​ ​​ ​​ each domain – such as education, healthcare or criminal justice – legacies of bias and ​​ ​​ ​​ ​​ ​​ ​​ ​​ ​​ ​​ ​​ ​​ ​​ ​​ ​​ movements toward equality have their own histories and practices. (extracted from25.pdf)
The third ​​ ​​ ​​ ​​ ​​ ​​ ​​ ​​ ​​ ​​ ​​ ​​ ​​ section, on Rights and Liberties, begins by recognizing the recent rise of political ​​ ​​​ ​​ ​​ ​​​ ​​ ​​ ​​ ​​ ​​ ​​ ​​ authoritarianism, and asks about the role of AI systems in either supporting or eroding ​​ ​​ ​​ ​​ ​​ ​​ ​​ ​​ ​​ ​​ ​​ ​​ ​​ citizens’ rights and liberties in areas like criminal justice, law enforcement, housing, hiring, ​​ ​​ ​​ ​​ ​​ ​​ ​​ ​​ ​​ ​​ ​​ ​​ lending and other domains. (extracted from25.pdf)
​​ ​​ ​​ ​​ ​​ ​​ ​​ ​​ ​​ ​​ ​​ The danger of bias increases when these systems are applied, often in non-transparent ​​ ​​ ​​ ​​ ​​ ​​ ​​ ​​ ​​ ​​ ​​ ​​ ways, to critical institutions like criminal justice and healthcare. (extracted from25.pdf)
How will AI’s use in the criminal justice system ​​ ​​ ​​ ​​ ​​ ​​ ​​ ​​ ​​ ​​ ​​ ​​ ​​ affect our understanding of due process and the principle of equal justice under the law? (extracted from25.pdf)
As AI systems promise new forms of ​​ ​​ ​​ ​​ ​​ ​​ ​​ ​​ ​​ ​​ ​​ ​​ ​​ ​​ technical efficiency in the service of safety, we may need to confront a fundamental ​​ ​​ ​​ ​​ ​​ ​​ ​​ ​​ ​​ ​​ ​​ ​​ ​​ tension between technological efficiency and a commitment to ideals of justice. (extracted from25.pdf)
​​ Scholars like Kate Crawford and Jason Schultz have identified a series of conflicts between ​​ ​​ ​​ ​​ ​​ ​​ ​​ ​​ ​​ ​​ ​​ ​​ ​​ 123 AI techniques and constitutional due process requirements, such as how AI techniques ​​ ​​ ​​ ​​ ​​ ​​ ​​ ​​ ​​ ​​ ​​ affect procedural considerations and equal justice under the law. (extracted from25.pdf)
​ AI Now 2017 Report 27 ​​ ​​ ​​ 129 requirement presently exists under the GDPR and more generally how competing ​​ ​​ ​​ ​​ ​​ ​​ ​​ ​​ ​​ ​​ interpretations or explanations might be technically formulated and understood by ​​ ​​ ​​ ​​ ​​ ​​ ​​ ​​ ​​ different stakeholders.130 ​​ The criminal justice system’s implementation of risk assessment algorithms provides an ​​ ​​ ​​ ​​ ​​ ​​ ​​ ​​ ​​ ​​ 131 example of the legal system’s use of AI and its attendant risks. (extracted from25.pdf)
​​ ​​​ ​​ ​​ ​​​ ​ 135 Rebecca Wexler, “Life, Liberty, and Trade Secrets: Intellectual Property in the Criminal Justice System,” SSRN preprint: ​​ ​​ ​​ ​​ ​​ ​​ ​​ ​​ ​​ ​​ ​​ ​​ ​​ ​​ ​​ https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2920883. (extracted from25.pdf)
​​ ​​ ​​ ​​ ​​ ​​ ​​ ​​ ​​​ ​​​ 137 Richard Berk, Hoda Heidari, Shahin Jabbari, Michael Kearns and Aaron Roth, “Fairness in Criminal Justice Risk Assessments: ​​ ​​ ​​ ​​ ​​ ​​ ​​ ​​ ​​ ​​ ​​ ​​ ​​ ​​ ​​ ​​ The State of the Art,” arXiv:1703.09207, March 27, 2017. (extracted from25.pdf)
Our Values Justice Trust Respect Compassion How We Work Our approach combines global collaboration, scientific rigor, and strategic action to shape a future where AI technologies align with human values and societal well-being. (extracted from538.html)
In fact, without being able to explain decisions taken by autonomous systems, it is difficult to justify them: it would seem inconceivable to accept what cannot be 1 15 justified in areas as crucial to the life of an individual as access to credit, employment, accommodation, justice and health. (extracted from468.pdf)
Several of the measures that were adopted in recent years have widened and improved group access to litigation; in particular, the French Act for the Modernization of Justice in the Twenty-First Century introduced the ‘personal data’ class action which allows associations of consumer protection to act when infringements to existing legislation occur. (extracted from468.pdf)
Other sions need to acknowledge that there is a human rights rights are also affected; for example, an automated framework setting binding legal obligations around system used in the justice system, which is based on AI, which should be seen as a starting point for any poor quality data, can negatively impact on the right evaluation of the opportunities and challenges brought to a fair trial and effective remedy, as well as on the by new technologies. (extracted from326.pdf)
See for 7 Committee on Civil Liberties, Justice and Home Affairs example: Salganik, M. (extracted from326.pdf)
Data protection legislation offers min- tion and the criminal justice system. (extracted from326.pdf)
The use of AI imal guidance on the topic – the principle of data and algorithms in the area of justice needs testing, accuracy in the General Data Protection Regulation as highlighted in the CEPEJ European ethical char- (GDPR)34 is related to data quality, but in a very nar- ter.30 One potential problem might be the use of row sense as it only focuses on the obligation to biased data for automated systems.31 In addition, keep personal data accurate and up to date. (extracted from326.pdf)
15 Data quality and artificial intelligence – mitigating bias and error to protect fundamental rights References Access Now (2018), Human Rights in the age of Committee on Civil Liberties, Justice and Home Artificial Intelligence. (extracted from326.pdf)
Affairs (2018), Opinion of the Committee on Civil Liberties, Justice and Home Affairs for the Alpaydin, E. (extracted from326.pdf)
(forthcoming), ‘Dirty Data, Bad Predictions: How (2017), ‘Men Also Like Shopping: Reducing Gender Bias Civil Rights Violations Impact Police Data, Predictive Amplification using Corpus-level Constraints’, paper Policing Systems, and Justice’, New York University given at the 2017 Conference on Empirical Methods Law Review Online. (extracted from326.pdf)
18 FRA – EUROPEAN UNION AGENCY FOR FUNDAMENTAL RIGHTS Schwarzenbergplatz 11 – 1040 Vienna – Austria Tel: +43 158030-0 – Fax: +43 158030-699 fra.europa.eu © European Union Agency for Fundamental Rights, 2019 facebook.com/fundamentalrights linkedin.com/company/eu-fundamental-rights-agency Print: ISBN 978-92-9474-605-4, doi:10.2811/615718 twitter.com/EURightsAgency PDF: ISBN 978-92-9474-606-1, doi:10.2811/546219 TK-01-19-330-EN-C (print); TK-01-19-330-EN-N (PDF) Further information: The following FRA publications offer further information relevant to the topic of the paper: • #BigData: Discrimination in data-supported decision making (2018) http://fra.europa.eu/en/publication/2018/big-data-discrimination • Under watchful eyes: biometrics, EU IT systems and fundamental rights (2018) http://fra.europa.eu/en/publication/2018/biometrics-rights-protection • Fundamental rights and the interoperability of EU information systems: borders and security (2017) http://fra.europa.eu/en/publication/2017/fundamental-rights-interoperability • Surveillance by intelligence services: fundamental rights safeguards and remedies in the EU - Volume II: field perspectives and legal update (2017) http://fra.europa.eu/en/publication/2017/surveillance-intelligence-socio-lega • Surveillance by intelligence services: fundamental rights safeguards and remedies in the European Union - Mapping Member States’ legal frameworks (2015) http://fra.europa.eu/en/publication/2015/surveillance-intelligence-services • The impact on fundamental rights of the proposed Regulation on the European Travel Information and Authorisation System (ETIAS) (2017) http://fra.europa.eu/en/opinion/2017/etias-impact • Handbook on European data protection law - 2018 edition (2018) https://fra.europa.eu/en/publication/2018/handbook-european-data-protection-law • Handbook on European law relating to access to justice (2016) https://fra.europa.eu/en/publication/2016/handbook-european-law-relating-access-justice • Handbook on European non-discrimination law – 2018 edition (2018) http://fra.europa.eu/en/publication/2018/handbook-european-law-non-discrimination (extracted from326.pdf)
Accountability , explainability , privacy , justice , but also other values such as robustness or safety are most easily operationalized mathematically and thus tend to be implemented in terms of technical solutions. (extracted from703.html)
With reference to the findings of psychologist Carol Gilligan, one could argue at this point that the way AI ethics is performed and structured constitutes a typical instantiation of a male-dominated justice ethics (Gilligan 1982 ). (extracted from703.html)
In the 1980s, Gilligan demonstrated in empirical studies that women do not, as men typically do, address moral problems primarily through a “calculating”, “rational”, “logic-oriented” ethics of justice, but rather interpret them within a wider framework of an “empathic”, “emotion-oriented” ethics of care. (extracted from703.html)
What does it mean to implement justice or transparency in AI-systems? (extracted from703.html)
In view of AI ethics, approaches that focus on virtues aim at cultivating a moral character, expressing technomoral virtues such as honesty, justice, courage, empathy, care, civility, or magnanimity, to name just a few (Vallor 2016 ). (extracted from703.html)
This implies that the purposes for which AI systems are developed and applied are not in accordance with societal values or fundamental rights such as beneficence, non-maleficence, justice, and explicability (Taddeo and Floridi 2018 ; Pekka et al. (extracted from703.html)
C’est typiquement le raisonnement utilisé par la justice pour adapter la jurisprudence à une nouvelle situation. (extracted from497.pdf)
En particulier, ce premier rapport se conclut sur l’idée que les technologies d’intelligence artificielle font émerger un potentiel d’amélioration de la vie des citoyens en ouvrant de nouveaux marchés et de nouvelles opportunités permettant de résoudre certains des grands enjeux sociétaux : la santé, les transports, l’éducation, l’énergie, l’environnement, la justice, la sécurité ou encore l’efficacité du gouvernement. (extracted from497.pdf)
Le rapport insiste sur l’importance d’assurer la justice, la transparence et la responsabilité des systèmes, dès la phase de conception. (extracted from497.pdf)
5 Article « Le droit à l’épreuve de l’intelligence artificielle » du 28 novembre 2016 paru dans la revue Village de la Justice. (extracted from497.pdf)
Lors d’un colloque à New York sur les défis posés par l’émergence de l’intelligence artificielle, organisé le 14 octobre 2015 par l’Institut de recherche sur la criminalité et la justice des Nations Unies (UNICRI), Max Tegmark était invité avec un autre expert3 à s’exprimer devant quelques 1 L’Institut a été fondé en mars 2014 par Max Tegmark cosmologiste au MIT, Jaan Tallinn co- fondateur de Skype, Anthony Aguirre physicien à l’UCSC et deux étudiants (Viktoriya Krakovna et Meia Chita-Tegmark), figurent à son conseil consultatif l’informaticien Stuart J. (extracted from497.pdf)
Lors d’un colloque à New York sur les défis posés par l’émergence de l’intelligence artificielle, organisé le 14 octobre 2015 par l’Institut de recherche sur la criminalité et la justice des Nations unies (Unicri), Max Tegmark était invité avec un autre expert à s’exprimer devant quelque 130 délégués. (extracted from497.pdf)
20 UK, High Court of Justice (Queens’ Bench Division – Divisional Court Cardiff), The Queen (OTAO) Bridges and Chief Constable of South Wales Police and others, [2019] EWCH 2341 (Admin), 4 September 2019. (extracted from327.pdf)
Experts FRA interviewed mentioned that potential future use of facial recognition technologies for the police could 50 UK, High Court of Justice (Queens’ Bench Division – Divisional target large events and gatherings as well as every Court Cardiff), The Queen (OTAO) Bridges and Chief Constable of South Wales Police and others, [2019] EWCH 2341 (Admin), 4 September 2019, para. (extracted from327.pdf)
Scale IT Systems in the Area of Freedom, Security and Justice 64 Annex to the Commission Implementing Decision laying (eu-LISA), and amending Regulation (EC) No 1987/2006 and down the specifications for the quality, resolution and use of Council Decision 2007/533/JHA and repealing Regulation fingerprints and facial image for biometric verification and (EU) No 1077/2011, OJ L 295, 21.11.2018, pp. (extracted from327.pdf)
In this framework, the iBorderCtrl Finland, Latvia, Sweden and the Netherlands, under the leadership of the Estonian Ministry of Justice. (extracted from327.pdf)
The Court of Justice of the EU (CJEU) has confirmed in its case law that the fundamental right to dignity is part of EU law.78 6.2. (extracted from327.pdf)
150 International Justice and Public Safety Network (2011), 146 FRA (2018), Under watchful eyes: biometrics, EU IT systems and Privacy Impact Assessment Report for the Utilization of Facial fundamental rights, Luxembourg, Publications Office, March Recognition Technologies to Identify Subjects in the Field, 30 2018, p. (extracted from327.pdf)
Justice, Equality and Law Reform, Ireland, Attorney General, 8 152 Human Rights Council (2019), Surveillance and human May 2014, para. (extracted from327.pdf)
See also: FRA and CoE (2016), Handbook on European law relating to access to justice, Luxembourg, Publications Office, June 2016, p. (extracted from327.pdf)
Case 36: Automated credit scoring is not qualifying ADM if a human ultimately decides whether to grant a loan or not In anearly pre-GDPR ruling from 2014, the German Federal Court of Justice (Bundesgerichtsoft) stated that “credit-scoring only amounts to an automated individual decision where the responsible body takes a decision with a legal consequence for the person concerned or a decision that has a significant impact on the person concerned, solely on the basis of a score result without further examination of the content. (extracted from333.pdf)
Article 2 from the initial version of the law stated that “Aucune décision de justice impliquant une appréciation sur un comportement humain ne peut avoir pour fondement un traitement automatisé d’informations donnant une définition du profil ou de la personnalité de l’intéressé. (extracted from333.pdf)
150 CJEU, Order of the President of the Court of Justice “Deletion” in Case C-552/21, January 25, 2022, ECLI:EU:C:2022:105. (extracted from333.pdf)
AI Now produces interdisciplinary research to help ensure that AI systems are accountable to the communities and contexts they are meant to serve, and that they are applied in ways that promote justice and equity. (extracted from18.pdf)
Our most recent publications include: ● Dirty Data, Bad Predictions: How Civil Rights Violations Impact Police Data, Predictive Policing Systems, and Justice,​ an article on how “dirty-policing” practices and policies shape the environment and the methodology by which data is created, raising the risk of creating inaccurate, skewed, or systematically biased “dirty data.” ● Anatomy of an AI System,​ a large-scale map and longform essay produced in partnership with S​ HARE Lab,​ which investigates the human labor, data, and planetary resources required to operate an Amazon Echo. (extracted from18.pdf)
​Advances in understanding of bias, fairness, and justice in machine learning research make it clear that assessments of risks and harms are imperative. (extracted from18.pdf)
This includes examples such as criminal justice advocates working to halt the use of discriminatory predictive policing tools, tenants-rights groups opposing facial recognition in housing, and a coalition of Latinx activists, tech workers, and students exposing and protesting lucrative tech company contracts with military and border agencies. (extracted from18.pdf)
Not only are James’ views counter to Google’s stated values, but they are directly counter to the project of ensuring that the development and application of AI prioritizes justice over profit.”​73​ Following the backlash, Google dissolved ATEAC after a little over a week.7​ 4 Yet even if one believes that corporate AI ethics might help guide better tech practices on some level, it is clear that change in the design, development, and implementation of AI systems largely occurs when there is pressure on companies from workers, the press, and policymakers. (extracted from18.pdf)
Barbara Grosz, a professor of natural sciences, imagines a world in which “every time a computer scientist logs on to write an algorithm or build a system, a message will flash across the screen that asks, ‘Have you thought about the ethical implications of what you’re doing?’”8​ 0​ The Design Justice Network takes this further, centering justice, not ethics, and calling on developers and designers to center affected communities in the process of creating technology together.​81 AI developers and researchers make important determinations that can affect billions of people, and helping them consider whom the technology benefits and harms is important. (extracted from18.pdf)
As during the dot-com boom and foreclosure crisis, numerous organizations and collectives formed to organize for housing justice. (extracted from18.pdf)
Accordingly, housing justice groups such as Social Housing Now (Căsi Sociala Acum) are in the midst of organizing against evictions and for the development of social housing.​97 Back in the North, there have been new forms of international solidarity in the works against AI displacement. (extracted from18.pdf)
Thus they have been organizing marches, Google bus blockades, and City Council demonstrations.​98 ​Much of this has taken place in solidarity with organizers and groups in Berlin such as Google Is Not a Good Neighbor (​Google ist kein guter Nachbar)​, which in 2018 collectively blocked Google from launching a new tech campus in the neighborhood of Kreuzberg.9​ 9​ Solidarity has also been found among New York City organizers who successfully fought the development of a new Amazon campus in 2019, and with activists in Toronto committed to thwarting gentrification induced by Sidewalk Labs.​100 During demonstrations, banners, light projections, video clips, and statements of support have expressed international solidarity, revealing a new trend toward urban justice.1​ 01 ​Much work remains to link struggles against forms of tech-sector displacement worldwide. (extracted from18.pdf)
Because AI technologies are often applied in ways that amplify and exacerbate historical patterns of inequality and discrimination, it is these historical practices—not AI systems alone—to which organizers and communities seeking justice are reacting. (extracted from18.pdf)
In July, Mijente joined Media Justice (an organization at the helm of San Francisco’s facial-recognition ban)​114 ​and Tech Workers Coalition1​ 15 ​to host Take Back Tech. (extracted from18.pdf)
This work helped shed light on lucrative tech company contracts with military and border agencies, and mobilized tech workers and students, while also emphasizing the human cost of a deportation campaign rife with human rights abuses.1​ 18​ Protesters catalyzed by the campaign have held regular demonstrations at Palantir’s headquarters in Palo Alto and at its New York City offices.​119 Organizations such as Never Again Action,​120 ​and Jews for Racial and Economic Justice (JFREJ)​121 ​have also led highly visible actions against Amazon, organizing street protests and sit-ins in Amazon bookstores to protest against the company’s ongoing work providing cloud computing services to ICE.​122​ And Immigrant rights groups such as Make the Road New York,1​ 23 along with Mijente, JFREJ, and other advocates, have reached out to academics and computer science and technology professionals through petitions, demanding that prominent conferences drop Palantir as a sponsor, given the company’s role in empowering ICE.1​ 24 ​Community-organized opposition to Palantir’s role in ICE’s detention of immigrants resulted in UC Berkeley’s Privacy Law Scholars Conference,​125 ​Lesbians Who Tech,1​ 26​ and the Grace Hopper Celebration all pulling Palantir as a sponsor.​127 Athena, a recently launched coalition, takes this further. (extracted from18.pdf)
But they also organized around issues like Amazon’s treatment of warehouse workers and its sale of surveillance tech.1​ 29 ​Athena expands on this multi-issue approach, recognizing that Amazon is at the heart of a set of interlocking issues, including worker rights at warehouses, climate justice, and mass surveillance. (extracted from18.pdf)
The pushback against AI thus builds upon the social justice work that organizers have engaged in for a much longer time. (extracted from18.pdf)
Worker organizing around AI is also part of a broader tech-worker movement focused on a broad range of social justice issues, including displacement,1​ 45​ two-tiered workforces and the exploitation of contract workers,1​ 46​ and climate change. (extracted from18.pdf)
Building on the emergence of globally oriented data protection approaches such as the European Union’s General Data Protection Regulation (GDPR), policymakers are moving quickly, driven both by the current sense of urgency to regulate the mass deployment of AI technologies lacking discernible safeguards and by the failure of ethical frameworks to adequately answer the call for accountability and justice. (extracted from18.pdf)
Addressing the specific case of forensic algorithms like automated software used to analyze DNA and predict potential suspects, the Justice in Forensic Algorithms Act of 2019​246 ​prohibits companies from withholding information about their system, such as its source code, from a defendant in a criminal proceeding on trade-secrecy grounds. (extracted from18.pdf)
The documents showed that the Federal Bureau of Investigation (FBI) and ICE were using state driver’s license databases as “the bedrock of an unprecedented surveillance infrastructure” that relied on facial-recognition technology.2​ 89​ The US Justice Department also recently announced plans to collect DNA data from migrants crossing the border, which could create more invasive monitoring of immigrants without any real limits.​290 Outside the US, governments are equally eager to pilot AI systems at border checkpoints. (extracted from18.pdf)
Though it declined to provide any details on how it is being used by customers, it indicated retail as a potential use case, illustrating how stores can feed live images of shoppers to detect emotional and demographic trends.4​ 01 Employment has also experienced a surge in the use of affect recognition, with companies like HireVue and VCV offering to screen job candidates for qualities like “grit” and to track how often they smile.4​ 02​ Call center programs Cogito and Empath use voice-analysis algorithms to monitor the reactions of customers and signal to call agents when they sound distressed.4​ 03 ​Similar programs have been proposed as an assistive technology for people with autism,4​ 04 ​while Boston-based company BrainCo is creating headbands that purport to detect and quantify students’ attention levels through brain-activity detection,4​ 05 ​despite studies that outline significant risks associated with the deployment of emotional AI in the classroom.​406 Affect-recognition software has also joined risk assessment as a tool in criminal justice. (extracted from18.pdf)
This is particularly concerning in contexts such as employment, education, and criminal justice. (extracted from18.pdf)
Despite the fact that social sciences and humanities approaches have a long history in information security and risk management,​491 ​research that addresses both social and technical dimensions in security is necessary, but still relatively nascent.4​ 92 ​Central in this challenge is redrawing the boundaries of analysis and design to expand beyond the algorithm,4​ 93 ​and securing channels for all affected stakeholders to democratically steer system development and to dissent when concerns arise.​494 CONCLUSION Despite the growth of ethical frameworks, AI systems continue to be deployed rapidly across domains of considerable social significance—in healthcare, education, employment, criminal justice, and many others—without appropriate safeguards or accountability structures in place. (extracted from18.pdf)
See Vidushi Marda, “Introduction” in APC, Article 19, and SIDA, “Artificial Intelligence: Human Rights, Social Justice and Development,” Global Information Watch 2019, November 2019, https://giswatch.org/sites/default/files/gisw2019_artificial_intelligence.pdf.​ 65. (extracted from18.pdf)
Design Justice Network Principles, accessed November 24, 2019, https://designjustice.org/read-the-principles​. (extracted from18.pdf)
“The Anti-Eviction Mapping Project: Counter Mapping and Oral History Toward Bay Area Housing Justice.” ​Annals of the American Association of Geographers ​108, no. (extracted from18.pdf)
Media Justice, accessed November 24, 2019, h​ ttps://mediajustice.org/ 115. (extracted from18.pdf)
Jews for Racial and Economic Justice, accessed November 24, 2019, h​ ttps://jfrej.org/​. (extracted from18.pdf)
Sasha Costanza Chock, “Design Justice: Towards an Intersectional Feminist Framework for Design Theory and Practice,” ​Proceedings of the Design Research Society 2018,​ June 3, 2018, https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3189696​. (extracted from18.pdf)
Amazon Employees for Climate Justice, “Open letter to Jeff Bezos and the Amazon Board of Directors,” Medium, April 10, 2019, https://medium.com/@amazonemployeesclimatejustice/public-letter-to-jeff-bezos-and-the-amazon-board- of-directors-82a8405f5e38​. (extracted from18.pdf)
Takano Introduces the Justice in Forensic Algorithms Act to Protect Defendants’ Due Process Rights in the Criminal Justice System,” Takano, September 17, 2019, https://takano.house.gov/newsroom/press-releases/rep-takano-introduces-the-justice-in-forensic-algorith ms-act-to-protect-defendants-due-process-rights-in-the-criminal-justice-system​. (extracted from18.pdf)
Bobby Allyn and Joel Rose, “Justice Department Announces Plan to Collect DNA from Migrants Crossing the Border,” NPR, October 21, 2019, https://www.npr.org/2019/10/21/772035602/justice-department-announces-plan-to-collect-dna-from-migr ants-crossing-the-bord.​ 291. (extracted from18.pdf)
Gray and Siddharth Surl, G​ host Work: How to Stop Silicon Valley from Building a New Global Underclass ​(Boston: Houghton Mifflin Harcourt, 2019); Kate Crawford and Vladan Joler, Anatomy of an AI System, 2018, ​https://anatomyof.ai​ ; Muqing Zhang, “Colonialism Is Alive in the Exploited Tech Work Force”, Outline​, June 6, 2019 https://theoutline.com/post/7533/colonialism-is-alive-in-the-exploited-tech-work-force?zd=2&zi=exrbzkaf​; APC, Article 19, and SIDA, “GISWatch 2019 - Artificial Intelligence: Human rights, social justice and development,” November 2019, h​ ttps://giswatch.org/sites/default/files/gisw2019_artificial_intelligence.pdf​; AI Now 2019 Report | 87 Noopur Raval, “Developing a Framework for Postcolonial Digital Labor,” unpublished manuscript, 2017, https://www.academia.edu/35413303/Developing_a_framework_for_postcolonial_digital_labor​. (extracted from18.pdf)
Part of the reason we must hold humans accountable is because only humans can be dissuaded by human justice. (extracted from685.html)
We often mistakenly think that justice is largely about compensation, but in fact not every wrong can be righted, so wrongs must also be prevented by disproportionate dissuasion for those wrongs successfully addressed. (extracted from685.html)
FAIRNESS AND JUSTICE Designers, developers and users of AI systems (AI stakeholders) must respect: • Applicable laws in New Zealand and other relevant jurisdictions • Human rights recognised under domestic and international law • Rights of Māori articulated in Te Tiriti o Waitangi • Democratic values including the electoral process and informed public debate • Principles of equality and fairness so that AI systems do not unjustly harm, exclude, disempower or discriminate against individuals or particular groups. (extracted from24.pdf)
care or justice)? (extracted from30.pdf)
Retrieved 23 Note to EGE (2018): AI should contribute to global justice and equal 05 01, 2018, from https://ec.europa.eu/research/ege/pdf/ege_ ai_ access to the benefits and advantages that AI, robotics and ‘autonomous’ statement_2018.pdf systems can bring. (extracted from30.pdf)
26 See: https://www.theguardian.com/news/series/cambridge-analytica-files 31 Note to EGE (2018): Sustainability: AI technology must be in line with the human responsibility to ensure the basic preconditions for life on 27 Note to EGE (2018): Rule of law, access to justice and the right to our planet, continued prospering for mankind and preservation of a redress and a fair trial provide the necessary framework for ensuring the good environment for future generations. (extracted from30.pdf)
6 A/HRC/48/31 Artificial intelligence in law enforcement, national security, criminal justice and border management 22. (extracted from657.pdf)
States are increasingly integrating AI systems into law enforcement, national security, criminal justice and border management37 systems. (extracted from657.pdf)
18; and Court of Justice of the European Union, Digital Rights Ireland and Others, C-293/12 and C-594/12, para. (extracted from657.pdf)
See also Court of Justice of the European Union, Maximilan Schrems v. (extracted from657.pdf)
12 A/HRC/48/31 are particularly high, such as law enforcement, national security,83 criminal justice, social protection, employment, health care, education and the financial sector, should have priority. (extracted from657.pdf)
These systems affect government approaches to policing and the administration of justice, determine the accessibility of public services, decide who has a chance to be recruited for a job, and affect what information people see and can share online. (extracted from657.pdf)
Those the middle of a racial justice crisis and a pandemic, concerns keep getting louder from all elements of which are disproportionately affecting people of society. (extracted from723.pdf)
Texts adopted - Civil Law Rules on Robotics - Thursday, 16 February 2017 Choisissez la langue de votre document : bg - български es - español cs - čeština da - dansk de - Deutsch et - eesti keel el - ελληνικά en - English fr - français ga - Gaeilge hr - hrvatski it - italiano lv - latviešu valoda lt - lietuvių kalba hu - magyar mt - Malti nl - Nederlands pl - polski pt - português ro - română sk - slovenčina sl - slovenščina fi - suomi sv - svenska Index Previous Next Full text Procedure : 2015/2103(INL) Document stages in plenary Document selected : A8-0005/2017 Texts tabled : A8-0005/2017 Debates : PV 15/02/2017 - 14 CRE 15/02/2017 - 14 Votes : PV 16/02/2017 - 6.9 Explanations of votes Texts adopted : P8_TA(2017)0051 Texts adopted 253k 74k Thursday, 16 February 2017 - Strasbourg Civil Law Rules on Robotics P8_TA(2017)0051 A8-0005/2017 Resolution Annex European Parliament resolution of 16 February 2017 with recommendations to the Commission on Civil Law Rules on Robotics ( 2015/2103(INL) ) The European Parliament, – having regard to Article 225 of the Treaty on the Functioning of the European Union, – having regard to Council Directive 85/374/EEC (1) , – having regard to the study on Ethical Aspects of Cyber-Physical Systems carried out on behalf of the Parliament's Science and Technology Options Assessment (STOA) Panel and managed by the Scientific Foresight Unit (STOA), European Parliamentary Research Service; – having regard to Rules 46 and 52 of its Rules of Procedure, – having regard to the report of the Committee on Legal Affairs and the opinions of the Committee on Transport and Tourism, the Committee on Civil Liberties, Justice and Home Affairs, the Committee on Employment and Social Affairs, the Committee on the Environment, Public Health and Food Safety, the Committee on Industry, Research and Energy and the Committee on the Internal Market and Consumer Protection ( A8-0005/2017 ), Introduction A. (extracted from294.html)
Points out that the guiding ethical framework should be based on the principles of beneficence, non-maleficence, autonomy and justice, on the principles and values enshrined in Article 2 of the Treaty on European Union and in the Charter of Fundamental Rights, such as human dignity, equality, justice and equity, non-discrimination, informed consent, private and family life and data protection, as well as on other underlying principles and values of the Union law, such as non-stigmatisation, transparency, autonomy, individual responsibility and social responsibility, and on existing ethical practices and codes; 14. (extracted from294.html)
Researchers in the field of robotics should commit themselves to the highest ethical and professional conduct and abide by the following principles: Beneficence – robots should act in the best interests of humans; Non-maleficence – the doctrine of ‘first, do no harm’, whereby robots should not harm a human; Autonomy – the capacity to make an informed, un-coerced decision about the terms of interaction with robots; Justice – fair distribution of the benefits associated with robotics and affordability of homecare and healthcare robots in particular. (extracted from294.html)
LICENCE FOR DESIGNERS – You should take into account the European values of dignity, autonomy and self-determination, freedom and justice before, during and after the process of design, development and delivery of such technologies including the need not to harm, injure, deceive or exploit (vulnerable) users. (extracted from294.html)
In addition to strong enforcement of the General Data Protection Regulation (GDPR) and safeguards such as human rights impacts assessments, software transparency and the availability of datasets for public scrutiny, it is vital that the upcoming regulatory proposal establishes in law clear limitations as to what can be considered lawful uses of AI, to unequivocally address the following issues: • the enabling of biometric mass surveillance and monitoring of public spaces; • the exacerbation of structural discrimination, exclusion and collective harms; • the restriction of and discriminatory access to vital services such as health-care and social security; • the surveillance of workers and infringement of workers’ fundamental rights; • the impeding of fair access to justice and procedural rights; • the use of systems which make inferences and predictions about our most sensitive characteristics, behaviours and thoughts; • and, crucially, the manipulation or control of human behaviour and associated threats to human dignity, agency, and collective democracy. (extracted from253.pdf)
Use of risk assessment tools in the criminal justice system and pre-trial context The use of algorithms in criminal justice matters to profile individuals within legal decision-making processes presents severe threats to fundamental rights. (extracted from253.pdf)
In addition, substantial evidence has shown that the introduction of such systems in criminal justice systems in Europe and elsewhere has resulted in unjust and discriminatory outcomes. (extracted from253.pdf)
We argue that legal limits must be imposed on AI risk assessment systems in the criminal justice context. (extracted from253.pdf)
Legal restrictions or legislative red-lines on the uses which contravene fundamental rights, including, but not limited to, uses of AI at the border, predictive policing, systems which restrict access to social rights and benefits, and risk-assessment tools in the criminal justice context; 3. (extracted from253.pdf)
Yours sincerely, European Digital Rights (EDRi), including: Access Now Bits of Freedom Chaos Computer Club D3 - Defesa dos Direitos Digitais Electronic Privacy Information Center (EPIC) Fitug Hermes Center Homo Digitalis IT-Pol Denmark Iuridicum Remedium Metamorphosis Foundation Panoptykon Foundation Privacy International Statewatch Other signatories: AI Now Institute, NYU Algorithm Watch Amnesty International App Drivers and Couriers Union (ADCU) Associazione Certi Diritti Associazione Luca Coscioni Associazione per gli Studi Giuridici sull'Immigrazione Big Brother Watch Center for Intersectional Justice (CIJ) Democratic Society Digitale Freiheit Dutch Section - International Commission of Jurists (NJCM) Each One Teach One (EOTO) e.V. (extracted from253.pdf)
Algorithms in decision-making 1 Contents Summary 3 1 Introduction 7 Our inquiry 10 2 Applications and bias 11 Data sharing 11 In the health sector 11 In the criminal justice system 13 In the web and social media sector 14 Government data sharing and getting value from its data 15 Bias 18 Training data 19 Insufficient data 20 Correlation without causation 21 Lack of representation in the algorithm development community 22 3 Accountability and transparency 24 Accountability 24 Principles and codes 25 Audit and certification 26 Ethics boards 27 Transparency 27 The right to explanation 29 4 The Centre for Data Ethics & Innovation, research and the regulatory environment 32 ‘Automated’ decisions 33 Consent 35 Data protection impact assessments 36 The Information Commissioner’s powers 37 Sector regulation 39 2 Algorithms in decision-making Conclusions and recommendations 41 Formal minutes 45 Witnesses 46 Published written evidence 47 List of Reports from the Committee during the current Parliament 50 Algorithms in decision-making 3 Summary Algorithms have long been used to aid decision-making, but in the last few years the growth of ‘big data’ and ‘machine learning’ has driven an increase in algorithmic decision-making—in finance, the legal sector, the criminal justice system, education, and healthcare, as well as recruitment decisions, giving loans or targeting adverts on social media, and there are plans for autonomous vehicles to be on public roads in the UK. (extracted from535.pdf)
The range of different industries in which machine learning is already being put to use includes finance (including access to loans and insurance), the legal sector, the criminal justice system, education, and healthcare, as well as recruitment decisions and targeting adverts on social media,12 and there are plans for driverless vehicles to be on public roads in the UK in the near future.13 Hetan Shah from the Royal Statistical Society believed that “it is best to understand this as a ubiquitous technology and to think of it almost as a public infrastructure”.14 The Royal Academy of Engineering believes that as more data are generated, an increase in the use of machine learning algorithms will allow organisations to consider a much broader range of datasets or inputs than was previously possible, providing “an opportunity for better decision-making by combining human and machine intelligence in a smart way”.15 Algorithms driven by machine learning bring certain risks as well as benefits. (extracted from535.pdf)
Our predecessor Committee’s report on Big Data expounded the “enormous benefits in prospect for the economy and for people’s lives” from making public data ‘open’.37 In our current inquiry we examined the way data sharing is affecting three sectors in particular—in healthcare, criminal justice and social media. (extracted from535.pdf)
Eleonora Harwich of Reform thought that “the standardisation of clinical codes, which are going to be replaced by a standard system” would be “a positive step forward”.65 In the criminal justice system 18. (extracted from535.pdf)
In the criminal justice system, algorithms are being used by some police forces for facial image recognition. (extracted from535.pdf)
We heard in our inquiry about how Durham Constabulary is also using algorithms to “assist decision making relating to whether a suspect could be eligible for a deferred prosecution”73 (Box 2), as well as their wider and more controversial use in the US for decisions on bail, parole and sentencing (paragraph 38).74 Durham Constabulary believed that AI’s ability to assess risk from past behaviours is being used to get “consistency in decision making” about targeted interventions for offenders.75 HM Inspectorate of Constabulary concluded in 2017 that the wider use of the technology used at Durham could “improve effectiveness, release officer capacity, and is likely to be cost effective”.76 64 Qq232, 234, 235 65 Q230 66 Big Brother Watch, Big Brother Watch Briefing for Short Debate on the use of facial recognition technology in security and policing in the House of Lords (March 2018), p8 67 Letter from Baroness Williams of Trafford to the Committee, 28 March 2018 68 Oxford Internet Institute (ALG0031) 69 UCL Jill Dando Institute of Security and Crime Science (ALG0048) para 5 70 Marion Oswald and Sheena Urwin submission; See also, “Pre-crime software recruited to track gang of thieves“, New Scientist, 11 March 2015 71 RUSI, Big Data and Policing 2017 (September 2017), p20 72 UCL Jill Dando Institute of Security and Crime Science 73 Sheena Urwin, Head of Criminal Justice, Durham Constabulary (ADM0032) 74 Institute of Mathematics and its Applications (ALG0028) para 19 75 Durham Constabulary (ALG0041) 76 HM Inspectorate of Constabulary, PEEL: Police Effectiveness 2016 (March 2017), p33 14 Algorithms in decision-making Box 2: Durham Constabulary’s use of algorithms The Harm Assessment Risk Tool (HART), designed as a result of a collaboration between Durham Constabulary and Dr Barnes of University of Cambridge, is a decision support system used to assist officers in deciding whether a suspect is eligible for deferred prosecution based on the future risk of offending. (extracted from535.pdf)
The HART algorithm being piloted and evaluated by Durham Constabulary does not utilise data from other police force areas, nor indeed from national IT systems.77 The Royal United Services Institute’s ‘Big Data and Policing’ review in 2017 concluded that “because the system was only using Durham Police’s data, offences committed in other areas would not be considered, and dangerous criminals might not be identified”.78 HM Inspectorate of Constabulary found that “most forces have not yet explored fully the use of new and emerging techniques and analysis to direct operational activity at a local level”.79 Marion Oswald, Director of the Centre for Information Rights, and Sheena Urwin of Durham Constabulary, noted that only 14% of UK police forces were using algorithmic data analysis or decision-making for intelligence work.80 Professor Louise Amoore questioned whether there is a “place for inference or correlation in the criminal justice system”81 since, unlike normal evidence, it cannot be cross-examined or questioned.82 Jamie Grace from Sheffield Hallam University accepted its use but wanted “a single [independent] oversight body and regulator for the use of police databases and algorithmic analysis in criminal justice”.83 In the web and social media sector 22. (extracted from535.pdf)
They are also moving into areas where the benefits to those applying them may not be matched by the benefits to those subject to their ‘decisions’—in some aspects of the criminal justice system, for example, and algorithms using social media datasets. (extracted from535.pdf)
The Alan Turing Institute told us that when automated decision-making is applied “current legislation does very little to protect individuals from being discriminated” against.119 Where algorithms are used in the criminal justice system it is imperative that algorithms are not unfairly discriminatory. (extracted from535.pdf)
Professor Amoore warned that there may exist “areas of our social, political or economic lives where we might want to say there is no place for algorithmic decision- making”.144 She also questioned the use of inference and correlation in the criminal justice system, and suggested that its use in the US for sentencing “constitutes a violation of due process or overt discrimination”.145 (In the UK, Durham Constabulary was using an algorithm to help determine whether a low-risk offender is suitable for ‘deferred prosecution’.)146 The risk is compounded, as Professor Amoore explained, when the algorithm’s results do not allow challenge: Whereas with conventional tools like DNA, or a photograph, or a CCTV image, or the evidence that has been given by an eye witness, there is always the possibility of this cross-examination and the questioning: ‘How did you arrive at that judgment?’” With machine learning algorithms that method is obviated.147 140 The Royal Society (ALG0056) para 13 141 Institute of Mathematics and its Applications (ADM0008) para 23 142 Q10 143 Wired, ‘UK police are using AI to inform custodial decisions – but it could be discriminating against the poor’, 1 March 2018 144 Q27 145 Kehl, Danielle, Priscilla Guo, and Samuel Kessler. (extracted from535.pdf)
Algorithms in the Criminal Justice System: Assessing the Use of Risk Assessments in Sentencing, accessed 5 April 2018 146 Sheena Urwin, Head of Criminal Justice, Durham Constabulary (ADM0032) 147 Q28 22 Algorithms in decision-making 41. (extracted from535.pdf)
In some of our evidence, there was a desire for algorithms within the criminal justice system to be restricted to advisory roles. (extracted from535.pdf)
DeepMind told us that they were working on a ‘verifiable data audit’ project using digital ledgers (‘blockchains’) to give people cryptographic proof that their data are being used in particular ways.265 256 Q150 257 Sheena Urwin, Head of Criminal Justice, Durham Constabulary (ADM0032) 258 EU GDPR, ‘GDPR Key Changes’, accessed 20 March 2018 259 Information Commissioner’s Office (ALG0038) 260 Science and Technology Committee, Fourth Report of Session 2015–16, The big data dilemma, HC 468 261 Q57 262 Q365 263 Science and Technology Committee, Fourth Report of Session 2015–16, The big data dilemma, HC 468, para 66 264 Q66 265 Q245 [Dr Dominic King] 36 Algorithms in decision-making 82. (extracted from535.pdf)
They are also moving into areas where the benefits to those applying them may not be matched by the benefits to those subject to their ‘decisions’—in some aspects of the criminal justice system, for example, and algorithms using social media datasets. (extracted from535.pdf)
Q89–148 Sheena Urwin, Head of Criminal Justice, Durham Constabulary; and Professor Kate Bowers, Academic Director, UCL Jill Dando Institute. (extracted from535.pdf)
URL: https://standards.ieee.org/content/dam/ieee-standards/standards/web/documents/other/eadv2_glossary.pdf Equity By definition, equity is concerned with justice. (extracted from247.pdf)
URL: https://www.iso.org/obp/ui/fr/#iso:std:iso- iec:22989:dis:ed-1:v1:en 3.6 F Fairness Fairness refers to a variety of ideas known as equity, impartiality, egalitarianism, non-discrimination and justice. (extracted from247.pdf)
URL: http://www.incompleteideas.net/book/ebook/the-book.html Rights That which is considered proper, correct, or consonant with justice, and related uses; The standard of permitted and forbidden action within a particular sphere. (extracted from247.pdf)
AI tightly developed under human oversight and control), embed the principles of fairness and justice in algorithms, applied in all phases of AI systems’ design, implementation and testing, while following through bias complaints and other undesired effects reporting. (extracted from279.pdf)
During the 71st session of the United Nations General Assembly, on 29 September 2016, UNICRI (United Nations International Crime and Justice research Institute) announced the opening of the first Centre on AI and Robotics 30 PE 626.074 European Artificial Intelligence (AI) leadership, the path for an integrated vision within the United Nations system. (extracted from279.pdf)
AI tightly developed under human oversight and control), embed the principles of fairness and justice in algorithms, applied in all phases of AI systems’ design, implementation and testing, while following through bias complaints and other undesired effects reporting. (extracted from279.pdf)
Apply EUR-Lex Access to European Union law This document is an excerpt from the EUR-Lex website You are here EUROPA EUR-Lex home EUR-Lex - 52021DC0118 - EN Help Print Menu EU law Treaties Treaties currently in force Founding Treaties Accession Treaties Other treaties and protocols Chronological overview Legal acts Consolidated texts International agreements Preparatory documents EFTA documents Lawmaking procedures Summaries of EU legislation Browse by EU institutions European Parliament European Council Council of the European Union European Commission Court of Justice of the European Union European Central Bank European Court of Auditors European Economic and Social Committee European Committee of the Regions Browse by EuroVoc EU case-law Case-law Reports of cases Directory of case-law Official Journal Access the Official Journal Official Journal L series daily view Official Journal C series daily view Browse the Official Journal Legally binding printed editions Special editions National law and case-law National transposition National case-law JURE case-law Information Themes in focus EUR-Lex developments Statistics ELI register About ELI Technical information ELI implementation overview Resources for implementing ELI ELI highlights ELI testimonials Legislation in schema.org EU budget online Quick search Use quotation marks to search for an "exact phrase". (extracted from125.html)
Using Green Public Procurement criteria 34 can boost demand for a green digital transformation The digital transformation should also enable modern and efficient justice systems 35 , enforcement of consumer rights and an increased effectiveness of public action including law enforcement and investigation capacities 36 – what is illegal offline is also illegal online, and law enfor cement must be best equipped to deal with more and more sophisticated digital crimes. (extracted from125.html)
The digital principles are rooted in primary EU law, notably the Treaty on European Union (TEU), the Treaty on the Functioning of the European Union (TFEU), the Charter of Fundament al Rights and the case-law of the Court of Justice of the European Union, as well as in secondary legislation 37 . (extracted from125.html)
(34) https://ec.europa.eu/environment/gpp/eu_gpp_criteria_en.htm (35) Communication from the Commission on the Digitalisation of justice in the European Union A toolbox of opportunities, COM(2020) 710 final. (extracted from125.html)
Help pages Contact Sitemap Follow us X Legal Legal notice Cookies policy Accessibility Privacy statement Information About EUR-Lex Newsletter Useful links Other services European Data EU tenders EU research results EU Whoiswho EU publications N-Lex EU Law Tracker Discover more on europa.eu Contact the EU Call us 00 800 6 7 8 9 10 11 Use other telephone options Write to us via our contact form Meet us at one of the EU centres Social media Search for EU social media channels Legal Languages on our websites Privacy policy Legal notice Cookies EU institutions European Parliament European Council Council of the European Union European Commission Court of Justice of the European Union (CJEU) European Central Bank (ECB) European Court of Auditors European External Action Service (EEAS) European Economic and Social Committee European Committee of Regions (CoR) European Investment Bank European Ombudsman European Data Protection Supervisor (EDPS) European Data Protection Board European Personnel Selection Office Publications Office of the European Union Agencies Switch to mobile Switch to desktop (extracted from125.html)
These include freedom, dignity and autonomy, privacy and data protection, non-discrimination and equality, diversity, fairness, social justice, and internationally recognized labor rights. (extracted from523.pdf)
Some interventions might aim to improve completion and contribute to social justice, fairness and non-discrimination, in line with the G20 Principles. (extracted from523.pdf)
At EU level, the processing of biometric data has been actively encouraged and directly supported over the past years in the context of EU-level large-scale information technology (IT) systems in the area of freedom, security and justice (AFSJ). (extracted from286.pdf)
A review of this architecture and of the most relevant rules on biometrics and on automated decision-making in EU data protection law, as well as of the most important case law in this area emanating from the Court of Justice of the EU (CJEU) and the European Court of Human Rights (ECtHR), shows that ongoing technological developments are taking place amid – and possibly also somehow despite – existing rights and principles, which might thus possibly need to be reinforced, clarified, or at least fine-tuned. (extracted from286.pdf)
The processing of biometric data has been actively supported at EU level23 in the context of EU-level large-scale IT systems in the Area of Freedom, Security and Justice (AFSJ). (extracted from286.pdf)
It also presents the most important case law in this area emanating from the Court of Justice of the EU (CJEU) and the European Court of Human Rights (ECtHR). (extracted from286.pdf)
Legal aid shall be made available to those who lack sufficient resources in so far as such aid is necessary to ensure effective access to justice.' 51 In this sense, see for instance EDPB and EDPS, Joint Opinion 5/2021 12. (extracted from286.pdf)
Fundamental rights case law Analysing the case law of the Court of Justice of the EU (CJEU) on biometrics, a series of points stand out. (extracted from286.pdf)
As underlined in a report for the Committee on Equality and Non-Discrimination of the Council of Europe's Parliamentary Assembly, certain flaws in a the criminal justice system can have 'far-reaching human rights consequences' (Lacroix 2020 12). (extracted from286.pdf)
Other areas mentioned as involving the qualification of an AI system as high risk are management and operation of critical infrastructure, educational and vocational training; employment, workers management and access to self-employment; access to and enjoyment of essential private services and public services and benefits; law enforcement; migration, asylum and border control management; administration of justice and democratic processes. (extracted from286.pdf)
This could relate to 'emotion recognition.' '(e) AI systems intended to be used by law enforcement authorities for predicting the occurrence or reoccurrence of an actual or potential criminal offence based on profiling of 42 Person identification, human rights and ethical principles natural persons as referred to in Article 3(4) of Directive (EU) 2016/680 or assessing personality traits and characteristics or past criminal behaviour of natural persons or groups.' '(f) AI systems intended to be used by law enforcement authorities for profiling of natural persons as referred to in Article 3(4) of Directive (EU) 2016/680 in the course of detection, investigation or prosecution of criminal offences.' '(g) AI systems intended to be used for crime analytics regarding natural persons, allowing law enforcement authorities to search complex related and unrelated large data sets available in different data sources or in different data formats in order to identify unknown patterns or discover hidden relationships in the data.' Under heading 7, on 'Migration, asylum and border control management', stand out: '(a) AI systems intended to be used by competent public authorities as polygraphs and similar tools or to detect the emotional state of a natural person;' 'd) AI systems intended to assist competent public authorities for the examination of applications for asylum, visa and residence permits and associated complaints with regard to the eligibility of the natural persons applying for a status.' Under heading 8, on 'Administration of justice and democratic processes', are mentioned: '(a) AI systems intended to assist a judicial authority in researching and interpreting facts and the law and in applying the law to a concrete set of facts.' The proposal foresees that Annex III might be amended by the European Commission via delegated acts (proposed Article 7(1) AIA). (extracted from286.pdf)
European Union Agency for the Operational Management of Large-Scale IT Systems in the Area of Freedom, Security and Justice (eu-LISA). (extracted from286.pdf)
justice, public housing, welfare and health provision are examples of areas where “black box” systems should not be 2. (extracted from721.pdf)
29 In “Fairness in Criminal Justice Risk Assessments: The State of the to the design of the system based on reported issues? (extracted from721.pdf)
Berk et al, 2017, Fairness in Criminal Justice Risk Assessments: The State of the Art https://arxiv.org/ 30 Ai Now Institute 2017 Report abs/1703.09207 12 How to Prevent Discriminatory Outcomes in Machine Learning Bringing principles of non-discrimination Step 3: Being transparent about efforts to identify, to life: Human rights due diligence for prevent, and mitigate human rights risks machine learning For leadership, this step involves explicitly encouraging transparency. (extracted from721.pdf)
Perceived fairness of web based applicant ‐ screening procedures: Weighing the rules of justice and the role of individual differences. (extracted from641.pdf)
"Why Machine Learning May Lead to Unfairness: Evidence from Risk Assessment for Juvenile Justice in Catalonia", Best Paper Award, International Conference on AI and Law, 2019 Certain AI programmes for facial analysis display gender and racial bias, demonstrating low errors for determining the gender of lighter-skinned men but high errors in determining gender for darker-skinned women. (extracted from133.pdf)
Individuals and legal entities may face difficulties with effective access to justice in situations where such decisions may negatively affect them. (extracted from133.pdf)
This is without prejudice to the question whether, for the purpose of liability to end-users or other parties suffering harm and ensuring effective access to justice, which party should be liable for any damage caused. (extracted from133.pdf)
WHAT IS NEEDED NEXT 32 3.1 From Fairness to Justice 32 3.2 Infrastructural Thinking 33 3.3 Accounting for Hidden Labor in AI Systems 34 3.4 Deeper Interdisciplinarity 36 3.5 Race, Gender and Power in AI 37 3.6 Strategic Litigation and Policy Interventions 39 3.7 Research and Organizing: An Emergent Coalition 40 CONCLUSION 42 ENDNOTES 44 This work is licensed under a C reative Commons Attribution-NoDerivatives 4.0 International License 2 ABOUT THE AI NOW INSTITUTE The AI Now Institute at New York University is an interdisciplinary research institute dedicated to understanding the social implications of AI technologies. (extracted from26.pdf)
AI Now produces interdisciplinary research to help ensure that AI systems are accountable to the communities and contexts they are meant to serve, and that they are applied in ways that promote justice and equity. (extracted from26.pdf)
Our workshop on Immigration, Data, and Automation in the Trump Era, co-hosted with the Brennan Center for Justice and the Center for Privacy and Technology at Georgetown Law, focused on the Trump Administration’s use of data harvesting, predictive analytics, and machine learning to target immigrant communities. (extracted from26.pdf)
Domains like health, education, criminal justice, and welfare all have their own histories, regulatory frameworks, and hazards. (extracted from26.pdf)
Facial recognition technology poses its own dangers, reinforcing skewed and potentially discriminatory practices, from criminal justice to education to employment, and presents risks to human rights and civil liberties in multiple countries. (extracted from26.pdf)
Yet, without a framework that accounts for social and political contexts and histories, these mathematical formulas for fairness will almost inevitably miss key factors, and can serve to paper over deeper problems in ways that ultimately increase harm or ignore justice. (extracted from26.pdf)
Broadening perspectives and expanding research into AI fairness and bias beyond the merely mathematical is critical to ensuring we are capable of addressing the core issues and moving the focus from parity to justice. (extracted from26.pdf)
Expanding AI fairness research beyond a focus on mathematical parity and statistical fairness toward issues of justice 2. (extracted from26.pdf)
9 INTRODUCTION The Social Challenges of AI in 2018 The past year has seen accelerated integration of powerful artificial intelligence systems into core social institutions, against a backdrop of rising inequality, political populism, and industry scandals.1 There have been major movements from both inside and outside technology companies pushing for greater accountability and justice. (extracted from26.pdf)
Facial recognition amplifies civil rights concerns Concerns are intensifying that facial recognition increases racial discrimination and other biases in the criminal justice system. (extracted from26.pdf)
Moreover, the false positives disproportionately occurred among non-white members of Congress, with an error rate of nearly 40% compared to only 5% for white members.5 2 Such results echo a string of findings that have demonstrated that facial recognition technology is, on average, better at detecting light-skinned people than dark-skinned people, and better at detecting men than women.5 3 In its response to the ACLU, Amazon acknowledged that “the Rekognition results can be significantly skewed by using a facial database that is not appropriately representative.”5 4 Given the deep and historical racial biases in the criminal justice system, most law enforcement databases are unlikely to be “appropriately representative.”5 5 Despite these serious flaws, ongoing pressure from civil rights groups, and protests from Amazon employees over the potential for misuse of these technologies, Amazon Web Services CEO Andrew Jassy recently told employees that “we feel really great and really strongly about the value that Amazon Rekognition is providing our customers of all sizes and all types of industries in law enforcement and out of law enforcement.”5 6 Nor is Amazon alone in implementing facial recognition technologies in unaccountable ways. (extracted from26.pdf)
17 1.2 The Risks of Automated Decision Systems in Government Over the past year, we have seen a substantial increase in the adoption of Automated Decision Systems (ADS) across government domains, including criminal justice, child welfare, education, and immigration. (extracted from26.pdf)
For years, criminal justice advocates and researchers have pushed for the elimination of cash bail, which has been shown to disproportionately harm individuals based on race and socioeconomic status while at the same time failing to enhance public safety.8 9 In response, New Jersey and California recently passed legislation aimed at addressing this concern. (extracted from26.pdf)
However, instead of simply ending cash bail, they replaced it with a pretrial assessment system designed to algorithmically generate “risk” scores that claim to predict whether a person should go free or be detained in jail while awaiting trial.9 0 The shift from policies such as cash bail to automated systems and risk assessment scoring is still relatively new, and is proceeding even without substantial research examining the potential to amplify discrimination within the criminal justice system. (extracted from26.pdf)
New Jersey’s law went into effect in 2017, and while the state has experienced a decline in its pretrial population, advocates have expressed worry that racial disparities in the risk 20 assessment system persist.9 1 Similarly, when California’s legislation passed earlier this year, many of the criminal justice advocates who pushed for the end of cash bail, and supported an earlier version of the bill, opposed its final version due to the risk assessment requirement.9 2 Education policy is also feeling the impact of automated decision systems. (extracted from26.pdf)
The National Association for the Advancement of Colored People (NAACP) and the Lawyers’ Committee for Civil Rights and 21 Economic Justice opposed the plan because of the school district’s failure to appreciate that parents of color and lower-income parents often rely on jobs that lack work schedule flexibility and may not be able to afford additional child care.9 9 These failed efforts demonstrate two important issues that policymakers must consider when evaluating the use of these systems. (extracted from26.pdf)
3.1 From Fairness to Justice Any debate about bias and fairness should approach issues of power and hierarchy, looking at who is in a position to produce and profit from these systems, whose values are embedded in these systems, who sets their “objective functions,” and which contexts they are intended to work within.1 65 Echoing the Association for Computing Machinery (ACM) researcher’s call for an acknowledgement of “negative implications” as a requirement for peer review, much more attention must be paid to the ways that AI can be used as a tool for exploitation and control.1 66 We must also be cautious not to reframe political questions as technical concerns.1 67 When framed as technical “fixes,” debiasing solutions rarely allow for questions about the appropriateness or efficacy of an AI system altogether, or for an interrogation of the institutional context into which the “fixed” AI system will ultimately be applied. (extracted from26.pdf)
For example, making a facial recognition system perform equally on people with light and dark skin may be a type of technical progress in terms of parity, but if that technology is disproportionately used on people of color and low-income communities, is it really “fair?” This is why definitions of fairness face a hard limit if they remain purely contained within the technical domain: in short, “parity is not justice.”1 69 32 3.2 Infrastructural Thinking In order to better understand and track the complexities of AI systems, we need to look beyond the technology and the hype to account for the broader context of how AI is shaping and shaped by social and material forces. (extracted from26.pdf)
Rafael Reif described it.1 92 Such initiatives are critical: as AI becomes more deeply embedded in areas like healthcare, criminal justice, hiring, housing, and educational systems, experts from these domains are essential if we are to ensure AI works as envisioned. (extracted from26.pdf)
For these important connections to grow, more protections are needed, including a commitment from technology companies to provide protections for conscientious objectors who do not want to work on military or policing contracts, along with protections for employees involved in labor organizing and ethical whistleblowers.2 34 The last year revealed many of the hardest challenges for accountability and justice as AI systems moved deeper into the social world. (extracted from26.pdf)
Natalie Ram, “Innovating Criminal Justice,” N orthwestern University Law Review 112, no. (extracted from26.pdf)
For a more general description of justice as fairness, see: John Rawls, J ustice as Fairness: A Restatement, ed. (extracted from26.pdf)
Ben Green, “‘Fair’ Risk Assessments: A Precarious Approach for Criminal Justice Reform” (5th Workshop on Fairness, Accountability, and Transparency in Machine Learning, Stockholm, 2018), https://scholar.harvard.edu/files/bgreen/files/18-fatml.pdf. (extracted from26.pdf)
This can most clearly be shown by comparing the sets of principles with the set of four core principles commonly used in bioethics: beneficence, non-maleficence, autonomy, and justice. (extracted from32.pdf)
4.4 Justice: Promoting Prosperity and Preserving Solidarity The last of the four classic bioethics principles is justice, which is typically invoked in relation to the distribution of resources, such as new and experimental treatment options or simply the general availability of conventional healthcare. (extracted from32.pdf)
The importance of “justice” is explicitly cited in the Montreal Declaration, which argues that “the development of AI should promote justice and seek to eliminate all 1 3 AI4People—An Ethical Framework for a Good AI Society:… 699 types of discrimination”, while the Asilomar Principles include the need for both “shared benefit” and “shared prosperity” from AI. (extracted from32.pdf)
Under its principle named “Jus- tice, equity and solidarity”, the EGE argues that AI should “contribute to global justice and equal access to the benefits” of AI technologies. (extracted from32.pdf)
Across the documents, justice variously relates to (a) Using AI to correct past wrongs such as eliminating unfair discrimination; (b) Ensuring that the use of AI creates benefits that are shared (or at least shareable); and (c) Preventing the creation of new harms, such as the undermining of existing social structures. (extracted from32.pdf)
Notable also are the different ways in which the position of AI, vis-à-vis people, is characterised in relation to justice. (extracted from32.pdf)
In Asilomar and EGE respectively, it is AI technologies themselves that “should benefit and empower as many people as pos- sible” and “contribute to global justice”, whereas in Montreal, it is “the develop- ment of AI” that “should promote justice” (italics added). (extracted from32.pdf)
Develop appropriate legal procedures and improve the IT infrastructure of the justice system to permit the scrutiny of algorithmic decisions in court. (extracted from32.pdf)
Accept additional cookies Reject additional cookies View cookies Hide cookie message Skip to main content GOV.UK Navigation menu Menu Menu Services and information Benefits Births, death, marriages and care Business and self-employed Childcare and parenting Citizenship and living in the UK Crime, justice and the law Disabled people Driving and transport Education and learning Employing people Environment and countryside Housing and local services Money and tax Passports, travel and living abroad Visas and immigration Working, jobs and pensions Government activity Departments Departments, agencies and public bodies News News stories, speeches, letters and notices Guidance and regulation Detailed guidance, regulations and rules Research and statistics Reports, analysis and official statistics Policy papers and consultations Consultations and strategy Transparency Data, Freedom of Information releases and corporate reports Search GOV.UK × Search GOV.UK Search Search GOV.UK Search Home Independent report COVID-19 repository and public attitudes retrospective The CDEI has published new research on the use of AI and data-driven technology in the UK’s COVID-19 response, highlighting insights into public attitudes, as well as trends it has identified. (extracted from84.html)
Cancel Services and information Benefits Births, death, marriages and care Business and self-employed Childcare and parenting Citizenship and living in the UK Crime, justice and the law Disabled people Driving and transport Education and learning Employing people Environment and countryside Housing and local services Money and tax Passports, travel and living abroad Visas and immigration Working, jobs and pensions Government activity Departments News Guidance and regulation Research and statistics Policy papers and consultations Transparency How government works Get involved Support links Help Privacy Cookies Accessibility statement Contact Terms and conditions Rhestr o Wasanaethau Cymraeg Government Digital Service All content is available under the Open Government Licence v3.0 , except where otherwise stated © Crown copyright (extracted from84.html)
Four of them are core principles commonly used in bioethics: beneficence , non-maleficence , autonomy , and justice. (extracted from0.html)
Keywords: accountability, autonomy, artificial intelligence, beneficence, ethics, explicability, fairness, intelligibility, justice, non-maleficence. (extracted from0.html)
This convergence can most clearly be shown by comparing the sets of principles with the four core principles commonly used in bioethics: beneficence, non-maleficence , autonomy , and justice (Beauchamp & Childress, 2012). (extracted from0.html)
Justice: Promoting Prosperity, Preserving Solidarity, Avoiding Unfairness The decision to make or delegate decisions does not take place in a vacuum. (extracted from0.html)
The consequences of this disparity in autonomy are addressed in the principle of justice. (extracted from0.html)
The importance of ‘justice’ is explicitly cited in the Montreal Declaration, which argues that “the development of AI should promote justice and seek to eliminate all types of discrimination,” while the Asilomar Principles include the need for both “shared benefit” and “shared prosperity” from AI. (extracted from0.html)
Under its principle named “Justice, equity and solidarity,” the EGE argues that AI should “contribute to global justice and equal access to the benefits” of AI technologies. (extracted from0.html)
It also warns against the risk of bias in datasets used to train AI systems, and – unique among the documents – argues for the need to defend against threats to “solidarity,” including “systems of mutual assistance such as in social insurance and healthcare.” Elsewhere ‘justice’ has still other meanings (especially in the sense of fairness ), variously relating to the use of AI to correct past wrongs such as eliminating unfair discrimination, promoting diversity, and preventing the rise of new threats to justice. (extracted from0.html)
The diverse ways in which justice is characterised hints at a broader lack of clarity over AI as a human-made reservoir of ‘smart agency.’ Put simply, are we (humans) the patient, receiving the ‘treatment’ of AI, the doctor prescribing it? (extracted from0.html)
Table 1: The five principles in the six documents analyzed and their occurrence in three recent documents Beneficence Nonmaleficence Autonomy Justice Explicability AIUK • • • • • Asilomar • • • • • EGE • • • • • IEEE • • • Montreal • • • • • Partnership • • • • AI4People • • • • • EC HLEG • • • • • OECD • • • • • The development and use of AI hold the potential for both positive and negative impact on society, to alleviate or to amplify existing inequalities, to cure old problems, or to cause new ones. (extracted from0.html)
This list was designed by consensus of a large diverse interdisciplinary committee to give the public something better than Asimov’s Laws (which covered beneficence & justice), but extended to five in order to bring in transparency and accountability. (extracted from0.html)
In turn, impact police data, predictive policing systems, and justice’, NYU Law Review, Vol. (extracted from325.pdf)
Beyond that, gender identity is mentioned only in Recital 9 of the Victims’ Rights Directive75 in the context of criminal law.76 According to the Court of Justice of the European Union, gender identity is only partly covered by the principle of equal treatment between men and women.77 Legal protection against discrimination based on religion is currently also limited under EU law.78 Nevertheless, one may argue that many comments referring to people who identify as lesbian, gay, bisexual, transgender and intersex (LGBTI), Jewish or Muslim fall under either the Racial Equality Directive or the Gender Goods and Services Directive, because discrimination based on sexual orientation, gender identity or religion predominantly affects a specific race or gender. (extracted from325.pdf)
Furthermore, there is a lack of representative and high- quality datasets to develop algorithms, and taking into account differences in speech patterns and changing patterns of speech remains difficult.88 The use of algorithms may further increase the opacity of content moderation and further increase challenges linked to fairness and justice.89 Without proper safeguards, such tools can lead to censorship and biased enforcement of laws and platforms’ terms and conditions.90 A potential increase in discrimination is just one of the challenges when using algorithms to support speech detection for content moderation purposes. (extracted from325.pdf)
Article 29 Working Party (2017a), Opinion on some key issues of the Law Enforcement Directive (EU 2016/680), WP 258, Brussels, European Commission Directorate-General Justice and Consumers. (extracted from325.pdf)
Article 29 Working Party (2017b), Guidelines on automated individual decision- making and profiling for the purposes of Regulation 2016/679, WP251rev.01, Brussels, European Commission Directorate-General Justice, p. (extracted from325.pdf)
CJEU (Court of Justice of the European Union) (1991), C-184/89, Helga Nimz v. (extracted from325.pdf)
(2021), Automating Injustice – The use of artificial intelligence and automated decision-making systems in criminal justice in Europe, London, Fair Trials. (extracted from325.pdf)
FRA (2016), Ensuring justice for hate crime victims: Professional perspectives, Luxembourg, Publications Office. (extracted from325.pdf)
(2019), ‘Dirty data, bad predictions: How civil rights violations impact police data, predictive policing systems, and justice’, NYU Law Review, Vol. (extracted from325.pdf)
The Law Society (2019), Algorithms in the criminal justice system, London, The Law Society, p. (extracted from325.pdf)
(2017) “Social justice, epidemiology and health inequalities”, European Journal of Epidemiology, available at https://doi.org/10.1007/ s10654-017-0286-3 134. (extracted from330.pdf)
49 Dr Luke Oakden-Rayner, Radiologist and PhD candidate with the School of Public Health at the University of Adelaide Dr Claudia Pagliari, Senior Lecturer in Primary Care and Informatics and Director of Global eHealth at the University of Edinburgh Imogen Parker, Head of Justice, Citizens and Digital Society Programmes at The Nuffield Foundation Dr Ali Parsa, Founder and CEO of Babylon Health Bakul Patel, Associate Center Director for Digital Health at the Food and Drug Administration (FDA) Nicola Perrin, Head of Understanding Patient Data Carol Platt, Innovation Associate at Alder Hey Children’s Hospital Professor Nasir Rajpoot, Professor in Computational Pathology at the Department of Computer Science, University of Warwick Professor Daniel Ray, Director of Data at NHS Digital Professor Geraint Rees, Dean of the UCL Faculty of Life Sciences and Professor of Cognitive Neurology at University College London Dr Travis Rieder, Assistant Director for Education Initiatives, Director of the Master of Bioethics degree program and Research Scholar at the Berman Institute of Bioethics Professor Renato Rocha Souza, Professor at the Applied Mathematics School, Fundação Getulio Vargas Professor Ferdinando Rodriguez y Baena, Professor of Medical Robotics in the Department of Mechanical Engineering at Imperial College London Dr Caroline Rubin, Vice-President for Clinical Radiology at the Royal College of Radiologists and Consultant Radiologist at the University Hospital Southampton NHS Foundation Trust Dr Benedict Rumbold, Research Fellow in the Department of Philosophy at University College London Professor Burkhard Schafer, Professor of Computational Legal Theory at the University of Edinburgh’s School of Law Professor Stefan Schulz, Professor of Medical Informatics at Medical University Graz, Austria Allan Tucker, Senior Lecturer of Computer Science at Brunel University Professor Rhema Vaithianathan, Co-Director of the Centre for Social Data Analytics at the University of Auckland Jenny Westaway, Head of the Office of the National Data Guardian Hugh Whittall, Director of the Nuffield Council on Bioethics John Wilkinson, Director of Devices at the Medicines and Healthcare products Regulatory Agency (MHRA) Professor Stephen Wilkinson, Professor of Bioethics 50 ETHICAL, SOCIAL, AND POLITICAL CHALLENGES OF ARTIFICIAL INTELLIGENCE IN HEALTH D: Patients and members of the public who contributed to this report Alex Brownrigg Mariana Campos Ann Cawley Annabel Dawson Ruth Day Eric Deeson Fran Husson Elaine Manna John Marsh Richard Melville Ballerand Dave McCormick Kath Pollock Bob Ruane Edward Sherley-Price Chris Warner Marney Williams ETHICAL, SOCIAL, AND POLITICAL CHALLENGES OF ARTIFICIAL INTELLIGENCE IN HEALTH 51 E: List of attendees at expert roundtable Professor Richard Ashcroft, Professor of Bioethics at Queen Mary University of London Shirley Cramer CBE, Chief Executive of the Royal Society for Public Health Professor Bobbie Farsides, Professor of Professor of Clinical and Biomedical Ethics at the University of Sussex Professor John Fox, Professor at the Department of Engineering Science at the University of Oxford Professor Nina Hallowell, Associate Professor at the Nuffield Department of Public Health, University of Oxford Dr Hugh Harvey, Clinical Lead for Kheiron Medical and Royal College of Radiologists Informatics Committee Member Eleonora Harwich, Head of Digital and Technological Innovation at Reform Dr Geraint Lewis, Chief Data Officer at NHS England and an Honorary Clinical Senior Lecturer at University College London Maxine Mackintosh, PhD candidate at University College London’s Farr Institute of Health Informatics and co-founder of One HealthTech Dr Benedict Rumbold, Research Fellow in the Department of Philosophy at University College London Professor Ilina Singh, Professor of Neuroscience & Society at the Department of Psychiatry at the University of Oxford and Co-Director of the Wellcome Trust Centre for Ethics Dr Nicola Strickland, President of the Royal College of Radiologists and Consultant Radiologist at the Imperial College Healthcare NHS Trust Professor Stephen Wilkinson, Professor of Bioethics 52 ETHICAL, SOCIAL, AND POLITICAL CHALLENGES OF ARTIFICIAL INTELLIGENCE IN HEALTH F: Methodology by which patient/public contributors were recruited Patients and members of the public that were interviewed or that participated in our roundtable on the 22nd February 2018 were recruited via one of two methods. (extracted from330.pdf)
equality, justice and equity, non­discrimination, 2 Callegaro, M. (extracted from324.pdf)
is racial bias in the risk scores used in the US crimi­ The situation is further complicated by the absence nal justice system. (extracted from324.pdf)
Taken as a whole, the principles by improving living conditions and health, facilitating articulated lay the foundation for cultivating social justice, creating wealth, bolstering public safety, trust toward artificially intelligent systems. (extracted from683.pdf)
AIS must avoid using acquired data to lock individuals into a user profile, fix their personal identity, or confine them to a filtering bubble, which would restrict and confine their possibilities for personal development — especially in fields such as education, justice, or business. (extracted from683.pdf)
From Principles to Practice An interdisciplinary framework to operationalise AI ethics AI Ethics Impact Group led by From Principles to Practice An interdisciplinary framework to operationalise AI ethics 4 CONTENTS EXECUTIVE SUMMARY 6 1 INTRODUCTION 8 1.1 Challenges of practically implementing AI ethics 10 1.2 Multimethod framework as solution 12 1.3 Handling AI ethics in practice 14 2 VALUES, CRITERIA, INDICATORS, OBSERVABLES (VCIO) AND THE AI ETHICS LABEL IN DETAIL 15 2.1 How to apply VCIO to AI ethics: Three illustrated examples 17 2.1.1 A pplying the VCIO approach to transparency as a value 20 2.1.2 A pplying the VCIO approach to justice as a value 22 2.1.3 A pplying the VCIO approach to accountability as a value 24 2.2 Values constituting the AI ethics rating 26 2.2.1 Transparency 26 2.2.2 Accountability 27 2.2.3 Privacy 28 2.2.4 Justice 28 2.2.5 Reliability 29 2.2.6 Environmental sustainability 30 2.3 How VCIO underpins the ratings in the AI Ethics Label 31 3 CLASSIFYING AN AI’S APPLICATION CONTEXT 35 3.1 The risk matrix 35 3.2 Dimensions of the risk matrix 37 3.2.1 Intensity of potential harm (x-axis) 37 3.2.2 Dependence on the decision (y-axis) 38 3.3 Recommendation for classes 38 4 CONCLUSION AND WHERE TO GO FROM HERE 41 4.1 Putting it all together 41 4.2 Next steps 42 5 BIBLIOGRAPHY 45 6 ABOUT THE AUTHORS 48 Imprint 54 5 EXECUTIVE SUMMARY Artificial intelligence (AI) increasingly pervades all areas of life. (extracted from33.pdf)
In chapter two, we present the VCIO model (values, criteria, indicators, and observables) for the operationalisation and measurement of otherwise abstract principles and demonstrate the functioning of the model for the values of transparency, justice and accountability. (extracted from33.pdf)
6 EXECUTIVE SUMMARY For the proposed AI Ethics Label, we carefully suggest six values, namely justice, environmental sustainability, accountability, transparency, privacy, and reliability, based on contemporary discourse and operability. (extracted from33.pdf)
8 INTRODUCTION • W e offer practical examples for applying the VCIO model to selected values such as transparency, accountability and justice. (extracted from33.pdf)
So how we implement and prioritise values such as justice (here includes fairness or non-discrimination) and transparency in practice, depends to some extent on the field of application and the cultural context an AI system operates in. (extracted from33.pdf)
A system used in the justice sector must necessarily exhibit higher levels of privacy and fairness than a system used in the organisation of industrial production. (extracted from33.pdf)
The implementation of values such as justice and transparency requires multiple measures throughout the complex development and implementation process of AI systems. (extracted from33.pdf)
with regards to justice, accountability, transparency) independent of the system’s application context (see VCIO approach, chapter 2). (extracted from33.pdf)
12 INTRODUCTION Taking the energy efficiency label as a guide, a label showing a rating of an AI system’s ethical characteristics could then look as follows: FIGURE 1 The AI Ethics Label with six selected values Transparency Accountability Privacy Justice Reliability Environmental Sustainability 13 INTRODUCTION 1.3 Handling AI ethics in practice Orientation for Taken together, our approach for the operationalisation of general principles (VCIO), the stakeholders to bring context-independent rating of ethical characteristics, the proposal of the introduction of an AI ethics into practice AI ethics label and the classification of different application contexts through a risk matrix provides a framework for bringing AI ethics from principles to practice. (extracted from33.pdf)
For application fields that are classified in one of the higher risk levels, they may demand that an AI system (1) must carry an ethics label that shows the rating for values such as transparency, robustness, or justice and (2) satisfy minimum levels within the rating. (extracted from33.pdf)
For example, the demand that algorithms should not discriminate finds consensus; the debate, however, begins with the question of what is understood by discrimination (justice), how to check whether it exists, and how to deal with conflicts between different values. (extracted from33.pdf)
They VCIO: values, criteria, are defined at the highest level (as justice or transparency, for example). (extracted from33.pdf)
2.1 How to apply VCIO to AI ethics: Three illustrated examples In the following, we illustrate how to apply the VCIO model by focusing on three values, Operationalising values namely transparency, justice, and accountability.7 The findings from the Algo.Rules project, with indicators, observables an initiative by the Bertelsmann Stiftung and the iRights.Lab, have been essential for the and potential value conflicts development of the framework and this chapter in particular. (extracted from33.pdf)
(1) T he regulator has to hierarchise the values concerning the context of an AI application In practice, where values such as privacy, (also see risk matrix), reliability or justice come into conflict with e.g. (extracted from33.pdf)
2.1.1 Applying the VCIO approach to transparency as a value (page 20/21) Justice The criteria subsumed under the value of justice in this example pertain to classic aspects Justice with aspects of of algorithmic fairness such as bias prevention and assessment but emphasise a process algorithmic fairness and perspective to include a broader set of ethical considerations. (extracted from33.pdf)
These aspects are, for inclusion example, inclusion, represented by criteria such as participatory procedures, or social justice considerations, and a criterion for the assessment of trade-offs generated by the employment of the AI system in question. (extracted from33.pdf)
In this sense, justice refers to a broader set of ethical considerations than the often-used term fairness, which mostly focuses on algorithmic outcomes themselves. (extracted from33.pdf)
2.1.2 Applying the VCIO approach to justice as a value (page 22/23) Accountability The value of accountability refers to problems that arise in connection with the complex Accountability refers to allocation or clarification of responsibility relationships in the use of AI. (extracted from33.pdf)
21 VALUES, CRITERIA, INDICATORS, OBSERVABLES (VCIO) AND THE AI ETHICS LABEL IN DETAIL 22 eulaV JUSTICE JUSTICE airetirC eulaV Identifying Social justice and assessing Assessment of different sources of potential biases to ensure fairness1 Detection and prevention of biases to ensure fairness Participatory procedures considerations trade-offs srotacidnI airetirC Have trade- Has the Has the Have the Were Has due care Have the Is a special Have the Is there an Are Are Is there Are potential Who has Can anyone Is there a Are the offs been training data input design requirements, possible self- been taken applied checking working external mechanisms in simulations transparent biases access to the initiate an participation stakeholders identified and been analysed (sensors, user goals and task reinforcing with regard to methods (e.g. (extracted from33.pdf)
selbavresbO srotacidnI Yes, with Yes, Yes, review on a Yes, and Yes, periodically Yes, and the Yes Yes, continual Yes, employing Yes, by an Yes, public Yes, simulations Yes, review Yes, publicly There is Yes Yes, on a Yes, there is a the help of a demographic regular basis continual output data checks external independent access designed for mechanisms unrestricted regular basis stakeholder regular external parity, equality reviews are design is evaluation institution the specific use and error access documentation technology of odds and conducted periodically mechanisms case sources are available impact opportunities reviewed made public for trusted assessment are ensured intermediaries selbavresbO 2.1.2 Applying the VCIO approach to justice as a value Yes, by a trade association or Yes, with Yes, after Yes, checks another related Yes, but made the help of changes of the are made after institution available only an external application or changes to the in summarized technology Only limited Yes, but only its environment Yes, but only Yes, but only application Yes, but only Yes, but access Yes, but form Yes, internally Access is Yes, but proof Yes, but only Yes, but no impact assessment once once during the internal will only only general restricted of being once (e.g. (extracted from33.pdf)
VALUES, CRITERIA, INDICATORS, OBSERVABLES (VCIO) AND THE AI ETHICS LABEL IN DETAIL 23 eulaV JUSTICE JUSTICE airetirC eulaV Identifying Social justice and assessing Assessment of different sources of potential biases to ensure fairness1 Detection and prevention of biases to ensure fairness Participatory procedures considerations trade-offs srotacidnI airetirC Have trade- Has the Has the Have the Were Has due care Have the Is a special Have the Is there an Are Are Is there Are potential Who has Can anyone Is there a Are the offs been training data input design requirements, possible self- been taken applied checking working external mechanisms in simulations transparent biases access to the initiate an participation stakeholders identified and been analysed (sensors, user goals and task reinforcing with regard to methods (e.g. (extracted from33.pdf)
Building on a meta-analysis of relevant publications, we settled on six values: transparency, accountability, privacy, justice, reliability, and environmental sustainability. (extracted from33.pdf)
2.2.4 Justice Justice as algorithmic non- Questions of justice include problems of equal treatment and the fair distribution of certain discrimination and question goods. (extracted from33.pdf)
This includes aspects of social justice, in particular, “hidden” work, which is essential for the operation of AI systems. (extracted from33.pdf)
The aspects of social justice that the VCIO model adds to this debate focus on the said Click work as an aspect “hidden” work that goes into the operation of AI systems. (extracted from33.pdf)
These services include of social justice precarious, potentially health-damaging labour, particularly so-called click work required for machine learning (Irani 2015). (extracted from33.pdf)
2.2.6 Environmental sustainability Resource-saving Environmental sustainability is a form of intergenerational justice and describes the infrastructures to ensure obligation towards future generations to ensure and preserve their living conditions. (extracted from33.pdf)
This intergenerational justice obligation is typically geared towards a careful use of natural resources, e.g., to combat pollution and to preserve biodiversity as well as mitigate the worst effects of climate change. (extracted from33.pdf)
Justice Reliability Environmental Sustainability Rating with 5–7 levels, indicated with letters A–G 32 VALUES, CRITERIA, INDICATORS, OBSERVABLES (VCIO) AND THE AI ETHICS LABEL IN DETAIL However, one drawback of a system using minimum requirements may be that it gives Ensuring incentives for few incentives to strive for individual indicator ratings that go beyond the minimum higher ratings despite requirements. (extracted from33.pdf)
33 VALUES, CRITERIA, INDICATORS, OBSERVABLES (VCIO) AND THE AI ETHICS LABEL IN DETAIL FIGURE 5 Illustration of the composition of the whole system rating using minimum requirements Observables Indicator 1.1.1 Criterion 1.1 A Observables Indicator 1.1.2 B A D B Transparency C G Observables Indicator 1.2.1 Criterion 1.2 Accountability A Observables Indicator 1.2.2 C A D C D Privacy Observables Indicator 2.1.1 Criterion 2.1 A Observables Indicator 2.1.2 G A Justice B C Observables Indicator 2.2.1 Criterion 2.2 Reliability A Observables Indicator 2.2.2 C A D C F Environmental Sustainability Observables Indicator 3.1.1 Criterium 3.1 A Observables Indicator 3.1.2 C A D 34 3 CLASSIFYING AN AI’S APPLICATION CONTEXT The AI Ethics Label provides at a glance information about the ethically relevant characteristics of an AI system. (extracted from33.pdf)
To assess this, the following issues must be regarded: can have, look at the impact on a number of people or • I mpact on fundamental rights, equality or social justice: Does an AI have a negative access to resources and impact on a natural, legal persons’ fundamental rights or are social justice mechanisms whether society as a whole (e.g. (extracted from33.pdf)
Andatthetimeoftheacquisition,Googleemphasizedthatitsprivacypoliciesprohibitedthecompany fromcombiningitsowndatastreamswiththoseobtainedfromotherwebsites.Butthecompany quietlywalkedbackthisinternalpolicyin2016-notably,apointatwhichthecompanyhadamassed greatermarketpowerandthuswaslessexposedtotheriskuserswouldflocktoacompetitor platform.71Followingthechange,Googlecouldcombineallitsuserdataintoasingleuseridentification thatitcouldintegrateintoitsbuyingtoolstoenableuniquelyprecisetargetingofparticularusers—an extremelyvaluableadvantageforGoogleAds’advertisingclients.72Thisalsomadeitharderfor publisherstotrackusersthemselvesbymaskingtheseuseridentifiers.73Thesenegativee ectsled publisherstocomplainthattheacquisitionwasanticompetitive;theFTC,however,optednottoblock themergerfromgoingforward.74 67SeeFederalTradeCommission,“RemarksofChairLinaM.KhanasPreparedforDelivery,”IAPPGlobalPrivacySummit2022,Washington,D.C.,April 11,2022, https://www.ftc.gov/system/files/ftc_gov/pdf/Remarks%20of%20Chair%20Lina%20M.%20Khan%20at%20IAPP%20Global%20Privacy%20Summit%2 02022.pdf;;JacquesCrémer,Yves-AlexandredeMontjoyeandHeikeSchweitzer,“Competitionpolicyforthedigitalera”,EuropeanCommission, 2019,https://ec.europa.eu/competition/publications/reports/kd0419345enn.pdf;AutoritédelaconcurrenceandBundeskartellamt,“Competition LawandData”,Bundeskartellamt,May10,2016, https://www.bundeskartellamt.de/SharedDocs/Publikation/DE/Berichte/Big%20Data%20Papier.pdf?__blob=publicationFile&v=2;TheUnited StatesDepartmentofJustice,“AssistantAttorneyGeneralJonathanKanteroftheAntitrustDivisionDeliversRemarksattheKeystoneConference onAntitrust,Regulation&thePoliticalEconomy”,TheUnitedStatesDepartmentofJustice,March2,2023, https://www.justice.gov/opa/speech/assistant-attorney-general-jonathan-kanter-antitrust-division-delivers-remarks-keystone;FederalTrade Commission,“FTCHearing#6:Privacy,BigData,andCompetition”,FTC,November6-8,2018, https://www.ftc.gov/news-events/events/2018/11/ftc-hearing-6-privacy-big-data-competition 68KatharineKemp,“Concealeddatapracticesandcompetitionlaw:whyprivacymatters”,EuropeanCompetitionJournal16,no.2-3(2020):628-672, https://doi.org/10.1080/17441056.2020.1839228 69SeeLouiseStoryandMiguelHelft,“GoogleBuysDoubleClickfor$3.1Billion,”NewYorkTimes,April14,2007, https://www.nytimes.com/2007/04/14/technology/14DoubleClick.html;andSteveLohr,“ThisDealHelpedTurnGoogleintoanAdPowerhouse.Is ThataProblem?”NewYorkTimes,September21,2020,https://www.nytimes.com/2020/09/21/technology/google-doubleclick-antitrust-ads.html. (extracted from27.pdf)
84FederalTradeCommission,“StatementofCommissionerAlvaroM.BedoyaJoinedbyCommissionerRebeccaKellySlaughterRegarding Amazon.com,Inc.’sAcquisitionof1LifeHealthcare,Inc.”,FTC,February27,2023, https://www.ftc.gov/system/files/ftc_gov/pdf/2210191amazononemedicalambstmt.pdf 85AndrewI.Gavil,“CompetitiveEdge:ThesilverliningforantitrustenforcementintheSupremeCourt’sembraceof“textualism””EquitableGrowth, July28,2021, https://equitablegrowth.org/competitive-edge-the-silver-lining-for-antitrust-enforcement-in-the-supreme-courts-embrace-of-textualism/;Baker, Jonathan;Farrell,Joseph;Gavil,Andrew;Gaynor,Martin;Kades,Michael;Katz,Michael;Kimmelman,Gene;Melamed,A.;Rose,Nancy;Salop,Steven; ScottMorton,Fiona;andShapiro,Carl,"JointResponsetotheHouseJudiciaryCommitteeontheStateofAntitrustLawandImplicationsfor ProtectingCompetitioninDigitalMarkets"CongressionalandOtherTestimony,18,2020, https://digitalcommons.wcl.american.edu/pub_disc_cong/18/ 86FederalTradeCommission,“FederalTradeCommissionandJusticeDepartmentSeektoStrengthenEnforcementAgainstIllegalMergers,”January 18,2022, https://www.ftc.gov/news-events/news/press-releases/2022/01/federal-trade-commission-justice-department-seek-strengthen-enforcement-ag ainst-illegal-mergers. (extracted from27.pdf)
110ExamplesincludetheEuropeanDataProtectionSupervisor’s2014workshoponPrivacy,Consumers,CompetitionandBigData, https://edps.europa.eu/data-protection/our-work/publications/reports/report-edps-workshop-privacy-consumers-competition-and_en,theFTC’s updatedstatementonUnfairMethodsofCompetition,FTC,“PolicyStatementRegardingtheScopeofUnfairMethodsofCompetitionUnderSection 5oftheFederalTradeCommissionAct”,November10,2022, https://www.ftc.gov/legal-library/browse/policy-statement-regarding-scope-unfair-methods-competition-under-section-5-federal-trade-commis sion;itsannouncementofrevisionstotheMergerGuidelines,FTC,“FederalTradeCommissionandJusticeDepartmentSeektoStrengthen EnforcementAgainstIllegalMergers,”pressrelease,January18,2022, https://www.ftc.gov/news-events/news/press-releases/2022/01/federal-trade-commission-justice-department-seek-strengthen-enforcement-ag ainst-illegal-mergers,CompetitionandMarketsAuthority,“CMA-ICOJointStatementonCompetitionandDataProtectionLaw,”May19,2021, https://www.gov.uk/government/publications/cma-ico-joint-statement-on-competition-and-data-protection-law;andAustralianCompetition& ConsumerCommission,“DigitalPlatformServicesInquiry2020–25:September2022InterimReport,”November11,2022, https://www.accc.gov.au/focus-areas/inquiries-ongoing/digital-platform-services-inquiry-2020-25/september-2022-interim-report. (extracted from27.pdf)
290FederalTradeCommission,“FederalTradeCommissionandJusticeDepartmentSeektoStrengthenEnforcementAgainstIllegalMergers,”press release,January18,2022, https://www.ftc.gov/news-events/news/press-releases/2022/01/federal-trade-commission-justice-department-seek-strengthen-enforcement-ag ainst-illegal-mergers. (extracted from27.pdf)
Roane,AmrahSalomón,BrunoSeraphin,andElenaSobrino,“WaterJusticeandTechnology:TheCOVID-19Crisis,ComputationalResourceControl, andWaterReliefPolicy,”AINowInstitute,January10,2022,https://ainowinstitute.org/water-justice-technology.html. (extracted from27.pdf)
For example, the use of artificial intelligence tools by law enforcement and the criminal justice system could have an impact on an individual’s right to be free from arbitrary arrest or to equality before the law; surveillance technologies could impact on the right to peaceful assembly; the use of social media platforms could impact the right to mental health; and property rental platforms could alter housing markets, possibly impacting the right to an adequate standard of living. (extracted from654.pdf)
Accept additional cookies Reject additional cookies View cookies Hide cookie message Skip to main content GOV.UK Navigation menu Menu Menu Services and information Benefits Births, death, marriages and care Business and self-employed Childcare and parenting Citizenship and living in the UK Crime, justice and the law Disabled people Driving and transport Education and learning Employing people Environment and countryside Housing and local services Money and tax Passports, travel and living abroad Visas and immigration Working, jobs and pensions Government activity Departments Departments, agencies and public bodies News News stories, speeches, letters and notices Guidance and regulation Detailed guidance, regulations and rules Research and statistics Reports, analysis and official statistics Policy papers and consultations Consultations and strategy Transparency Data, Freedom of Information releases and corporate reports Search GOV.UK × Search GOV.UK Search Search GOV.UK Search Home Business and industry Science and innovation Artificial intelligence National AI Strategy Department for Science, Innovation & Technology Office for Artificial Intelligence Department for Digital, Culture, Media & Sport Department for Business, Energy & Industrial Strategy Guidance National AI Strategy - HTML version Updated 18 December 2022 Contents Our ten-year plan to make Britain a global AI superpower Executive summary Summary of key actions Introduction Pillar 1: Investing in the long-term needs of the AI ecosystem Pillar 2: Ensuring AI benefits all sectors and regions Pillar 3: Governing AI effectively Next steps Print this page © Crown copyright 2022 This publication is licensed under the terms of the Open Government Licence v3.0 except where otherwise stated. (extracted from109.html)
Cancel Services and information Benefits Births, death, marriages and care Business and self-employed Childcare and parenting Citizenship and living in the UK Crime, justice and the law Disabled people Driving and transport Education and learning Employing people Environment and countryside Housing and local services Money and tax Passports, travel and living abroad Visas and immigration Working, jobs and pensions Government activity Departments News Guidance and regulation Research and statistics Policy papers and consultations Transparency How government works Get involved Support links Help Privacy Cookies Accessibility statement Contact Terms and conditions Rhestr o Wasanaethau Cymraeg Government Digital Service All content is available under the Open Government Licence v3.0 , except where otherwise stated © Crown copyright (extracted from109.html)
whether the use case can be addressed at all in a data- – Domain: some domains – such as health, justice and driven manner. (extracted from720.pdf)
justice) in which the in your RFP and require tenderers to describe their AI solution will be applied, machine learning/data strategies on how to address these shortcomings. (extracted from720.pdf)
To further ensure trustworthy AI, most governments are adopting new legislative frameworks for AI technologies, which are often sector- specific and cover areas such as autonomous driving, healthcare and e-justice. (extracted from244.pdf)
The Federal Ministry also highlights the use of AI for evaluating judicial proceedings, as mentioned in its National e-justice strategy. (extracted from244.pdf)
ensuring that citizens can make informed and independent decisions) and to human dignity, equality and justice (i.e. (extracted from244.pdf)
In addition, public sector operators should be secured sufficient resources and incentives to engage in such development, paying particular attention to sort out the rights of the outcomes of co-development;  The Ministry of Justice and the Ministry of Finance are currently examining national regulation of automated decision-making. (extracted from244.pdf)
5.13.4 Regulation Hungary’s national AI strategy aims to ensure a responsible, reliable and human-centred utilisation of AI technologies by means of the following policies:  Creating an ethical framework: developing an AI code of conduct by the first half of 2021 in collaboration between the Ministry of Justice, the Ministry for Innovation and Technology, AI Innovation Hub and the Central Statistical Office;  Setting up an Artificial Intelligence Regulation and Ethics Knowledge Centre: the aim is to create and coordinate an extensive pool of experts to help resolve legal issues and matters of ethics relating to the regulation of AI and the implementation of the strategy;  Establishing a regulatory framework for AI: the objective is to amend the current regulatory system to suit AI and to align it to EU regulations;  Building data management regulation: the objective is to set up regulations for the use and exchange of public and private data and to define rules regarding data monetisation. (extracted from244.pdf)
The Latvian strategy identifies priority sectors with a high potential for AI applications in the country, such as transport (Intelligent transport systems), culture, justice (AI as support for decision making and drafting legislation), agriculture (automated control), and translation. (extracted from244.pdf)
Regarding ethics, the government adopts the European ethical guidelines outlined by the European Commission for the Efficiency of Justice. (extracted from244.pdf)
66 1) International bodies and AI, 2) Swiss intelligence of interests in the European AI (Digital Europe Programme) activities, 3) Changes in the world of work, 4) AI in industry and services, 5) AI in education, 6) Application of AI in science and research, 7) AI in cybersecurity and security policy, 8) AI, Media & Public, 9) Automated mobility and AI, 10) AI in healthcare, 11) AI in the financial sector, 12) AI in agriculture, 13) Energy, climate, environment and AI, 14) AI in administration, 15) Further development of the general legal framework on AI, 16) AI in justice, 17) AI, data and intellectual property law. (extracted from244.pdf)
While the establishment of the legal basis is ensured by a wide range of institutions, the FDFA will specifically focus on the following policies to further develop the general legal framework on AI:  Examining the emergence of AI-specific international law and its impact on Switzerland;  Following-up developments with regard to the visibility of AI systems in interaction with consumers;  Monitoring developments in AI-based decision-making in the justice system (predictive justice). (extracted from244.pdf)
14 February, 2024 Signatories Closed AI Licensing for a Better Future: On Addressing Both Present Harms and Emerging Threats This joint open letter by Encode Justice and the Future of Life Institute calls for the implementation of three concrete US policies in order to address current and future harms of AI. (extracted from332.html)
Mobility, DG GROW Roberto Viola, Director-General, Communications Networks, Content and Technology Paul Nemitz, Principal Advisor in the Directorate General for Justice and Consumers GETTING IN TOUCH WITH THE EU In person All over the European Union there are hundreds of Europe Direct information centres. (extracted from250.pdf)
Proposal for a regulation of the European Parliament and of the Council laying down harmonised rules on artificial intelligence (artificial intelligence act) and amending certain Union legislative acts Committees Internal Market and Consumer Protection (IMCO) and COM(2021)206 responsible: Civil Liberties, Justice and Home Affairs (LIBE) 21.4.2021 (jointly under Rule 58) 2021/0106(COD) Rapporteurs: Brando Benifei (S&D, Italy) and Dragoş Tudorache (Renew, Romania) Ordinary legislative Shadow rapporteurs: Deirdre Clune, Axel Voss (EPP); Petar Vitanov (S&D); procedure (COD) Svenja Hahn, (Renew); Sergey Lagodinsky, (Parliament and Kim Van Sparrentak (Greens/EFA); Rob Rooken, Council on equal Kosma Złotowski (ECR); Jean-Lin Lacapelle, Jaak footing – formerly Madison (ID); Cornelia Ernst, Kateřina Konecna (The 'co-decision') Left) Procedure Regulation (EU) 2024/1689 completed. (extracted from278.pdf)
In Parliament, the file was assigned jointly (under Rule 58) to the Committee on Internal Market and Consumer Protection (IMCO) and the Committee on Civil Liberties, Justice and Home Affairs (LIBE), with Brando Benifei (S&D, Italy) and Dragoş Tudorache, Renew, Romania) appointed as rapporteurs. (extracted from278.pdf)
creditworthiness evaluation), law enforcement, border control, administration of justice and democratic processes, biometric identification, categorisation and emotion recognition systems (outside the prohibited categories). (extracted from278.pdf)
HOUSE OF LORDS Justice and Home Affairs Committee 1st Report of Session 2021–22 Technology rules? (extracted from223.pdf)
The advent of new technologies in the justice system Ordered to be printed 21 March 2022 and published 30 March 2022 Published by the Authority of the House of Lords HL Paper 180 Justice and Home Affairs Committee The Justice and Home Affairs Committee was appointed by the House of Lords on 14 April 2021 to consider justice and home affairs, including the domestic criminal justice system, and international cooperation in respect of criminal justice, civil justice, migration and asylum. (extracted from223.pdf)
Membership The Members of the Justice and Home Affairs Committee are: Lord Blunkett Baroness Kennedy of The Shaws Baroness Chakrabarti Baroness Pidding Lord Dholakia Baroness Primarolo Baroness Hallett Lord Ricketts Baroness Hamwee (Chair) Baroness Sanderson of Welton Lord Hunt of Wirral Baroness Shackleton of Belgravia Declaration of interests See Appendix 1. (extracted from223.pdf)
A full list of Members’ interests can be found in the Register of Lords’ Interests: http://www.parliament.uk/hlregister Publications All publications of the Committee are available at: http://www.parliament.uk/ 519/justice-and-home-affairs-committee/ Parliament Live Live coverage of debates and public sessions of the Committee’s meetings are available at: http://www.parliamentlive.tv Further information Further information about the House of Lords and its Committees, including guidance to witnesses, details of current inquiries and forthcoming meetings is available at: http://www.parliament.uk/business/lords Committee staff The staff who worked on this inquiry were Sam Kenny (Clerk), Achille Versaevel (Policy Analyst) and Amanda McGrath (Committee Operations Officer). (extracted from223.pdf)
Contact details General correspondence should be addressed to the Clerk of the Justice and Home Affairs Committee, Committee Office, House of Lords, London SW1A 0PW. (extracted from223.pdf)
THE ADVEnT OF nEW TECHnOLOGIES In THE JUSTICE SYSTEM 3 SUMMARY In recent years, and without many of us realising it, Artificial Intelligence has begun to permeate every aspect of our personal and professional lives. (extracted from223.pdf)
Our Committee has limited its investigation to only one area–how these advanced technologies are used in our justice system. (extracted from223.pdf)
Algorithms are being used to improve crime detection, aid the security categorisation of prisoners, streamline entry clearance processes at our borders and generate new insights that feed into the entire criminal justice pipeline. (extracted from223.pdf)
When deployed within the justice system, AI technologies have serious implications for a person’s human rights and civil liberties. (extracted from223.pdf)
Without transparency, there can not only be no scrutiny, but no 4 THE ADVEnT OF nEW TECHnOLOGIES In THE JUSTICE SYSTEM accountability for when things go wrong. (extracted from223.pdf)
Proper trials methodology is fully embedded into medical science but there are no minimum scientific or ethical standards that an AI tool must meet before it can be used in the criminal justice sphere. (extracted from223.pdf)
THE ADVEnT OF nEW TECHnOLOGIES In THE JUSTICE SYSTEM 5 Yet without sufficient safeguards, supervision, and caution, advanced technologies may have a chilling effect on a range of human rights, undermine the fairness of trials, weaken the rule of law, further exacerbate existing inequalities, and fail to produce the promised effectiveness and efficiency gains. (extracted from223.pdf)
The advent of new technologies in the justice system CHAPTER 1: INTRODUCTION 1. (extracted from223.pdf)
Within the application of the law, we included a broad view of the justice system, examining instances where advanced tools were used to discover, deter, rehabilitate, or punish people who breach the law in England and Wales, as well as border management. (extracted from223.pdf)
8 THE ADVEnT OF nEW TECHnOLOGIES In THE JUSTICE SYSTEM Automated decision making (ADM): ADM is the process of making a decision by automated means without any human involvement. (extracted from223.pdf)
Written evidence from Dr Miri Zilka, Dr Adrian Weller and Detective Sergeant Laurence Cartwright laid out some categories of tools used in the justice system. (extracted from223.pdf)
THE ADVEnT OF nEW TECHnOLOGIES In THE JUSTICE SYSTEM 9 (b) Data analysis: software and tools primarily used to analyse data to create insights. (extracted from223.pdf)
We heard the most about tools used by the Home Office, the Ministry of Justice, HM Prisons and Probation Service, and individual police forces. (extracted from223.pdf)
We were told, for example, about the use of polygraphs to monitor sex offenders on parole and manage their level of compliance with parole conditions.14 8 Written evidence from Dr Miri Zilka, Detective Sergeant Laurence Cartwright and Dr Adrian Weller (nTL0040) 9 Written evidence from Association of Police and Crime Commissioners, national Police Chiefs’ Council and Police Digital Service (nTL0049) 10 Written evidence from Avon and Somerset Police (nTL0052) 11 Written evidence from the Serious Fraud Office (nTL0034) 12 Written evidence from Public Law Project (nTL0046) 13 Written evidence from Liberty (nTL0020) 14 Written evidence from Dr Kyriakos n Kotsoglou (nTL0007) 10 THE ADVEnT OF nEW TECHnOLOGIES In THE JUSTICE SYSTEM 6. (extracted from223.pdf)
THE ADVEnT OF nEW TECHnOLOGIES In THE JUSTICE SYSTEM 11 the end of which it would become costly for the customer to opt out. (extracted from223.pdf)
Avon and Somerset Constabulary thought their use of data analytics placed “better insights into the hands of those delivering the business to help empower and support more effective decision making.”23 The Rt Hon Kit Malthouse MP, the Minister for Crime and Policing at the Home Office and Ministry of Justice, told us that he was “very excited about the use of artificial intelligence and machine learning in policing.”24 We also acknowledge that, as many submissions pointed out, advanced tools can provide substantial assistance towards enacting the crucial duties of the police to protect and prevent harm. (extracted from223.pdf)
Matthew Gill, Senior Fellow at the Institute for Government, facilitated a seminar for us to consider the institutional and regulatory frameworks which 22 Q 45 (Professor Elizabeth Joh) 23 Written evidence from Avon and Somerset Police (nTL0052) 24 Q 99 (Kit Malthouse MP) 25 Q 39 (Professor Elizabeth Joh) 26 Written evidence from SAS UK&I (nTL0041) 27 Written evidence from the Information Commissioner’s Office (nTL0016) 12 THE ADVEnT OF nEW TECHnOLOGIES In THE JUSTICE SYSTEM may be put in place. (extracted from223.pdf)
• In 2020, the Scottish Parliament Justice Sub-Committee on Policing published Facial recognition: how policing in Scotland makes use of this technology.30 • In 2021, the European Parliament Committee on Civil Liberties, Justice and Home Affairs published a report on “artificial intelligence in criminal law and its use by the police and judicial authorities in criminal matters”.31 • In 2021, nATO adopted its first Artificial Intelligence Strategy, including principles of the responsible use of AI in Defence and announcing further work to set international AI standards.32 • In 2021, UnESCO adopted a Recommendation on the Ethics of Artificial Intelligence and is working towards establishing the first-ever global normative instrument on the ethics of AI.33 15. (extracted from223.pdf)
We decided to examine the use of these tools throughout the “criminal justice pipeline”34 and in border management, identifying where change was needed, and identifying some principles for the safe and ethical use of such tools. (extracted from223.pdf)
(Report of Session 2017–19, HL Paper 100) 29 Council of Europe, CAHAI Ad Hoc Committee on Artificial Intelligence, ‘Terms of Reference’: https://www.coe.int/en/web/artificial-intelligence/cahai [accessed 6 February 2022] 30 The Scottish Parliament, Justice Sub-Committee on Policing, Facial Recognition: How Policing in Scotland Makes Use of This Technology (1st Report, Session 5, SP Paper 678) 31 Committee on Civil Liberties, Justice and Home Affairs, Report on artificial intelligence in criminal law and its use by the police and judicial authorities in criminal matters (13 July 2021): https://www.europarl. (extracted from223.pdf)
org/ark:/48223/pf0000380455 [accessed 6 February 2022] 34 Written evidence from Dr Miri Zilka, Detective Sergeant Laurence Cartwright and Dr Adrian Weller (nTL0040) THE ADVEnT OF nEW TECHnOLOGIES In THE JUSTICE SYSTEM 13 Chapter 3, we look at transparency: its necessity and proposals to increase it. (extracted from223.pdf)
The key issues we have identified, however, hold true for a much wider context: their application to all functions of the justice system and to border management. (extracted from223.pdf)
14 THE ADVEnT OF nEW TECHnOLOGIES In THE JUSTICE SYSTEM CHAPTER 2: LEGAL AND INSTITUTIONAL FRAMEWORKS 18. (extracted from223.pdf)
It was also indicated that “public failures” could “lead to not just operational defects or inefficiencies, but miscarriages of justice”,42 and that where weaknesses were exposed, they “exacerbate the low level and negative trend in public trust for relevant technology”.43 Professor nigel Harvey and Tobias Harvey referred to accountability for errors and misuse, saying that the use of algorithms “may leave people open to dangers for which no person can be identified as responsible”.44 “A chilling effect”45 22. (extracted from223.pdf)
Various contributors told us that the use of some technologies, notably the use of live facial recognition, created fear or disquiet, and that this risked 35 Written evidence from the Home Office (nTL0055) 36 QQ 103–104 (Kit Malthouse MP) 37 Royal Court of Justice, R v The Chief Constable of South Wales Police, [2020] EWCA Civ 1058. (extracted from223.pdf)
(nTL0022) 43 Written evidence from Archie Drake and Perry Keller (nTL0011) 44 Written evidence from Professor nigel Harvey and Tobias Harvey (nTL0025) 45 Written evidence from Dr Joe Purshouse, Dr nessa Lynch, Dr Marcin Betkier and Professor Liz Campbell (nTL0021) THE ADVEnT OF nEW TECHnOLOGIES In THE JUSTICE SYSTEM 15 damaging the democratic process. (extracted from223.pdf)
16 THE ADVEnT OF nEW TECHnOLOGIES In THE JUSTICE SYSTEM The right to a fair trial 23. (extracted from223.pdf)
We were concerned that, in some instances, the use of advanced tools at certain points of the criminal justice pipeline may impede an individual’s right to a fair trial: whether by a lack of awareness that they were being used, unreliable evidence, or an inability to understand and therefore challenge proceedings. (extracted from223.pdf)
Kotsoglou (nTL0006) 55 Written evidence from Big Brother Watch (nTL0037) THE ADVEnT OF nEW TECHnOLOGIES In THE JUSTICE SYSTEM 17 discrimination”,56 and Professor nigel Harvey and Tobias Harvey wrote that “learning algorithms based on historical data would preserve bias”.57 28. (extracted from223.pdf)
56 Written evidence from Liberty (nTL0020) 57 Written evidence from Professor nigel Harvey and Tobias Harvey (nTL0025) 58 Q 60 (Professor Karen Yeung) 59 Written evidence from Liberty (nTL0020) 18 THE ADVEnT OF nEW TECHnOLOGIES In THE JUSTICE SYSTEM Figure 1: Predictive policing—a vicious circle? (extracted from223.pdf)
We have noted over 30 public bodies, initiatives, and programmes playing a role in the governance of new technologies for the application of the law 60 Q 99 (Kit Malthouse MP) THE ADVEnT OF nEW TECHnOLOGIES In THE JUSTICE SYSTEM 19 (see Box 4). (extracted from223.pdf)
Its three key areas of work are “to pilot new forms of data stewardship and governance”; to increase assurance practices around AI; and, to assist public sector bodies looking to procure technologies— ”facilitating the delivery of transformative data and AI projects in the public sector”.63 • The AI Council is an independent advisory committee which “works to support the growth of AI in the UK” and aims to increase skills, “work on public perception”, and “[explore] how to develop and deploy safe, fair, legal and ethical data sharing frameworks”.64 61 Office for Artificial Intelligence, ‘About us’: https://www.gov.uk/government/organisations/office-for- artificial-intelligence/about [accessed 6 February 2022] 62 Department for Digital, Culture, Media and Sport, National AI Strategy (September 2021): https:// assets.publishing.service.gov.uk/government/uploads/system/uploads/attachment_data/file/1020402 [accessed 24 February 2022] 63 Centre for Data Ethics and Innovation, ‘About us’: https://www.gov.uk/government/organisations/ centre-for-data-ethics-and-innovation/about [accessed 6 February 2022] 64 HM Government, AI Council: https://www.gov.uk/government/groups/ai-council [accessed 6 February 2022] 20 THE ADVEnT OF nEW TECHnOLOGIES In THE JUSTICE SYSTEM Box 4: List of entities and programmes • Her Majesty’s Inspectorate of Constabulary and Fire and Rescue Services • The AI Council • The Association of Police and Crime Commissioners (APCC), and its various working groups and initiatives, including the APCC Biometrics and Data Ethics Working Group • The Biometrics and Forensics Ethics Group • The Biometrics and Surveillance Camera Commissioner • The Centre for Data Ethics and Innovation • The College of Policing • The Data Analytics Community of Practice • The Equalities and Human Rights Commission • The Forensic Science Regulator • The Home Office Digital, Data and Technology function • The Independent Office for Police Conduct • The Information Commissioner’s Office • The national Crime Agency, and its TRACER programme • The national Data Analytics Solution • The national Digital and Data Ethics Guidance Group • The national Digital Exploitation Centre • The national Police Chiefs’ Council, and its eleven co-ordination committees, each responsible for a specific aspect related to new technologies • The national Police Ethics Group • The national Policing Chief Scientific Adviser • The Office for AI • The Police Digital Service, its Data Office and Chief Data Officer • The Police Rewired initiative • The Police Science, Technology, Analysis and Research (STAR) fund • The Police, Science, and Technology Investment Board • The Royal Statistical Society • The Science Advisory Council to the national Policing Chief Scientific Adviser • The Senior Data Governance Panel within the Ministry of Justice • The specialist and generalist ethics committees of some police forces • The Tackling Organised Exploitation programme THE ADVEnT OF nEW TECHnOLOGIES In THE JUSTICE SYSTEM 21 33. (extracted from223.pdf)
65 Q 98 (Professor Paul Taylor) 66 Written evidence from Association of Police and Crime Commissioners, national Police Chiefs’ Council and Police Digital Service (nTL0049) 67 Department for Digital, Culture, Media & Sport, Data: A new direction (10 September 2021), para 409: https://assets.publishing.service.gov.uk/government/uploads/system/uploads/attachment_data/ file/1022315/Data_Reform_Consultation_Document__Accessible_.pdf [accessed 28 January 2022] 68 Q 99 (Kit Malthouse MP) 69 Q 108 (Kit Malthouse MP) 22 THE ADVEnT OF nEW TECHnOLOGIES In THE JUSTICE SYSTEM Figure 2: “Family tree” of relevant governance arrangements Decision Local accountability ensures contextualised and timely decisions making Public SLT, ethics Police and Crime Public consultation committee, Chief Constable Commissioner technic e a t l c e . (extracted from223.pdf)
Dr Christopher Lawless referred to a series of bodies that all play “key roles in oversight” but which “vary in their remit and the extent of their powers”.70 Robin Allen QC and Dee Masters believed that “there has been too much thinking in ‘silos’”.71 There may also be confusion over responsibilities—Dr Lawless gave the example of facial recognition technology 70 Written evidence from Dr Christopher Lawless (nTL0029) 71 Written evidence from Robin Allen QC and Dee Masters (nTL0019) THE ADVEnT OF nEW TECHnOLOGIES In THE JUSTICE SYSTEM 23 to argue that there is a “potentially significant lacuna [in governance] where it is unclear who is statutorily responsible for regulation and oversight”.72 36. (extracted from223.pdf)
79 Q 99 (Kit Malthouse MP), see also Home Office, New Biometrics and Surveillance Camera Commissioner appointed (15 March 2021): https://www.gov.uk/government/news/new-biometrics-and-surveillance- camera-commissioner-appointed [accessed 28 January 2022] 80 Q 84 (Professor Paul Taylor) 24 THE ADVEnT OF nEW TECHnOLOGIES In THE JUSTICE SYSTEM • In September 2021, the Government published its National Artificial Intelligence Strategy. (extracted from223.pdf)
While the Minister said that, as a Minister in both the Home Office and the Ministry of Justice, he was the “living embodiment” of cross- departmental working, this does not appear to have impacted strategic 81 Department for Business, Energy & Industrial Strategy, ‘Guidance national AI Strategy’ (22 September 2021): https://www.gov.uk/government/publications/national-ai-strategy/national-ai- strategy-html-version [accessed 1 February 2022] 82 Department for Digital, Culture, Media & Sport, Data: A new direction, para 409. (extracted from223.pdf)
83 DCMS Consultation: ‘Data: A new direction’ Response by the Biometrics and Surveillance Camera Commissioner: https://assets.publishing.service.gov.uk/government/uploads/system/uploads/ attachment_data/file/1030248/BSCC_DCMS_Consultation_Response.pdf [accessed 1 February 2022] 84 Written evidence from Association of Police and Crime Commissioners, national Police Chiefs’ Council and Police Digital Service (nTL0049) 85 HL Deb, 3 november 2021, cols 1301–1305 THE ADVEnT OF nEW TECHnOLOGIES In THE JUSTICE SYSTEM 25 thinking on the use of new technologies in applying the law, and there is no indication of any collective governmental effort towards a single strategy.86 On the contrary, there are many indications of siloed thinking. (extracted from223.pdf)
Similarly, BEIS and DCMS are collectively responsible for the implementation of the National AI Strategy, which focuses on businesses and the benefits of innovation and does not appear to have considered the needs of the Ministry of Justice or the Home Office at any length, or AI’s potential in their sectors. (extracted from223.pdf)
David Tucker, Faculty Lead on Crime and Criminal Justice at the College of Policing, told us: “We have seen that where decisions are challenged or doubted cases go to court and affect the way policing operates. (extracted from223.pdf)
The Appeal Court said that there was an absence of policy, so we are filling that gap and moving to apply these principles to this piece of technology”.87 86 Q 100 (Kit Malthouse MP) 87 Q 89 (David Tucker) 26 THE ADVEnT OF nEW TECHnOLOGIES In THE JUSTICE SYSTEM Box 5: The Bridges case and the Public Sector Equality Duty 1. (extracted from223.pdf)
88 Ministry of Justice, Public sector equality duty (6 July 2021): https://www.gov.uk/government/ publications/public-sector-equality-duty [accessed 4 February 2022] 89 Royal Court of Justice, R v The Chief Constable of South Wales Police, [2020] EWCA Civ 1058 90 Q 110 (Kit Malthouse MP) 91 Q 92 (Alun Michael) THE ADVEnT OF nEW TECHnOLOGIES In THE JUSTICE SYSTEM 27 We have also been told that both domestic courts and the European Court of Human Rights had been relied upon in the past.92 49. (extracted from223.pdf)
(nTL0022) 93 Written evidence from nCC Group (nTL0005) 94 Written evidence from the Bar Council (nTL0048) 95 Written evidence from nCC Group (nTL0005) 96 Written evidence from Archie Drake and Perry Keller (nTL0011) 97 Written evidence from the Home Office (nTL0055) 98 Q 73 (David Lewis) 99 Written evidence from nCC Group (nTL0005) 100 Q 36 (Dr David Leslie) 28 THE ADVEnT OF nEW TECHnOLOGIES In THE JUSTICE SYSTEM of the EU Commission’s proposed AI regulation” (see Box 6). (extracted from223.pdf)
Robin Allen QC and Dee Masters argued that “public actors and private companies need clear, pragmatic and effective regulatory frameworks because it provides a safety net within which ‘good’ AI can be developed whilst also protecting the fundamental rights of the public.”104 Archie Drake and Perry Keller had a similar view, stating that “legal uncertainty tends to harm business and innovation as well as public trust in the criminal justice system (and technology).”105 In a joint submission, three police bodies wrote that “Government should seek to clarify public appetite for new technologies and legislate so that policing has a clearer basis on which to make policies and decisions about deployment.”106 55. (extracted from223.pdf)
Professor Sandra Wachter, Associate Professor at the University of Oxford, thought that “soft regulation would be irresponsible” because the criminal justice system is “one of the most high- risk areas [she] can think of”.107 Dr Joe Purshouse and his co-contributors reflected that while guidance documents may be cited in court, they “do not provide actionable grounds for an individual to make a complaint”, adding that “non-compliance would not impact on the admissibility of any material gleaned.”108 101 Written evidence from Public Law Project (nTL0046) 102 Written evidence from Professor Pete Fussey, Dr Daragh Murray and Dr Amy Stevens (nTL0017) 103 Ibid. (extracted from223.pdf)
104 Written evidence from Robin Allen QC and Dee Masters (nTL0019) 105 Written evidence from Archie Drake and Perry Keller (nTL0011) 106 Written evidence from the Association of Police and Crime Commissioners (APCC), national Police Chiefs’ Council (nPCC), and Police Digital Service (PDS) (nTL0049) 107 Q 73 (Dr Liam Owens and Professor Sandra Wachter) 108 Written evidence from Dr Joe Purshouse, Dr nessa Lynch, Dr Marcin Betkier and Professor Liz Campbell (nTL0021) THE ADVEnT OF nEW TECHnOLOGIES In THE JUSTICE SYSTEM 29 Box 6: The EU Artificial Intelligence Regulation Proposal 1. (extracted from223.pdf)
30 THE ADVEnT OF nEW TECHnOLOGIES In THE JUSTICE SYSTEM of the national Police Chiefs’ Council, who, among others, thought that any new legal framework should be adopted at national level, but that “it would be helpful if it was not too divergent from international regulation”.115 The practicalities 58. (extracted from223.pdf)
Others favoured a value- and principles-based approach, with Alun Michael saying that “the values and principles need to be established in law”.118 As our witnesses pointed out, “certain things with AI will always be the same … we will always have a data issue, a bias issue and an explainability issue.”119 Professor Raab similarly told us that among the “plethora” of guidance, research and reviews, a consensus had emerged on some principles: “privacy protection, accountability, fairness, non-discrimination, justice, transparency, safety and cybersecurity, serving the common good, explainability, and human oversight”.120 As we highlighted in paragraph 56, the Minister himself said that the Government’s preferred approach was to produce a set of principles—our view is that these should be translated into statute. (extracted from223.pdf)
We 115 Q 73 (David Lewis) 116 Written evidence from nCC Group (nTL0005) 117 Written evidence from Professor Pete Fussey, Dr Daragh Murray and Dr Amy Stevens (nTL0017) 118 Q 98 (Alun Michael) 119 Q 69 (Professor Sandra Wachter) 120 Written evidence from Professor Charles Raab (nTL0014) THE ADVEnT OF nEW TECHnOLOGIES In THE JUSTICE SYSTEM 31 acknowledge the demands that the development of new legislation would place on parliamentary time and Government capacity, and that legislation is not a ‘quick fix’. (extracted from223.pdf)
32 THE ADVEnT OF nEW TECHnOLOGIES In THE JUSTICE SYSTEM 64. (extracted from223.pdf)
The Home Office told us about guidance on the CAID programme (the Child Abuse Image Database, a facial recognition tool which helps identify victims and offenders)126, while guidance on facial recognition is currently being developed by the College of Policing, and the Ministry of Justice is working with the Alan Turing Institute to extend existing guidance on the use of data-driven technologies within the justice system.127 There is also various guidance available from the Surveillance Camera and Information Commissioners128, and a host of guidance from non-governmental sources such as the ALGO-care framework (a practical decision-making framework for the policing context).129 The Metropolitan Police Service noted that the application of the “variety of guidance, opinion, codes, directions and proposals for ethical frameworks … risks confusion and inconsistency”.130 126 Written evidence from the Home Office (nTL0055) 127 College of Policing, ‘Police use of live facial recognition technology—have your say’ (17 May 2021): https://www.college.police.uk/article/police-use-live-facial-recognition-technology-have-your-say [accessed 26 January 2022] and written evidence from the Ministry of Justice (nTL0053) 128 Information Commissioner’s Office, ‘Guidance index’: https://ico.org.uk/for-organisations/guidance- index/ and Biometrics and Surveillance Camera Commissioner, ‘Surveillance camera guidance, tools and templates’ (22 October 2018): https://www.gov.uk/government/collections/surveillance-camera- guidance-tools-and-templates [accessed 7 February 2022] 129 Marion Oswald, ‘Algorithmic risk assessment policing models: lessons from the Durham HART model and ‘Experimental’ proportionality’, Information & Communications Technology Law, vol.27, (3 April 2018): https://www.tandfonline.com/doi/full/10.1080/13600834.2018.1458455 [accessed 7 February 2022] 130 Written evidence from the Metropolitan Police Service (nTL0031) THE ADVEnT OF nEW TECHnOLOGIES In THE JUSTICE SYSTEM 33 Box 9: Application of the Equality Act 2010 • To illustrate the confusion in the application of law, several submissions referred to the Equality Act 2010. (extracted from223.pdf)
As Professor Raab pointed out, comprehensive and practical guidance on 131 Written evidence from Archie Drake and Perry Keller (nTL0011) 132 Cloisters, In the matter of automated data processing in Government decision making (7 September 2019): https://www.cloisters.com/wp-content/uploads/2019/10/Open-opinion-pdf-version-1.pdf [accessed 25 January 2022] 133 Centre for Data Ethics and Innovation, Review into bias in algorithmic decision-making (november 2020), p 12: https://assets.publishing.service.gov.uk/government/uploads/system/uploads/attachment_data/ file/957259/Review_into_bias_in_algorithmic_decision-making.pdf [accessed 2 February 2022] 134 Written evidence from Professor Pete Fussey, Dr Daragh Murray and Dr Amy Stevens (nTL0017) 135 Written evidence from Professor Charles Raab (nTL0014) 136 Written evidence from Robin Allen QC and Dee Masters (nTL0019) 137 College of Policing, ‘APP content’ (4 november 2015): https://www.app.college.police.uk/app- content/ [accessed 4 February 2022] 138 Written evidence from the Serious Fraud Office (nTL0034) 34 THE ADVEnT OF nEW TECHnOLOGIES In THE JUSTICE SYSTEM the use of types of technologies will require consistent review and ongoing updates as tools are used in operational settings, and practical operational issues identified.139 It could not therefore be expected that such guidance will ever tackle all of the specificities of particular tools. (extracted from223.pdf)
The Government should require that national guidance for the use of advanced technological tools in policing and criminal justice is drawn up and, as part of their response to this report, should outline concrete plans for this. (extracted from223.pdf)
142 Written evidence from the Association of Police and Crime Commissioners, national Police Chiefs’ Council, and Police Digital Service (nTL0049) 143 Written evidence from the Law Society of England and Wales (nTL0023) 144 Written evidence from Professor Pete Fussey, Dr Daragh Murray and Dr Amy Stevens (nTL0017) 145 Written evidence from BAE Systems (nTL0056) 146 Written evidence from the Information Commissioner’s Office (nTL0016) THE ADVEnT OF nEW TECHnOLOGIES In THE JUSTICE SYSTEM 35 potential influence on individuals, groups and society”147, this trust is critical. (extracted from223.pdf)
Kit Malthouse MP said that forces must be allowed to fail “before we jump on everything.” It is important to note that the Minister was speaking in this context about technology which proves “not to be terribly useful”152, rather than about a failure to comply with minimum standards or where a miscarriage of justice had occurred. (extracted from223.pdf)
In particular, they thought that the Home Secretary, the Lord Chancellor and Secretary of State for Justice, and the Minister for Crime and Policing should be answerable for “how the Government’s vision of technological change in the system safeguards its effectiveness and legitimacy.”156 The Minister for Crime and Policing agreed that he, and Government as a whole, are “broadly—whether [they] like it or not—responsible for most things.”157 147 Written evidence from Dr Matthias Wienroth et al. (extracted from223.pdf)
(nTL0022) 148 Committee on Standards in Public Life, ‘The Seven Principles of Public Life’ (31 May 1995): https:// www.gov.uk/government/publications/the-7-principles-of-public-life/the-7-principles-of-public- life--2 [accessed 27 January 2022] 149 Written evidence from Public Law Project (nTL0046) 150 Q 72 (Dr Liam Owens) 151 Q 76 (Dr Liam Owens) 152 Q 106 (Kit Malthouse MP) 153 Written evidence from Privacy International (nTL0051) 154 Written evidence from Dr Miri Zilka, Detective Sergeant Laurence Cartwright and Dr Adrian Weller (nTL0040) and Q 31 (Professor Michael Wooldridge) 155 Written evidence from the Bar Council (nTL0048) 156 Written evidence from Archie Drake and Perry Keller (nTL0011) 157 Q 107 (Kit Malthouse MP) 36 THE ADVEnT OF nEW TECHnOLOGIES In THE JUSTICE SYSTEM • Chief Constables. (extracted from223.pdf)
(nTL0012) 167 Written evidence from nCC Group (nTL0005) 168 Written evidence from BAE Systems (nTL0056) 169 Written evidence from Dr Christopher Lawless (nTL0029) THE ADVEnT OF nEW TECHnOLOGIES In THE JUSTICE SYSTEM 37 Lack of recourse 82. (extracted from223.pdf)
174 Written evidence from Robin Allen QC and Dee Masters (nTL0019) 175 Written evidence from Liberty (nTL0020) and Big Brother Watch (nTL0037) 38 THE ADVEnT OF nEW TECHnOLOGIES In THE JUSTICE SYSTEM to ban “clearly harmful or high-risk applications of technology” which lack robust accountability arrangements.176 87. (extracted from223.pdf)
176 Written evidence from Archie Drake and Perry Keller (nTL0011) 177 Written evidence from the Metropolitan Police Service (nTL0031) 178 Committee on Civil Liberties, Justice and Home Affairs, Report on artificial intelligence in criminal law and its use by the police and judicial authorities in criminal matters (13 July 2021): https://www.europarl. (extracted from223.pdf)
europa.eu/doceo/document/A-9-2021–0232_En.html [accessed 3 February 2022] 179 United nations Human Rights Office of the High Commissioner, Artificial intelligence risks to privacy demand urgent action — Bachelet (15 September 2021): https://www.ohchr.org/en/2021/09/artificial- intelligence-risks-privacy-demand-urgent-action-bachelet [accessed 3 February 2022] THE ADVEnT OF nEW TECHnOLOGIES In THE JUSTICE SYSTEM 39 CHAPTER 3: TRANSPARENCY 90. (extracted from223.pdf)
(nTL0022) 185 Written evidence from Public Law Project (nTL0046), see also written evidence from Dr Joe Purshouse, Dr nessa Lynch, Dr Marcin Betkier and Professor Liz Campbell (nTL0021) 186 Written evidence from Robin Allen QC and Dee Masters (nTL0019), see also Q 65 (Peter Dawson) 187 Q 78 (Professor Sandra Wachter) 188 Written evidence from The Bar Council (nTL0048) 189 Written evidence from Public Law Project (nTL0046) 190 Q 57 (Professor Karen Yeung) 40 THE ADVEnT OF nEW TECHnOLOGIES In THE JUSTICE SYSTEM of particular technologies but also for ensuring that the decision-making process for the use of technology is open to public scrutiny.”191 94. (extracted from223.pdf)
The Home Office told us that they were “supporting law enforcement organisations to address … the need for transparency”,192 and that “policing is committed to being transparent.”193 The Ministry of Justice also informed us about an annual review of “analytical algorithms—only a small subset of [which] involve data and decisions about individuals”. (extracted from223.pdf)
196 Written evidence from Robin Allen QC and Dee Masters (nTL0019) 197 Written evidence from the Metropolitan Police Service (nTL0031) THE ADVEnT OF nEW TECHnOLOGIES In THE JUSTICE SYSTEM 41 the use of technological solutions as they cannot know who is using what, for how long, for what purpose, or with what safeguards. (extracted from223.pdf)
This risks undermining trust in the police, the justice system, and the rule of law. (extracted from223.pdf)
42 THE ADVEnT OF nEW TECHnOLOGIES In THE JUSTICE SYSTEM to constantly refine and refresh the model to comply with appropriate ethical and legal oversight and governance.”203 While the full study is expected to be published (at a date yet to be confirmed), there are no commitments as to what information it will contain. (extracted from223.pdf)
Box 10: Previous support for a register • In 2018, the House of Commons Science and Technology Committee recommended that “the Government should produce, publish, and maintain a list of where algorithms with significant impacts are being used within Central Government, along with projects underway or planned for public service algorithms”.207 • A 2019 report by the Law Society of England and Wales concluded that “a national register of algorithmic systems in the criminal justice system should be created”.208 203 Police Professional, ‘Artificial intelligence ‘marginally better’ at predicting re-offending’ (25 January 2022): https://www.policeprofessional.com/news/artificial-intelligence-marginally-better-at-predict ing-reoffending/ [accessed 24 February 2022] 204 Q 65 (Silkie Carlo, Peter Dawson, Professor Karen Yeung) and Q 78 (David Lewis, Dr Liam Owens, Professor Sandra Wachter) 205 Written evidence from Robin Allen QC and Dee Masters (nTL0019) 206 Written evidence from Professor Colin Gavaghan (nTL0047), see also written evidence from Archie Drake and Perry Keller (nTL0011) and Professor Lilian Edwards, Professor Derek McAuley, Dr Lachlan Urquhart and Dr Jiahong Chen (nTL0035). (extracted from223.pdf)
207 Science and Technology Committee, Algorithms in decision-making (Fourth Report, Session 2017– 2019, HC 351) 208 Q 11 (Professor Sylvie Delacroix) see also the Law Society of England and Wales, Algorithm use in the criminal justice system report, p 66: https://www.lawsociety.org.uk/en/topics/research/algorithm-use-in- the-criminal-justice-system-report [accessed 10 January 2022]. (extracted from223.pdf)
THE ADVEnT OF nEW TECHnOLOGIES In THE JUSTICE SYSTEM 43 • A 2020 report by the Royal United Services Institute (RUSI) commissioned by the Centre for Data Ethics and Innovation (CDEI) found that “the nPCC and APCC should … maintain a high-level catalogue for all algorithms used by police forces nationwide”.209 David Lewis told us that the nPCC had recently accepted this recommendation, although caveating that “there is a matter of degree to be debated”.210 There is no indication that such a catalogue would be published. (extracted from223.pdf)
One contributor thought that “it is not always feasible or even desirable to make algorithms in criminal justice fully transparent.”214 In the following paragraphs we examine those arguments. (extracted from223.pdf)
For instance, the information published on a register “could be used to infer how a [Machine Learning] model would make a specific legal decision, and thus what inputs could be crafted to manipulate a desired legal 209 RUSI, Data Analytics and Algorithms in Policing in England and Wales (February 2020), p xi: https:// static.rusi.org/rusi_pub_165_2020_01_algorithmic_policing_babuta_final_web_copy.pdf [accessed 21 January 2022] 210 Q 78 (David Lewis) 211 Centre for Data Ethics and Innovation, Review into bias in algorithmic decision making (november 2020): https://assets.publishing.service.gov.uk/government/uploads/system/uploads/attachment_data/ file/957259/Review_into_bias_in_algorithmic_decision-making.pdf [accessed 21 January 2022] 212 Commission on Race and Ethnic Disparities, Independent report, Forward, introduction and full recommendations, (28 April 2021): https://www.gov.uk/government/publications/the-report-of-the- commission-on-race-and-ethnic-disparities/foreword-introduction-and-full-recommendations#full- recommendations [accessed 10 January 2022] 213 Written evidence from the Independent Office for Police Conduct (nTL0054) 214 Written evidence from Dr Miri Zilka, Detective Sergeant Laurence Cartwright and Dr Adrian Weller (nTL0040) 215 Q 78 (Dr Liam Owens) 216 Written evidence from BAE Systems (nTL0056) 217 Q 78 (Professor Sandra Wachter) and Q 73 (Professor Sandra Wachter) 44 THE ADVEnT OF nEW TECHnOLOGIES In THE JUSTICE SYSTEM outcome.”218 Indeed, several witnesses were worried that a technological solution “could be ‘gamed’ by criminals” if algorithms were published.219 This is what BAE Systems calls “data poisoning”220 and the nCC Group calls “adversarial Machine Learning”.221 107. (extracted from223.pdf)
When some circumscribed their recommendation to “the application of the law”228 or “the criminal justice system”229 only, others advised that it should cover “the public sector”230 or “Government”231 in general. (extracted from223.pdf)
226 Written evidence from the Information Commissioner’s Office (nTL0016) 227 Written evidence from Professor Colin Gavaghan (nTL0047) 228 Written evidence from Big Brother Watch (nTL0037) 229 Q 11 (Professor Sylvie Delacroix) 230 Q 65 (Professor Karen Yeung) 231 Written evidence from the Public Law Project (nTL0046) 232 Written evidence from Professor Colin Gavaghan (nTL0047) 233 Written evidence from Big Brother Watch (nTL0037) THE ADVEnT OF nEW TECHnOLOGIES In THE JUSTICE SYSTEM 45 • The Public Law Project considered that each entry in the register should be accompanied with “executable versions of listed algorithms” and an explanation of how the technology works.234 • Citing the EU’s proposed AI Regulation currently being discussed within the European Union as a reference (see Box 6), Professor Sandra Wachter suggested that the register could include algorithms themselves, the data on which they are trained, as well as information on tests carried out and on oversight mechanisms.235 • Several witnesses asked for “detailed impact assessments”236 to be included, such as Equality Impact Assessments237 or Human Rights Impact Assessments.238 The Algorithmic Transparency Standard 109. (extracted from223.pdf)
240 Cabinet Office, UK government publishes pioneering standard for algorithmic transparency 46 THE ADVEnT OF nEW TECHnOLOGIES In THE JUSTICE SYSTEM allow public bodies the time to submit entries without diverting effort away from operational activities. (extracted from223.pdf)
THE ADVEnT OF nEW TECHnOLOGIES In THE JUSTICE SYSTEM 47 CHAPTER 4: HUMAN-TECHNOLOGY INTERACTIONS 114. (extracted from223.pdf)
(nTL0012) 248 Article 22 of Regulation (EU) 2016/679 of 23 April 2016 on the protection of natural persons with regard to the processing of personal data and on the free movement of such data, and repealing Directive 95/46/EC (General Data Protection Regulation) (Article 22) 4 May 2016 (OJ L 119/1), see written evidence from Professor nigel Harvey and Tobias Harvey (nTL0025) see also written evidence from Professor Lilian Edwards, Professor Derek McAuley, Dr Lachlan Urquhart and Dr Jiahong Chen (nTL0035) 48 THE ADVEnT OF nEW TECHnOLOGIES In THE JUSTICE SYSTEM enforcement body may not take a qualifying significant decision based solely on automated processing unless that decision is required or authorised by law.249 These provisions aim to guarantee that there is a “human in the loop” but only for narrowly specified decisions. (extracted from223.pdf)
251 Oral evidence taken on 27 October 2021 (Session 2021–22) Q 13 (The Rt Hon Priti Patel MP, Home Secretary) 252 Q 86 (Professor Paul Taylor) 253 See, for example, written evidence from the Law Society of England and Wales (nTL0023), Big Brother Watch (nTL0037) and the Public Law Project (nTL0046) 254 Q 33 (Professor Michael Wooldridge) 255 European Commission, Guidelines on Automated individual decision making and Profiling for the purposes of regulation 2016/679 (wp251rev.01), 22 August 2018: https://ec.europa.eu/newsroom/article29/ redirection/document/49826 [accessed 24 January 2022] THE ADVEnT OF nEW TECHnOLOGIES In THE JUSTICE SYSTEM 49 • reviewers must ‘weigh-up’ and ‘interpret’ the recommendation, consider all available input data, and also take into account other additional factors.”256 122. (extracted from223.pdf)
Evidence from the Ministry of Justice stated that “operational decisions are informed by analytical tools rather than being automatic consequences of tool outputs.”257 Similarly, the Home Office stated that they “strongly disagree … that we are allowing sensitive decisions to be delegated to machines in a way that is either contrary to the law or the core principles of the [criminal justice system]”.258 Interactions to date 123. (extracted from223.pdf)
We were told, for instance, that attendees to an event about facial recognition (many of whom had access to and used facial recognition technology) “had a limited understanding of both face recognition technology and human face recognition.”264 A supplier of some tools had found that “criminal justice professionals are typically also lacking an expert, or even good, understanding.”265 Dee Masters and Robin Allen QC had, similarly, “become increasingly aware of the lack of understanding by regulators and the general public of the way in which AI systems are being used in their field of activity.”266 256 Information Commissioner’s Office, ‘Guidance on AI and data protection’: https://ico.org.uk/for- organisations/guide-to-data-protection/key-dp-themes/guidance-on-artificial-intelligence-and-data- protection/ [accessed 17 January 2022] 257 Written evidence from the Ministry of Justice (nTL0053) 258 Written evidence from the Home Office (nTL0055) 259 Written evidence from BAE systems (nTL0056) 260 Q 8 (Professor Charles Raab) 261 Written evidence from Professor nigel Harvey and Tobias Harvey (nTL0025) 262 Written evidence from nCC Group (nTL0005) 263 Written evidence from Archie Drake and Perry Keller (nTL0011) 264 Written evidence from Dr Eilidh noyes and Dr Reuben Moreton (nTL0026) 265 Written evidence from SAS UK&I (nTL0041) 266 Written evidence from Robin Allen QC and Dee Masters (nTL0019) 50 THE ADVEnT OF nEW TECHnOLOGIES In THE JUSTICE SYSTEM 125. (extracted from223.pdf)
Professor Carole McCartney, Professor of Law and Criminal Justice at the University of northumbria, explained this further: “If the humans do not understand the technology and how it is working, how will they spot if it has failed or if they have made a mistake? (extracted from223.pdf)
The Prison Reform Trust were similarly concerned about confidence to challenge outcomes which might appear discriminatory: “managers and policy makers are likely to be less inclined to ‘look under the bonnet’ when the technology they find there is unfamiliar.”268 The lack of understanding does not only apply to the people who are using the tool, but to those who commission it—and those who interact with its outputs later ‘down the justice pipeline’, including judiciary reviewing the conduct and findings of an investigation. (extracted from223.pdf)
Amy Stevens (nTL0017) 270 Police Professional, Artificial intelligence ‘marginally better’ at predicting re-offending (25 January 2022): https://www.policeprofessional.com/news/artificial-intelligence-marginally-better-at-predicting- reoffending/ [accessed 24 February 2022] THE ADVEnT OF nEW TECHnOLOGIES In THE JUSTICE SYSTEM 51 • An automated triage system used by the Home Office, known as the sham marriage algorithm, is used to help “determine whether a proposed marriage should be investigated as a ‘sham’”. (extracted from223.pdf)
280 The Human Rights, Big Data and Technology Project, Independent Report on the London Metropolitan Police Service’s Trial of Live Facial Recognition Technology (July 2019), p 10: http://repository.essex.ac.uk /24946/1/ London-Met-Police-Trial-of-Facial-Recognition-Tech-Report-2.pdf [accessed 7 February 2022] 52 THE ADVEnT OF nEW TECHnOLOGIES In THE JUSTICE SYSTEM 128. (extracted from223.pdf)
The Home Office should, in conjunction with the Ministry of Justice and the College of Policing, undertake or commission appropriate research to determine how the use of predictive algorithms affects decision making, and under what circumstances meaningful human interaction is most likely. (extracted from223.pdf)
283 Q 87 (David Tucker) 284 Written evidence from the Public Law Project (nTL0046) 285 Written evidence from Dr Eilidh noyes and Dr Reuben Moreton (nTL0026) 286 Q 70 (David Lewis) 287 Q 58 (Peter Dawson) THE ADVEnT OF nEW TECHnOLOGIES In THE JUSTICE SYSTEM 53 ongoing288, and regularly reviewed.289 It could also address a skills shortage in the workforce: references were made in the evidence to the difficulty in attracting employees highly skilled in technological tools who can command extremely high salaries in the private sector.290 134. (extracted from223.pdf)
David Tucker, Head of Criminal Justice at the College of Policing, told the Committee that “we have to wait for a moment of maturity, because if we do not we run the risk of trying to give guidance on something that has not settled down and is developing.”291 We are unconvinced by this argument. (extracted from223.pdf)
There is also ensuring that we have systems and processes in place so that does not occur.”293 The Ministry of Justice also outlined some of its safeguarding systems, which included “clear guidance” for “when the data should and should not be used, and support (and sometimes training) … made available to staff”.294 136. (extracted from223.pdf)
It would not be reasonable to expect every police officer, or every Ministry of Justice official (to take but two examples), to be trained in data analytics and in the specificities of sophisticated technological solutions. (extracted from223.pdf)
(nTL0012) 290 Written evidence from the Serious Fraud Office (nTL0034) 291 Q 91 (David Tucker) 292 RUSI, ‘Data analytics and algorithms in policing in England and Wales: Towards a new policy framework’ (2020): https://rusi.org/explore-our-research/publications/occasional-papers/data-analyti cs-and-algorithms-policing-england-and-wales-towards-new-policy-framework [accessed 9 March 2022] 293 Q 89 (Professor Paul Taylor) 294 Written evidence from the Ministry of Justice (nTL0053) 54 THE ADVEnT OF nEW TECHnOLOGIES In THE JUSTICE SYSTEM a part,295 and thus also need a thorough understanding of how algorithmic tools work. (extracted from223.pdf)
As part of continuing professional development, training should also be made available to lawyers, members of the Judiciary, and other professionals involved in the justice system. (extracted from223.pdf)
The Ministry of Justice referred to their sexual offender predictive tool, which is supported by an “overarching” policy framework to “support consistency of use”.298 Evidence from the Prison Reform Trust acknowledged the need for structures and policies enabling a clear route for challenge, but emphasised that these must work well in practice. (extracted from223.pdf)
295 See paras 23–26 296 Independent Chief Inspector of Borders and Immigration, An inspection of entry clearance processing operations in Croydon and Istanbul 297 Written evidence from Professor Colin Gavaghan (nTL0047) 298 Written evidence from the Ministry of Justice (nTL0053) THE ADVEnT OF nEW TECHnOLOGIES In THE JUSTICE SYSTEM 55 144. (extracted from223.pdf)
Professor Karen Yeung, for instance, warned against tools that produce “a nice colour” such as “a little risk assessment, red, green or yellow” because they are so “easy to interpret” that they do not encourage challenge or critical thinking.304 In a similar vein, the Ministry of Justice told us that when designing “tools which 299 Q 57 (Peter Dawson) 300 Written evidence from the Prison Reform Trust (nTL0004) 301 Independent Chief Inspector of Borders and Immigration, An inspection of entry clearance processing operations in Croydon and Istanbul 302 Q 8 (Professor Delacroix) 303 Written evidence from Gary Pugh (nTL0036) 304 Q 57 (Professor Karen Yeung) 56 THE ADVEnT OF nEW TECHnOLOGIES In THE JUSTICE SYSTEM are used to support decisions about individuals”, they “strive to ensure that the tool is designed to be intuitive.”305 150. (extracted from223.pdf)
If achieved, algorithmic explainability would therefore provide more compelling explanations of decisions than humans currently provide.314 Certainly, procurement managers need to understand how the tools they are commissioning or purchasing function.315 Importantly, Professor Wachter added that introducing an explainability requirement on technologies used for the application of the law could help with this: “When people do not want to tell you how [a technological solution] is working, it is either because they do not want to or because they do 305 Written evidence from the Ministry of Justice (nTL0053) 306 Written evidence from the Bar Council (nTL0048) 307 Written evidence from BAE Systems (nTL0056) 308 See Marion Oswald, ‘Algorithm-assisted decision-making in the public sector: framing the issues using administrative law rules governing discretionary power’, University of Winchester, (6 August 2018), p 8–9: https://royalsocietypublishing.org/doi/10.1098/rsta.2017.0359 [accessed 2 February 2022] 309 Written evidence from Big Brother Watch (nTL0037) 310 Written evidence from Dr Miri Zilka, Detective Sergeant Laurence Cartwright and Dr Adrian Weller (nTL0040) 311 Written evidence from the Serious Fraud Office (nTL0034) and Archie Drake and Perry Keller (nTL0011) 312 Written evidence from Professor Lilian Edwards, Professor Derek McAuley, Dr Lachlan Urquhart and Dr Jiahong Chen (nTL0035) and Q 11 (Professor Sylvie Delacroix) 313 Written evidence from the Royal Statistical Society (nTL0033) 314 Written evidence from Professor Colin Gavaghan (nTL0047) 315 Chapter 5 addresses this from the angle of evaluations. (extracted from223.pdf)
THE ADVEnT OF nEW TECHnOLOGIES In THE JUSTICE SYSTEM 57 not know. (extracted from223.pdf)
I do not think either is acceptable, especially in the criminal justice sector. (extracted from223.pdf)
58 THE ADVEnT OF nEW TECHnOLOGIES In THE JUSTICE SYSTEM CHAPTER 5: EVALUATION AND OVERSIGHT 156. (extracted from223.pdf)
The Law Society of England and Wales told us that: “Bias, both conscious and unconscious, can be baked into algorithms and undermine consistently reliable results, and that using algorithms without questioning them or explaining them to the public could lead to decisions which threaten human rights and undermine trust in the justice system”.323 158. (extracted from223.pdf)
ethnicity-facts-figures.service.gov.uk/crime-justice-and-the-law/policing/stop-and-search/ latest#byethnicity [accessed 7 February 2022] 327 Q 71 (Professor Sandra Wachter) THE ADVEnT OF nEW TECHnOLOGIES In THE JUSTICE SYSTEM 59 the application of the categorisation algorithm and on which it depends. (extracted from223.pdf)
60 THE ADVEnT OF nEW TECHnOLOGIES In THE JUSTICE SYSTEM Assessments are a common approach to complying with the Public Sector Equality Duty,335 while Data Protection Impact Assessments are required for data processing operations which are likely to result in a high risk to individuals.336 163. (extracted from223.pdf)
THE ADVEnT OF nEW TECHnOLOGIES In THE JUSTICE SYSTEM 61 and in demonstrating trustworthiness in ways that are more convincing than slogans and pledges, or compliance with legal requirements.344 Community consultation 166. (extracted from223.pdf)
(nTL0022) 350 Written evidence from the Metropolitan Police Service (nTL0031) 351 Written evidence from Big Brother Watch (nTL0037) 62 THE ADVEnT OF nEW TECHnOLOGIES In THE JUSTICE SYSTEM Technical considerations “The system will fail” 170. (extracted from223.pdf)
THE ADVEnT OF nEW TECHnOLOGIES In THE JUSTICE SYSTEM 63 Box 12: Scientific standards In this report, a reference to ‘scientific standards’ is intended to mean a regime of quality standards and processes consistently applied to a person or body developing, maintaining or manufacturing a particular scientific product or technology, providing a scientific service, or incorporating a scientific method into their public service, combined with independent regulatory enforcement. (extracted from223.pdf)
64 THE ADVEnT OF nEW TECHnOLOGIES In THE JUSTICE SYSTEM issues”.362 For instance, we heard that “the accuracy of face recognition technology depends on … the environment in which the technology is deployed” because “in real settings, images may be of suboptimal quality or environmental conditions may be inhibitory to realising the full accuracy of face recognition technology.”363 176. (extracted from223.pdf)
371 Q 40 (Professor Colin Gavaghan) 372 Q 66 (Silkie Carlo), Q 46 (Dr Rosamunde van Brakel), and Q 4 (Professor Charles Raab) 373 Written evidence from Archie Drake and Perry Keller, Kings College London (nTL0011) THE ADVEnT OF nEW TECHnOLOGIES In THE JUSTICE SYSTEM 65 confirmed. (extracted from223.pdf)
66 THE ADVEnT OF nEW TECHnOLOGIES In THE JUSTICE SYSTEM was also critical of the Government’s justification for the increased use of polygraph testing—a technological solution with controversial scientific grounds. (extracted from223.pdf)
David Spreadborough, a forensic analyst, told us that “a technology introduced in the judiciary system should be validated and approved by people technically competent on the matter.”389 BAE Systems agreed that a designated body could “develop a certificate of conformance (or ‘Kitemark’/CE label) for approved AI applications”.390 nCC Group concurred that it is “essential that clear processes are established to vet technologies before they are deployed”391, whereas Dr Liam Owens of technology provider Semantics21 told us about a “review” by “an intermediary”.392 388 Written evidence from the Information Commissioner’s Office (nTL0016) 389 Written evidence from David Spreadborough (nTL0015) 390 Written evidence from BAE Systems (nTL0056) 391 Written evidence from nCC Group (nTL0005) 392 Q 78 (Dr Liam Owens) THE ADVEnT OF nEW TECHnOLOGIES In THE JUSTICE SYSTEM 67 Privacy International agreed with them and detailed mechanisms by which a technological solution should be “approved for use”.393 187. (extracted from223.pdf)
The Ministry of Justice referred us to a peer-reviewed research study that evaluated the OASys Sexual reoffending Predictor (OSP) before this technological solution was approved by the Sexual Offending Management Board of Her Majesty’s Prison and Probation Service.394 Similarly, the Public Law Project drew our attention to the proposed AI Regulation in the European Union, which foresees central “certification indicating conformity to regulatory standards.”395 188. (extracted from223.pdf)
Former Deputy Chief Constable David Lewis told us that “there probably should be more centralised procurement”, alluding to the success of “regional procurement hubs” bringing police forces together.398 BAE Systems agreed 393 Written evidence from Privacy International (nTL0051) 394 Written evidence from the Ministry of Justice (nTL0053) 395 Written evidence from Public Law Project (nTL0046) 396 College of Policing, Fundamental review of the College of Policing: https://assets.college.police.uk/s3fs- public/2022–02/Fundamental-review-of-the-College-of-Policing.pdf [accessed 24 February 2022] 397 HMICFRS, Inspectorate with College standards letter (10 February 2022): https://www.justicein spectorates.gov.uk/hmicfrs/publication-html/inspectorate-relationship-with-college-standards-letter/ [accessed 24 February 2022] 398 Q 77 (David Lewis) 68 THE ADVEnT OF nEW TECHnOLOGIES In THE JUSTICE SYSTEM that they “would support some form of centralised AI procurement within policing and justice.”399 192. (extracted from223.pdf)
The Ministry of Justice and HM Prison & Probation Service have developed several of them, such as: • the Offender Group Reconviction Score (OGRS), a “predictor of proven reoffending within one and two years of noncustodial sentence or discharge from custody”404 • the Offender Assessment System (OASys), which “aims to assess the risk of harm offenders pose to others and how likely an offender is to reoffend”405 • the Digital Categorisation Service (DCS), an algorithm used to support decisions on security categorisations in prisons. (extracted from223.pdf)
405 Written evidence from Big Brother Watch (nTL0037) 406 Written evidence from the Prison Reform Trust (nTL0004) THE ADVEnT OF nEW TECHnOLOGIES In THE JUSTICE SYSTEM 69 Police and the University College London.407 The national Data Analytics Solution, a nationwide project sponsored by the Home Office and led by West Midlands Police in partnership with the national Crime Agency and Accenture, also falls in this category.408 We were told that, in new Zealand, such partnerships were “by far the most common practice” when the government procures technological solutions.409 196. (extracted from223.pdf)
415 Written evidence from the nCC Group (nTL0005) 416 Written evidence from Dr Eilidh noyes and Dr Reuben Moreton (nTL0026) 417 Q 24 (Professor Charles Raab) 70 THE ADVEnT OF nEW TECHnOLOGIES In THE JUSTICE SYSTEM has been overtaken by marketing” because of salespeople who “will take something they do not understand and shout a number that they do not understand” to make accuracy claims.418 200. (extracted from223.pdf)
publishing.service.gov.uk/government/uploads/system/uploads/attachment_data/file/990469/ Guidelines_for_AI_procurement.pdf [accessed 26 January 2022] 424 World Economic Forum, AI Procurement Guidelines (11 June 2020): https://www.weforum.org/reports/ ai-procurement-in-a-box/ai-government-procurement-guidelines [accessed 26 January 2022] 425 Q 73 (David Lewis) THE ADVEnT OF nEW TECHnOLOGIES In THE JUSTICE SYSTEM 71 Chiefs’ Council, and the Police Digital Service confirmed that they “inform contract implementation and management”.426 203. (extracted from223.pdf)
BAE Systems argued that the guidelines should be “more technical”, more “supplier-focused”, and more specific to “policing and the justice context”.427 Professor Wachter told us that these “vague” guidelines were “not good enough” because they were too soft to induce change in procurement practices.428 Dr Liam Owens agreed that the guidelines are “very broad” and “non-specific” and would need to be better tailored to address the needs of technology providers in the context of the application of the law.429 Furthermore, this document refers to ‘guidance’ as well as ‘guidelines’. (extracted from223.pdf)
432 World Economic Forum, AI Procurement Guidelines 72 THE ADVEnT OF nEW TECHnOLOGIES In THE JUSTICE SYSTEM used in the application of the law. (extracted from223.pdf)
THE ADVEnT OF nEW TECHnOLOGIES In THE JUSTICE SYSTEM 73 The West Midlands Ethics Committee model 211. (extracted from223.pdf)
These included: • A recruitment based on merit, 443; with independent membership444 with a range of expertise; 445 • A commitment to publish meetings papers, minutes and conclusions;446 • The Committee’s independence from the police force whose use of technology it is scrutinising; and447 • The Committee’s remit to consider technological solutions throughout their lifecycle.448 441 See, for instance, Q 2 (Professor Carole McCartney), Q 110 (Kit Malthouse MP), also written evidence from Archie Drake and Perry Keller (nTL0011), and defenddigitalme (nTL0044) 442 Written evidence from Association of Police and Crime Commissioners, national Police Chiefs’ Council and Police Digital Service (nTL0049), see also Q 11 (Professor McCartney), Q 48 (Dr Rosamunde van Brakel), and written evidence from BAE Systems (nTL0056) 443 Written evidence from Association of Police and Crime Commissioners, national Police Chiefs’ Council and Police Digital Service (nTL0049) 444 Written evidence from Association of Police and Crime Commissioners, national Police Chiefs’ Council and Police Digital Service (nTL0049) and BAE Systems (nTL0056) 445 Q 11 (Professor McCartney), Q 48 (Dr Rosamunde van Brakel) and written evidence from BAE Systems (nTL0056) 446 Q 11 (Professor McCartney), written evidence from Association of Police and Crime Commissioners, national Police Chiefs’ Council and Police Digital Service (nTL0049), and BAE Systems (nTL0056) 447 Written evidence from Association of Police and Crime Commissioners, national Police Chiefs’ Council and Police Digital Service (nTL0049), Archie Drake and Perry Keller (nTL0011) and BAE Systems (nTL0056) 448 Written evidence from Association of Police and Crime Commissioners, national Police Chiefs’ Council and Police Digital Service (nTL0049) and BAE Systems (nTL0056) 74 THE ADVEnT OF nEW TECHnOLOGIES In THE JUSTICE SYSTEM Box 14: The West Midlands Police Ethics Committee • The West Midlands Police and Crime Commissioner (PCC) and West Midlands Police (WMP) jointly established a specialist Ethics Committee in early 2019. (extracted from223.pdf)
pdf?x41638 [accessed 1 February 2022] 450 HL Deb, 3 november 2021, cols 1301–1305 451 Written evidence from the Home Office (nTL0055) THE ADVEnT OF nEW TECHnOLOGIES In THE JUSTICE SYSTEM 75 which everything in society operates”. (extracted from223.pdf)
76 THE ADVEnT OF nEW TECHnOLOGIES In THE JUSTICE SYSTEM SUMMARY OF CONCLUSIONS AND RECOMMENDATIONS Legal and institutional frameworks 1. (extracted from223.pdf)
(Paragraph 66) THE ADVEnT OF nEW TECHnOLOGIES In THE JUSTICE SYSTEM 77 11. (extracted from223.pdf)
The Government should require that national guidance for the use of advanced technological tools in policing and criminal justice is drawn up and, as part of their response to this report, should outline concrete plans for this. (extracted from223.pdf)
This risks undermining trust in the police, the justice system, and the rule of law. (extracted from223.pdf)
(Paragraph 113) 78 THE ADVEnT OF nEW TECHnOLOGIES In THE JUSTICE SYSTEM Human-technology interactions 21. (extracted from223.pdf)
The Home Office should, in conjunction with the Ministry of Justice and the College of Policing, undertake or commission appropriate research to determine how the use of predictive algorithms affects decision making, and under what circumstances meaningful human interaction is most likely. (extracted from223.pdf)
As part of continuing professional development, training should also be made available to lawyers, members of the Judiciary, and other professionals involved in the justice system. (extracted from223.pdf)
(Paragraph 183) THE ADVEnT OF nEW TECHnOLOGIES In THE JUSTICE SYSTEM 79 30. (extracted from223.pdf)
With the assurance brought by the certification process and the register of algorithms, police forces and other 80 THE ADVEnT OF nEW TECHnOLOGIES In THE JUSTICE SYSTEM public bodies would remain free to procure the technological solutions of their choice, as long as the products have been certified. (extracted from223.pdf)
(Paragraph 219) THE ADVEnT OF nEW TECHnOLOGIES In THE JUSTICE SYSTEM 81 APPENDIx 1: LIST OF MEMBERS AND DECLARATIONS OF INTEREST Members Lord Blunkett Baroness Chakrabarti Lord Dholakia Baroness Hallett Baroness Hamwee (Chair) Lord Hunt of Wirral Baroness Kennedy of The Shaws Baroness Pidding Baroness Primarolo Lord Ricketts Baroness Sanderson of Welton Baroness of Shackleton of Belgravia Declarations of Interest Lord Blunkett Non- Financial: Non-executive Chairman, Cyber Essentials Direct Limited Directorship: Director and Chairman of the Board, University of Law Limited (subsidiary and affiliated institution of Global University Systems and Interactive Pro Limited) Baroness Chakrabarti No relevant interests to declare Lord Dholakia Trustee of the Police Foundation which produced a report on the Strategic Review of Policing in England and Wales on 8 March 2022 Baroness Hallett Retired judge Baroness Hamwee No relevant interests to declare Lord Hunt of Wirral Partner, DAC Beachcroft LLP (International commercial law firm) Honorary Bencher, Inner Temple Baroness Kennedy of The Shaws Member of Microsoft Technology and Human Rights Advisory Council Baroness Pidding No relevant interests to declare Baroness Primarolo Non-Executive Director on the Board of Thompson’s Solicitors LLP. (extracted from223.pdf)
Lord Ricketts No relevant interests to declare Baroness Sanderson of Welton No relevant interests to declare Baroness Shackleton of Belgravia No relevant interests to declare other than those on the Register 82 THE ADVEnT OF nEW TECHnOLOGIES In THE JUSTICE SYSTEM Specialist Adviser Dr Marion Oswald Associate Professor, Northumbria Law School Advisory Board member, Centre for Data Ethics and Innovation Senior Research Associate, The Alan Turing Institute Associate Fellow, Royal United Services Institute for Defence and Security Studies; Independent Chair, West Midlands Police & Crime Commissioner and West Midlands Police Data Ethics Committee; Member, New Zealand Police Expert Panel on Emergent Technologies; Advisory board member of the UKRI Trustworthy Autonomous Systems Hub; Member of the Royal Society Working Group on Privacy Enhancing Technologies (2018–19 and reconstituted 2021); Member of National Statistician’s Data Ethics Advisory Committee since its foundation (2016-date); Executive member, British & Irish Law, Education & Technology Association; Member, Arts and Humanities Research Council Peer Review College. (extracted from223.pdf)
THE ADVEnT OF nEW TECHnOLOGIES In THE JUSTICE SYSTEM 83 APPENDIx 2: LIST OF WITNESSES Evidence is published online at https://committees.parliament.uk/committee/519/ justice-and-home-affairs-committee/publications/ and available for inspection at the Parliamentary Archives (020 7219 3074). (extracted from223.pdf)
Oral evidence in chronological order * Professor Sylvie Delacroix, Professor in Law and QQ 1–24 Ethics at University of Birmingham * Professor Carole McCartney, Professor of Law and QQ 1–24 Criminal Justice at northumbria University ** Professor Charles Raab, Professorial Fellow, Politics QQ 1–24 and International Relations, School of Social and Political Science at The University of Edinburgh) * Dr David Leslie, Ethics Theme Lead at Alan Turing QQ 25–38 Institute * Professor Michael Wooldridge, Head of Department QQ 25–38 of Computer Science, Professor of Computer Science at University of Oxford ** Professor Colin Gavaghan, Director new Zealand QQ 39–51 Law Foundation Centre for Law and Policy in Emerging Technologies at University of Otago * Professor Elizabeth E Joh, Martin Luther King Jr. (extracted from223.pdf)
QQ 39–51 Professor of Law at University of California, Davis * Dr Rosamunde Elise van Brakel, Co-Director, QQ 39–51 Surveillance Studies network, Associate Professor Tilburg University/Vrije Universiteit Brussel (VUB) ** Silkie Carlo, Director, Big Brother Watch QQ 52–67 ** Peter Dawson, Director, Prison Reform Trust QQ 52–67 * Professor Karen Yeung, Interdisciplinary Professorial QQ 52–67 Fellow in Law, Ethics and Informatics, Birmingham Law School at The University of Birmingham * David Lewis, Former Deputy Chief Constable and QQ 68–82 former ethics lead nPCC at Dorset Police * Dr Liam Owens, Founder and Chief Executive QQ 68–82 Officer, Semantics 21 ** Professor Sandra Wachter, Associate Professor and QQ 68–82 Senior Research Fellow at University of Oxford ** Professor Paul Taylor, Chief Scientific Adviser, QQ 83–98 national Police Chiefs’ Council 84 THE ADVEnT OF nEW TECHnOLOGIES In THE JUSTICE SYSTEM ** Alun Michael, Police and Crime Commissioner for QQ 83–98 South Wales and Joint Lead for Data and Bioethics, Association of Police and Crime Commissioners ** Darryl Preston, Police and Crime Commissioner for QQ 83–98 Cambridgeshire and Peterborough and Joint Lead for Data and Bioethics, Association of Police and Crime Commissioners * David Tucker, Faculty Lead on Crime and Criminal QQ 83–98 Justice, College of Policing. (extracted from223.pdf)
* The Rt Hon Kit Malthouse MP, Minister of State for QQ 99–111 Crime and Policing at the Home Office and Ministry of Justice * Dr Christophe Prince, Director for Data and QQ 99–111 Identity, Home Office Alphabetical list of all witnesses Robin Allen QC, Barristers at A1 Law Consultancy/ nTL0019 Cloisters Chambers Dr Arianna Andreangeli, Senior Lecturer in nTL0038 Competition Law, Edinburgh Law School, nTL0039 University of Edinburgh Dr Philip Avenell, Managing Director and Forensic nTL0024 Biologist at Forensic Access Avon and Somerset Police nTL0052 BAE Systems nTL0056 Professor Melanie Bailey, Professor at University of nTL0024 Surrey The Bar Council nTL0048 Dr Marcin Betkier, Lecturer in Law at Victoria nTL0021 University of Wellington Dr Stephen Bleay, Senior Lecturer in Forensic nTL0024 Science at London South Bank University Katy Bourne OBE, Sussex Police and Crime nTL0045 Commissioner Dr Rebecca Brown, Research Fellow at University of nTL0030 Oxford Professor Dame Vicki Bruce DBE, Professor Emerita nTL0012 at newcastle University Professor A Mike Burton, Professor of Psychology at nTL0012 University of York Professor Liz Campbell, Professor and Francine V nTL0021 Mcniff Chair in Criminal Jurisprudence at Monash University THE ADVEnT OF nEW TECHnOLOGIES In THE JUSTICE SYSTEM 85 ** Silkie Carlo, Director, Big Brother Watch nTL0037 (QQ 52–67) Detective Sergeant Laurence Cartwright, Data nTL0040 Analytics lead at Sussex Police Dr Jiahong Chen, Lecturer in Law at Sheffield Law nTL0035 School, University of Sheffield The Crown Prosecution Service nTL0018 Dr Benjamin Davies, Wellcome Trust Society & nTL0030 Ethics Research Fellow at University of Oxford ** Peter Dawson, Director, The Prison Reform Trust nTL0004 (QQ 52–67) defenddigitalme nTL0044 Dr Delphine Defossez (Lecturer in Law), nTL0022 northumbria University * Professor Sylvie Delacroix, Professor in Law and Ethics at University of Birmingham (QQ 1–24) Professor Thomas Douglas, Professor of Applied nTL0013 Philosophy University of Oxford nTL0030 Archie Drake, Research Associate at Kings College, nTL0011 London Professor Gary Edmond, Professor of Law at UnSW nTL0012 Sydney Professor Lilian Edwards, Professor of Law, nTL0035 Innovation and Society at newcastle Law School, newcastle University Professor Seena Fazel, Professor of Forensic nTL0030 Psychiatry & Wellcome Trust Senior Research Fellow in Clinical Science at University of Oxford Dr Lisa Forsberg, British Academy Postdoctoral nTL0013 Fellow at University of Oxford nTL0030 Professor Simona Francese, Professor of Forensic nTL0024 and Bioanalytical Mass Spectrometry at Sheffield Hallam University Professor Pete Fussey, Professor of Sociology, nTL0017 University of Essex Professor Angela Gallop, Professor of Practice/ nTL0024 Director of Forensic Science at University of Strathclyde/Forensic Access ** Professor Colin Gavaghan, Director new Zealand nTL0047 Law Foundation Centre for Law and Policy in Emerging Technologies at University of Otago (QQ 39–51) 86 THE ADVEnT OF nEW TECHnOLOGIES In THE JUSTICE SYSTEM Dr Jamie Grace, Senior Lecturer in Law at Sheffield nTL0001 Hallam University Professor nigel Harvey, Professor of Judgment and nTL0025 Decision Research at UCL London Tobias Harvey, Student of Law at Kings College nTL0025 London Dr Binesh Hass, Research Fellow at University of nTL0030 Oxford The Home Office nTL0055 Independent Office for Police Conduct nTL0054 The Information Commissioner’s Office (ICO) nTL0016 Istanbul Bar Association nTL0028 * Professor Elizabeth E Joh, Martin Luther King Jr. (extracted from223.pdf)
Professor of Law at University of California, Davis (QQ 39–51) Perry Keller, Reader in Media and Information nTL0011 Law, Director of Doctoral Studies at King’s College London Professor Paul Kelly, Professor of Inorganic nTL0024 Chemistry at University of Loughborough Professor Richard I.Kemp, Professor of Psychology nTL0012 at UnSW Sydney Dr Kyriakos n Kotsoglou, Senior Lecturer in Law, nTL0006 northumbria University/Research Fellow, University nTL0007 of Lausanne The Law Society of England and Wales nTL0023 Dr Christopher Lawless, Associate Professor at nTL0029 Durham University * Dr David Leslie, Ethics Theme Lead at Alan Turing Institute (QQ 25–38) * David Lewis, Former Deputy Chief Constable and former ethics lead nPCC at Dorset Police (QQ 68– 82) Liberty nTL0020 Sjors Ligthart LLM PhD candidate at Tilburg nTL0013 University Dr nessa Lynch, Associate Professor of Law at nTL0021 Victoria University of Wellington * The Rt Hon Kit Malthouse MP, Minister of State for Crime and Policing at the Home Office and Ministry of Justice (QQ 99–111) Stephen Mason, Associate Research Fellow, Institute nTL0002 of Advanced Legal Studies THE ADVEnT OF nEW TECHnOLOGIES In THE JUSTICE SYSTEM 87 Dee Masters, Barristers at A1 Law Consultancy/ nTL0019 Cloisters Chambers Professor Derek McAuley, Director of Horizon nTL0035 Digital Economy Research Institute at University of nottingham ** Professor Carole McCartney, Professor of Law nTL0022 and Criminal Justice at northumbria University (QQ 1–24) medConfidential nTL0050 The Metropolitan Police Service nTL0031 Professor Gerben Meynen, Professor of Forensic nTL0013 Psychiatry and Bioethics at Utrecht University and VU University Amsterdam ** Alun Michael, Police and Crime Commissioner for nTL0049 South Wales and Joint Lead for Data and Bioethics, nTL0057 Association of Police and Crime Commissioners (QQ 83–98) Migrants’ Rights network nTL0042 The Ministry of Justice nTL0053 Abhishek Mishra, Doctoral Student at University of nTL0030 Oxford Dr Brent Mittelstadt of the Oxford Internet Institute nTL0058 (OII) Dr Reuben Moreton, Reli Ltd nTL0026 Professor Ruth Morgan, Professor of Crime and nTL0024 Forensic Sciences at University College London Dr Daragh Murray, Senior Lecturer in Human nTL0017 Rights at University of Essex nCC Group nTL0005 Dr Eilidh noyes, University of Huddersfield nTL0026 * Dr Liam Owens, Founder and Chief Executive Officer, Semantics 21 (QQ 68–82) Ms Angela Paul (PhD candidate in Law), nTL0022 northumbria University Police Scotland nTL0043 Dr Susan Pope, DnA expert—chair of the Forensic nTL0024 Science Regulator DnA Specialist Group & is an assessor for the netherlands Register of Court Experts at Principal Forensic Services ** Darryl Preston, Police and Crime Commissioner for nTL0049 Cambridgeshire and Peterborough and Joint Lead for nTL0057 Data and Bioethics, Association of Police and Crime Commissioners (QQ 83–98) 88 THE ADVEnT OF nEW TECHnOLOGIES In THE JUSTICE SYSTEM Dr Christophe Prince, Director for Data and Identity, Home Office (QQ 99–111) Privacy International nTL0051 Public Law Project nTL0046 nTL0059 Gary Pugh, Forensic Science Regulator nTL0036 Dr Jonathan Pugh, Parfit Radcliffe Senior Research nTL0030 Fellow at University of Oxford Dr Joe Purshouse, Senior Lecturer in Criminal Law nTL0021 and Justice at University of Sheffield ** Professor Charles Raab, Professorial Fellow, nTL0014 School of Social and Political Science, University of Edinburgh Fellow, The Alan Turing Institute (QQ 1–24) Dr Liam Ralph (Lecturer in Criminology and nTL0022 Policing) at the Centre for Crime and Policing & the Science and Justice Research, northumbria University Dr Kay L. (extracted from223.pdf)
Mehera San Roque, UnSW nTL0012 Sydney SAS UK&I nTL0041 Professor Julian Savulescu, Professor of Practical nTL0030 Ethics at University of Oxford Serious Fraud Office nTL0034 Professor Ilina Singh, Professor of neuroscience and nTL0030 Society at University of Oxford David Spreadborough, CFVA, Forensic Analyst at nTL0015 Amped Software Dr Amy Stevens, Senior Research Officer, Human nTL0017 Rights, Big Data and Technology Project at University of Essex Dr Clare Sutherland, Senior Lecturer at University nTL0012 of Aberdeen ** Professor Paul Taylor, Chief Scientific Adviser, nTL0049 national Police Chiefs’ Council (QQ 83–98) nTL0057 Dr Alice Towler, Research Fellow at UnSW Sydney nTL0012 ** David Tucker, Faculty Lead on Crime and Criminal nTL0057 Justice, College of Policing (QQ 83–98) THE ADVEnT OF nEW TECHnOLOGIES In THE JUSTICE SYSTEM 89 Professor Gillian Tully, Professor of Practice for nTL0024 Forensic Science Policy and Regulation at King’s College London UCL Centre for the Forensic Sciences nTL0010 Dr Lachlan Urquhart, Lecturer in Technology nTL0035 Law, and Co-Investigator of the UKRI Trustworthy Autonomous Systems node in Governance and Regulation at School of Law University of Edinburgh * Dr Rosamunde Elise van Brakel, Co-Director, Surveillance Studies network, Associate Professor Tilburg University/Vrije Universiteit Brussel (VUB) (QQ 39–51) ** Professor Sandra Wachter, Associate Professor and nTL0058 Senior Research Fellow at Oxford Internet Institute (OII), University of Oxford (QQ 68–82) Dr Adrian Weller, Principal Research Fellow in nTL0040 Machine Learning at the University of Cambridge Dr David White, Senior Lecturer at UnSW Sydney nTL0012 Dr Matthias Wienroth (Vice-Chancellor’s Senior nTL0022 Fellow in Criminology/Sociology), northumbria University Professor Kim Wolff, Director King’s Forensics at nTL0024 King’s College London * Professor Michael Woodridge, Head of Department of Computer Science, Professor of Computer Science at University of Oxford (QQ 25–38) * Professor Karen Yeung, Interdisciplinary Professorial Fellow in Law, Ethics and Informatics, Birmingham Law School at The University of Birmingham (QQ 52–67) Dr Miri Zilka, Research Associate in Machine nTL0040 Learning at the University of Cambridge 90 THE ADVEnT OF nEW TECHnOLOGIES In THE JUSTICE SYSTEM APPENDIx 3: CALL FOR EVIDENCE Scope of the inquiry The Committee seeks to explore the use of new technologies in the application of the law and the experience of people currently or previously engaged with them. (extracted from223.pdf)
Are safeguards needed to ensure that THE ADVEnT OF nEW TECHnOLOGIES In THE JUSTICE SYSTEM 91 technologies cannot be used to serve purposes incompatible with a democratic society? (extracted from223.pdf)
92 THE ADVEnT OF nEW TECHnOLOGIES In THE JUSTICE SYSTEM APPENDIx 4: ABBREVIATIONS, ACRONYMS AND TECHNICAL TERMS ADM Automated Decision Making AFR Automated Facial Recognition AI Artificial Intelligence Algorithm A series of instructions for performing a calculation or solving a problem, especially with a computer. (extracted from223.pdf)
THE ADVEnT OF nEW TECHnOLOGIES In THE JUSTICE SYSTEM 93 ML Machine Learning nDAS national Data Analytics Solution (a national analytics capability being developed by West Midlands Police in conjunction with the Home Office). (extracted from223.pdf)
PAGE 4 STEP 1: DATA GENERATION AND ACQUISITION JUSTICE The recommendations based on justice are intended to ensure reciprocity, so that when data are collected, subjects are offered benefits that are proportionate to the data you request. (extracted from592.pdf)
With reference to the basic orientation of justice, the physician can convince Patient U. (extracted from592.pdf)
PAGE 10 STEP 2: DATA STORAGE AND MANAGEMENT JUSTICE Justice requires enabling customers to use their data for the benefit of third parties and the common good. (extracted from592.pdf)
Source: https:// deepmind.com/blog/article/specifying-ai-safety-problems PAGE 16 STEP 3: DATA ANALYSIS AND KNOWLEDGE GENERATION JUSTICE The recommendations on justice address the problem of indirect discrimination as it relates to data-based models and predictions. (extracted from592.pdf)
PAGE 22 STEP 4: USE OF DATA-BASED PRODUCTS AND SERVICES JUSTICE Justice requires ensuring that the use of data-based products and services does not lead to undesirable social effects such as indirect discrimination or the stigma- tisation or exclusion of groups of people. (extracted from592.pdf)
If the app were made compulsory, or even linked to significant social advantages (such as the right to use public transportation), not knowing what groups are notified and inequalities in the rate of false positives would imply ignoring important elements about justice in deployment. (extracted from592.pdf)
Thus, the app as designed protects privacy, but prevents the collection on information relevant to the justice implications of the app’s use and functionality. (extracted from592.pdf)
There is thus a tension between justice and privacy in the deployment of the app. (extracted from592.pdf)
Texts adopted - Fundamental rights implications of big data - Tuesday, 14 March 2017 Choisissez la langue de votre document : bg - български es - español cs - čeština da - dansk de - Deutsch et - eesti keel el - ελληνικά en - English fr - français ga - Gaeilge hr - hrvatski it - italiano lv - latviešu valoda lt - lietuvių kalba hu - magyar mt - Malti nl - Nederlands pl - polski pt - português ro - română sk - slovenčina sl - slovenščina fi - suomi sv - svenska Index Previous Next Full text Procedure : 2016/2225(INI) Document stages in plenary Document selected : A8-0044/2017 Texts tabled : A8-0044/2017 Debates : PV 13/03/2017 - 15 CRE 13/03/2017 - 15 Votes : PV 14/03/2017 - 6.12 Explanations of votes Texts adopted : P8_TA(2017)0076 Texts adopted 203k 61k Tuesday, 14 March 2017 - Strasbourg Fundamental rights implications of big data P8_TA(2017)0076 A8-0044/2017 European Parliament resolution of 14 March 2017 on fundamental rights implications of big data: privacy, data protection, non-discrimination, security and law-enforcement ( 2016/2225(INI) ) The European Parliament, – having regard to Article 16 of the Treaty on the Functioning of the European Union, – having regard to Articles 1, 7, 8, 11, 14, 21, 47 and 52 of the Charter of Fundamental Rights of the European Union, – having regard to the guidelines for the regulation of computerised personal data files of the United Nations General Assembly in its Resolution 45/95 of 14 December 1990, – having regard to Regulation (EU) 2016/679 of the European Parliament and of the Council of 27 April 2016 on the protection of natural persons with regard to the processing of personal data and on the free movement of such data, and repealing Directive 95/46/EC (General Data Protection Regulation) (1) (GDPR), and to Directive (EU) 2016/680 of the European Parliament and of the Council of 27 April 2016 on the protection of natural persons with regard to the processing of personal data by competent authorities for the purposes of the prevention, investigation, detection or prosecution of criminal offences or the execution of criminal penalties, and on the free movement of such data, and repealing Council Framework Decision 2008/977/JHA (2) , – having regard to the Communication from the Commission to the European Parliament, the Council, the European Economic and Social Committee and the Committee of the Regions entitled ‘A Digital Single Market Strategy for Europe’ of 6 May 2015 ( COM(2015)0192 ), – having regard to the Council of Europe Convention for the protection of individuals with regard to automatic processing of personal data of 28 January 1981 (ETS No 108) and its Additional Protocol of 8 November 2001 (ETS No 181) (3) , – having regard to Recommendation CM/Rec(2010)13 of the Committee of Ministers of the Council of Europe to Member States on the protection of individuals with regard to automatic processing of personal data in the context of profiling of 23 November 2010 (4) , – having regard to Opinion 7/2015 of the European Data Protection Supervisor of 19 November 2015 entitled ‘Meeting the challenges of big data – A call for transparency, user control, data protection by design and accountability’ (5) , – having regard to Opinion 8/2016 of the European Data Protection Supervisor of 23 September 2016 entitled ‘EDPS Opinion on coherent enforcement of fundamental rights in the age of big data’ (6) , – having regard to the statement of the Article 29 Data Protection Working Party on the impact of the development of big data on the protection of individuals with regard to the processing of their personal data in the EU of 16 September 2014 (7) , – having regard to Rule 52 of its Rules of Procedure, – having regard to the report of the Committee on Civil Liberties, Justice and Home Affairs ( A8-0044/2017 ), A. (extracted from293.html)
Recalls that in accordance with Article 15 of Directive 2000/31/EC, Member States shall neither impose a general obligation on the providers of transmission, storage and hosting services to monitor the information which they transmit or store, nor a general obligation to actively seek facts or circumstances suggesting illegal activity; reiterates in particular that the Court of Justice of the European Union, in the cases C-360/10 and C-70/10, rejected measures for the ‘active monitoring’ of almost all users of the services concerned (internet access providers in one case, a social network in the other) and specified that any injunction requiring a hosting services provider to undertake general monitoring shall be precluded; Non- discrimination 19. (extracted from293.html)
(3) http://www.coe.int/en/web/conventions/full-list/-/conventions/treaty/108 (4) https://search.coe.int/cm/Pages/result_details.aspx?ObjectID=09000016805cdd00 (5) https://secure.edps.europa.eu/EDPSWEB/webdav/site/mySite/shared/Documents/Consultation/Opinions/2015/15-11-19_Big_Data_EN.pdf (6) https://secure.edps.europa.eu/EDPSWEB/webdav/site/mySite/shared/Documents/Consultation/Opinions/2016/16-09-23_BigData_opinion_EN.pdf (7) http://ec.europa.eu/justice/data-protection/article-29/documentation/opinion-recommendation/files/2014/wp221_en.pdf (8) Opinion 8/2016 of the European Data Protection Supervisor of 23 September 2016, p. (extracted from293.html)
How Responsible AI can improve business and preserve value | PwC Skip to content Skip to footer Industries Services Issues About us Careers More Search Menu Industries Industries See all industries Consumer markets Energy, utilities and resources Financial services Government and public sector Health industries Industrials and services Private equity and principal investors Technology, media and telecommunications Menu Industries See all industries Menu Industries Consumer markets Consumer goods Hospitality and leisure Retail Transportation and logistics Menu Industries Energy, utilities and resources Chemicals Mining and metals Oil and gas Power and utilities Menu Industries Financial services Asset and wealth management Banking and capital markets Insurance Real estate Menu Industries Government and public sector Defence Education and skills Public safety, justice and security Menu Industries Health industries Health services Pharmaceuticals and life sciences Menu Industries Industrials and services Aerospace, defence and security Automotive Business services Engineering and construction Industrial manufacturing Space Menu Industries Private equity and principal investors Capital projects and infrastructure Menu Industries Technology, media and telecommunications Entertainment and media Technology Telecommunications Featured Value in motion Industry edge Engine by Starling: From launching a bank to launching a software business Menu Services Services See all services Alliances Artificial Intelligence Audit and assurance services Business model reinvention Business transformation Consulting Crisis management Deals Entrepreneurial and private business Family business Forensics Legal business solutions Managed Services Risk services Strategy Sustainability and climate change Tax Trust Workforce Menu Services See all services Menu Services Alliances Menu Services Artificial Intelligence Menu Services Audit and assurance services Actuarial services Capital markets Corporate reporting Financial audit IFRS Internal audit Next Generation Audit Risk assurance Sustainability assurance Menu Services Business model reinvention Menu Services Business transformation Menu Services Consulting Cloud transformation Digital operations Deals Finance transformation Forensics Front office transformation HR transformation Risk and regulation Strategy Technology Workforce Menu Services Crisis management Menu Services Deals Acquisitions Capital markets Corporate finance Deals strategy Joint ventures and alliances M&A legal M&A tax Performance and Restructuring​ Sovereign wealth funds Menu Services Entrepreneurial and private business Menu Services Family business Menu Services Forensics Menu Services Legal business solutions Employment Entity governance and compliance International business reorganisations Mergers and acquisitions NewLaw Menu Services Managed Services Menu Services Risk services Menu Services Strategy Menu Services Sustainability and climate change Climate risk, resilience and adaptation Energy solutions Impact management for sustainable business strategy Legal and sustainability Net zero transformation Social sustainability Sustainability assurance Sustainable capital Sustainability reporting Tax and sustainability Menu Services Tax Indirect taxes International tax services Mergers and acquisitions Sightline Tax code of conduct Tax controversy and dispute resolution Tax policy and administration Tax reporting and strategy Transfer pricing Menu Services Trust Menu Services Workforce Culture, leadership and change Inclusion Employment law Employment tax and payroll HR transformation and technology Organisational design People analytics and insights People in deals Retirement and pensions Reward and benefits Workforce risk Workforce strategy Featured Climate risk, resilience and adaptation Business transformation Sustainability assurance Menu Issues Issues See all issues Business model reinvention Business transformation C-suite insights Cybersecurity Climate and sustainability Megatrends Risk and regulation Technology Trust Upskilling Workforce Menu Issues See all issues Menu Issues Business model reinvention Menu Issues Business transformation Menu Issues C-suite insights Accelerating performance Global CEO Survey PwC at Davos strategy+business digital issue Take on Tomorrow: a strategy+business podcast The Leadership Agenda Menu Issues Cybersecurity Menu Issues Climate and sustainability Menu Issues Megatrends Menu Issues Risk and regulation Menu Issues Technology Menu Issues Trust Menu Issues Upskilling Menu Issues Workforce Featured Value in motion PwC’s 28th Annual Global CEO Survey The Fearless Future: 2025 Global AI Jobs Barometer Menu About us About us See more About Us Alumni Analyst relations Client case studies Ethics and compliance Committing to Net Zero Corporate sustainability Inclusion PwC's Global Annual Review Global regulatory affairs Human rights policy Network Leadership, Governance, and Structure New Ventures and Innovation News room Purpose, values and behaviours PwC office locations PwC's Code of Conduct Strategy& Tax Code of Conduct Third party code of conduct Transparency Report Menu About us See more About Us Menu About us Alumni Menu About us Analyst relations Menu About us Client case studies Menu About us Ethics and compliance Menu About us Committing to Net Zero Menu About us Corporate sustainability Menu About us Inclusion Menu About us PwC's Global Annual Review Menu About us Global regulatory affairs Menu About us Human rights policy Menu About us Network Leadership, Governance, and Structure Menu About us New Ventures and Innovation Menu About us News room Menu About us Purpose, values and behaviours Menu About us PwC office locations Menu About us PwC's Code of Conduct Menu About us Strategy& Menu About us Tax Code of Conduct Menu About us Third party code of conduct Menu About us Transparency Report Featured Value in motion Committing to Net Zero Global Transparency Report 2024 Menu Careers Careers Find out more about careers Search for a job Menu Careers Find out more about careers Menu Careers Search for a job Featured PwC’s Global Annual Review Committing to net zero The CEO’s ESG dilemma Loading Results No Match Found View All Results Designing, building and operating AI that delivers real-world impact Responsible AI Artificial intelligence is transforming business by streamlining activities, enhancing customer offerings, making workers more effective and speeding up innovation—prompting executives to deploy intelligent applications and agentic systems. (extracted from552.html)
Koenig, Jacob Metcalf, Arvind Narayanan, Alondra Nelson, Frank Pasquale Ten simple rules for responsible big data research www.ploscompbiol.org www.ploscompbiol.org endstream endobj 3 0 obj < >/XObject< >>>/CropBox[0 0 612 792]/MediaBox[0 0 612 792]/Parent 9 0 R/Annots 10 0 R/Contents 11 0 R/TrimBox[0 0 612 792]>> endobj 10 0 obj [12 0 R 13 0 R 14 0 R 15 0 R 16 0 R 17 0 R 18 0 R 19 0 R 20 0 R 21 0 R 22 0 R 23 0 R 24 0 R 25 0 R 26 0 R] endobj 12 0 obj < >/Border[0 0 0]/A 27 0 R>> endobj 27 0 obj < > endobj 13 0 obj < >/Border[0 0 0]/Dest(Rpcbi.1005399.ref001)>> endobj 14 0 obj < >/Border[0 0 0]/Dest(Rpcbi.1005399.ref002)>> endobj 15 0 obj < >/Border[0 0 0]/Dest(Rpcbi.1005399.ref003)>> endobj 16 0 obj < >/Border[0 0 0]/Dest(Rpcbi.1005399.ref004)>> endobj 17 0 obj < >/Border[0 0 0]/Dest(Rpcbi.1005399.ref005)>> endobj 18 0 obj < >/Border[0 0 0]/Dest(Rpcbi.1005399.ref006)>> endobj 19 0 obj < >/Border[0 0 0]/Dest(Rpcbi.1005399.ref007)>> endobj 20 0 obj < >/Border[0 0 0]/Dest(Rpcbi.1005399.ref008)>> endobj 21 0 obj < >/Border[0 0 0]/Dest(Rpcbi.1005399.ref009)>> endobj 22 0 obj < >/Border[0 0 0]/Dest(Rpcbi.1005399.ref009)>> endobj 23 0 obj < >/Border[0 0 0]/Dest(Rpcbi.1005399.ref010)>> endobj 24 0 obj < >/Border[0 0 0]/Dest(Rpcbi.1005399.ref011)>> endobj 25 0 obj < >/Border[0 0 0]/Dest(Rpcbi.1005399.ref012)>> endobj 26 0 obj < >/Border[0 0 0]/A 28 0 R>> endobj 28 0 obj < > endobj 11 0 obj [29 0 R 30 0 R 31 0 R 32 0 R 33 0 R 34 0 R 35 0 R 36 0 R 37 0 R 38 0 R 39 0 R 40 0 R 41 0 R 42 0 R 43 0 R] endobj 29 0 obj < >stream q 0.83 0.64 0.02 0 k 407.5654 537.3354 m 514.148 537.3354 l h f* 560.7496 511.3134 m 565.3984 511.3134 l h f* 231.0236 387.6094 m 235.7858 387.6094 l h f* 203.4142 309.6 m 208.1764 309.6 l h f* 210.3874 309.6 m 215.1496 309.6 l h f* 233.4614 296.6173 m 238.2236 296.6173 l h f* 237.0331 283.6346 m 241.7953 283.6346 l h f* 488.3528 270.5953 m 493.115 270.5953 l h f* 390.7843 218.6079 m 395.4898 218.6079 l h f* 482.9669 205.6252 m 487.7291 205.6252 l h f* 271.5024 166.6205 m 276.2646 166.6205 l h f* 278.5323 166.6205 m 288.0567 166.6205 l h f* 355.9181 81.9213 m 365.3858 81.9213 l h f* 435.7984 81.9213 m 445.3228 81.9213 l h f* 0 g 1 j 1 J 0 w 10 0 0 10 211.9748 707.4141 cm BT /F6 1 Tf 1 TL -0.004 Tc 0 0 Td (The)Tj 1.7858 0 Td (variety)Tj 2.9253 0 Td (in)Tj 1.0318 0 Td (data)Tj 1.9162 0 Td (sources,)Tj 3.4186 0 Td (research)Tj 3.5546 0 Td (topics,)Tj 2.8233 0 Td (and)Tj 1.7291 0 Td (methodological)Tj 6.4176 0 Td (approaches)Tj 4.7395 0 Td (in)Tj 1.0318 0 Td (big)Tj 1.4627 0 Td (data)Tj -34.0325 -1.3039 Td (belies)Tj 2.4491 0 Td (a)Tj 0.6519 0 Td (one-size-fits-all)Tj 6.3553 0 Td (checklist;)Tj 3.9117 0 Td (as)Tj 1.0148 0 Td (a)Tj 0.6577 0 Td (result,)Tj 2.6702 0 Td (these)Tj 2.2564 0 Td (rules)Tj 2.1486 0 Td (are)Tj 1.4457 0 Td (less)Tj 1.6157 0 Td (specific)Tj 3.1918 0 Td (than)Tj 2.0239 0 Td (some)Tj 2.3357 0 Td (might)Tj 2.5966 0 Td (hope.)Tj -35.3251 -1.2982 Td (Rather,)Tj 3.118 0 Td (we)Tj 1.3323 0 Td (exhort)Tj 2.812 0 Td (researchers)Tj 4.7111 0 Td (to)Tj 1.0261 0 Td (recognize)Tj 4.0592 0 Td (the)Tj 1.474 0 Td (human)Tj 3.0728 0 Td (participants)Tj 4.9606 0 Td (and)Tj 1.7291 0 Td (complex)Tj 3.6226 0 Td (systems)Tj 3.2996 0 Td (con-)Tj -35.2174 -1.2983 Td (tained)Tj 2.7098 0 Td (within)Tj 2.8063 0 Td (their)Tj 2.1033 0 Td (data)Tj 1.9106 0 Td (and)Tj 1.7234 0 Td (make)Tj 2.3868 0 Td (grappling)Tj 4.0421 0 Td (with)Tj 1.9956 0 Td (ethical)Tj 2.829 0 Td (questions)Tj 4.0195 0 Td (part)Tj 1.8312 0 Td (of)Tj 1.0091 0 Td (their)Tj 2.1033 0 Td (standard)Tj 3.7133 0 Td (work-)Tj -35.1833 -1.2983 Td (flow.)Tj 2.1713 0 Td (Towards)Tj 3.7133 0 Td (this)Tj 1.6725 0 Td (end,)Tj 1.9502 0 Td (we)Tj 1.3266 0 Td (structure)Tj 3.8211 0 Td (the)Tj 1.4683 0 Td (first)Tj 1.8085 0 Td (five)Tj 1.6497 0 Td (rules)Tj 2.1544 0 Td (around)Tj 3.1351 0 Td (how)Tj 1.9388 0 Td (to)Tj 1.0262 0 Td (reduce)Tj 2.914 0 Td (the)Tj 1.4683 0 Td (chance)Tj 3.0047 0 Td (of)Tj -35.223 -1.3039 Td (harm)Tj 2.364 0 Td (resulting)Tj 3.719 0 Td (from)Tj 2.1941 0 Td (big)Tj 1.4513 0 Td (data)Tj 1.8992 0 Td (research)Tj 3.5489 0 Td (practices;)Tj 3.9572 0 Td (the)Tj 1.457 0 Td (second)Tj 3.0047 0 Td (five)Tj 1.6384 0 Td (rules)Tj 2.143 0 Td (focus)Tj 2.3187 0 Td (on)Tj 1.2642 0 Td (ways)Tj 2.143 0 Td (researchers)Tj -33.1027 -1.2982 Td (can)Tj 1.627 0 Td (contribute)Tj 4.388 0 Td (to)Tj 1.0262 0 Td (building)Tj 3.5773 0 Td (best)Tj 1.8084 0 Td (practices)Tj 3.7418 0 Td (that)Tj 1.7801 0 Td (fit)Tj 1.0715 0 Td (their)Tj 2.109 0 Td (disciplinary)Tj 4.9039 0 Td (and)Tj 1.7291 0 Td (methodological)Tj -27.7623 -1.2983 Td (approaches.)Tj 4.9605 0 Td (At)Tj 1.2019 0 Td (the)Tj 1.4684 0 Td (core)Tj 1.9445 0 Td (of)Tj 1.0148 0 Td (these)Tj 2.2564 0 Td (rules,)Tj 2.3754 0 Td (we)Tj 1.3266 0 Td (challenge)Tj 3.9571 0 Td (big)Tj 1.4627 0 Td (data)Tj 1.9105 0 Td (researchers)Tj 4.7112 0 Td (who)Tj 1.9389 0 Td (consider)Tj 3.651 0 Td (their)Tj -34.1799 -1.3039 Td (data)Tj 1.9105 0 Td (disentangled)Tj 5.2951 0 Td (from)Tj 2.2053 0 Td (the)Tj 1.4683 0 Td (ability)Tj 2.6929 0 Td (to)Tj 1.0262 0 Td (harm)Tj 2.3754 0 Td (to)Tj 1.0261 0 Td (reexamine)Tj 4.3937 0 Td (their)Tj 2.109 0 Td (assumptions.)Tj 5.4481 0 Td (The)Tj 1.7802 0 Td (examples)Tj 3.9117 0 Td (in)Tj -35.6425 -1.2983 Td (this)Tj 1.6724 0 Td (paper)Tj 2.4831 0 Td (show)Tj 2.3017 0 Td (how)Tj 1.9389 0 Td (often)Tj 2.2847 0 Td (even)Tj 2.0693 0 Td (seemingly)Tj 4.2293 0 Td (innocuous)Tj 4.439 0 Td (and)Tj 1.7291 0 Td (anonymized)Tj 5.1647 0 Td (data)Tj 1.9162 0 Td (have)Tj 2.0579 0 Td (produced)Tj -32.2863 -1.2983 Td (unanticipated)Tj 5.7372 0 Td (ethical)Tj 2.8403 0 Td (questions)Tj 4.0195 0 Td (and)Tj 1.7291 0 Td (detrimental)Tj 4.8756 0 Td (impacts.)Tj -18.0055 -1.3039 Td (This)Tj -0.01 Tc 1.9899 0 Td (paper)Tj 2.4547 0 Td (is)Tj 0.8334 0 Td (a)Tj 0.6463 0 Td (result)Tj 2.4151 0 Td (of)Tj 1.0035 0 Td (a)Tj 0.6463 0 Td (two-year)Tj 3.6963 0 Td (National)Tj 3.6454 0 Td (Science)Tj 3.1634 0 Td (Foundation)Tj 4.8529 0 Td (\(NSF\)-funded)Tj 5.76 0 Td (project)Tj 2.9763 0 Td (that)Tj -35.2797 -1.2982 Td (established)Tj 4.5297 0 Td (the)Tj 1.4513 0 Td (Council)Tj 3.3619 0 Td (for)Tj 1.3663 0 Td (Big)Tj 1.5137 0 Td (Data,)Tj 2.3187 0 Td (Ethics,)Tj 2.8516 0 Td (and)Tj 1.7121 0 Td (Society,)Tj 3.2485 0 Td (a)Tj 0.6463 0 Td (group)Tj 2.5795 0 Td (of)Tj 1.0035 0 Td (20)Tj 1.1565 0 Td (scholars)Tj 3.4072 0 Td (from)Tj 2.1827 0 Td (a)Tj 0.6463 0 Td (wide)Tj -33.9758 -1.2983 Td (range)Tj 2.432 0 Td (of)Tj 1.0035 0 Td (social,)Tj 2.6419 0 Td (natural,)Tj 3.2541 0 Td (and)Tj 1.7121 0 Td (computational)Tj 5.9754 0 Td (sciences)Tj 3.4016 0 Td (\()Tj 0.83 0.64 0.02 0 k (http://bdes.datasociety.n)Tj 9.9609 0 Td (et/)Tj 0 g (\).)Tj 1.8028 0 Td (The)Tj 1.7688 0 Td (Council)Tj -33.9531 -1.3039 Td (was)Tj 1.678 0 Td (charged)Tj 3.3506 0 Td (with)Tj 1.9729 0 Td (providing)Tj 4.0932 0 Td (guidance)Tj 3.7927 0 Td (to)Tj 1.0091 0 Td (the)Tj 1.457 0 Td (NSF)Tj 1.9389 0 Td (on)Tj 1.2643 0 Td (how)Tj 1.9218 0 Td (to)Tj 1.0092 0 Td (best)Tj 1.7858 0 Td (encourage)Tj 4.2916 0 Td (ethical)Tj 2.8006 0 Td (practices)Tj 3.685 0 Td (in)Tj -36.0507 -1.2983 Td (scientific)Tj 3.7246 0 Td (and)Tj 1.7122 0 Td (engineering)Tj 4.9095 0 Td (research,)Tj 3.7247 0 Td (utilizing)Tj 3.4696 0 Td (big)Tj 1.44 0 Td (data)Tj 1.8879 0 Td (research)Tj 3.5092 0 Td (methods)Tj 3.6454 0 Td (and)Tj 1.7121 0 Td (infrastructures)Tj 6.0037 0 Td ([)Tj 0.83 0.64 0.02 0 k (1)Tj 0 g (].)Tj ET Q q 1 j 1 J 0 w 11.9999 0 0 11.9999 200.0125 483.7039 cm BT /F2 1 Tf 1 TL -0.0033 Tc 0 0 Td (1.)Tj 1.0441 0 Td (Acknowledge)Tj 6.6284 0 Td (that)Tj 2.0362 0 Td (data)Tj 2.2583 0 Td (are)Tj 1.696 0 Td (people)Tj 3.4253 0 Td (and)Tj 1.9937 0 Td (can)Tj 1.9276 0 Td (do)Tj 1.4409 0 Td (harm)Tj ET Q q 1 j 1 J 0 w 10 0 0 10 200.0125 466.696 cm BT /F6 1 Tf 1 TL -0.004 Tc 0 0 Td (One)Tj 1.9388 0 Td (of)Tj 1.0148 0 Td (the)Tj 1.4684 0 Td (most)Tj 2.2053 0 Td (fundamental)Tj 5.3178 0 Td (rules)Tj 2.1486 0 Td (of)Tj 1.0148 0 Td (responsible)Tj 4.7509 0 Td (big)Tj 1.4626 0 Td (data)Tj 1.9106 0 Td (research)Tj 3.5603 0 Td (is)Tj 0.8447 0 Td (the)Tj 1.4683 0 Td (steadfast)Tj 3.651 0 Td (recognition)Tj -32.7569 -1.2982 Td (that)Tj 1.7744 0 Td (most)Tj 2.211 0 Td (data)Tj 1.9106 0 Td (represent)Tj 3.9571 0 Td (or)Tj 1.0999 0 Td (impact)Tj 2.982 0 Td (people.)Tj 3.0784 0 Td (Simply)Tj 2.999 0 Td (starting)Tj 3.2655 0 Td (with)Tj 1.9956 0 Td (the)Tj 1.474 0 Td (assumption)Tj 4.8586 0 Td (that)Tj 1.7801 0 Td (all)Tj 1.1452 0 Td (data)Tj -34.5314 -1.3039 Td (are)Tj 1.4456 0 Td (people)Tj 2.8573 0 Td (until)Tj 2.109 0 Td (proven)Tj 3.0387 0 Td (otherwise)Tj 4.0875 0 Td (places)Tj 2.6249 0 Td (the)Tj 1.4683 0 Td (difficulty)Tj 3.8098 0 Td (of)Tj 1.0148 0 Td (disassociating)Tj 5.7429 0 Td (data)Tj 1.9106 0 Td (from)Tj 2.2053 0 Td (specific)Tj 3.1918 0 Td (indi-)Tj -35.5065 -1.2983 Td (viduals)Tj 3.0387 0 Td (front)Tj 2.2337 0 Td (and)Tj 1.7291 0 Td (center.)Tj 2.931 0 Td (This)Tj 1.9899 0 Td (logic)Tj 2.126 0 Td (is)Tj 0.8504 0 Td (readily)Tj 2.9423 0 Td (evident)Tj 3.1634 0 Td (for)Tj 1.389 0 Td (ªriskyº)Tj 2.9537 0 Td (datasets,)Tj 3.583 0 Td (e.g.,)Tj 1.7858 0 Td (social)Tj 2.4604 0 Td (media)Tj -33.1764 -1.2983 Td (with)Tj 1.9955 0 Td (inflammatory)Tj 5.709 0 Td (language,)Tj 3.9911 0 Td (but)Tj 1.5534 0 Td (even)Tj 2.0693 0 Td (seemingly)Tj 4.2292 0 Td (benign)Tj 2.9821 0 Td (data)Tj 1.9105 0 Td (can)Tj 1.6271 0 Td (contain)Tj 3.2485 0 Td (sensitive)Tj 3.6226 0 Td (and)Tj 1.7291 0 Td (private)Tj -34.6674 -1.2982 Td (information,)Tj 5.3007 0 Td (e.g.,)Tj 1.7858 0 Td (it)Tj 0.7824 0 Td (is)Tj 0.8504 0 Td (possible)Tj 3.4072 0 Td (to)Tj 1.0261 0 Td (extract)Tj 2.9367 0 Td (data)Tj 1.9105 0 Td (on)Tj 1.2756 0 Td (the)Tj 1.474 0 Td (exact)Tj 2.2621 0 Td (heart)Tj 2.2733 0 Td (rates)Tj 2.109 0 Td (of)Tj 1.0148 0 Td (people)Tj 2.8573 0 Td (from)Tj 2.2053 0 Td (YouTube)Tj -33.4712 -1.3039 Td (videos)Tj 2.7609 0 Td ([)Tj 0.83 0.64 0.02 0 k (2)Tj 0 g (].)Tj 1.5987 0 Td (Even)Tj 2.2167 0 Td (data)Tj 1.9162 0 Td (that)Tj 1.7745 0 Td (seemingly)Tj 4.2292 0 Td (have)Tj 2.0636 0 Td (nothing)Tj 3.3789 0 Td (to)Tj 1.0262 0 Td (do)Tj 1.2585 0 Td (with)Tj 1.9956 0 Td (people)Tj 2.8516 0 Td (might)Tj 2.6022 0 Td (impact)Tj 2.9764 0 Td (individuals')Tj -32.6492 -1.2983 Td (lives)Tj 1.9672 0 Td (in)Tj 1.0318 0 Td (unexpected)Tj 4.7905 0 Td (ways,)Tj 2.3754 0 Td (e.g.,)Tj 1.7802 0 Td (oceanographic)Tj 6.0661 0 Td (data)Tj 1.9105 0 Td (that)Tj 1.7745 0 Td (change)Tj 3.0387 0 Td (the)Tj 1.4627 0 Td (risk)Tj 1.7121 0 Td (profiles)Tj 3.1975 0 Td (of)Tj 1.0091 0 Td (communities')Tj -32.1163 -1.2983 Td (and)Tj 1.7177 0 Td (properties')Tj 4.4844 0 Td (values)Tj 2.6589 0 Td (or)Tj 1.0885 0 Td (Exchangeable)Tj 5.6806 0 Td (Image)Tj 2.6929 0 Td (Format)Tj 3.1691 0 Td (\(EXIF\))Tj 2.982 0 Td (records)Tj 3.1975 0 Td (from)Tj 2.1997 0 Td (photos)Tj 2.9253 0 Td (that)Tj 1.7688 0 Td (contain)Tj -34.5654 -1.3039 Td (location)Tj 3.4525 0 Td (coordinates)Tj 4.8869 0 Td (and)Tj 1.7291 0 Td (reveal)Tj 2.5739 0 Td (the)Tj 1.4683 0 Td (photographer's)Tj 6.2646 0 Td (movement)Tj 4.5127 0 Td (or)Tj 1.0998 0 Td (even)Tj 2.0693 0 Td (home)Tj 2.5001 0 Td (location.)Tj -29.361 -1.2983 Td (Harm)Tj 2.6135 0 Td (can)Tj 1.6271 0 Td (also)Tj 1.7688 0 Td (result)Tj 2.4491 0 Td (when)Tj 2.4038 0 Td (seemingly)Tj 4.2292 0 Td (innocuous)Tj 4.439 0 Td (datasets)Tj 3.3562 0 Td (about)Tj 2.4945 0 Td (population-wide)Tj 6.8428 0 Td (effects)Tj 2.7269 0 Td (are)Tj -36.1471 -1.2982 Td (used)Tj 2.0579 0 Td (to)Tj 1.0261 0 Td (shape)Tj 2.4832 0 Td (the)Tj 1.4683 0 Td (lives)Tj 1.9729 0 Td (of)Tj 1.0148 0 Td (individuals)Tj 4.6488 0 Td (or)Tj 1.0998 0 Td (stigmatize)Tj 4.269 0 Td (groups,)Tj 3.1918 0 Td (often)Tj 2.2847 0 Td (without)Tj 3.3335 0 Td (procedural)Tj 4.5694 0 Td (recourse)Tj -33.4202 -1.3039 Td ([)Tj 0.83 0.64 0.02 0 k (3)Tj 0 g (,)Tj 0.83 0.64 0.02 0 k (4)Tj 0 g (].)Tj 2.3017 0 Td (For)Tj 1.627 0 Td (example,)Tj 3.7701 0 Td (social)Tj 2.4605 0 Td (network)Tj 3.5489 0 Td (maps)Tj 2.3528 0 Td (for)Tj 1.3889 0 Td (services)Tj 3.3052 0 Td (such)Tj 2.0579 0 Td (as)Tj 1.0148 0 Td (Twitter)Tj 3.1748 0 Td (can)Tj 1.6271 0 Td (determine)Tj 4.32 0 Td (credit-wor-)Tj -32.9497 -1.2983 Td (thiness)Tj 3.0047 0 Td ([)Tj 0.83 0.64 0.02 0 k (5)Tj 0 g (],)Tj 1.5987 0 Td (opaque)Tj 3.1351 0 Td (recidivism)Tj 4.3993 0 Td (scores)Tj 2.6646 0 Td (can)Tj 1.6214 0 Td (shape)Tj 2.4831 0 Td (criminal)Tj 3.5943 0 Td (justice)Tj 2.778 0 Td (decisions)Tj 3.9004 0 Td (in)Tj 1.0318 0 Td (a)Tj 0.652 0 Td (racially)Tj 3.0954 0 Td (disparate)Tj -33.9588 -1.2983 Td (manner)Tj 3.3618 0 Td ([)Tj 0.83 0.64 0.02 0 k (6)Tj 0 g (],)Tj 1.6044 0 Td (and)Tj 1.7291 0 Td (categorization)Tj 5.8791 0 Td (based)Tj 2.4718 0 Td (on)Tj 1.2755 0 Td (zip)Tj 1.4174 0 Td (codes)Tj 2.4604 0 Td (resulted)Tj 3.4016 0 Td (in)Tj 1.0374 0 Td (less)Tj 1.6101 0 Td (access)Tj 2.6475 0 Td (to)Tj 1.0262 0 Td (Amazon)Tj 3.6226 0 Td (Prime)Tj -33.5449 -1.3039 Td (same-day)Tj 4.0251 0 Td (delivery)Tj 3.3902 0 Td (service)Tj 2.9424 0 Td (for)Tj 1.3833 0 Td (African-Americans)Tj 7.9199 0 Td (in)Tj 1.0375 0 Td (United)Tj 3.0217 0 Td (States)Tj 2.5115 0 Td (cities)Tj 2.262 0 Td ([)Tj 0.83 0.64 0.02 0 k (7)Tj 0 g (].)Tj 1.5988 0 Td (These)Tj 2.5681 0 Td (high-profile)Tj -32.6605 -1.2982 Td (cases)Tj 2.2223 0 Td (show)Tj 2.296 0 Td (that)Tj 1.7802 0 Td (apparently)Tj 4.4674 0 Td (neutral)Tj 3.067 0 Td (data)Tj 1.9162 0 Td (can)Tj 1.6271 0 Td (yield)Tj 2.1373 0 Td (discriminatory)Tj 6.1398 0 Td (outcomes,)Tj 4.3143 0 Td (thereby)Tj 3.2258 0 Td (com-)Tj -33.1934 -1.2983 Td (pounding)Tj 4.1272 0 Td (social)Tj 2.4604 0 Td (inequities.)Tj -5.3914 -1.2983 Td (Other)Tj 2.5908 0 Td (cases)Tj 2.2224 0 Td (show)Tj 2.3017 0 Td (that)Tj 1.7745 0 Td (ªpublicº)Tj 3.4922 0 Td (datasets)Tj 3.3619 0 Td (are)Tj 1.4456 0 Td (easily)Tj 2.4095 0 Td (adapted)Tj 3.3789 0 Td (for)Tj 1.3889 0 Td (highly)Tj 2.7099 0 Td (invasive)Tj 3.4299 0 Td (research)Tj 3.5547 0 Td (by)Tj -35.2571 -1.3039 Td (incorporating)Tj 5.7372 0 Td (other)Tj 2.3414 0 Td (data,)Tj 2.1317 0 Td (such)Tj 2.0522 0 Td (as)Tj 1.0092 0 Td (Hague)Tj 2.8346 0 Td (et)Tj 0.9354 0 Td (al.'s)Tj 1.6951 0 Td ([)Tj 0.83 0.64 0.02 0 k (8)Tj 0 g (])Tj 1.3663 0 Td (use)Tj 1.5251 0 Td (of)Tj 1.0091 0 Td (property)Tj 3.6737 0 Td (records)Tj 3.1974 0 Td (and)Tj 1.7235 0 Td (geographic)Tj 4.6148 0 Td (pro-)Tj -35.8467 -1.2983 Td (filing)Tj 2.3017 0 Td (techniques)Tj 4.5297 0 Td (to)Tj 1.0205 0 Td (allegedly)Tj 3.6907 0 Td (identify)Tj 3.2938 0 Td (the)Tj 1.4683 0 Td (pseudonymous)Tj 6.3042 0 Td (artist)Tj 2.2507 0 Td (Banksy)Tj 3.0954 0 Td ([)Tj 0.83 0.64 0.02 0 k (9)Tj 0 g (].)Tj 1.5988 0 Td (In)Tj 1.1112 0 Td (particular,)Tj 4.3369 0 Td (data)Tj -35.0019 -1.2982 Td (ungoverned)Tj 5.0172 0 Td (by)Tj 1.1792 0 Td (substantive)Tj 4.7055 0 Td (consent)Tj 3.3279 0 Td (practices,)Tj 3.9628 0 Td (whether)Tj 3.4809 0 Td (social)Tj 2.4548 0 Td (media)Tj 2.6929 0 Td (or)Tj 1.0998 0 Td (the)Tj 1.4684 0 Td (residual)Tj 3.3788 0 Td (DNA)Tj 2.3755 0 Td (we)Tj -35.1437 -1.3039 Td (continually)Tj 4.7224 0 Td (leave)Tj 2.1997 0 Td (behind)Tj 3.0217 0 Td (us,)Tj 1.3323 0 Td (may)Tj 1.9332 0 Td (seem)Tj 2.245 0 Td (public)Tj 2.7099 0 Td (but)Tj 1.5534 0 Td (can)Tj 1.6271 0 Td (cause)Tj 2.3868 0 Td (unintentional)Tj 5.6862 0 Td (breaches)Tj 3.6907 0 Td (of)Tj 1.0148 0 Td (privacy)Tj -34.1232 -1.2983 Td (and)Tj 1.7291 0 Td (other)Tj 2.347 0 Td (harms)Tj 2.7326 0 Td ([)Tj 0.83 0.64 0.02 0 k (9)Tj 0 g (,)Tj 0.83 0.64 0.02 0 k (10)Tj 0 g (].)Tj -5.6125 -1.2983 Td (Start)Tj 2.0976 0 Td (with)Tj 2.0012 0 Td (the)Tj 1.4684 0 Td (assumption)Tj 4.8642 0 Td (that)Tj 1.7745 0 Td (data)Tj 1.9162 0 Td (are)Tj 1.4456 0 Td (people)Tj 2.8573 0 Td (\(until)Tj 2.4492 0 Td (proven)Tj 3.0387 0 Td (otherwise\),)Tj 4.6488 0 Td (and)Tj 1.7348 0 Td (use)Tj 1.5307 0 Td (it)Tj 0.7823 0 Td (to)Tj 1.0262 0 Td (guide)Tj -34.8319 -1.3039 Td (your)Tj 2.0806 0 Td (analysis.)Tj 3.5489 0 Td (No)Tj 1.4627 0 Td (one)Tj 1.7008 0 Td (gets)Tj 1.7688 0 Td (an)Tj 1.2019 0 Td (automatic)Tj 4.2292 0 Td (pass)Tj 1.8936 0 Td (on)Tj 1.2755 0 Td (ethics.)Tj ET Q q 1 j 1 J 0 w 11.9999 0 0 11.9999 200.0125 112.9889 cm BT /F2 1 Tf 1 TL -0.0033 Tc 0 0 Td (2.)Tj 1.0441 0 Td (Recognize)Tj 5.1874 0 Td (that)Tj 2.0362 0 Td (privacy)Tj 3.6851 0 Td (is)Tj 1.0441 0 Td (more)Tj 2.641 0 Td (than)Tj 2.3197 0 Td (a)Tj 0.7701 0 Td (binary)Tj 3.1984 0 Td (value)Tj ET Q q 1 j 1 J 0 w 10 0 0 10 200.0125 95.9811 cm BT /F6 1 Tf 1 TL -0.004 Tc 0 0 Td (Breaches)Tj 3.7643 0 Td (of)Tj 1.0148 0 Td (privacy)Tj 3.1408 0 Td (are)Tj 1.4513 0 Td (key)Tj 1.5931 0 Td (means)Tj 2.8063 0 Td (by)Tj 1.1792 0 Td (which)Tj 2.6532 0 Td (big)Tj 1.457 0 Td (data)Tj 1.9162 0 Td (research)Tj 3.5546 0 Td (can)Tj 1.6271 0 Td (do)Tj 1.2529 0 Td (harm,)Tj 2.6022 0 Td (and)Tj 1.7291 0 Td (it)Tj 0.7824 0 Td (is)Tj 0.8504 0 Td (important)Tj -33.3749 -1.2982 Td (to)Tj 1.0261 0 Td (recognize)Tj 4.0592 0 Td (that)Tj 1.7801 0 Td (privacy)Tj 3.1408 0 Td (is)Tj 0.8504 0 Td (contextual)Tj 4.3936 0 Td ([)Tj 0.83 0.64 0.02 0 k (11)Tj 0 g (])Tj 1.8482 0 Td (and)Tj 1.7291 0 Td (situational)Tj 4.4107 0 Td ([)Tj 0.83 0.64 0.02 0 k (12)Tj 0 g (],)Tj 2.075 0 Td (not)Tj 1.576 0 Td (reducible)Tj 3.9402 0 Td (to)Tj 1.0204 0 Td (a)Tj 0.6577 0 Td (simple)Tj ET Q q 1 j 1 J 0 w 576 737.1 m 36 737.1 l 36 737.6 l 576 737.6 l f* 36 741.2598 169.285 23.6976 re W* n q 169.2283 0 0 23.6409 36 741.3165 cm q /I0 Do Q Q Q q 0 0 612 792 re W* n 1 j 1 J 0 w 7.9999 0 0 7.9999 391.9748 745.7952 cm BT /F0 1 Tf 1 TL -0.005 Tc 0 0 Td (Ten)Tj 1.9204 0 Td (Simple)Tj 3.2245 0 Td (Rules)Tj 2.7283 0 Td (for)Tj 1.3678 0 Td (Respons)Tj 3.8764 0 Td (ible)Tj 1.7362 0 Td (Big)Tj 1.637 0 Td (Data)Tj 2.3032 0 Td (Researc)Tj 3.6567 0 Td (h)Tj ET Q q 1 j 1 J 0 w 36 48.0002 m 576 48.0002 l 576 47.5002 l 36 47.5002 l f* 0.83 0.64 0.02 0 k 142.5827 34.9228 m 295.8236 34.9228 l h f* 0 g 7.9999 0 0 7.9999 36 36 cm BT /F0 1 Tf 1 TL -0.005 Tc 0 0 Td (PLOS)Tj 2.8559 0 Td (Computationa)Tj 6.1229 0 Td (l)Tj 0.4322 0 Td (Biology)Tj 3.4371 0 Td (|)Tj 0.83 0.64 0.02 0 k 0.4748 0 Td (https:/)Tj 2.6788 0 Td (/doi.org/10.13)Tj 6.0094 0 Td (71/journal.p)Tj 5.1166 0 Td (cbi.1005399)Tj 0 g 6.3497 0 Td (March)Tj 2.9551 0 Td (30,)Tj 1.5874 0 Td (2017)Tj 27.1207 0 Td (2)Tj 0.7654 0 Td (/)Tj ET endstream endobj 30 0 obj < >stream endstream endobj 31 0 obj < >stream endstream endobj 32 0 obj < >stream endstream endobj 33 0 obj < >stream endstream endobj 34 0 obj < >stream endstream endobj 35 0 obj < >stream endstream endobj 36 0 obj < >stream endstream endobj 37 0 obj < >stream endstream endobj 38 0 obj < >stream endstream endobj 39 0 obj < >stream endstream endobj 40 0 obj < >stream endstream endobj 41 0 obj < >stream endstream endobj 42 0 obj < >stream endstream endobj 43 0 obj < >stream BT 66.402 0 Td (10)Tj ET Q endstream endobj 44 0 obj < >/XObject< >>>/CropBox[0 0 612 792]/MediaBox[0 0 612 792]/Parent 9 0 R/Annots 45 0 R/Contents 46 0 R/TrimBox[0 0 612 792]>> endobj 45 0 obj [47 0 R 48 0 R 49 0 R 50 0 R 51 0 R 52 0 R 53 0 R 54 0 R 55 0 R 56 0 R 57 0 R 58 0 R 59 0 R 60 0 R 61 0 R 62 0 R 63 0 R] endobj 47 0 obj < >/Border[0 0 0]/Dest(Rpcbi.1005399.ref013)>> endobj 48 0 obj < >/Border[0 0 0]/Dest(Rpcbi.1005399.ref010)>> endobj 49 0 obj < >/Border[0 0 0]/Dest(Rpcbi.1005399.ref014)>> endobj 50 0 obj < >/Border[0 0 0]/Dest(Rpcbi.1005399.ref015)>> endobj 51 0 obj < >/Border[0 0 0]/Dest(Rpcbi.1005399.ref016)>> endobj 52 0 obj < >/Border[0 0 0]/Dest(Rpcbi.1005399.ref017)>> endobj 53 0 obj < >/Border[0 0 0]/Dest(Rpcbi.1005399.ref018)>> endobj 54 0 obj < >/Border[0 0 0]/Dest(Rpcbi.1005399.ref019)>> endobj 55 0 obj < >/Border[0 0 0]/Dest(Rpcbi.1005399.ref020)>> endobj 56 0 obj < >/Border[0 0 0]/Dest(Rpcbi.1005399.ref021)>> endobj 57 0 obj < >/Border[0 0 0]/Dest(Rpcbi.1005399.ref022)>> endobj 58 0 obj < >/Border[0 0 0]/Dest(Rpcbi.1005399.ref023)>> endobj 59 0 obj < >/Border[0 0 0]/Dest(Rpcbi.1005399.ref024)>> endobj 60 0 obj < >/Border[0 0 0]/Dest(Rpcbi.1005399.ref025)>> endobj 61 0 obj < >/Border[0 0 0]/Dest(Rpcbi.1005399.ref026)>> endobj 62 0 obj < >/Border[0 0 0]/Dest(Rpcbi.1005399.ref027)>> endobj 63 0 obj < >/Border[0 0 0]/A 64 0 R>> endobj 64 0 obj < > endobj 46 0 obj [65 0 R 66 0 R 67 0 R 68 0 R 69 0 R 70 0 R 71 0 R 72 0 R 73 0 R 74 0 R 75 0 R 76 0 R 77 0 R 78 0 R 79 0 R 80 0 R 81 0 R 82 0 R] endobj 65 0 obj < >stream q 0.83 0.64 0.02 0 k 355.1811 615.3449 m 364.7055 615.3449 l h f* 478.3181 511.3134 m 487.8425 511.3134 l h f* 412.4409 485.348 m 421.9654 485.348 l h f* 426.4441 485.348 m 435.9118 485.348 l h f* 507.2882 459.326 m 516.7559 459.326 l h f* 454.1102 315.9496 m 463.578 315.9496 l h f* 337.0961 302.9102 m 346.6205 302.9102 l h f* 351.0992 302.9102 m 360.6236 302.9102 l h f* 532.2331 263.9055 m 541.7575 263.9055 l h f* 449.2913 237.9402 m 458.8157 237.9402 l h f* 272.863 198.9354 m 282.3874 198.9354 l h f* 203.4142 172.9134 m 212.9386 172.9134 l h f* 529.3984 159.9307 m 538.8661 159.9307 l h f* 277.9087 133.9087 m 287.4331 133.9087 l h f* 538.3559 120.926 m 547.8803 120.926 l h f* 250.7528 81.9213 m 260.2772 81.9213 l h f* 0 g 1 j 1 J 0 w 10 0 0 10 200.0125 707.4141 cm BT /F6 1 Tf 1 TL -0.004 Tc 0 0 Td (public/private)Tj 5.7939 0 Td (binary.)Tj 3.0161 0 Td (Just)Tj 1.7291 0 Td (because)Tj 3.3052 0 Td (something)Tj 4.4277 0 Td (has)Tj 1.5363 0 Td (been)Tj 2.109 0 Td (shared)Tj 2.8573 0 Td (publicly)Tj 3.4016 0 Td (does)Tj 2.0239 0 Td (not)Tj 1.5647 0 Td (mean)Tj 2.4378 0 Td (any)Tj 1.6441 0 Td (sub-)Tj -35.8467 -1.3039 Td (sequent)Tj 3.3108 0 Td (use)Tj 1.5307 0 Td (would)Tj 2.7099 0 Td (be)Tj 1.1509 0 Td (unproblematic.)Tj 6.3382 0 Td (Looking)Tj 3.5489 0 Td (at)Tj 0.9525 0 Td (a)Tj 0.6519 0 Td (single)Tj 2.5342 0 Td (Instagram)Tj 4.2916 0 Td (photo)Tj 2.5795 0 Td (by)Tj 1.1792 0 Td (an)Tj 1.2019 0 Td (individual)Tj 4.286 0 Td (has)Tj -36.2662 -1.2982 Td (different)Tj 3.6566 0 Td (ethical)Tj 2.8403 0 Td (implications)Tj 5.176 0 Td (than)Tj 2.0296 0 Td (looking)Tj 3.2542 0 Td (at)Tj 0.9524 0 Td (someone's)Tj 4.388 0 Td (full)Tj 1.5251 0 Td (history)Tj 3.0103 0 Td (of)Tj 1.0148 0 Td (all)Tj 1.1452 0 Td (social)Tj 2.4605 0 Td (media)Tj 2.6929 0 Td (posts.)Tj -34.1459 -1.2983 Td (Privacy)Tj 3.1804 0 Td (depends)Tj 3.5546 0 Td (on)Tj 1.2756 0 Td (the)Tj 1.4683 0 Td (nature)Tj 2.8233 0 Td (of)Tj 1.0148 0 Td (the)Tj 1.4684 0 Td (data,)Tj 2.1373 0 Td (the)Tj 1.474 0 Td (context)Tj 3.1804 0 Td (in)Tj 1.0375 0 Td (which)Tj 2.6476 0 Td (they)Tj 1.9275 0 Td (were)Tj 2.1203 0 Td (created)Tj 3.1238 0 Td (and)Tj -32.4338 -1.2983 Td (obtained,)Tj 3.9514 0 Td (and)Tj 1.7235 0 Td (the)Tj 1.4626 0 Td (expectations)Tj 5.1761 0 Td (and)Tj 1.7234 0 Td (norms)Tj 2.8233 0 Td (of)Tj 1.0035 0 Td (those)Tj 2.33 0 Td (who)Tj 1.9333 0 Td (are)Tj 1.4399 0 Td (affected.)Tj 3.5433 0 Td (Understand)Tj 4.989 0 Td (that)Tj 1.7745 0 Td (your)Tj 2.0749 0 Td (atti-)Tj -35.9487 -1.3039 Td (tude)Tj 1.9955 0 Td (towards)Tj 3.4016 0 Td (acceptable)Tj 4.354 0 Td (use)Tj 1.5307 0 Td (and)Tj 1.7291 0 Td (privacy)Tj 3.1464 0 Td (may)Tj 1.9276 0 Td (not)Tj 1.5704 0 Td (correspond)Tj 4.7791 0 Td (with)Tj 2.0013 0 Td (those)Tj 2.3357 0 Td (whose)Tj 2.7213 0 Td (data)Tj 1.9162 0 Td (you)Tj 1.7064 0 Td (are)Tj -35.1153 -1.2982 Td (using,)Tj 2.6135 0 Td (as)Tj 1.0148 0 Td (privacy)Tj 3.1407 0 Td (preferences)Tj 4.7962 0 Td (differ)Tj 2.3868 0 Td (across)Tj 2.6759 0 Td (and)Tj 1.7291 0 Td (within)Tj 2.8176 0 Td (societies.)Tj -19.9784 -1.2983 Td (For)Tj 1.627 0 Td (example,)Tj 3.7758 0 Td (Tene)Tj 2.228 0 Td (and)Tj 1.7291 0 Td (Polonetsky)Tj 4.6204 0 Td ([)Tj 0.83 0.64 0.02 0 k (13)Tj 0 g (])Tj 1.8539 0 Td (explore)Tj 3.1748 0 Td (how)Tj 1.9389 0 Td (pushing)Tj 3.4355 0 Td (past)Tj 1.8255 0 Td (social)Tj 2.4605 0 Td (norms,)Tj 3.0557 0 Td (particularly)Tj -32.9213 -1.3039 Td (in)Tj 1.0318 0 Td (novel)Tj 2.4037 0 Td (situations)Tj 4.0876 0 Td (created)Tj 3.1237 0 Td (by)Tj 1.1792 0 Td (new)Tj 1.8765 0 Td (technologies,)Tj 5.4482 0 Td (is)Tj 0.8447 0 Td (perceived)Tj 4.0535 0 Td (by)Tj 1.1792 0 Td (individuals)Tj 4.6488 0 Td (as)Tj 1.0148 0 Td (ªcreepyº)Tj 3.6227 0 Td (even)Tj -34.5144 -1.2983 Td (when)Tj 2.4037 0 Td (they)Tj 1.9219 0 Td (do)Tj 1.2586 0 Td (not)Tj 1.5704 0 Td (violate)Tj 2.8516 0 Td (data)Tj 1.9162 0 Td (protection)Tj 4.3823 0 Td (regulations)Tj 4.6658 0 Td (or)Tj 1.0999 0 Td (privacy)Tj 3.1407 0 Td (laws.)Tj 2.1714 0 Td (Social)Tj 2.5738 0 Td (media)Tj 2.6929 0 Td (apps)Tj 2.0466 0 Td (that)Tj -34.6958 -1.2983 Td (utilize)Tj 2.6645 0 Td (users')Tj 2.4775 0 Td (locations)Tj 3.8097 0 Td (to)Tj 1.0261 0 Td (push)Tj 2.1544 0 Td (information,)Tj 5.3007 0 Td (corporate)Tj 4.0762 0 Td (tracking)Tj 3.5206 0 Td (of)Tj 1.0148 0 Td (individuals')Tj 4.8642 0 Td (social)Tj 2.4605 0 Td (media)Tj 2.6929 0 Td (and)Tj -36.0621 -1.3039 Td (private)Tj 2.9876 0 Td (communications)Tj 6.9676 0 Td (to)Tj 1.0261 0 Td (gain)Tj 1.9389 0 Td (customer)Tj 3.9515 0 Td (intelligence,)Tj 5.0229 0 Td (and)Tj 1.7348 0 Td (marketing)Tj 4.3427 0 Td (based)Tj 2.4718 0 Td (on)Tj 1.2755 0 Td (search)Tj 2.761 0 Td (pat-)Tj -34.4804 -1.2982 Td (terns)Tj 2.2223 0 Td (have)Tj 2.0579 0 Td (been)Tj 2.126 0 Td (perceived)Tj 4.0479 0 Td (by)Tj 1.1848 0 Td (some)Tj 2.3301 0 Td (to)Tj 1.0261 0 Td (be)Tj 1.1509 0 Td (ªcreepyº)Tj 3.617 0 Td (or)Tj 1.0998 0 Td (even)Tj 2.0693 0 Td (outright)Tj 3.4809 0 Td (breaches)Tj 3.6907 0 Td (of)Tj 1.0148 0 Td (privacy.)Tj 3.3675 0 Td (Like-)Tj -34.486 -1.2983 Td (wise,)Tj 2.1769 0 Td (distributing)Tj 4.9266 0 Td (health)Tj 2.6816 0 Td (records)Tj 3.2031 0 Td (is)Tj 0.8504 0 Td (a)Tj 0.652 0 Td (necessary)Tj 4.0195 0 Td (part)Tj 1.8425 0 Td (of)Tj 1.0148 0 Td (receiving)Tj 3.8664 0 Td (health)Tj 2.6759 0 Td (care,)Tj 2.0976 0 Td (but)Tj 1.5534 0 Td (this)Tj 1.6724 0 Td (same)Tj -33.2331 -1.3039 Td (sharing)Tj 3.1917 0 Td (brings)Tj 2.744 0 Td (new)Tj 1.8765 0 Td (ethical)Tj 2.8403 0 Td (concerns)Tj 3.8267 0 Td (when)Tj 2.4038 0 Td (it)Tj 0.7824 0 Td (goes)Tj 1.9785 0 Td (beyond)Tj 3.1861 0 Td (providers)Tj 4.0195 0 Td (to)Tj 1.0262 0 Td (marketers.)Tj -26.6795 -1.2983 Td (Privacy)Tj 3.1861 0 Td (also)Tj 1.7688 0 Td (goes)Tj 1.9786 0 Td (beyond)Tj 3.1861 0 Td (single)Tj 2.5341 0 Td (individuals)Tj 4.6488 0 Td (and)Tj 1.7292 0 Td (extends)Tj 3.2654 0 Td (to)Tj 1.0262 0 Td (groups)Tj 2.9707 0 Td ([)Tj 0.83 0.64 0.02 0 k (10)Tj 0 g (].)Tj 2.0749 0 Td (This)Tj 1.9899 0 Td (is)Tj 0.8447 0 Td (particularly)Tj -32.3997 -1.2982 Td (resonant)Tj 3.7133 0 Td (for)Tj 1.3833 0 Td (communities)Tj 5.4822 0 Td (who)Tj 1.9389 0 Td (have)Tj 2.0636 0 Td (been)Tj 2.1203 0 Td (on)Tj 1.2756 0 Td (the)Tj 1.4683 0 Td (receiving)Tj 3.8664 0 Td (end)Tj 1.7178 0 Td (of)Tj 1.0148 0 Td (discriminatory)Tj 6.1398 0 Td (data-driven)Tj -32.1843 -1.2983 Td (policies)Tj 3.2314 0 Td (historically,)Tj 4.8472 0 Td (such)Tj 2.0636 0 Td (as)Tj 1.0148 0 Td (the)Tj 1.4684 0 Td (practice)Tj 3.3789 0 Td (of)Tj 1.0148 0 Td (redlining)Tj 3.8834 0 Td ([)Tj 0.83 0.64 0.02 0 k (14)Tj 0 g (,)Tj 0.83 0.64 0.02 0 k 1.7405 0 Td (15)Tj 0 g (].)Tj 1.7347 0 Td (Other)Tj 2.5852 0 Td (examples)Tj 3.9118 0 Td (include)Tj 3.1861 0 Td (commu-)Tj -34.0608 -1.3039 Td (nity)Tj 1.7858 0 Td (mapsÐmade)Tj 5.4821 0 Td (to)Tj 1.0262 0 Td (identify)Tj 3.2938 0 Td (problematic)Tj 5.0343 0 Td (properties)Tj 4.2803 0 Td (or)Tj 1.0942 0 Td (an)Tj 1.2075 0 Td (assertion)Tj 3.7871 0 Td (of)Tj 1.0148 0 Td (land)Tj 1.9785 0 Td (rightsÐbeing)Tj -29.9846 -1.2983 Td (reused)Tj 2.8516 0 Td (by)Tj 1.1849 0 Td (others)Tj 2.7099 0 Td (to)Tj 1.0204 0 Td (identify)Tj 3.2995 0 Td (opportunities)Tj 5.6296 0 Td (for)Tj 1.3833 0 Td (redevelopment)Tj 6.2078 0 Td (or)Tj 1.0999 0 Td (exploitation)Tj 5.0003 0 Td ([)Tj 0.83 0.64 0.02 0 k (16)Tj 0 g (].)Tj 2.0749 0 Td (Thus,)Tj 2.4718 0 Td (reus-)Tj -34.9339 -1.2983 Td (ing)Tj 1.5023 0 Td (a)Tj 0.652 0 Td (seemingly)Tj 4.2292 0 Td (public)Tj 2.7099 0 Td (dataset)Tj 2.9934 0 Td (could)Tj 2.4548 0 Td (run)Tj 1.6668 0 Td (counter)Tj 3.3221 0 Td (to)Tj 1.0205 0 Td (the)Tj 1.474 0 Td (original)Tj 3.3222 0 Td (privacy)Tj 3.1464 0 Td (intents)Tj 2.965 0 Td (of)Tj 1.0148 0 Td (those)Tj 2.3358 0 Td (who)Tj -34.8092 -1.3039 Td (created)Tj 3.118 0 Td (it)Tj 0.7881 0 Td (and)Tj 1.7291 0 Td (raise)Tj 2.0806 0 Td (questions)Tj 4.0195 0 Td (about)Tj 2.4945 0 Td (whether)Tj 3.4752 0 Td (it)Tj 0.7824 0 Td (represents)Tj 4.32 0 Td (responsible)Tj 4.7565 0 Td (big)Tj 1.457 0 Td (data)Tj 1.9162 0 Td (research.)Tj -29.7409 -1.2982 Td (Situate)Tj 2.948 0 Td (and)Tj 1.7291 0 Td (contextualize)Tj 5.4935 0 Td (your)Tj 2.0863 0 Td (data)Tj 1.9105 0 Td (to)Tj 1.0262 0 Td (anticipate)Tj 4.1329 0 Td (privacy)Tj 3.1407 0 Td (breaches)Tj 3.6907 0 Td (and)Tj 1.7291 0 Td (minimize)Tj 4.0479 0 Td (harm.)Tj 2.5965 0 Td (The)Tj -35.7276 -1.2983 Td (availability)Tj 4.5297 0 Td (or)Tj 1.0998 0 Td (perceived)Tj 4.0479 0 Td (publicness)Tj 4.405 0 Td (of)Tj 1.0148 0 Td (data)Tj 1.9162 0 Td (does)Tj 2.0352 0 Td (not)Tj 1.5761 0 Td (guarantee)Tj 4.1442 0 Td (lack)Tj 1.8199 0 Td (of)Tj 1.0148 0 Td (harm,)Tj 2.6021 0 Td (nor)Tj 1.6441 0 Td (does)Tj 2.041 0 Td (it)Tj 0.7823 0 Td (mean)Tj -34.6731 -1.3039 Td (that)Tj 1.7744 0 Td (data)Tj 1.9162 0 Td (creators)Tj 3.4129 0 Td (consent)Tj 3.3279 0 Td (to)Tj 1.0261 0 Td (researchers)Tj 4.7112 0 Td (using)Tj 2.3867 0 Td (their)Tj 2.109 0 Td (data.)Tj ET Q q 1 j 1 J 0 w 11.9999 0 0 11.9999 200.0125 360 cm BT /F2 1 Tf 1 TL -0.0033 Tc 0 0 Td (3.)Tj 1.0441 0 Td (Guard)Tj 3.1512 0 Td (against)Tj 3.6898 0 Td (the)Tj 1.7102 0 Td (reidentification)Tj 7.3371 0 Td (of)Tj 1.1622 0 Td (your)Tj 2.3717 0 Td (data)Tj ET Q q 1 j 1 J 0 w 10 0 0 10 200.0125 342.9921 cm BT /F6 1 Tf 1 TL -0.004 Tc 0 0 Td (It)Tj 0.856 0 Td (is)Tj 0.8504 0 Td (problematic)Tj 5.0343 0 Td (to)Tj 1.0261 0 Td (assume)Tj 3.1465 0 Td (that)Tj 1.7744 0 Td (data)Tj 1.9163 0 Td (cannot)Tj 2.982 0 Td (be)Tj 1.1508 0 Td (reidentified.)Tj 5.074 0 Td (There)Tj 2.5795 0 Td (are)Tj 1.4514 0 Td (numerous)Tj 4.3029 0 Td (examples)Tj 3.9118 0 Td (of)Tj -36.0564 -1.2982 Td (researchers)Tj 4.7111 0 Td (with)Tj 1.9956 0 Td (good)Tj 2.228 0 Td (intentions)Tj 4.286 0 Td (and)Tj 1.7347 0 Td (seemingly)Tj 4.2293 0 Td (good)Tj 2.2224 0 Td (methods)Tj 3.685 0 Td (failing)Tj 2.7382 0 Td (to)Tj 1.0262 0 Td (anonymize)Tj 4.6374 0 Td (data)Tj 1.9106 0 Td (suffi-)Tj -35.4045 -1.2983 Td (ciently)Tj 2.8799 0 Td (to)Tj 1.0262 0 Td (prevent)Tj 3.2598 0 Td (the)Tj 1.4683 0 Td (later)Tj 1.9899 0 Td (identification)Tj 5.5899 0 Td (of)Tj 1.0148 0 Td (specific)Tj 3.1918 0 Td (individuals)Tj 4.6488 0 Td ([)Tj 0.83 0.64 0.02 0 k (17)Tj 0 g (];)Tj 2.0749 0 Td (in)Tj 1.0375 0 Td (other)Tj 2.3471 0 Td (cases,)Tj 2.4491 0 Td (these)Tj -32.978 -1.3039 Td (efforts)Tj 2.7552 0 Td (were)Tj 2.1203 0 Td (extremely)Tj 4.1499 0 Td (superficial)Tj 4.3427 0 Td ([)Tj 0.83 0.64 0.02 0 k (18)Tj 0 g (,)Tj 0.83 0.64 0.02 0 k 1.7404 0 Td (19)Tj 0 g (].)Tj 1.7348 0 Td (When)Tj 2.6872 0 Td (datasets)Tj 3.3562 0 Td (thought)Tj 3.3732 0 Td (to)Tj 1.0262 0 Td (be)Tj 1.1508 0 Td (anonymized)Tj 5.1647 0 Td (are)Tj 1.4457 0 Td (com-)Tj -35.0473 -1.2983 Td (bined)Tj 2.4944 0 Td (with)Tj 1.9956 0 Td (other)Tj 2.3471 0 Td (variables,)Tj 3.9458 0 Td (it)Tj 0.7823 0 Td (may)Tj 1.9276 0 Td (result)Tj 2.4491 0 Td (in)Tj 1.0375 0 Td (unexpected)Tj 4.7962 0 Td (reidentification,)Tj 6.6103 0 Td (much)Tj 2.5172 0 Td (like)Tj 1.6554 0 Td (a)Tj 0.6519 0 Td (chemical)Tj -33.2104 -1.2982 Td (reaction)Tj 3.4922 0 Td (resulting)Tj 3.7304 0 Td (from)Tj 2.2053 0 Td (the)Tj 1.4684 0 Td (addition)Tj 3.5999 0 Td (of)Tj 1.0148 0 Td (a)Tj 0.652 0 Td (final)Tj 2.0012 0 Td (ingredient.)Tj -16.968 -1.3039 Td (While)Tj 2.6532 0 Td (the)Tj 1.4683 0 Td (identificatory)Tj 5.5956 0 Td (power)Tj 2.7269 0 Td (of)Tj 1.0148 0 Td (birthdate,)Tj 4.0988 0 Td (gender,)Tj 3.2032 0 Td (and)Tj 1.7291 0 Td (zip)Tj 1.4173 0 Td (code)Tj 2.1033 0 Td (is)Tj 0.8504 0 Td (well)Tj 1.8198 0 Td (known)Tj 3.0047 0 Td ([)Tj 0.83 0.64 0.02 0 k (20)Tj 0 g (],)Tj 2.075 0 Td (there)Tj -34.9566 -1.2983 Td (are)Tj 1.4456 0 Td (a)Tj 0.6576 0 Td (number)Tj 3.4186 0 Td (of)Tj 1.0148 0 Td (other)Tj 2.3471 0 Td (parametersÐparticularly)Tj 10.1876 0 Td (the)Tj 1.4684 0 Td (metadata)Tj 3.8891 0 Td (associated)Tj 4.2576 0 Td (with)Tj 1.9956 0 Td (digital)Tj 2.7269 0 Td (activityÐ)Tj -33.4089 -1.2983 Td (that)Tj 1.7744 0 Td (may)Tj 1.9332 0 Td (be)Tj 1.1509 0 Td (as)Tj 1.0148 0 Td (or)Tj 1.0942 0 Td (even)Tj 2.0692 0 Td (more)Tj 2.3414 0 Td (useful)Tj 2.5966 0 Td (for)Tj 1.3889 0 Td (identifying)Tj 4.5751 0 Td (individuals)Tj 4.6488 0 Td ([)Tj 0.83 0.64 0.02 0 k (21)Tj 0 g (].)Tj 2.075 0 Td (Surprising)Tj 4.3936 0 Td (to)Tj 1.0262 0 Td (many,)Tj 2.6985 0 Td (unla-)Tj -34.7808 -1.3039 Td (beled)Tj 2.347 0 Td (network)Tj 3.549 0 Td (graphsÐsuch)Tj 5.6579 0 Td (as)Tj 1.0148 0 Td (location)Tj 3.4526 0 Td (and)Tj 1.7291 0 Td (movement,)Tj 4.7452 0 Td (DNA)Tj 2.3754 0 Td (profiles,)Tj 3.4242 0 Td (call)Tj 1.5704 0 Td (records)Tj 3.2088 0 Td (from)Tj -33.0744 -1.2983 Td (mobile)Tj 2.9876 0 Td (phone)Tj 2.744 0 Td (data,)Tj 2.1429 0 Td (and)Tj 1.7292 0 Td (even)Tj 2.0692 0 Td (high-resolution)Tj 6.4176 0 Td (satellite)Tj 3.2145 0 Td (images)Tj 2.9934 0 Td (of)Tj 1.0148 0 Td (the)Tj 1.4683 0 Td (earthÐcan)Tj 4.6034 0 Td (be)Tj 1.1509 0 Td (used)Tj 2.0636 0 Td (to)Tj -34.5994 -1.2982 Td (reidentify)Tj 4.0875 0 Td (people)Tj 2.8573 0 Td ([)Tj 0.83 0.64 0.02 0 k (22)Tj 0 g (].)Tj 2.0749 0 Td (More)Tj 2.4151 0 Td (important)Tj 4.2803 0 Td (than)Tj 2.0296 0 Td (specifying)Tj 4.235 0 Td (the)Tj 1.4683 0 Td (variables)Tj 3.719 0 Td (that)Tj 1.7802 0 Td (allow)Tj 2.3357 0 Td (for)Tj 1.389 0 Td (reidentifica-)Tj -32.6719 -1.3039 Td (tion,)Tj 2.0636 0 Td (however,)Tj 3.838 0 Td (is)Tj 0.8504 0 Td (the)Tj 1.4684 0 Td (realization)Tj 4.4277 0 Td (that)Tj 1.7801 0 Td (it)Tj 0.788 0 Td (is)Tj 0.8448 0 Td (difficult)Tj 3.3562 0 Td (to)Tj 1.0261 0 Td (recognize)Tj 4.0592 0 Td (these)Tj 2.2563 0 Td (vulnerable)Tj 4.3994 0 Td (points)Tj 2.7212 0 Td (a)Tj 0.652 0 Td (priori)Tj -34.5314 -1.2983 Td ([)Tj 0.83 0.64 0.02 0 k (23)Tj 0 g (].)Tj 2.0749 0 Td (Factors)Tj 3.1408 0 Td (discounted)Tj 4.6374 0 Td (today)Tj 2.4378 0 Td (as)Tj 1.0148 0 Td (irrelevant)Tj 4.0592 0 Td (or)Tj 1.0998 0 Td (inherently)Tj 4.3313 0 Td (harmlessÐsuch)Tj 6.5253 0 Td (as)Tj 1.0148 0 Td (battery)Tj 3.0104 0 Td (usageÐ)Tj -33.3465 -1.2983 Td (may)Tj 1.9275 0 Td (very)Tj 1.9219 0 Td (well)Tj 1.8198 0 Td (prove)Tj 2.4945 0 Td (to)Tj 1.0261 0 Td (be)Tj 1.1509 0 Td (a)Tj 0.6519 0 Td (significant)Tj 4.388 0 Td (vector)Tj 2.6986 0 Td (of)Tj 1.0148 0 Td (personal)Tj 3.634 0 Td (identification)Tj 5.5842 0 Td (tomorrow)Tj 4.286 0 Td ([)Tj 0.83 0.64 0.02 0 k (24)Tj 0 g (].)Tj 2.0749 0 Td (For)Tj -34.6731 -1.2982 Td (example,)Tj 3.7757 0 Td (the)Tj 1.4683 0 Td (addition)Tj 3.5943 0 Td (of)Tj 1.0148 0 Td (spatial)Tj 2.778 0 Td (location)Tj 3.4525 0 Td (can)Tj 1.6271 0 Td (turn)Tj 1.9672 0 Td (social)Tj 2.4548 0 Td (media)Tj 2.6929 0 Td (posts)Tj 2.2677 0 Td (into)Tj 1.8369 0 Td (a)Tj 0.6576 0 Td (means)Tj 2.8063 0 Td (of)Tj 1.0148 0 Td (identify-)Tj -33.4089 -1.3039 Td (ing)Tj 1.5023 0 Td (home)Tj 2.4945 0 Td (location)Tj 3.4526 0 Td ([)Tj 0.83 0.64 0.02 0 k (25)Tj 0 g (],)Tj 2.0749 0 Td (and)Tj 1.7291 0 Td (Google's)Tj 3.6624 0 Td (reverse)Tj 3.05 0 Td (image)Tj 2.6306 0 Td (search)Tj 2.7609 0 Td (can)Tj 1.6214 0 Td (connect)Tj 3.3959 0 Td (previously)Tj 4.3483 0 Td (separate)Tj -32.7229 -1.2983 Td (personal)Tj 3.6283 0 Td (activitiesÐsuch)Tj 6.4799 0 Td (as)Tj 1.0148 0 Td (dating)Tj 2.761 0 Td (and)Tj 1.7291 0 Td (professional)Tj 5.0513 0 Td (profilesÐin)Tj 4.9379 0 Td (unanticipated)Tj 5.7373 0 Td (ways)Tj 2.1543 0 Td ([)Tj 0.83 0.64 0.02 0 k (26)Tj 0 g (].)Tj 2.075 0 Td (Even)Tj -35.5689 -1.2983 Td (data)Tj 1.9048 0 Td (about)Tj 2.4888 0 Td (groupsÐªaggregate)Tj 8.056 0 Td (statisticsºÐcan)Tj 6.3042 0 Td (have)Tj 2.0523 0 Td (serious)Tj 3.0274 0 Td (implications)Tj 5.1704 0 Td (if)Tj 0.771 0 Td (they)Tj 1.9162 0 Td (reveal)Tj 2.5625 0 Td (that)Tj 1.7745 0 Td (cer-)Tj -36.0281 -1.3039 Td (tain)Tj 1.7688 0 Td (communities,)Tj 5.7089 0 Td (for)Tj 1.3833 0 Td (example,)Tj 3.7757 0 Td (suffer)Tj 2.4775 0 Td (from)Tj 2.2053 0 Td (stigmatized)Tj 4.7962 0 Td (diseases)Tj 3.3789 0 Td (or)Tj 1.0998 0 Td (social)Tj 2.4548 0 Td (behavior)Tj 3.7134 0 Td (much)Tj 2.5171 0 Td (more)Tj -35.2797 -1.2983 Td (than)Tj 2.0295 0 Td (others)Tj 2.7043 0 Td ([)Tj 0.83 0.64 0.02 0 k (27)Tj 0 g (].)Tj ET Q q 1 j 1 J 0 w 576 737.1 m 36 737.1 l 36 737.6 l 576 737.6 l f* 36 741.2598 169.285 23.6976 re W* n q 169.2283 0 0 23.6409 36 741.3165 cm q /I0 Do Q Q Q q 0 0 612 792 re W* n 1 j 1 J 0 w 7.9999 0 0 7.9999 391.9748 745.7952 cm BT /F0 1 Tf 1 TL -0.005 Tc 0 0 Td (Ten)Tj 1.9204 0 Td (Simple)Tj 3.2245 0 Td (Rules)Tj 2.7283 0 Td (for)Tj 1.3678 0 Td (Respons)Tj 3.8764 0 Td (ible)Tj 1.7362 0 Td (Big)Tj 1.637 0 Td (Data)Tj 2.3032 0 Td (Researc)Tj 3.6567 0 Td (h)Tj ET Q q 1 j 1 J 0 w 36 48.0002 m 576 48.0002 l 576 47.5002 l 36 47.5002 l f* 0.83 0.64 0.02 0 k 142.5827 34.9228 m 295.8236 34.9228 l h f* 0 g 7.9999 0 0 7.9999 36 36 cm BT /F0 1 Tf 1 TL -0.005 Tc 0 0 Td (PLOS)Tj 2.8559 0 Td (Computationa)Tj 6.1229 0 Td (l)Tj 0.4322 0 Td (Biology)Tj 3.4371 0 Td (|)Tj 0.83 0.64 0.02 0 k 0.4748 0 Td (https:/)Tj 2.6788 0 Td (/doi.org/10.13)Tj 6.0094 0 Td (71/journal.p)Tj 5.1166 0 Td (cbi.1005399)Tj 0 g 6.3497 0 Td (March)Tj 2.9551 0 Td (30,)Tj 1.5874 0 Td (2017)Tj 27.1207 0 Td (3)Tj 0.7654 0 Td (/)Tj ET endstream endobj 66 0 obj < >stream endstream endobj 67 0 obj < >stream endstream endobj 68 0 obj < >stream endstream endobj 69 0 obj < >stream endstream endobj 70 0 obj < >stream endstream endobj 71 0 obj < >stream endstream endobj 72 0 obj < >stream endstream endobj 73 0 obj < >stream endstream endobj 74 0 obj < >stream endstream endobj 75 0 obj < >stream endstream endobj 76 0 obj < >stream endstream endobj 77 0 obj < >stream endstream endobj 78 0 obj < >stream endstream endobj 79 0 obj < >stream endstream endobj 80 0 obj < >stream endstream endobj 81 0 obj < >stream endstream endobj 82 0 obj < >stream BT 66.402 0 Td (10)Tj ET Q endstream endobj 83 0 obj < >/XObject< >>>/CropBox[0 0 612 792]/MediaBox[0 0 612 792]/Parent 9 0 R/Annots 84 0 R/Contents 85 0 R/TrimBox[0 0 612 792]>> endobj 84 0 obj [86 0 R 87 0 R 88 0 R] endobj 86 0 obj < >/Border[0 0 0]/Dest(Rpcbi.1005399.ref028)>> endobj 87 0 obj < >/Border[0 0 0]/Dest(Rpcbi.1005399.ref029)>> endobj 88 0 obj < >/Border[0 0 0]/A 89 0 R>> endobj 89 0 obj < > endobj 85 0 obj [90 0 R 91 0 R 92 0 R 93 0 R 94 0 R] endobj 90 0 obj < >stream q 0.83 0.64 0.02 0 k 217.1339 544.1386 m 226.6583 544.1386 l h f* 291.8551 232.1008 m 301.3228 232.1008 l h f* 0 g 1 j 1 J 0 w 10 0 0 10 211.9748 707.4141 cm BT /F6 1 Tf 1 TL -0.004 Tc 0 0 Td (Identify)Tj 3.3732 0 Td (possible)Tj 3.4072 0 Td (vectors)Tj 3.0614 0 Td (of)Tj 1.0148 0 Td (reidentification)Tj 6.3779 0 Td (in)Tj 1.0375 0 Td (your)Tj 2.0806 0 Td (data.)Tj 2.1373 0 Td (Work)Tj 2.5625 0 Td (to)Tj 1.0261 0 Td (minimize)Tj 4.0422 0 Td (them)Tj 2.2904 0 Td (in)Tj 1.0375 0 Td (your)Tj -34.6448 -1.3039 Td (published)Tj 4.1215 0 Td (results)Tj 2.8119 0 Td (to)Tj 1.0262 0 Td (the)Tj 1.4683 0 Td (greatest)Tj 3.2938 0 Td (extent)Tj 2.6816 0 Td (possible.)Tj ET Q q 1 j 1 J 0 w 11.9999 0 0 11.9999 200.0125 666.1984 cm BT /F2 1 Tf 1 TL -0.0033 Tc 0 0 Td (4.)Tj 1.0441 0 Td (Practice)Tj 4.0677 0 Td (ethical)Tj 3.3591 0 Td (data)Tj 2.2583 0 Td (sharing)Tj ET Q q 1 j 1 J 0 w 10 0 0 10 200.0125 649.1905 cm BT /F6 1 Tf 1 TL -0.004 Tc 0 0 Td (For)Tj 1.627 0 Td (some)Tj 2.3301 0 Td (projects,)Tj 3.6 0 Td (sharing)Tj 3.1974 0 Td (data)Tj 1.9162 0 Td (is)Tj 0.8448 0 Td (an)Tj 1.2018 0 Td (expectation)Tj 4.8302 0 Td (of)Tj 1.0148 0 Td (the)Tj 1.4684 0 Td (human)Tj 3.0784 0 Td (participants)Tj 4.9549 0 Td (involved)Tj 3.651 0 Td (and)Tj 1.7291 0 Td (thus)Tj -35.4441 -1.2982 Td (a)Tj 0.6519 0 Td (key)Tj 1.5931 0 Td (part)Tj 1.8425 0 Td (of)Tj 1.0148 0 Td (ethical)Tj 2.8403 0 Td (research.)Tj 3.7814 0 Td (For)Tj 1.627 0 Td (example,)Tj 3.7758 0 Td (in)Tj 1.0318 0 Td (rare)Tj 1.8255 0 Td (genetic)Tj 3.067 0 Td (disease)Tj 3.0161 0 Td (research,)Tj 3.7814 0 Td (biological)Tj 4.0932 0 Td (samples)Tj -33.9418 -1.2983 Td (are)Tj 1.4456 0 Td (shared)Tj 2.8687 0 Td (in)Tj 1.0318 0 Td (the)Tj 1.474 0 Td (hope)Tj 2.194 0 Td (of)Tj 1.0148 0 Td (finding)Tj 3.1294 0 Td (cures,)Tj 2.5568 0 Td (making)Tj 3.2485 0 Td (dissemination)Tj 5.8507 0 Td (a)Tj 0.6519 0 Td (condition)Tj 4.1159 0 Td (of)Tj 1.0148 0 Td (participation.)Tj 5.5956 0 Td (In)Tj -36.1925 -1.3039 Td (other)Tj 2.347 0 Td (projects,)Tj 3.6057 0 Td (questions)Tj 4.0195 0 Td (of)Tj 1.0148 0 Td (the)Tj 1.4683 0 Td (larger)Tj 2.5342 0 Td (public)Tj 2.7042 0 Td (goodÐan)Tj 4.1329 0 Td (admittedly)Tj 4.5184 0 Td (difficult)Tj 3.3562 0 Td (to)Tj 1.0261 0 Td (define)Tj 2.6986 0 Td (category)Tj -33.4259 -1.2983 Td (Ðprovide)Tj 4.2122 0 Td (compelling)Tj 4.6828 0 Td (arguments)Tj 4.4674 0 Td (for)Tj 1.389 0 Td (sharing)Tj 3.1917 0 Td (data,)Tj 2.143 0 Td (e.g.,)Tj 1.7858 0 Td (the)Tj 1.4684 0 Td (NIH-sponsored)Tj 6.5366 0 Td (database)Tj 3.6397 0 Td (of)Tj 1.0148 0 Td (Geno-)Tj -34.5314 -1.2982 Td (types)Tj 2.2733 0 Td (and)Tj 1.7291 0 Td (Phenotypes)Tj 4.8416 0 Td (\(dbGaP\),)Tj 3.8664 0 Td (which)Tj 2.6532 0 Td (makes)Tj 2.7553 0 Td (deidentified)Tj 5.0059 0 Td (genomic)Tj 3.6794 0 Td (data)Tj 1.9105 0 Td (widely)Tj 2.8233 0 Td (available)Tj 3.668 0 Td (to)Tj -35.206 -1.3039 Td (researchers,)Tj 4.9379 0 Td (democratizing)Tj 5.981 0 Td (access,)Tj 2.8743 0 Td (or)Tj 1.0999 0 Td (the)Tj 1.4683 0 Td (justice)Tj 2.778 0 Td (claim)Tj 2.4094 0 Td (made)Tj 2.4264 0 Td (by)Tj 1.1792 0 Td (the)Tj 1.4684 0 Td (Institute)Tj 3.5829 0 Td (of)Tj 1.0205 0 Td (Medicine)Tj 3.9912 0 Td (about)Tj -35.2174 -1.2983 Td (the)Tj 1.4626 0 Td (value)Tj 2.3017 0 Td (of)Tj 1.0092 0 Td (mandating)Tj 4.558 0 Td (that)Tj 1.7745 0 Td (individual-level)Tj 6.429 0 Td (data)Tj 1.9105 0 Td (from)Tj 2.1997 0 Td (clinical)Tj 3.0727 0 Td (trials)Tj 2.194 0 Td (be)Tj 1.1452 0 Td (shared)Tj 2.863 0 Td (among)Tj 2.9877 0 Td (research-)Tj -33.9078 -1.2983 Td (ers)Tj 1.3719 0 Td ([)Tj 0.83 0.64 0.02 0 k (28)Tj 0 g (].)Tj 2.0806 0 Td (Asking)Tj 3.0387 0 Td (participants)Tj 4.9606 0 Td (for)Tj 1.3833 0 Td (broad,)Tj 2.795 0 Td (as)Tj 1.0148 0 Td (opposed)Tj 3.583 0 Td (to)Tj 1.0204 0 Td (narrowly)Tj 3.8381 0 Td (structured)Tj 4.354 0 Td (consent)Tj 3.3278 0 Td (for)Tj 1.389 0 Td (down-)Tj -34.1572 -1.3039 Td (stream)Tj 2.9253 0 Td (data)Tj 1.9162 0 Td (management)Tj 5.4311 0 Td (makes)Tj 2.7553 0 Td (it)Tj 0.788 0 Td (easier)Tj 2.5002 0 Td (to)Tj 1.0261 0 Td (share)Tj 2.3357 0 Td (data.)Tj 2.1373 0 Td (Careful)Tj 3.1748 0 Td (research)Tj 3.5603 0 Td (design)Tj 2.812 0 Td (and)Tj 1.7291 0 Td (guidance)Tj -33.0914 -1.2983 Td (from)Tj 2.2053 0 Td (IRBs)Tj 2.1203 0 Td (can)Tj 1.6271 0 Td (help)Tj 1.9332 0 Td (clarify)Tj 2.7042 0 Td (consent)Tj 3.3335 0 Td (processes.)Tj 4.1953 0 Td (However,)Tj 4.0762 0 Td (we)Tj 1.3266 0 Td (caution)Tj 3.2258 0 Td (that)Tj 1.7801 0 Td (even)Tj 2.0693 0 Td (when)Tj 2.4038 0 Td (broad)Tj 2.5682 0 Td (con-)Tj -35.5689 -1.2982 Td (sent)Tj 1.8481 0 Td (was)Tj 1.7008 0 Td (obtained)Tj 3.7304 0 Td (upfront,)Tj 3.5092 0 Td (researchers)Tj 4.7112 0 Td (should)Tj 2.914 0 Td (consider)Tj 3.651 0 Td (the)Tj 1.4683 0 Td (best)Tj 1.8085 0 Td (interests)Tj 3.5773 0 Td (of)Tj 1.0148 0 Td (the)Tj 1.4684 0 Td (human)Tj 3.0784 0 Td (partici-)Tj -34.4804 -1.3039 Td (pant,)Tj 2.245 0 Td (proactively)Tj 4.6147 0 Td (considering)Tj 4.9323 0 Td (the)Tj 1.4683 0 Td (likelihood)Tj 4.235 0 Td (of)Tj 1.0148 0 Td (privacy)Tj 3.1407 0 Td (breaches)Tj 3.6964 0 Td (and)Tj 1.7291 0 Td (reidentification)Tj 6.3779 0 Td (issues.)Tj -33.4542 -1.2983 Td (This)Tj 1.9842 0 Td (is)Tj 0.8504 0 Td (of)Tj 1.0148 0 Td (particular)Tj 4.1102 0 Td (concern)Tj 3.4639 0 Td (for)Tj 1.389 0 Td (human)Tj 3.0784 0 Td (DNA)Tj 2.3754 0 Td (data,)Tj 2.1373 0 Td (which)Tj 2.6532 0 Td (is)Tj 0.8447 0 Td (uniquely)Tj 3.7247 0 Td (identifiable.)Tj -26.43 -1.2983 Td (These)Tj 2.5681 0 Td (types)Tj 2.2734 0 Td (of)Tj 1.0148 0 Td (projects,)Tj 3.6057 0 Td (howeverÐin)Tj 5.3517 0 Td (which)Tj 2.6532 0 Td (rules)Tj 2.1487 0 Td (of)Tj 1.0148 0 Td (use)Tj 1.5307 0 Td (and)Tj 1.7291 0 Td (sharing)Tj 3.1975 0 Td (are)Tj 1.4456 0 Td (well)Tj 1.8199 0 Td (governed)Tj 3.9458 0 Td (by)Tj -35.4952 -1.2982 Td (informed)Tj 3.9741 0 Td (consent)Tj 3.3279 0 Td (and)Tj 1.7291 0 Td (right)Tj 2.1486 0 Td (of)Tj 1.0148 0 Td (withdrawalÐare)Tj 6.8485 0 Td (increasingly)Tj 5.0343 0 Td (the)Tj 1.4683 0 Td (exception)Tj 4.0932 0 Td (rather)Tj 2.6476 0 Td (than)Tj 2.0296 0 Td (the)Tj 1.4683 0 Td (rule)Tj -35.7843 -1.3039 Td (for)Tj 1.3832 0 Td (big)Tj 1.4627 0 Td (data.)Tj 2.1373 0 Td (In)Tj 1.1112 0 Td (our)Tj 1.6214 0 Td (digital)Tj 2.7269 0 Td (society,)Tj 3.1805 0 Td (we)Tj 1.3266 0 Td (are)Tj 1.4513 0 Td (followed)Tj 3.651 0 Td (by)Tj 1.1792 0 Td (data)Tj 1.9162 0 Td (clouds)Tj 2.812 0 Td (composed)Tj 4.3029 0 Td (of)Tj 1.0148 0 Td (the)Tj 1.474 0 Td (trace)Tj 2.1714 0 Td (ele-)Tj -34.9226 -1.2983 Td (ments)Tj 2.6702 0 Td (of)Tj 1.0148 0 Td (daily)Tj 2.1486 0 Td (lifeÐcredit)Tj 4.6828 0 Td (card)Tj 1.9729 0 Td (transactions,)Tj 5.3008 0 Td (medical)Tj 3.3618 0 Td (test)Tj 1.5988 0 Td (results,)Tj 3.033 0 Td (closed-circuit)Tj 5.6409 0 Td (television)Tj -31.4246 -1.2983 Td (\(CCTV\))Tj 3.5319 0 Td (images)Tj 2.9877 0 Td (and)Tj 1.7348 0 Td (video,)Tj 2.6249 0 Td (smart)Tj 2.5001 0 Td (phone)Tj 2.7439 0 Td (apps,)Tj 2.2791 0 Td (etc.Ðcollected)Tj 6.032 0 Td (under)Tj 2.6192 0 Td (mandatory)Tj 4.6148 0 Td (terms)Tj 2.4945 0 Td (of)Tj 1.0148 0 Td (ser-)Tj -35.1777 -1.3039 Td (vice)Tj 1.7858 0 Td (rather)Tj 2.6475 0 Td (than)Tj 2.0296 0 Td (responsible)Tj 4.7508 0 Td (research)Tj 3.5603 0 Td (design)Tj 2.812 0 Td (overseen)Tj 3.7304 0 Td (by)Tj 1.1848 0 Td (university)Tj 4.1896 0 Td (compliance)Tj 4.8302 0 Td (officers.)Tj 3.3789 0 Td (While)Tj -34.8999 -1.2983 Td (we)Tj 1.3266 0 Td (might)Tj 2.5965 0 Td (wish)Tj 2.0636 0 Td (to)Tj 1.0204 0 Td (have)Tj 2.0637 0 Td (the)Tj 1.4683 0 Td (standards)Tj 4.0819 0 Td (of)Tj 1.0148 0 Td (informed)Tj 3.9741 0 Td (consent)Tj 3.3335 0 Td (and)Tj 1.7292 0 Td (right)Tj 2.1486 0 Td (of)Tj 1.0148 0 Td (withdrawal,)Tj 4.9209 0 Td (these)Tj 2.2507 0 Td (in-)Tj -35.0076 -1.2982 Td (formal)Tj 2.8856 0 Td (big)Tj 1.457 0 Td (data)Tj 1.9162 0 Td (sources)Tj 3.1918 0 Td (are)Tj 1.4513 0 Td (gathered)Tj 3.6907 0 Td (by)Tj 1.1792 0 Td (agents)Tj 2.7553 0 Td (other)Tj 2.347 0 Td (than)Tj 2.0296 0 Td (the)Tj 1.4684 0 Td (researcherÐprivate)Tj 8.039 0 Td (software)Tj -32.4111 -1.3039 Td (companies,)Tj 4.7451 0 Td (state)Tj 2.0353 0 Td (agencies,)Tj 3.7927 0 Td (and)Tj 1.7291 0 Td (telecommunications)Tj 8.3622 0 Td (firms.)Tj 2.5568 0 Td (These)Tj 2.5625 0 Td (data)Tj 1.9162 0 Td (are)Tj 1.4514 0 Td (only)Tj 1.9729 0 Td (accessible)Tj 4.0932 0 Td (to)Tj -35.2174 -1.2983 Td (researchers)Tj 4.7111 0 Td (after)Tj 2.0353 0 Td (their)Tj 2.1089 0 Td (creation,)Tj 3.7191 0 Td (making)Tj 3.2484 0 Td (it)Tj 0.7881 0 Td (impossible)Tj 4.4957 0 Td (to)Tj 1.0261 0 Td (gain)Tj 1.9332 0 Td (informed)Tj 3.9742 0 Td (consent)Tj 3.3278 0 Td (a)Tj 0.652 0 Td (priori,)Tj 2.7496 0 Td (and)Tj -34.7695 -1.2983 Td (contacting)Tj 4.4333 0 Td (the)Tj 1.4683 0 Td (human)Tj 3.0785 0 Td (participants)Tj 4.9605 0 Td (retroactively)Tj 5.1931 0 Td (for)Tj 1.3833 0 Td (permission)Tj 4.6658 0 Td (is)Tj 0.8447 0 Td (often)Tj 2.2847 0 Td (forbidden)Tj 4.1896 0 Td (by)Tj 1.1792 0 Td (the)Tj -33.681 -1.2982 Td (owner)Tj 2.7552 0 Td (of)Tj 1.0148 0 Td (the)Tj 1.4683 0 Td (data)Tj 1.9106 0 Td (or)Tj 1.0998 0 Td (is)Tj 0.8504 0 Td (impossible)Tj 4.4957 0 Td (to)Tj 1.0262 0 Td (do)Tj 1.2529 0 Td (at)Tj 0.9524 0 Td (scale.)Tj -15.6301 -1.304 Td (Of)Tj 1.2585 0 Td (course,)Tj 3.0558 0 Td (researchers)Tj 4.7111 0 Td (within)Tj 2.8176 0 Td (software)Tj 3.583 0 Td (companies)Tj 4.5241 0 Td (and)Tj 1.7291 0 Td (state)Tj 2.0353 0 Td (institutions)Tj 4.7678 0 Td (collecting)Tj 4.0705 0 Td (these)Tj -33.749 -1.2982 Td (data)Tj 1.9105 0 Td (have)Tj 2.0636 0 Td (a)Tj 0.652 0 Td (special)Tj 2.8913 0 Td (responsibility)Tj 5.6182 0 Td (to)Tj 1.0262 0 Td (address)Tj 3.2258 0 Td (the)Tj 1.4683 0 Td (terms)Tj 2.4945 0 Td (under)Tj 2.6192 0 Td (which)Tj 2.6532 0 Td (data)Tj 1.9105 0 Td (are)Tj 1.4513 0 Td (collected;)Tj 3.9629 0 Td (but)Tj 1.5533 0 Td (that)Tj -35.5008 -1.2983 Td (does)Tj 2.0352 0 Td (not)Tj 1.5761 0 Td (exempt)Tj 3.1691 0 Td (the)Tj 1.4683 0 Td (end-user)Tj 3.7531 0 Td (of)Tj 1.0148 0 Td (shared)Tj 2.8686 0 Td (data.)Tj 2.1373 0 Td (In)Tj 1.1055 0 Td (short,)Tj 2.5115 0 Td (the)Tj 1.474 0 Td (burden)Tj 3.1238 0 Td (of)Tj 1.0148 0 Td (ethical)Tj 2.8403 0 Td (use)Tj 1.5307 0 Td (\(see)Tj 1.7688 0 Td (Rules)Tj 2.3981 0 Td (1)Tj 0.6973 0 Td (to)Tj -36.4873 -1.3039 Td (3\))Tj 1.0318 0 Td (and)Tj 1.7347 0 Td (sharing)Tj 3.1918 0 Td (is)Tj 0.8504 0 Td (placed)Tj 2.7893 0 Td (on)Tj 1.2756 0 Td (the)Tj 1.474 0 Td (researcher,)Tj 4.5751 0 Td (since)Tj 2.2393 0 Td (the)Tj 1.474 0 Td (terms)Tj 2.4945 0 Td (of)Tj 1.0148 0 Td (service)Tj 2.9424 0 Td (under)Tj 2.6191 0 Td (which)Tj 2.6476 0 Td (the)Tj 1.474 0 Td (human)Tj -33.8284 -1.2983 Td (subjects')Tj 3.5943 0 Td (data)Tj 1.9105 0 Td (were)Tj 2.126 0 Td (produced)Tj 4.0421 0 Td (can)Tj 1.6271 0 Td (often)Tj 2.2847 0 Td (be)Tj 1.1509 0 Td (extremely)Tj 4.1442 0 Td (broad)Tj 2.5682 0 Td (with)Tj 1.9956 0 Td (little)Tj 2.0012 0 Td (protection)Tj 4.3824 0 Td (for)Tj 1.3889 0 Td (breaches)Tj -33.2161 -1.2982 Td (of)Tj 1.0147 0 Td (privacy.)Tj 3.3676 0 Td (In)Tj 1.1055 0 Td (these)Tj 2.2564 0 Td (circumstances,)Tj 6.1284 0 Td (researchers)Tj 4.7112 0 Td (must)Tj 2.228 0 Td (balance)Tj 3.2371 0 Td (the)Tj 1.4684 0 Td (requirements)Tj 5.5615 0 Td (from)Tj 2.2053 0 Td (funding)Tj -33.2841 -1.3039 Td (agencies)Tj 3.5659 0 Td (to)Tj 1.0261 0 Td (share)Tj 2.3358 0 Td (data)Tj 1.9162 0 Td ([)Tj 0.83 0.64 0.02 0 k (29)Tj 0 g (])Tj 1.8482 0 Td (with)Tj 1.9955 0 Td (their)Tj 2.109 0 Td (responsibilities)Tj 6.2135 0 Td (to)Tj 1.0261 0 Td (the)Tj 1.4684 0 Td (human)Tj 3.0784 0 Td (beings)Tj 2.7949 0 Td (behind)Tj 3.0217 0 Td (the)Tj 1.4684 0 Td (data)Tj 1.9162 0 Td (they)Tj -35.7843 -1.2983 Td (acquired.)Tj 3.9231 0 Td (A)Tj 0.9071 0 Td (researcher)Tj 4.3483 0 Td (needs)Tj 2.5058 0 Td (to)Tj 1.0261 0 Td (inform)Tj 3.0217 0 Td (funding)Tj 3.3902 0 Td (agencies)Tj 3.566 0 Td (about)Tj 2.4945 0 Td (possible)Tj 3.4129 0 Td (ethical)Tj 2.8403 0 Td (concerns)Tj -31.436 -1.2983 Td (before)Tj 2.7382 0 Td (the)Tj 1.4683 0 Td (research)Tj 3.5603 0 Td (begins)Tj 2.7893 0 Td (and)Tj 1.7291 0 Td (guard)Tj 2.5455 0 Td (against)Tj 3.0274 0 Td (reidentification)Tj 6.3836 0 Td (before)Tj 2.7382 0 Td (sharing.)Tj -25.7837 -1.3039 Td (Share)Tj 2.4548 0 Td (data)Tj 1.9162 0 Td (as)Tj 1.0148 0 Td (specified)Tj 3.719 0 Td (in)Tj 1.0318 0 Td (research)Tj 3.5546 0 Td (protocols,)Tj 4.1839 0 Td (but)Tj 1.5534 0 Td (proactively)Tj 4.6204 0 Td (address)Tj 3.2259 0 Td (concerns)Tj 3.8267 0 Td (of)Tj 1.0148 0 Td (potential)Tj -33.3125 -1.2983 Td (harm)Tj 2.3697 0 Td (from)Tj 2.2053 0 Td (informally)Tj 4.4051 0 Td (collected)Tj 3.7417 0 Td (big)Tj 1.457 0 Td (data.)Tj ET Q q 1 j 1 J 0 w 11.9999 0 0 11.9999 200.0125 153.0141 cm BT /F2 1 Tf 1 TL -0.0033 Tc 0 0 Td (5.)Tj 1.0441 0 Td (Consider)Tj 4.526 0 Td (the)Tj 1.7055 0 Td (strengths)Tj 4.7386 0 Td (and)Tj 1.989 0 Td (limitations)Tj 5.1922 0 Td (of)Tj 1.1575 0 Td (your)Tj 2.3716 0 Td (data;)Tj 2.589 0 Td (big)Tj 1.7197 0 Td (does)Tj 2.5371 0 Td (not)Tj -29.5703 -1.1669 Td (automatically)Tj 6.5575 0 Td (mean)Tj 2.8063 0 Td (better)Tj ET Q q 1 j 1 J 0 w 10 0 0 10 200.0125 122.0031 cm BT /F6 1 Tf 1 TL -0.004 Tc 0 0 Td (In)Tj 1.1055 0 Td (order)Tj 2.4207 0 Td (to)Tj 1.0262 0 Td (do)Tj 1.2585 0 Td (both)Tj 2.058 0 Td (accurate)Tj 3.5546 0 Td (and)Tj 1.7291 0 Td (responsible)Tj 4.7565 0 Td (big)Tj 1.457 0 Td (data)Tj 1.9162 0 Td (research,)Tj 3.7814 0 Td (it)Tj 0.7824 0 Td (is)Tj 0.8504 0 Td (important)Tj 4.2803 0 Td (to)Tj 1.0261 0 Td (ground)Tj 3.1691 0 Td (data-)Tj -35.172 -1.2982 Td (sets)Tj 1.661 0 Td (in)Tj 1.0375 0 Td (their)Tj 2.109 0 Td (proper)Tj 2.9253 0 Td (context)Tj 3.1861 0 Td (including)Tj 4.0422 0 Td (conflicts)Tj 3.583 0 Td (of)Tj 1.0148 0 Td (interests.)Tj 3.7984 0 Td (Context)Tj 3.4242 0 Td (also)Tj 1.7688 0 Td (affects)Tj 2.7326 0 Td (every)Tj 2.3471 0 Td (stage)Tj 2.2053 0 Td (of)Tj -35.8353 -1.3039 Td (research:)Tj 3.7813 0 Td (from)Tj 2.2054 0 Td (data)Tj 1.9105 0 Td (acquisition,)Tj 4.8586 0 Td (to)Tj 1.0204 0 Td (cleaning,)Tj 3.8041 0 Td (to)Tj 1.0261 0 Td (interpretation)Tj 5.794 0 Td (of)Tj 1.0148 0 Td (findings,)Tj 3.7191 0 Td (and)Tj 1.7291 0 Td (dissemination)Tj 5.8506 0 Td (of)Tj -36.714 -1.2983 Td (the)Tj 1.4683 0 Td (results.)Tj 3.033 0 Td (During)Tj 3.1351 0 Td (the)Tj 1.474 0 Td (step)Tj 1.8199 0 Td (of)Tj 1.0148 0 Td (data)Tj 1.9105 0 Td (acquisition,)Tj 4.8529 0 Td (it)Tj 0.788 0 Td (is)Tj 0.8504 0 Td (crucial)Tj 2.9083 0 Td (to)Tj 1.0262 0 Td (understand)Tj 4.7905 0 Td (both)Tj 2.0579 0 Td (the)Tj 1.474 0 Td (source)Tj 2.829 0 Td (of)Tj ET Q q 1 j 1 J 0 w 576 737.1 m 36 737.1 l 36 737.6 l 576 737.6 l f* 36 741.2598 169.285 23.6976 re W* n q 169.2283 0 0 23.6409 36 741.3165 cm q /I0 Do Q Q Q q 0 0 612 792 re W* n 1 j 1 J 0 w 7.9999 0 0 7.9999 391.9748 745.7952 cm BT /F0 1 Tf 1 TL -0.005 Tc 0 0 Td (Ten)Tj 1.9204 0 Td (Simple)Tj 3.2245 0 Td (Rules)Tj 2.7283 0 Td (for)Tj 1.3678 0 Td (Respons)Tj 3.8764 0 Td (ible)Tj 1.7362 0 Td (Big)Tj 1.637 0 Td (Data)Tj 2.3032 0 Td (Researc)Tj 3.6567 0 Td (h)Tj ET Q q 1 j 1 J 0 w 36 48.0002 m 576 48.0002 l 576 47.5002 l 36 47.5002 l f* 0.83 0.64 0.02 0 k 142.5827 34.9228 m 295.8236 34.9228 l h f* 0 g 7.9999 0 0 7.9999 36 36 cm BT /F0 1 Tf 1 TL -0.005 Tc 0 0 Td (PLOS)Tj 2.8559 0 Td (Computationa)Tj 6.1229 0 Td (l)Tj 0.4322 0 Td (Biology)Tj 3.4371 0 Td (|)Tj 0.83 0.64 0.02 0 k 0.4748 0 Td (https:/)Tj 2.6788 0 Td (/doi.org/10.13)Tj 6.0094 0 Td (71/journal.p)Tj 5.1166 0 Td (cbi.1005399)Tj 0 g 6.3497 0 Td (March)Tj 2.9551 0 Td (30,)Tj 1.5874 0 Td (2017)Tj 27.1207 0 Td (4)Tj 0.7654 0 Td (/)Tj ET endstream endobj 91 0 obj < >stream endstream endobj 92 0 obj < >stream endstream endobj 93 0 obj < >stream endstream endobj 94 0 obj < >stream BT 66.402 0 Td (10)Tj ET Q endstream endobj 95 0 obj < >/XObject< >>>/CropBox[0 0 612 792]/MediaBox[0 0 612 792]/Parent 9 0 R/Annots 97 0 R/Contents 98 0 R/TrimBox[0 0 612 792]>> endobj 96 0 obj < > endobj 99 0 obj < > endobj 100 0 obj < >stream %!PS-AdobeFont-1.0 %%This font is a conversion from MinionPro-Bold with the following copyright notice: %\050c\051 2000, 2002 Adobe Systems Incorporated. (extracted from544.html)
uÁOÕ*Ý®ÅÓÙ_pÙ§½Û øäÖZDÀ#Ä3û UõàÚ^&eèXÖÔñ÷ X¨Þ Æ×Ì `0RâÚB¢Òï{§:M¾(¤í5óö fÄ¬ë=æ¨t% (*\YwÜÐ&(PP³ÂûD¨dÏ·ôqû9µD9ª7Òèì² éùEX<Ü:×;²0( ´0vh´ß´~ ½käF8Õ¯ÜÏ¨·L a6bé¨ns&Ù ¿åE9u{ÞÇeÂ*Vw'¯(5#¦@ì åÂw^^mµ£HqêõMKM^Ëí£ÕBÀgÎÏÇàAE&I÷.®ïG1bm®ÇùÏMyr ±Þú`Ð¿¼X=ø®üÑø5*µärºi¤Í/ã¿òu$ÙæµU8¹é&rgeJ\³Uy>]¿ÙûqØËUÆ×E>{F0é è ò'àÅ å`ÒQ§fj¬Ü°Ù3?º *U@*U(lóÒdÆ¶,0+¹_+Gð±v!^² üZ?øÃ$x¿Ì» VÐzè@ ?N¹?}|¼jpvÖ_NÜ¥­ÙzKZi ZkPR{Q§wð)=&ß'»T¶P×uô "YbnfBèæÎ Þ Ú!oXöghVØë6Ó¶(×dÝRéðE Õ¹äË0 tFµ R?É©äWÂ`ÝXöf}ïï­ùñEaHt:KwøÚ` ê.*ë<¹{ç¢á°vV/d¼q                                0000000000000000000000000000000000000000000000000000000000000000 0000000000000000000000000000000000000000000000000000000000000000 0000000000000000000000000000000000000000000000000000000000000000 0000000000000000000000000000000000000000000000000000000000000000 0000000000000000000000000000000000000000000000000000000000000000 0000000000000000000000000000000000000000000000000000000000000000 0000000000000000000000000000000000000000000000000000000000000000 cleartomark endstream endobj 97 0 obj [101 0 R 102 0 R] endobj 101 0 obj < >/Border[0 0 0]/Dest(Rpcbi.1005399.ref030)>> endobj 102 0 obj < >/Border[0 0 0]/A 103 0 R>> endobj 103 0 obj < > endobj 98 0 obj [104 0 R 105 0 R 106 0 R 107 0 R] endobj 104 0 obj < >stream q 0.83 0.64 0.02 0 k 488.0693 146.948 m 497.5937 146.948 l h f* 0 g 1 j 1 J 0 w 10 0 0 10 200.0125 707.4141 cm BT /F6 1 Tf 1 TL -0.004 Tc 0 0 Td (the)Tj 1.4683 0 Td (data)Tj 1.9162 0 Td (and)Tj 1.7291 0 Td (the)Tj 1.4684 0 Td (rules)Tj 2.1486 0 Td (and)Tj 1.7291 0 Td (regulations)Tj 4.6715 0 Td (with)Tj 1.9956 0 Td (which)Tj 2.6532 0 Td (they)Tj 1.9219 0 Td (were)Tj 2.1259 0 Td (gathered.)Tj 3.9175 0 Td (This)Tj 1.9842 0 Td (is)Tj 0.8504 0 Td (especially)Tj 4.0138 0 Td (impor-)Tj -34.5937 -1.3039 Td (tant)Tj 1.7971 0 Td (in)Tj 1.0375 0 Td (cases)Tj 2.2223 0 Td (of)Tj 1.0148 0 Td (research)Tj 3.5546 0 Td (conducted)Tj 4.4277 0 Td (in)Tj 1.0375 0 Td (relatively)Tj 3.8381 0 Td (loose)Tj 2.2677 0 Td (regulatory)Tj 4.3143 0 Td (environments,)Tj 6.0207 0 Td (in)Tj 1.0319 0 Td (which)Tj 2.6532 0 Td (use)Tj 1.5307 0 Td (of)Tj -36.7481 -1.2982 Td (answers)Tj 3.4015 0 Td (to)Tj 1.0261 0 Td (research)Tj 3.5547 0 Td (questions)Tj 4.0195 0 Td (may)Tj 1.9332 0 Td (conflict)Tj 3.2201 0 Td (with)Tj 1.9956 0 Td (the)Tj 1.4683 0 Td (expectations)Tj 5.1874 0 Td (of)Tj 1.0148 0 Td (those)Tj 2.3414 0 Td (who)Tj 1.9389 0 Td (provided)Tj 3.8154 0 Td (the)Tj -34.9169 -1.2983 Td (data.)Tj 2.1373 0 Td (One)Tj 1.9388 0 Td (possible)Tj 3.4129 0 Td (approach)Tj 3.9515 0 Td (might)Tj 2.5965 0 Td (be)Tj 1.1509 0 Td (the)Tj 1.4683 0 Td (ethical)Tj 2.8403 0 Td (norms)Tj 2.829 0 Td (employed)Tj 4.1385 0 Td (to)Tj 1.0262 0 Td (track)Tj 2.245 0 Td (the)Tj 1.4683 0 Td (provenance)Tj 4.8699 0 Td (of)Tj -36.0734 -1.2983 Td (artifacts,)Tj 3.6169 0 Td (often)Tj 2.2904 0 Td (in)Tj 1.0318 0 Td (cooperation)Tj 5.023 0 Td (and)Tj 1.7348 0 Td (collaboration)Tj 5.5162 0 Td (with)Tj 1.9955 0 Td (the)Tj 1.4684 0 Td (communities)Tj 5.4821 0 Td (from)Tj 2.2111 0 Td (which)Tj 2.6475 0 Td (they)Tj 1.9219 0 Td (come)Tj -34.9396 -1.3039 Td (\(e.g.,)Tj 2.1259 0 Td (archaeologists)Tj 5.845 0 Td (working)Tj 3.5603 0 Td (in)Tj 1.0318 0 Td (indigenous)Tj 4.6658 0 Td (communities)Tj 5.4822 0 Td (to)Tj 1.0261 0 Td (determine)Tj 4.3257 0 Td (the)Tj 1.4683 0 Td (disposition)Tj 4.6488 0 Td (of)Tj 1.0148 0 Td (mate-)Tj -35.1947 -1.2982 Td (rial)Tj 1.5363 0 Td (culture\).)Tj 3.6057 0 Td (In)Tj 1.1055 0 Td (a)Tj 0.6576 0 Td (similar)Tj 2.9877 0 Td (manner,)Tj 3.5886 0 Td (computer)Tj 4.1103 0 Td (scientists)Tj 3.8267 0 Td (use)Tj 1.5307 0 Td (data)Tj 1.9162 0 Td (lineage)Tj 3.0274 0 Td (techniques)Tj 4.5297 0 Td (to)Tj 1.0262 0 Td (track)Tj 2.2393 0 Td (the)Tj -35.6879 -1.2983 Td (evolution)Tj 3.9968 0 Td (of)Tj 1.0148 0 Td (a)Tj 0.6576 0 Td (dataset)Tj 2.9934 0 Td (and)Tj 1.7348 0 Td (often)Tj 2.2847 0 Td (to)Tj 1.0204 0 Td (trace)Tj 2.1714 0 Td (bugs)Tj 2.0806 0 Td (in)Tj 1.0375 0 Td (the)Tj 1.4683 0 Td (data.)Tj -19.2641 -1.3039 Td (Being)Tj 2.5058 0 Td (mindful)Tj 3.4469 0 Td (of)Tj 1.0148 0 Td (the)Tj 1.4683 0 Td (data's)Tj 2.4888 0 Td (context)Tj 3.1861 0 Td (provides)Tj 3.651 0 Td (the)Tj 1.4684 0 Td (foundation)Tj 4.6714 0 Td (for)Tj 1.389 0 Td (clarifying)Tj 3.9855 0 Td (when)Tj 2.4038 0 Td (your)Tj 2.0806 0 Td (data)Tj -34.9566 -1.2983 Td (and)Tj 1.7291 0 Td (analysis)Tj 3.3278 0 Td (are)Tj 1.4457 0 Td (working)Tj 3.5603 0 Td (and)Tj 1.7291 0 Td (when)Tj 2.4038 0 Td (they)Tj 1.9275 0 Td (are)Tj 1.4457 0 Td (not.)Tj 1.8028 0 Td (While)Tj 2.6475 0 Td (it)Tj 0.7881 0 Td (is)Tj 0.8447 0 Td (tempting)Tj 3.8607 0 Td (to)Tj 1.0262 0 Td (interpret)Tj 3.736 0 Td (findings)Tj -32.275 -1.2983 Td (based)Tj 2.4717 0 Td (on)Tj 1.2756 0 Td (big)Tj 1.457 0 Td (data)Tj 1.9162 0 Td (as)Tj 1.0148 0 Td (a)Tj 0.652 0 Td (clear)Tj 2.1203 0 Td (outcome,)Tj 3.9515 0 Td (a)Tj 0.6519 0 Td (key)Tj 1.5988 0 Td (step)Tj 1.8198 0 Td (within)Tj 2.8119 0 Td (scientific)Tj 3.7871 0 Td (research)Tj 3.5546 0 Td (is)Tj 0.8504 0 Td (clearly)Tj 2.8176 0 Td (articulating)Tj -32.7512 -1.3039 Td (what)Tj 2.1656 0 Td (data)Tj 1.9105 0 Td (or)Tj 1.0999 0 Td (an)Tj 1.2019 0 Td (indicator)Tj 3.8664 0 Td (represent)Tj 3.9571 0 Td (and)Tj 1.7292 0 Td (what)Tj 2.1656 0 Td (they)Tj 1.9219 0 Td (do)Tj 1.2586 0 Td (not.)Tj 1.7971 0 Td (Are)Tj 1.7008 0 Td (your)Tj 2.0806 0 Td (findings)Tj 3.4923 0 Td (as)Tj 1.0148 0 Td (clear-cut)Tj 3.719 0 Td (if)Tj -35.0813 -1.2982 Td (your)Tj 2.0806 0 Td (interpretation)Tj 5.7939 0 Td (of)Tj 1.0148 0 Td (a)Tj 0.652 0 Td (social)Tj 2.4605 0 Td (media)Tj 2.6929 0 Td (posting)Tj 3.1804 0 Td (switches)Tj 3.566 0 Td (from)Tj 2.211 0 Td (a)Tj 0.6519 0 Td (recording)Tj 4.1273 0 Td (of)Tj 1.0148 0 Td (fact)Tj 1.6611 0 Td (to)Tj 1.0261 0 Td (the)Tj 1.4683 0 Td (perfor-)Tj -33.6016 -1.2983 Td (mance)Tj 2.8686 0 Td (of)Tj 1.0148 0 Td (a)Tj 0.652 0 Td (social)Tj 2.4604 0 Td (identity?)Tj 3.6737 0 Td (Given)Tj 2.6305 0 Td (the)Tj 1.4684 0 Td (messy,)Tj 2.8686 0 Td (almost)Tj 2.8857 0 Td (organic)Tj 3.2371 0 Td (nature)Tj 2.8233 0 Td (of)Tj 1.0148 0 Td (many)Tj 2.4775 0 Td (datasets)Tj 3.3562 0 Td (derived)Tj -33.4316 -1.3039 Td (from)Tj 2.2053 0 Td (social)Tj 2.4604 0 Td (actions,)Tj 3.2825 0 Td (it)Tj 0.7824 0 Td (is)Tj 0.8504 0 Td (fundamental)Tj 5.3178 0 Td (that)Tj 1.7744 0 Td (researchers)Tj 4.7169 0 Td (be)Tj 1.1451 0 Td (sensitive)Tj 3.6284 0 Td (to)Tj 1.0261 0 Td (the)Tj 1.4683 0 Td (potential)Tj 3.7588 0 Td (multiple)Tj -32.4168 -1.2983 Td (meanings)Tj 4.0875 0 Td (of)Tj 1.0148 0 Td (data.)Tj -3.9061 -1.2982 Td (For)Tj 1.627 0 Td (example,)Tj 3.7758 0 Td (is)Tj 0.8504 0 Td (a)Tj 0.6519 0 Td (Facebook)Tj 4.0479 0 Td (post)Tj 1.8992 0 Td (or)Tj 1.0998 0 Td (an)Tj 1.2019 0 Td (Instagram)Tj 4.2916 0 Td (photo)Tj 2.5795 0 Td (best)Tj 1.8085 0 Td (interpreted)Tj 4.6942 0 Td (as)Tj 1.0148 0 Td (an)Tj 1.2018 0 Td (approval/dis-)Tj -31.9405 -1.2983 Td (approval)Tj 3.702 0 Td (of)Tj 1.0148 0 Td (a)Tj 0.6519 0 Td (phenomenon,)Tj 5.8167 0 Td (a)Tj 0.652 0 Td (simple)Tj 2.8573 0 Td (observation,)Tj 5.125 0 Td (or)Tj 1.0998 0 Td (an)Tj 1.2019 0 Td (effort)Tj 2.3924 0 Td (to)Tj 1.0261 0 Td (improve)Tj 3.583 0 Td (status)Tj 2.5002 0 Td (within)Tj 2.8119 0 Td (a)Tj 0.652 0 Td (friend)Tj -35.087 -1.3039 Td (network?)Tj 3.9174 0 Td (While)Tj 2.6532 0 Td (any)Tj 1.6554 0 Td (of)Tj 1.0148 0 Td (these)Tj 2.2564 0 Td (interpretations)Tj 6.1511 0 Td (are)Tj 1.4514 0 Td (potentially)Tj 4.4617 0 Td (valid,)Tj 2.3754 0 Td (the)Tj 1.4683 0 Td (lack)Tj 1.8199 0 Td (of)Tj 1.0148 0 Td (context)Tj 3.1861 0 Td (makes)Tj 2.7552 0 Td (it)Tj -36.1811 -1.2983 Td (even)Tj 2.0692 0 Td (more)Tj 2.3414 0 Td (difficult)Tj 3.3562 0 Td (to)Tj 1.0205 0 Td (justify)Tj 2.6759 0 Td (the)Tj 1.4683 0 Td (choice)Tj 2.795 0 Td (of)Tj 1.0148 0 Td (one)Tj 1.6951 0 Td (understanding)Tj 6.0717 0 Td (over)Tj 1.9786 0 Td (another.)Tj 3.5546 0 Td (Reflecting)Tj 4.2236 0 Td (on)Tj 1.2756 0 Td (the)Tj -35.5405 -1.2983 Td (potential)Tj 3.7587 0 Td (multiple)Tj 3.5659 0 Td (meanings)Tj 4.0876 0 Td (of)Tj 1.0148 0 Td (data)Tj 1.9162 0 Td (fosters)Tj 2.8289 0 Td (greater)Tj 3.0047 0 Td (clarity)Tj 2.7156 0 Td (in)Tj 1.0375 0 Td (research)Tj 3.5546 0 Td (hypotheses)Tj 4.6204 0 Td (and)Tj 1.7348 0 Td (also)Tj -33.8397 -1.3039 Td (makes)Tj 2.7552 0 Td (researchers)Tj 4.7112 0 Td (aware)Tj 2.5625 0 Td (of)Tj 1.0148 0 Td (the)Tj 1.474 0 Td (other)Tj 2.347 0 Td (potential)Tj 3.7588 0 Td (uses)Tj 1.8935 0 Td (of)Tj 1.0148 0 Td (their)Tj 2.1089 0 Td (data.)Tj 2.1374 0 Td (Again,)Tj 2.8459 0 Td (the)Tj 1.4684 0 Td (act)Tj 1.3776 0 Td (of)Tj 1.0148 0 Td (interpreta-)Tj -32.4848 -1.2982 Td (tion)Tj 1.8425 0 Td (is)Tj 0.8447 0 Td (a)Tj 0.6519 0 Td (human)Tj 3.0784 0 Td (process,)Tj 3.4129 0 Td (and)Tj 1.7292 0 Td (because)Tj 3.3221 0 Td (the)Tj 1.4684 0 Td (judgments)Tj 4.4447 0 Td (of)Tj 1.0148 0 Td (those)Tj 2.3414 0 Td (\(re\)using)Tj 3.8607 0 Td (your)Tj 2.0807 0 Td (data)Tj 1.9162 0 Td (may)Tj 1.9275 0 Td (differ)Tj -33.9361 -1.2983 Td (from)Tj 2.2053 0 Td (your)Tj 2.0806 0 Td (own,)Tj 2.1827 0 Td (it)Tj 0.788 0 Td (is)Tj 0.8447 0 Td (essential)Tj 3.583 0 Td (to)Tj 1.0261 0 Td (clarify)Tj 2.7043 0 Td (both)Tj 2.0636 0 Td (the)Tj 1.4683 0 Td (strengths)Tj 3.8721 0 Td (and)Tj 1.7348 0 Td (shortcomings)Tj 5.6749 0 Td (of)Tj 1.0148 0 Td (the)Tj 1.4684 0 Td (data.)Tj -31.5154 -1.3039 Td (Document)Tj 4.507 0 Td (the)Tj 1.4684 0 Td (provenance)Tj 4.8755 0 Td (and)Tj 1.7291 0 Td (evolution)Tj 3.9969 0 Td (of)Tj 1.0148 0 Td (your)Tj 2.0806 0 Td (data.)Tj 2.143 0 Td (Do)Tj 1.4626 0 Td (not)Tj 1.5704 0 Td (overstate)Tj 3.7927 0 Td (clarity;)Tj 2.9424 0 Td (acknowl-)Tj -32.7796 -1.2983 Td (edge)Tj 2.0579 0 Td (messiness)Tj 4.1442 0 Td (and)Tj 1.7348 0 Td (multiple)Tj 3.566 0 Td (meanings.)Tj ET Q q 1 j 1 J 0 w 11.9999 0 0 11.9999 200.0125 347.0173 cm BT /F2 1 Tf 1 TL -0.0033 Tc 0 0 Td (6.)Tj 1.0441 0 Td (Debate)Tj 3.5197 0 Td (the)Tj 1.7102 0 Td (tough,)Tj 3.2693 0 Td (ethical)Tj 3.3591 0 Td (choices)Tj ET Q q 1 j 1 J 0 w 10 0 0 10 200.0125 330.0094 cm BT /F6 1 Tf 1 TL -0.004 Tc 0 0 Td (Research)Tj 3.804 0 Td (involving)Tj 3.9798 0 Td (human)Tj 3.0784 0 Td (participants)Tj 4.955 0 Td (at)Tj 0.9524 0 Td (federally)Tj 3.6397 0 Td (funded)Tj 3.0614 0 Td (institutions)Tj 4.7678 0 Td (is)Tj 0.8504 0 Td (governed)Tj 3.9401 0 Td (by)Tj 1.1849 0 Td (IRBs)Tj -34.2139 -1.2982 Td (charged)Tj 3.3902 0 Td (with)Tj 2.0012 0 Td (preventing)Tj 4.5354 0 Td (harm)Tj 2.3754 0 Td (through)Tj 3.4413 0 Td (well-established)Tj 6.548 0 Td (procedures)Tj 4.6771 0 Td (and)Tj 1.7291 0 Td (are)Tj 1.4457 0 Td (familiar)Tj 3.3505 0 Td (to)Tj 1.0261 0 Td (many)Tj -34.52 -1.3039 Td (researchers.)Tj 4.9379 0 Td (IRBs,)Tj 2.347 0 Td (however,)Tj 3.8381 0 Td (are)Tj 1.4457 0 Td (not)Tj 1.576 0 Td (the)Tj 1.4684 0 Td (sole)Tj 1.7574 0 Td (arbiter)Tj 2.8914 0 Td (of)Tj 1.0148 0 Td (ethics;)Tj 2.7496 0 Td (many)Tj 2.4774 0 Td (ethical)Tj 2.8403 0 Td (issues)Tj 2.5172 0 Td (involving)Tj 3.9798 0 Td (big)Tj -35.841 -1.2983 Td (data)Tj 1.9105 0 Td (are)Tj 1.4513 0 Td (outside)Tj 3.1294 0 Td (of)Tj 1.0148 0 Td (their)Tj 2.109 0 Td (governance)Tj 4.8189 0 Td (mandate.)Tj 3.9344 0 Td (Precisely)Tj 3.7474 0 Td (because)Tj 3.3222 0 Td (big)Tj 1.457 0 Td (data)Tj 1.9162 0 Td (researchers)Tj 4.7112 0 Td (often)Tj -33.5223 -1.2983 Td (encounter)Tj 4.2916 0 Td (situations)Tj 4.0875 0 Td (that)Tj 1.7802 0 Td (are)Tj 1.4513 0 Td (foreign)Tj 3.0897 0 Td (to)Tj 1.0262 0 Td (or)Tj 1.0941 0 Td (outside)Tj 3.1351 0 Td (of)Tj 1.0148 0 Td (the)Tj 1.4684 0 Td (mandate)Tj 3.7077 0 Td (of)Tj 1.0148 0 Td (IRBs,)Tj 2.347 0 Td (we)Tj 1.3266 0 Td (emphasize)Tj 4.405 0 Td (the)Tj -35.24 -1.3039 Td (importance)Tj 4.8302 0 Td (of)Tj 1.0148 0 Td (debating)Tj 3.6906 0 Td (the)Tj 1.4684 0 Td (issues)Tj 2.5228 0 Td (within)Tj 2.8119 0 Td (groups)Tj 2.9707 0 Td (of)Tj 1.0148 0 Td (peers.)Tj -19.128 -1.2982 Td (Rather)Tj 2.897 0 Td (than)Tj 2.0295 0 Td (a)Tj 0.6577 0 Td (bug,)Tj 1.9445 0 Td (the)Tj 1.4684 0 Td (lack)Tj 1.8198 0 Td (of)Tj 1.0148 0 Td (clear-cut)Tj 3.719 0 Td (solutions)Tj 3.8438 0 Td (and)Tj 1.7291 0 Td (governance)Tj 4.8189 0 Td (protocols)Tj 3.9571 0 Td (should)Tj 2.9197 0 Td (be)Tj 1.1508 0 Td (more)Tj -35.1663 -1.2983 Td (appropriately)Tj 5.5785 0 Td (understood)Tj 4.8189 0 Td (as)Tj 1.0148 0 Td (a)Tj 0.6519 0 Td (feature)Tj 2.9877 0 Td (that)Tj 1.7802 0 Td (researchers)Tj 4.7111 0 Td (should)Tj 2.9197 0 Td (embrace)Tj 3.6226 0 Td (within)Tj 2.812 0 Td (their)Tj 2.109 0 Td (own)Tj 1.9558 0 Td (work.)Tj -34.9622 -1.3039 Td (Discussion)Tj 4.5807 0 Td (and)Tj 1.7291 0 Td (debate)Tj 2.8347 0 Td (of)Tj 1.0148 0 Td (ethical)Tj 2.8403 0 Td (issues)Tj 2.5171 0 Td (is)Tj 0.8504 0 Td (an)Tj 1.2019 0 Td (essential)Tj 3.5829 0 Td (part)Tj 1.8426 0 Td (of)Tj 1.0147 0 Td (professional)Tj 5.0514 0 Td (developmentÐboth)Tj -29.0606 -1.2983 Td (within)Tj 2.8119 0 Td (and)Tj 1.7291 0 Td (between)Tj 3.532 0 Td (disciplinesÐas)Tj 6.1454 0 Td (it)Tj 0.7881 0 Td (can)Tj 1.627 0 Td (establish)Tj 3.6454 0 Td (a)Tj 0.6519 0 Td (mature)Tj 3.0955 0 Td (community)Tj 4.8869 0 Td (of)Tj 1.0148 0 Td (responsible)Tj 4.7508 0 Td (practi-)Tj -34.6788 -1.2983 Td (tioners.)Tj 3.2201 0 Td (Bringing)Tj 3.736 0 Td (these)Tj 2.2507 0 Td (debates)Tj 3.1975 0 Td (into)Tj 1.8368 0 Td (coursework)Tj 4.8926 0 Td (and)Tj 1.7348 0 Td (training)Tj 3.4185 0 Td (can)Tj 1.6271 0 Td (produce)Tj 3.515 0 Td (peer)Tj 1.9558 0 Td (reviewers)Tj 4.0025 0 Td (who)Tj -35.3874 -1.3039 Td (are)Tj 1.4456 0 Td (particularly)Tj 4.8132 0 Td (well)Tj 1.8198 0 Td (placed)Tj 2.795 0 Td (to)Tj 1.0261 0 Td (raise)Tj 2.075 0 Td (these)Tj 2.2507 0 Td (ethical)Tj 2.8403 0 Td (questions)Tj 4.0251 0 Td (and)Tj 1.7292 0 Td (spur)Tj 1.9955 0 Td (recognition)Tj 4.8472 0 Td (of)Tj 1.0148 0 Td (the)Tj 1.4684 0 Td (need)Tj 2.143 0 Td (for)Tj -36.2889 -1.2982 Td (these)Tj 2.2506 0 Td (conversations.)Tj -1.0544 -1.2983 Td (A)Tj 0.907 0 Td (precondition)Tj 5.4255 0 Td (of)Tj 1.0148 0 Td (any)Tj 1.6611 0 Td (formal)Tj 2.8857 0 Td (ethics)Tj 2.5171 0 Td (rules)Tj 2.1487 0 Td (or)Tj 1.0998 0 Td (regulations)Tj 4.6715 0 Td (is)Tj 0.8503 0 Td (the)Tj /F11 1 Tf 1.4684 0 Td (capacity)Tj /F6 1 Tf 3.6396 0 Td (to)Tj 1.0205 0 Td (have)Tj 2.0636 0 Td (such)Tj 2.058 0 Td (open-)Tj -34.6278 -1.2983 Td (ended)Tj 2.6702 0 Td (debates.)Tj 3.4185 0 Td (As)Tj 1.2643 0 Td (digital)Tj 2.7269 0 Td (social)Tj 2.4604 0 Td (scientist)Tj 3.4696 0 Td (and)Tj 1.7292 0 Td (ethicist)Tj 3.084 0 Td (Annette)Tj 3.4469 0 Td (Markham)Tj 4.1953 0 Td ([)Tj 0.83 0.64 0.02 0 k (30)Tj 0 g (])Tj 1.8538 0 Td (writes,)Tj 2.846 0 Td (ªwe)Tj 1.7234 0 Td (can)Tj -34.8885 -1.3039 Td (make)Tj 2.3924 0 Td ([data)Tj 2.2563 0 Td (ethics])Tj 2.8573 0 Td (an)Tj 1.2019 0 Td (easier)Tj 2.5002 0 Td (topic)Tj 2.2337 0 Td (to)Tj 1.0261 0 Td (broach)Tj 2.9934 0 Td (by)Tj 1.1792 0 Td (addressing)Tj 4.507 0 Td (ethics)Tj 2.5228 0 Td (as)Tj 1.0148 0 Td (being)Tj 2.4321 0 Td (about)Tj 2.4889 0 Td (choices)Tj 3.1577 0 Td (we)Tj -34.7638 -1.2982 Td (make)Tj 2.3924 0 Td (at)Tj 0.9524 0 Td (critical)Tj 2.948 0 Td (junctures;)Tj 4.1839 0 Td (choices)Tj 3.1578 0 Td (that)Tj 1.7745 0 Td (will)Tj 1.6667 0 Td (invariably)Tj 4.201 0 Td (have)Tj 2.0636 0 Td (impact.º)Tj 3.5943 0 Td (Given)Tj 2.6305 0 Td (the)Tj 1.4684 0 Td (nature)Tj 2.8232 0 Td (of)Tj 1.0148 0 Td (big)Tj -34.8715 -1.2983 Td (data,)Tj 2.1373 0 Td (bringing)Tj 3.6623 0 Td (technical,)Tj 4.0365 0 Td (scientific,)Tj 4.0138 0 Td (social,)Tj 2.6816 0 Td (and)Tj 1.7291 0 Td (humanistic)Tj 4.6998 0 Td (researchers)Tj 4.7112 0 Td (together)Tj 3.5319 0 Td (on)Tj 1.2756 0 Td (projects)Tj -32.4791 -1.3039 Td (enables)Tj 3.1634 0 Td (this)Tj 1.6724 0 Td (debate)Tj 2.8347 0 Td (to)Tj 1.0261 0 Td (emerge)Tj 3.1464 0 Td (as)Tj 1.0148 0 Td (a)Tj 0.652 0 Td (strength)Tj 3.5149 0 Td (because,)Tj 3.5433 0 Td (if)Tj 0.7767 0 Td (done)Tj 2.228 0 Td (well,)Tj 2.0466 0 Td (it)Tj 0.7824 0 Td (provides)Tj 3.651 0 Td (the)Tj 1.4683 0 Td (means)Tj 2.8063 0 Td (to)Tj -34.3273 -1.2983 Td (understand)Tj 4.7848 0 Td (the)Tj 1.474 0 Td (ethical)Tj 2.8403 0 Td (issues)Tj 2.5172 0 Td (from)Tj 2.2053 0 Td (a)Tj 0.6576 0 Td (range)Tj 2.4605 0 Td (of)Tj 1.0148 0 Td (perspectives)Tj 5.0626 0 Td (and)Tj 1.7292 0 Td (disrupt)Tj 3.0897 0 Td (the)Tj 1.474 0 Td (silos)Tj 1.9616 0 Td (of)Tj 1.0148 0 Td (disciplines)Tj ET Q q 1 j 1 J 0 w 576 737.1 m 36 737.1 l 36 737.6 l 576 737.6 l f* 36 741.2598 169.285 23.6976 re W* n q 169.2283 0 0 23.6409 36 741.3165 cm q /I0 Do Q Q Q q 0 0 612 792 re W* n 1 j 1 J 0 w 7.9999 0 0 7.9999 391.9748 745.7952 cm BT /F0 1 Tf 1 TL -0.005 Tc 0 0 Td (Ten)Tj 1.9204 0 Td (Simple)Tj 3.2245 0 Td (Rules)Tj 2.7283 0 Td (for)Tj 1.3678 0 Td (Respons)Tj 3.8764 0 Td (ible)Tj 1.7362 0 Td (Big)Tj 1.637 0 Td (Data)Tj 2.3032 0 Td (Researc)Tj 3.6567 0 Td (h)Tj ET Q q 1 j 1 J 0 w 36 48.0002 m 576 48.0002 l 576 47.5002 l 36 47.5002 l f* 0.83 0.64 0.02 0 k 142.5827 34.9228 m 295.8236 34.9228 l h f* 0 g 7.9999 0 0 7.9999 36 36 cm BT /F0 1 Tf 1 TL -0.005 Tc 0 0 Td (PLOS)Tj 2.8559 0 Td (Computationa)Tj 6.1229 0 Td (l)Tj 0.4322 0 Td (Biology)Tj 3.4371 0 Td (|)Tj 0.83 0.64 0.02 0 k 0.4748 0 Td (https:/)Tj 2.6788 0 Td (/doi.org/10.13)Tj 6.0094 0 Td (71/journal.p)Tj 5.1166 0 Td (cbi.1005399)Tj 0 g 6.3497 0 Td (March)Tj 2.9551 0 Td (30,)Tj 1.5874 0 Td (2017)Tj 27.1207 0 Td (5)Tj 0.7654 0 Td (/)Tj ET endstream endobj 105 0 obj < >stream endstream endobj 106 0 obj < >stream endstream endobj 107 0 obj < >stream BT 66.402 0 Td (10)Tj ET Q endstream endobj 108 0 obj < >/XObject< >>>/CropBox[0 0 612 792]/MediaBox[0 0 612 792]/Parent 9 0 R/Annots 109 0 R/Contents 110 0 R/TrimBox[0 0 612 792]>> endobj 109 0 obj [111 0 R 112 0 R 113 0 R 114 0 R 115 0 R 116 0 R 117 0 R 118 0 R 119 0 R] endobj 111 0 obj < >/Border[0 0 0]/Dest(Rpcbi.1005399.ref031)>> endobj 112 0 obj < >/Border[0 0 0]/Dest(Rpcbi.1005399.ref032)>> endobj 113 0 obj < >/Border[0 0 0]/Dest(Rpcbi.1005399.ref033)>> endobj 114 0 obj < >/Border[0 0 0]/Dest(Rpcbi.1005399.ref034)>> endobj 115 0 obj < >/Border[0 0 0]/Dest(Rpcbi.1005399.ref035)>> endobj 116 0 obj < >/Border[0 0 0]/A 120 0 R>> endobj 120 0 obj < > endobj 117 0 obj < >/Border[0 0 0]/Dest(Rpcbi.1005399.ref013)>> endobj 118 0 obj < >/Border[0 0 0]/Dest(Rpcbi.1005399.ref001)>> endobj 119 0 obj < >/Border[0 0 0]/A 121 0 R>> endobj 121 0 obj < > endobj 110 0 obj [122 0 R 123 0 R 124 0 R 125 0 R 126 0 R 127 0 R 128 0 R 129 0 R 130 0 R] endobj 122 0 obj < >stream q 0.83 0.64 0.02 0 k 203.4142 706.337 m 212.9386 706.337 l h f* 249.4488 680.315 m 258.9732 680.315 l h f* 393.052 680.315 m 402.5764 680.315 l h f* 217.1339 641.3102 m 226.6583 641.3102 l h f* 227.2819 615.3449 m 236.8063 615.3449 l h f* 203.4142 459.5528 m 341.7449 459.5528 l 341.7449 459.0992 l 203.4142 459.0992 l f* 295.1433 185.8961 m 304.611 185.8961 l h f* 555.1937 107.9433 m 559.9559 107.9433 l h f* 0 g 1 j 1 J 0 w 10 0 0 10 200.0125 707.4141 cm BT /F6 1 Tf 1 TL -0.004 Tc 0 0 Td ([)Tj 0.83 0.64 0.02 0 k (31)Tj 0 g (].)Tj 2.0749 0 Td (There)Tj 2.5795 0 Td (are)Tj 1.4457 0 Td (a)Tj 0.6576 0 Td (number)Tj 3.4129 0 Td (of)Tj 1.0148 0 Td (good)Tj 2.228 0 Td (models)Tj 3.1068 0 Td (for)Tj 1.3833 0 Td (interdisciplinary)Tj 6.8144 0 Td (ethics)Tj 2.5228 0 Td (research,)Tj 3.7814 0 Td (such)Tj 2.058 0 Td (as)Tj 1.0148 0 Td (the)Tj -34.0949 -1.3039 Td (trainings)Tj 3.7813 0 Td (offered)Tj 3.0501 0 Td (by)Tj 1.1792 0 Td (the)Tj 1.4683 0 Td (Science)Tj 3.2088 0 Td (and)Tj 1.7292 0 Td (Justice)Tj 2.8516 0 Td (research)Tj 3.5546 0 Td (center)Tj 2.7099 0 Td (at)Tj 0.9525 0 Td (the)Tj 1.4683 0 Td (University)Tj 4.3993 0 Td (of)Tj 1.0148 0 Td (California,)Tj -31.3679 -1.2982 Td (Santa)Tj 2.4094 0 Td (Cruz)Tj 2.194 0 Td ([)Tj 0.83 0.64 0.02 0 k (32)Tj 0 g (])Tj 1.8538 0 Td (and)Tj 1.7292 0 Td (Values)Tj 2.9083 0 Td (in)Tj 1.0318 0 Td (Design)Tj 3.0217 0 Td (curricula)Tj 3.8154 0 Td ([)Tj 0.83 0.64 0.02 0 k (33)Tj 0 g (].)Tj 2.075 0 Td (Research)Tj 3.8097 0 Td (ethics)Tj 2.5171 0 Td (consultation)Tj 5.1874 0 Td (services,)Tj -32.5528 -1.2983 Td (available)Tj 3.668 0 Td (at)Tj 0.9467 0 Td (some)Tj 2.3358 0 Td (universities)Tj 4.7905 0 Td (as)Tj 1.0148 0 Td (a)Tj 0.6519 0 Td (result)Tj 2.4492 0 Td (of)Tj 1.0148 0 Td (the)Tj 1.4683 0 Td (Clinical)Tj 3.3165 0 Td (and)Tj 1.7291 0 Td (Translational)Tj 5.5276 0 Td (Science)Tj 3.2088 0 Td (Award)Tj -32.122 -1.2983 Td (\(CTSA\))Tj 3.3278 0 Td (program)Tj 3.702 0 Td (of)Tj 1.0092 0 Td (the)Tj 1.4626 0 Td (National)Tj 3.685 0 Td (Institutes)Tj 3.9402 0 Td (of)Tj 1.0091 0 Td (Health)Tj 2.914 0 Td (\(NIH\),)Tj 2.965 0 Td (can)Tj 1.6214 0 Td (also)Tj 1.7632 0 Td (be)Tj 1.1452 0 Td (resources)Tj 3.9798 0 Td (for)Tj 1.3833 0 Td (research-)Tj -33.9078 -1.3039 Td (ers)Tj 1.3719 0 Td ([)Tj 0.83 0.64 0.02 0 k (34)Tj 0 g (].)Tj -0.1757 -1.2982 Td (Some)Tj 2.4491 0 Td (of)Tj 1.0148 0 Td (the)Tj 1.4683 0 Td (better-known)Tj 5.6749 0 Td (ªbig)Tj 1.8539 0 Td (dataº)Tj 2.3074 0 Td (ethical)Tj 2.8403 0 Td (casesÐi.e.,)Tj 4.5127 0 Td (the)Tj 1.4683 0 Td (Facebook)Tj 4.0479 0 Td (emotional)Tj 4.2689 0 Td (contagion)Tj -33.1027 -1.2983 Td (study)Tj 2.3867 0 Td ([)Tj 0.83 0.64 0.02 0 k (35)Tj 0 g (]Ðprovide)Tj 5.845 0 Td (extremely)Tj 4.1442 0 Td (productive)Tj 4.5411 0 Td (venues)Tj 2.9594 0 Td (for)Tj 1.3833 0 Td (cross-disciplinary)Tj 7.2793 0 Td (discussions.)Tj 4.9606 0 Td (Why)Tj -33.4996 -1.3039 Td (might)Tj 2.5965 0 Td (one)Tj 1.7007 0 Td (set)Tj 1.304 0 Td (of)Tj 1.0148 0 Td (scholars)Tj 3.4469 0 Td (see)Tj 1.4286 0 Td (this)Tj 1.6781 0 Td (as)Tj 1.0148 0 Td (a)Tj 0.652 0 Td (relatively)Tj 3.8381 0 Td (benign)Tj 2.982 0 Td (approach)Tj 3.9515 0 Td (while)Tj 2.3697 0 Td (other)Tj 2.3471 0 Td (groups)Tj 2.9707 0 Td (see)Tj 1.423 0 Td (signifi-)Tj -34.7185 -1.2983 Td (cant)Tj 1.9218 0 Td (ethical)Tj 2.8403 0 Td (shortcomings?)Tj 6.0491 0 Td (Where)Tj 2.931 0 Td (do)Tj 1.2586 0 Td (researchers)Tj 4.7112 0 Td (differ)Tj 2.381 0 Td (in)Tj 1.0375 0 Td (drawing)Tj 3.5206 0 Td (the)Tj 1.4684 0 Td (line)Tj 1.7064 0 Td (between)Tj 3.5263 0 Td (responsi-)Tj -33.3522 -1.2983 Td (ble)Tj 1.3946 0 Td (and)Tj 1.7291 0 Td (irresponsible)Tj 5.3915 0 Td (research)Tj 3.5603 0 Td (and)Tj 1.7291 0 Td (why?)Tj 2.2564 0 Td (Understanding)Tj 6.2758 0 Td (the)Tj 1.474 0 Td (different)Tj 3.6567 0 Td (ways)Tj 2.1486 0 Td (people)Tj 2.8573 0 Td (discuss)Tj 3.0501 0 Td (these)Tj -35.5235 -1.3039 Td (challenges)Tj 4.3199 0 Td (and)Tj 1.7291 0 Td (processes)Tj 3.9685 0 Td (provides)Tj 3.651 0 Td (an)Tj 1.2019 0 Td (important)Tj 4.2803 0 Td (check)Tj 2.5171 0 Td (for)Tj 1.3833 0 Td (researchers,)Tj 4.938 0 Td (especially)Tj 4.0195 0 Td (if)Tj 0.771 0 Td (they)Tj 1.9275 0 Td (come)Tj -34.7071 -1.2982 Td (from)Tj 2.2053 0 Td (disciplines)Tj 4.4277 0 Td (not)Tj 1.576 0 Td (focused)Tj 3.2769 0 Td (on)Tj 1.2755 0 Td (human)Tj 3.0785 0 Td (subject)Tj 3.016 0 Td (concerns.)Tj -17.6597 -1.2983 Td (Moreover,)Tj 4.3993 0 Td (the)Tj 1.4683 0 Td (high)Tj 2.007 0 Td (visibility)Tj 3.6113 0 Td (surrounding)Tj 5.244 0 Td (these)Tj 2.2564 0 Td (events)Tj 2.7269 0 Td (means)Tj 2.8063 0 Td (that)Tj 1.7801 0 Td (\(for)Tj 1.7235 0 Td (better)Tj 2.5455 0 Td (or)Tj 1.0942 0 Td (worse\))Tj 2.9083 0 Td (they)Tj -35.7673 -1.3039 Td (represent)Tj 3.9571 0 Td (the)Tj 1.4683 0 Td (ªpublicº)Tj 3.4923 0 Td (view)Tj 2.0523 0 Td (of)Tj 1.0148 0 Td (big)Tj 1.457 0 Td (data)Tj 1.9162 0 Td (research,)Tj 3.7814 0 Td (and)Tj 1.7291 0 Td (becoming)Tj 4.1839 0 Td (an)Tj 1.2019 0 Td (active)Tj 2.5171 0 Td (member)Tj 3.5887 0 Td (of)Tj 1.0148 0 Td (this)Tj 1.6724 0 Td (con-)Tj -35.0473 -1.2983 Td (versation)Tj 3.8834 0 Td (ensures)Tj 3.2371 0 Td (that)Tj 1.7802 0 Td (researchers)Tj 4.7111 0 Td (can)Tj 1.6271 0 Td (give)Tj 1.8312 0 Td (voice)Tj 2.2904 0 Td (to)Tj 1.0261 0 Td (their)Tj 2.109 0 Td (insights)Tj 3.3165 0 Td (rather)Tj 2.6475 0 Td (than)Tj 2.0296 0 Td (simply)Tj 2.8857 0 Td (being)Tj 2.4321 0 Td (at)Tj -35.807 -1.2982 Td (the)Tj 1.4683 0 Td (receiving)Tj 3.8664 0 Td (end)Tj 1.7178 0 Td (of)Tj 1.0148 0 Td (policy)Tj 2.6362 0 Td (decisions.)Tj 4.1329 0 Td (In)Tj 1.1055 0 Td (an)Tj 1.2019 0 Td (effort)Tj 2.3981 0 Td (to)Tj 1.0261 0 Td (help)Tj 1.9332 0 Td (these)Tj 2.2564 0 Td (debates)Tj 3.1918 0 Td (along,)Tj 2.6475 0 Td (the)Tj 1.4684 0 Td (Council)Tj 3.4072 0 Td (for)Tj -35.4725 -1.2983 Td (Big)Tj 1.5306 0 Td (Data,)Tj 2.3471 0 Td (Ethics,)Tj 2.8913 0 Td (and)Tj 1.7348 0 Td (Society)Tj 3.0671 0 Td (has)Tj 1.542 0 Td (produced)Tj 4.0479 0 Td (a)Tj 0.652 0 Td (number)Tj 3.4185 0 Td (of)Tj 1.0148 0 Td (case)Tj 1.8595 0 Td (studies)Tj 2.9877 0 Td (focused)Tj 3.2769 0 Td (specifically)Tj 4.575 0 Td (on)Tj 1.2756 0 Td (big)Tj -36.2208 -1.3039 Td (data)Tj 1.9105 0 Td (research)Tj 3.5603 0 Td (and)Tj 1.7291 0 Td (a)Tj 0.652 0 Td (white)Tj 2.4207 0 Td (paper)Tj 2.4832 0 Td (with)Tj 2.0012 0 Td (recommendations)Tj 7.4948 0 Td (to)Tj 1.0261 0 Td (start)Tj 1.9843 0 Td (these)Tj 2.2563 0 Td (important)Tj 4.2803 0 Td (conversations)Tj -31.7988 -1.2983 Td (\()Tj 0.83 0.64 0.02 0 k (http://bdes.datasociety.net/o)Tj 11.6729 0 Td (utput/)Tj 0 g (\).)Tj -10.4767 -1.2983 Td (Engage)Tj 3.1294 0 Td (your)Tj 2.0806 0 Td (colleagues)Tj 4.2746 0 Td (and)Tj 1.7348 0 Td (students)Tj 3.566 0 Td (about)Tj 2.4945 0 Td (ethical)Tj 2.8403 0 Td (practice)Tj 3.3732 0 Td (for)Tj 1.3889 0 Td (big)Tj 1.457 0 Td (data)Tj 1.9162 0 Td (research.)Tj ET Q q 1 j 1 J 0 w 11.9999 0 0 11.9999 200.0125 413.0078 cm BT /F2 1 Tf 1 TL -0.0033 Tc 0 0 Td (7.)Tj 1.0441 0 Td (Develop)Tj 4.0772 0 Td (a)Tj 0.77 0 Td (code)Tj 2.5371 0 Td (of)Tj 1.1622 0 Td (conduct)Tj 4.0866 0 Td (for)Tj 1.5449 0 Td (your)Tj 2.3717 0 Td (organization,)Tj 6.4111 0 Td (research)Tj -24.0049 -1.1669 Td (community,)Tj 5.7968 0 Td (or)Tj 1.2095 0 Td (industry)Tj ET Q q 1 j 1 J 0 w 10 0 0 10 200.0125 381.9968 cm BT /F6 1 Tf 1 TL -0.004 Tc 0 0 Td (The)Tj 1.7801 0 Td (process)Tj 3.1861 0 Td (of)Tj 1.0148 0 Td (debating)Tj 3.6907 0 Td (tough)Tj 2.5455 0 Td (choices)Tj 3.1521 0 Td (inserts)Tj 2.8516 0 Td (ethics)Tj 2.5229 0 Td (directly)Tj 3.2314 0 Td (into)Tj 1.8425 0 Td (the)Tj 1.4684 0 Td (workflow)Tj 4.0025 0 Td (of)Tj 1.0148 0 Td (research,)Tj -32.3034 -1.2982 Td (making)Tj 3.2484 0 Td (ªfaking)Tj 3.1125 0 Td (ethicsº)Tj 2.914 0 Td (as)Tj 1.0147 0 Td (unacceptable)Tj 5.4255 0 Td (as)Tj 1.0148 0 Td (faking)Tj 2.7213 0 Td (data)Tj 1.9105 0 Td (or)Tj 1.0998 0 Td (results.)Tj 3.0331 0 Td (Internalizing)Tj 5.3915 0 Td (these)Tj 2.2563 0 Td (debates,)Tj -33.1424 -1.3039 Td (rather)Tj 2.6475 0 Td (than)Tj 2.0239 0 Td (treating)Tj 3.3279 0 Td (them)Tj 2.2903 0 Td (as)Tj 1.0148 0 Td (an)Tj 1.2019 0 Td (afterthought)Tj 5.1874 0 Td (or)Tj 1.0998 0 Td (a)Tj 0.652 0 Td (problem)Tj 3.6113 0 Td (to)Tj 1.0261 0 Td (outsource,)Tj 4.3937 0 Td (is)Tj 0.8447 0 Td (key)Tj 1.5988 0 Td (for)Tj 1.3833 0 Td (successful)Tj -32.3034 -1.2983 Td (research,)Tj 3.7813 0 Td (particularly)Tj 4.8132 0 Td (when)Tj 2.4038 0 Td (using)Tj 2.3868 0 Td (trace)Tj 2.1713 0 Td (data)Tj 1.9162 0 Td (produced)Tj 4.0422 0 Td (by)Tj 1.1848 0 Td (people.)Tj 3.0785 0 Td (This)Tj 1.9842 0 Td (is)Tj 0.8504 0 Td (relevant)Tj 3.4185 0 Td (for)Tj 1.389 0 Td (all)Tj 1.1452 0 Td (re-)Tj -34.5654 -1.2983 Td (search)Tj 2.7609 0 Td (including)Tj 4.0422 0 Td (those)Tj 2.3357 0 Td (within)Tj 2.8119 0 Td (industry)Tj 3.5773 0 Td (who)Tj 1.9389 0 Td (have)Tj 2.058 0 Td (privileged)Tj 4.1839 0 Td (access)Tj 2.6475 0 Td (to)Tj 1.0262 0 Td (the)Tj 1.4683 0 Td (data)Tj 1.9162 0 Td (streams)Tj 3.2882 0 Td (of)Tj 1.0148 0 Td (digital)Tj -35.07 -1.2982 Td (daily)Tj 2.1486 0 Td (life.)Tj 1.6724 0 Td (Public)Tj 2.7496 0 Td (attention)Tj 3.8438 0 Td (to)Tj 1.0261 0 Td (the)Tj 1.4683 0 Td (ethical)Tj 2.8403 0 Td (use)Tj 1.5307 0 Td (of)Tj 1.0148 0 Td (these)Tj 2.2564 0 Td (data)Tj 1.9105 0 Td (should)Tj 2.9197 0 Td (not)Tj 1.5704 0 Td (be)Tj 1.1509 0 Td (avoided;)Tj 3.5886 0 Td (after)Tj 2.0353 0 Td (all,)Tj 1.3719 0 Td (these)Tj -35.0983 -1.3039 Td (datasets)Tj 3.3561 0 Td (are)Tj 1.4514 0 Td (based)Tj 2.4718 0 Td (on)Tj 1.2756 0 Td (an)Tj 1.2018 0 Td (infrastructure)Tj 5.7317 0 Td (that)Tj 1.7801 0 Td (billions)Tj 3.1691 0 Td (of)Tj 1.0148 0 Td (people)Tj 2.8516 0 Td (are)Tj 1.4514 0 Td (using)Tj 2.3867 0 Td (to)Tj 1.0262 0 Td (live)Tj 1.61 0 Td (their)Tj 2.109 0 Td (lives,)Tj 2.1997 0 Td (and)Tj -35.087 -1.2983 Td (there)Tj 2.262 0 Td (is)Tj 0.8504 0 Td (a)Tj 0.6519 0 Td (compelling)Tj 4.6828 0 Td (public)Tj 2.7099 0 Td (interest)Tj 3.2145 0 Td (that)Tj 1.7745 0 Td (research)Tj 3.5603 0 Td (is)Tj 0.8447 0 Td (done)Tj 2.228 0 Td (responsibly.)Tj -21.5828 -1.2983 Td (One)Tj 1.9388 0 Td (of)Tj 1.0148 0 Td (the)Tj 1.4741 0 Td (best)Tj 1.8084 0 Td (ways)Tj 2.1544 0 Td (to)Tj 1.0261 0 Td (cement)Tj 3.1578 0 Td (this)Tj 1.6724 0 Td (in)Tj 1.0318 0 Td (daily)Tj 2.1543 0 Td (practice)Tj 3.3789 0 Td (is)Tj 0.8447 0 Td (to)Tj 1.0262 0 Td (develop)Tj 3.3221 0 Td (codes)Tj 2.4605 0 Td (of)Tj 1.0148 0 Td (conduct)Tj 3.4752 0 Td (for)Tj 1.389 0 Td (use)Tj -35.5405 -1.3039 Td (in)Tj 1.0318 0 Td (your)Tj 2.0806 0 Td (organization)Tj 5.2837 0 Td (or)Tj 1.0999 0 Td (research)Tj 3.5546 0 Td (community)Tj 4.8869 0 Td (and)Tj 1.7291 0 Td (for)Tj 1.389 0 Td (inclusion)Tj 3.9118 0 Td (in)Tj 1.0374 0 Td (formal)Tj 2.8857 0 Td (education)Tj 4.1782 0 Td (and)Tj 1.7291 0 Td (ongo-)Tj -34.7978 -1.2983 Td (ing)Tj 1.5023 0 Td (training.)Tj 3.6453 0 Td (The)Tj 1.7802 0 Td (codes)Tj 2.4604 0 Td (can)Tj 1.6271 0 Td (provide)Tj 3.2882 0 Td (guidance)Tj 3.8381 0 Td (in)Tj 1.0374 0 Td (peer)Tj 1.9559 0 Td (review)Tj 2.8403 0 Td (of)Tj 1.0148 0 Td (publications)Tj 5.125 0 Td (and)Tj 1.7348 0 Td (in)Tj 1.0318 0 Td (funding)Tj -32.8816 -1.2982 Td (consideration.)Tj 5.93 0 Td (In)Tj 1.1055 0 Td (practice,)Tj 3.6056 0 Td (a)Tj 0.652 0 Td (highly)Tj 2.7099 0 Td (visible)Tj 2.7496 0 Td (case)Tj 1.8595 0 Td (of)Tj 1.0148 0 Td (unethical)Tj 3.9175 0 Td (research)Tj 3.5546 0 Td (brings)Tj 2.7382 0 Td (problems)Tj 3.9742 0 Td (to)Tj 1.0261 0 Td (an)Tj -34.8375 -1.3039 Td (entire)Tj 2.5511 0 Td (field,)Tj 2.1997 0 Td (not)Tj 1.5704 0 Td (just)Tj 1.6667 0 Td (to)Tj 1.0262 0 Td (those)Tj 2.3357 0 Td (directly)Tj 3.2315 0 Td (involved.)Tj 3.8777 0 Td (Moreover,)Tj 4.3937 0 Td (designing)Tj 4.0932 0 Td (codes)Tj 2.4661 0 Td (of)Tj 1.0148 0 Td (conduct)Tj 3.4753 0 Td (makes)Tj -33.9021 -1.2983 Td (researchers)Tj 4.7111 0 Td (more)Tj 2.3414 0 Td (successful.)Tj 4.388 0 Td (Issues)Tj 2.5909 0 Td (that)Tj 1.7801 0 Td (might)Tj 2.5965 0 Td (otherwise)Tj 4.0876 0 Td (be)Tj 1.1508 0 Td (ignored)Tj 3.3279 0 Td (until)Tj 2.1089 0 Td (they)Tj 1.9219 0 Td (blow)Tj 2.1657 0 Td (upÐe.g.,)Tj -33.1708 -1.2983 Td (Are)Tj 1.6951 0 Td (we)Tj 1.3322 0 Td (abiding)Tj 3.2372 0 Td (by)Tj 1.1792 0 Td (the)Tj 1.4683 0 Td (terms)Tj 2.4945 0 Td (of)Tj 1.0148 0 Td (service)Tj 2.9423 0 Td (or)Tj 1.0999 0 Td (users')Tj 2.4831 0 Td (expectations?)Tj 5.5559 0 Td (Does)Tj 2.245 0 Td (the)Tj 1.474 0 Td (general)Tj 3.1294 0 Td (public)Tj 2.7099 0 Td (consider)Tj -34.0608 -1.3039 Td (our)Tj 1.627 0 Td (research)Tj 3.5546 0 Td (ªcreepyº?)Tj 3.9912 0 Td ([)Tj 0.83 0.64 0.02 0 k (13)Tj 0 g (]Ðcan)Tj 4.1782 0 Td (be)Tj 1.1509 0 Td (addressed)Tj 4.1782 0 Td (thoughtfully)Tj 5.1364 0 Td (rather)Tj 2.6419 0 Td (than)Tj 2.0296 0 Td (in)Tj 1.0374 0 Td (a)Tj 0.652 0 Td (scramble)Tj 3.8097 0 Td (for)Tj 1.3833 0 Td (dam-)Tj -35.3704 -1.2982 Td (age)Tj 1.542 0 Td (control.)Tj 3.3448 0 Td (This)Tj 1.99 0 Td (is)Tj 0.8447 0 Td (particularly)Tj 4.8132 0 Td (relevant)Tj 3.4185 0 Td (to)Tj 1.0262 0 Td (public-facing)Tj 5.4878 0 Td (private)Tj 2.9877 0 Td (businesses)Tj 4.3597 0 Td (interested)Tj 4.1612 0 Td (in)Tj 1.0375 0 Td (avoid-)Tj -35.0133 -1.2983 Td (ing)Tj 1.5023 0 Td (potentially)Tj 4.456 0 Td (unfavorable)Tj 4.9663 0 Td (attention.)Tj -9.7284 -1.2983 Td (An)Tj 1.457 0 Td (additional)Tj 4.2746 0 Td (and)Tj 1.7291 0 Td (longer-term)Tj 5.0456 0 Td (advantage)Tj 4.2406 0 Td (of)Tj 1.0148 0 Td (developing)Tj 4.6035 0 Td (codes)Tj 2.4604 0 Td (of)Tj 1.0148 0 Td (conduct)Tj 3.4753 0 Td (is)Tj 0.8504 0 Td (that)Tj 1.7801 0 Td (it)Tj 0.7824 0 Td (is)Tj 0.8504 0 Td (clear)Tj -34.7752 -1.3039 Td (that)Tj 1.7744 0 Td (change)Tj 3.0444 0 Td (is)Tj 0.8504 0 Td (coming)Tj 3.2485 0 Td (to)Tj 1.0261 0 Td (big)Tj 1.457 0 Td (data)Tj 1.9162 0 Td (research.)Tj 3.7814 0 Td (The)Tj 1.7802 0 Td (NSF)Tj 1.9615 0 Td (funded)Tj 3.0614 0 Td (the)Tj 1.474 0 Td (Council)Tj 3.4016 0 Td (for)Tj 1.3833 0 Td (Big)Tj 1.5364 0 Td (Data,)Tj 2.347 0 Td (Ethics,)Tj -34.0438 -1.2983 Td (and)Tj 1.7234 0 Td (Society)Tj 3.0614 0 Td (as)Tj 1.0035 0 Td (a)Tj 0.6463 0 Td (means)Tj 2.8006 0 Td (of)Tj 1.0034 0 Td (getting)Tj 2.9821 0 Td (in)Tj 1.0261 0 Td (front)Tj 2.2223 0 Td (of)Tj 1.0092 0 Td (a)Tj 0.6463 0 Td (developing)Tj 4.5921 0 Td (issue)Tj 2.1486 0 Td (and)Tj 1.7235 0 Td (pending)Tj 3.5092 0 Td (regulatory)Tj 4.3143 0 Td (changes)Tj -34.4123 -1.2982 Td (within)Tj 2.8119 0 Td (federal)Tj 2.9367 0 Td (rules)Tj 2.1486 0 Td (for)Tj 1.3833 0 Td (the)Tj 1.4684 0 Td (protection)Tj 4.3823 0 Td (of)Tj 1.0148 0 Td (human)Tj 3.0784 0 Td (subjects)Tj 3.3789 0 Td (that)Tj 1.7801 0 Td (are)Tj 1.4457 0 Td (currently)Tj 3.8891 0 Td (under)Tj 2.6192 0 Td (review)Tj 2.8403 0 Td ([)Tj 0.83 0.64 0.02 0 k (1)Tj 0 g (].)Tj -35.1777 -1.3039 Td (Actively)Tj 3.4695 0 Td (developing)Tj 4.6035 0 Td (rules)Tj 2.1486 0 Td (for)Tj 1.389 0 Td (responsible)Tj 4.7508 0 Td (big)Tj 1.457 0 Td (data)Tj 1.9162 0 Td (research)Tj 3.5547 0 Td (within)Tj 2.8119 0 Td (a)Tj 0.6576 0 Td (research)Tj 3.5547 0 Td (community)Tj 4.8869 0 Td (is)Tj 0.8447 0 Td (a)Tj -36.0451 -1.2983 Td (key)Tj 1.593 0 Td (way)Tj 1.7915 0 Td (researchers)Tj 4.7111 0 Td (can)Tj 1.6271 0 Td (join)Tj 1.8028 0 Td (this)Tj 1.6725 0 Td (ongoing)Tj 3.5263 0 Td (process.)Tj ET Q q 1 j 1 J 0 w 576 737.1 m 36 737.1 l 36 737.6 l 576 737.6 l f* 36 741.2598 169.285 23.6976 re W* n q 169.2283 0 0 23.6409 36 741.3165 cm q /I0 Do Q Q Q q 0 0 612 792 re W* n 1 j 1 J 0 w 7.9999 0 0 7.9999 391.9748 745.7952 cm BT /F0 1 Tf 1 TL -0.005 Tc 0 0 Td (Ten)Tj 1.9204 0 Td (Simple)Tj 3.2245 0 Td (Rules)Tj 2.7283 0 Td (for)Tj 1.3678 0 Td (Respons)Tj 3.8764 0 Td (ible)Tj 1.7362 0 Td (Big)Tj 1.637 0 Td (Data)Tj 2.3032 0 Td (Researc)Tj 3.6567 0 Td (h)Tj ET Q q 1 j 1 J 0 w 36 48.0002 m 576 48.0002 l 576 47.5002 l 36 47.5002 l f* 0.83 0.64 0.02 0 k 142.5827 34.9228 m 295.8236 34.9228 l h f* 0 g 7.9999 0 0 7.9999 36 36 cm BT /F0 1 Tf 1 TL -0.005 Tc 0 0 Td (PLOS)Tj 2.8559 0 Td (Computationa)Tj 6.1229 0 Td (l)Tj 0.4322 0 Td (Biology)Tj 3.4371 0 Td (|)Tj 0.83 0.64 0.02 0 k 0.4748 0 Td (https:/)Tj 2.6788 0 Td (/doi.org/10.13)Tj 6.0094 0 Td (71/journal.p)Tj 5.1166 0 Td (cbi.1005399)Tj 0 g 6.3497 0 Td (March)Tj 2.9551 0 Td (30,)Tj 1.5874 0 Td (2017)Tj 27.1207 0 Td (6)Tj 0.7654 0 Td (/)Tj ET endstream endobj 123 0 obj < >stream endstream endobj 124 0 obj < >stream endstream endobj 125 0 obj < >stream endstream endobj 126 0 obj < >stream endstream endobj 127 0 obj < >stream endstream endobj 128 0 obj < >stream endstream endobj 129 0 obj < >stream endstream endobj 130 0 obj < >stream BT 66.402 0 Td (10)Tj ET Q endstream endobj 131 0 obj < >/XObject< >>>/CropBox[0 0 612 792]/MediaBox[0 0 612 792]/Parent 9 0 R/Annots 132 0 R/Contents 133 0 R/TrimBox[0 0 612 792]>> endobj 132 0 obj [134 0 R 135 0 R 136 0 R] endobj 134 0 obj < >/Border[0 0 0]/Dest(Rpcbi.1005399.ref036)>> endobj 135 0 obj < >/Border[0 0 0]/Dest(Rpcbi.1005399.ref037)>> endobj 136 0 obj < >/Border[0 0 0]/A 137 0 R>> endobj 137 0 obj < > endobj 133 0 obj [138 0 R 139 0 R 140 0 R 141 0 R 142 0 R 143 0 R] endobj 138 0 obj < >stream q 0.83 0.64 0.02 0 k 203.4142 350.3622 m 212.9386 350.3622 l h f* 203.4142 220.3654 m 212.9386 220.3654 l h f* 0 g 1 j 1 J 0 w 10 0 0 10 211.9748 707.4141 cm BT /F6 1 Tf 1 TL -0.004 Tc 0 0 Td (Establish)Tj 3.7927 0 Td (appropriate)Tj 4.8812 0 Td (codes)Tj 2.4605 0 Td (of)Tj 1.0148 0 Td (ethical)Tj 2.8403 0 Td (conduct)Tj 3.4752 0 Td (within)Tj 2.8176 0 Td (your)Tj 2.0807 0 Td (community.)Tj 5.108 0 Td (Make)Tj 2.4661 0 Td (industry)Tj -32.1333 -1.3039 Td (researchers)Tj 4.7111 0 Td (and)Tj 1.7291 0 Td (representatives)Tj 6.1965 0 Td (of)Tj 1.0148 0 Td (affected)Tj 3.3279 0 Td (communities)Tj 5.4821 0 Td (active)Tj 2.5228 0 Td (contributors)Tj 5.2044 0 Td (to)Tj 1.0262 0 Td (this)Tj 1.6781 0 Td (process.)Tj ET Q q 1 j 1 J 0 w 11.9999 0 0 11.9999 200.0125 663.9307 cm BT /F2 1 Tf 1 TL -0.0033 Tc 0 0 Td (8.)Tj 1.0441 0 Td (Design)Tj 3.5291 0 Td (your)Tj 2.3717 0 Td (data)Tj 2.2583 0 Td (and)Tj 1.989 0 Td (systems)Tj 4.1717 0 Td (for)Tj 1.5448 0 Td (auditability)Tj ET Q q 1 j 1 J 0 w 10 0 0 10 200.0125 646.9228 cm BT /F6 1 Tf 1 TL -0.004 Tc 0 0 Td (Although)Tj 3.9968 0 Td (codes)Tj 2.4491 0 Td (of)Tj 1.0091 0 Td (conduct)Tj 3.4639 0 Td (will)Tj 1.6555 0 Td (vary)Tj 1.9275 0 Td (depending)Tj 4.4617 0 Td (on)Tj 1.2643 0 Td (the)Tj 1.4626 0 Td (topic)Tj 2.2224 0 Td (and)Tj 1.7234 0 Td (research)Tj 3.549 0 Td (community,)Tj 5.1023 0 Td (a)Tj 0.6463 0 Td (partic-)Tj -34.9339 -1.2982 Td (ularly)Tj 2.5001 0 Td (important)Tj 4.2803 0 Td (element)Tj 3.4015 0 Td (is)Tj 0.8504 0 Td (designing)Tj 4.0932 0 Td (data)Tj 1.9162 0 Td (and)Tj 1.7292 0 Td (systems)Tj 3.2995 0 Td (for)Tj 1.3833 0 Td (auditability.)Tj 4.9776 0 Td (Responsible)Tj 5.0003 0 Td (internal)Tj -33.4316 -1.2983 Td (auditing)Tj 3.5546 0 Td (processes)Tj 3.9685 0 Td (flow)Tj 1.9445 0 Td (easily)Tj 2.4094 0 Td (into)Tj 1.8369 0 Td (audit)Tj 2.279 0 Td (systems)Tj 3.2995 0 Td (and)Tj 1.7291 0 Td (also)Tj 1.7689 0 Td (keep)Tj 2.0806 0 Td (track)Tj 2.2393 0 Td (of)Tj 1.0148 0 Td (factors)Tj 2.9027 0 Td (that)Tj 1.7801 0 Td (might)Tj 2.5966 0 Td (con-)Tj -35.4045 -1.3039 Td (tribute)Tj 2.9139 0 Td (to)Tj 1.0262 0 Td (problematic)Tj 5.0343 0 Td (outcomes.)Tj 4.3086 0 Td (Developing)Tj 4.8132 0 Td (automated)Tj 4.4901 0 Td (testing)Tj 2.8799 0 Td (processes)Tj 3.9685 0 Td (for)Tj 1.389 0 Td (assessing)Tj 3.7984 0 Td (prob-)Tj -34.6221 -1.2983 Td (lematic)Tj 3.1294 0 Td (outcomes)Tj 4.0875 0 Td (and)Tj 1.7291 0 Td (mechanisms)Tj 5.2101 0 Td (for)Tj 1.3833 0 Td (auditing)Tj 3.5546 0 Td (other's)Tj 2.9253 0 Td (work)Tj 2.2791 0 Td (during)Tj 2.931 0 Td (review)Tj 2.8403 0 Td (processes)Tj 3.9685 0 Td (can)Tj 1.627 0 Td (help)Tj -35.6652 -1.2982 Td (strengthen)Tj 4.4843 0 Td (research)Tj 3.5547 0 Td (as)Tj 1.0148 0 Td (a)Tj 0.6576 0 Td (whole.)Tj 2.8346 0 Td (The)Tj 1.7802 0 Td (goal)Tj 1.8708 0 Td (of)Tj 1.0148 0 Td (auditability)Tj 4.7508 0 Td (is)Tj 0.8448 0 Td (to)Tj 1.0261 0 Td (clearly)Tj 2.8233 0 Td (document)Tj 4.2916 0 Td (when)Tj 2.4038 0 Td (decisions)Tj -33.3522 -1.3039 Td (are)Tj 1.4456 0 Td (made)Tj 2.4265 0 Td (and,)Tj 1.9558 0 Td (if)Tj 0.7767 0 Td (necessary,)Tj 4.2463 0 Td (backtrack)Tj 4.1045 0 Td (to)Tj 1.0262 0 Td (an)Tj 1.2019 0 Td (earlier)Tj 2.7552 0 Td (dataset)Tj 2.9991 0 Td (and)Tj 1.7291 0 Td (address)Tj 3.2258 0 Td (the)Tj 1.4683 0 Td (issue)Tj 2.16 0 Td (at)Tj 0.9524 0 Td (the)Tj 1.4684 0 Td (root)Tj -33.9418 -1.2983 Td (\(e.g.,)Tj 2.1259 0 Td (if)Tj 0.771 0 Td (strategies)Tj 3.9232 0 Td (for)Tj 1.3889 0 Td (anonymizing)Tj 5.4935 0 Td (data)Tj 1.9106 0 Td (are)Tj 1.4513 0 Td (compromised\).)Tj -15.8682 -1.2983 Td (Designing)Tj 4.3029 0 Td (for)Tj 1.389 0 Td (auditability)Tj 4.7508 0 Td (also)Tj 1.7632 0 Td (brings)Tj 2.7439 0 Td (direct)Tj 2.5285 0 Td (benefits)Tj 3.3392 0 Td (to)Tj 1.0261 0 Td (researchers)Tj 4.7112 0 Td (by)Tj 1.1792 0 Td (providing)Tj 4.1499 0 Td (a)Tj 0.6519 0 Td (mecha-)Tj -33.732 -1.3039 Td (nism)Tj 2.2166 0 Td (for)Tj 1.3833 0 Td (double-checking)Tj 6.8882 0 Td (work)Tj 2.279 0 Td (and)Tj 1.7291 0 Td (forcing)Tj 3.0898 0 Td (oneself)Tj 3.016 0 Td (to)Tj 1.0262 0 Td (be)Tj 1.1508 0 Td (explicit)Tj 3.1294 0 Td (about)Tj 2.4945 0 Td (decisions,)Tj 4.1329 0 Td (increasing)Tj -32.5358 -1.2983 Td (understandability)Tj 7.2623 0 Td (and)Tj 1.7291 0 Td (replicability.)Tj 5.1704 0 Td (For)Tj 1.627 0 Td (example,)Tj 3.7701 0 Td (many)Tj 2.4774 0 Td (types)Tj 2.2791 0 Td (of)Tj 1.0148 0 Td (social)Tj 2.4548 0 Td (media)Tj 2.6929 0 Td (and)Tj 1.7291 0 Td (other)Tj 2.3471 0 Td (trace)Tj -34.5541 -1.2982 Td (data)Tj 1.9105 0 Td (are)Tj 1.4513 0 Td (unstructured,)Tj 5.6523 0 Td (and)Tj 1.7291 0 Td (answers)Tj 3.4015 0 Td (to)Tj 1.0262 0 Td (even)Tj 2.0749 0 Td (basic)Tj 2.211 0 Td (questions)Tj 4.0195 0 Td (such)Tj 2.058 0 Td (as)Tj 1.0148 0 Td (network)Tj 3.5489 0 Td (ties,)Tj 1.7972 0 Td (location,)Tj 3.6793 0 Td (and)Tj -35.5745 -1.2983 Td (randomness)Tj 5.1193 0 Td (depend)Tj 3.1918 0 Td (on)Tj 1.2756 0 Td (the)Tj 1.4683 0 Td (steps)Tj 2.1827 0 Td (taken)Tj 2.4207 0 Td (to)Tj 1.0205 0 Td (collect)Tj 2.7893 0 Td (and)Tj 1.7291 0 Td (collate)Tj 2.8006 0 Td (data.)Tj 2.143 0 Td (Systems)Tj 3.4129 0 Td (of)Tj 1.0148 0 Td (auditability)Tj 4.7451 0 Td (clar-)Tj -35.3137 -1.3039 Td (ify)Tj 1.2302 0 Td (how)Tj 1.9389 0 Td (different)Tj 3.6566 0 Td (datasets)Tj 3.3562 0 Td (\(and)Tj 2.0693 0 Td (the)Tj 1.4683 0 Td (subsequent)Tj 4.7055 0 Td (analysis\))Tj 3.668 0 Td (differ)Tj 2.3868 0 Td (from)Tj 2.2053 0 Td (each)Tj 2.0296 0 Td (other,)Tj 2.5739 0 Td (aiding)Tj 2.7269 0 Td (under-)Tj -34.0155 -1.2983 Td (standing)Tj 3.668 0 Td (and)Tj 1.7291 0 Td (creating)Tj 3.4526 0 Td (better)Tj 2.5455 0 Td (research.)Tj -10.199 -1.2982 Td (Plan)Tj 2.0125 0 Td (for)Tj 1.3833 0 Td (and)Tj 1.7292 0 Td (welcome)Tj 3.7473 0 Td (audits)Tj 2.6363 0 Td (of)Tj 1.0147 0 Td (your)Tj 2.0807 0 Td (big)Tj 1.4626 0 Td (data)Tj 1.9106 0 Td (practices.)Tj ET Q q 1 j 1 J 0 w 11.9999 0 0 11.9999 200.0125 421.4551 cm BT /F2 1 Tf 1 TL -0.0033 Tc 0 0 Td (9.)Tj 1.0441 0 Td (Engage)Tj 3.8126 0 Td (with)Tj 2.2157 0 Td (the)Tj 1.7056 0 Td (broader)Tj 3.9165 0 Td (consequences)Tj 7.1103 0 Td (of)Tj 1.1575 0 Td (data)Tj 2.2583 0 Td (and)Tj 1.989 0 Td (analysis)Tj -25.2096 -1.1669 Td (practices)Tj ET Q q 1 j 1 J 0 w 10 0 0 10 200.0125 390.444 cm BT /F6 1 Tf 1 TL -0.004 Tc 0 0 Td (It)Tj 0.856 0 Td (is)Tj 0.8504 0 Td (also)Tj 1.7688 0 Td (important)Tj 4.2803 0 Td (for)Tj 1.3833 0 Td (responsible)Tj 4.7565 0 Td (big)Tj 1.457 0 Td (data)Tj 1.9162 0 Td (researchers)Tj 4.7112 0 Td (to)Tj 1.0261 0 Td (think)Tj 2.3584 0 Td (beyond)Tj 3.1861 0 Td (the)Tj 1.4684 0 Td (traditional)Tj 4.422 0 Td (metrics)Tj -34.4407 -1.2982 Td (of)Tj 1.0147 0 Td (success)Tj 3.1011 0 Td (in)Tj 1.0318 0 Td (business)Tj 3.5773 0 Td (and)Tj 1.7348 0 Td (the)Tj 1.4684 0 Td (academy.)Tj 3.9628 0 Td (For)Tj 1.627 0 Td (example,)Tj 3.7701 0 Td (the)Tj 1.474 0 Td (energy)Tj 2.9027 0 Td (demands)Tj 3.8664 0 Td (for)Tj 1.3833 0 Td (digital)Tj 2.7269 0 Td (daily)Tj 2.1487 0 Td (life,)Tj -35.79 -1.2983 Td (a)Tj 0.6519 0 Td (key)Tj 1.5931 0 Td (source)Tj 2.8346 0 Td (of)Tj 1.0148 0 Td (big)Tj 1.4627 0 Td (data)Tj 1.9105 0 Td (for)Tj 1.389 0 Td (social)Tj 2.4548 0 Td (science)Tj 3.0954 0 Td (research,)Tj 3.7814 0 Td (are)Tj 1.4456 0 Td (significant)Tj 4.388 0 Td (in)Tj 1.0375 0 Td (this)Tj 1.6724 0 Td (era)Tj 1.4514 0 Td (of)Tj 1.0148 0 Td (climate)Tj 3.1294 0 Td (change)Tj -34.3273 -1.3039 Td ([)Tj 0.83 0.64 0.02 0 k (36)Tj 0 g (].)Tj 2.0749 0 Td (How)Tj 2.177 0 Td (might)Tj 2.6022 0 Td (big)Tj 1.457 0 Td (data)Tj 1.9162 0 Td (research)Tj 3.5546 0 Td (lessen)Tj 2.5852 0 Td (the)Tj 1.4683 0 Td (environmental)Tj 6.1115 0 Td (impact)Tj 2.9763 0 Td (of)Tj 1.0148 0 Td (data)Tj 1.9163 0 Td (analytics)Tj 3.685 0 Td (work?)Tj -33.5393 -1.2983 Td (For)Tj 1.627 0 Td (example,)Tj 3.7701 0 Td (should)Tj 2.9196 0 Td (researchers)Tj 4.7112 0 Td (take)Tj 1.8709 0 Td (the)Tj 1.474 0 Td (lead)Tj 1.8481 0 Td (in)Tj 1.0375 0 Td (asking)Tj 2.7893 0 Td (cloud)Tj 2.4548 0 Td (storage)Tj 3.0784 0 Td (providers)Tj 4.0195 0 Td (and)Tj 1.7348 0 Td (data)Tj 1.9105 0 Td (pro-)Tj -35.2457 -1.2982 Td (cessing)Tj 3.067 0 Td (centers)Tj 3.0728 0 Td (to)Tj 1.0261 0 Td (shift)Tj 1.9616 0 Td (to)Tj 1.0261 0 Td (sustainable)Tj 4.6261 0 Td (and)Tj 1.7291 0 Td (renewable)Tj 4.2803 0 Td (energy)Tj 2.9083 0 Td (sources?)Tj 3.566 0 Td (As)Tj 1.2642 0 Td (important)Tj 4.2803 0 Td (and)Tj 1.7348 0 Td (pub-)Tj -34.5427 -1.2983 Td (licly)Tj 1.8538 0 Td (visible)Tj 2.7496 0 Td (users)Tj 2.262 0 Td (of)Tj 1.0148 0 Td (the)Tj 1.474 0 Td (cloud,)Tj 2.6759 0 Td (big)Tj 1.457 0 Td (data)Tj 1.9162 0 Td (researchers)Tj 4.7112 0 Td (collectively)Tj 4.6318 0 Td (represent)Tj 3.9628 0 Td (an)Tj 1.2018 0 Td (interest)Tj 3.2088 0 Td (group)Tj 2.6079 0 Td (that)Tj -35.7276 -1.3039 Td (could)Tj 2.4491 0 Td (rally)Tj 1.9729 0 Td (behind)Tj 3.0217 0 Td (such)Tj 2.0636 0 Td (a)Tj 0.652 0 Td (call)Tj 1.5703 0 Td (for)Tj 1.3833 0 Td (change.)Tj -11.9167 -1.2983 Td (The)Tj 1.7858 0 Td (pursuit)Tj 3.0897 0 Td (of)Tj 1.0148 0 Td (citations,)Tj 3.8494 0 Td (reputation,)Tj 4.6375 0 Td (or)Tj 1.0998 0 Td (money)Tj 2.9764 0 Td (is)Tj 0.8447 0 Td (a)Tj 0.652 0 Td (key)Tj 1.5987 0 Td (incentive)Tj 3.8721 0 Td (for)Tj 1.389 0 Td (pushing)Tj 3.4355 0 Td (research)Tj 3.5547 0 Td (for-)Tj -34.9963 -1.2982 Td (ward,)Tj 2.4604 0 Td (but)Tj 1.5534 0 Td (it)Tj 0.788 0 Td (can)Tj 1.6214 0 Td (also)Tj 1.7688 0 Td (result)Tj 2.4491 0 Td (in)Tj 1.0375 0 Td (unintended)Tj 4.8586 0 Td (and)Tj 1.7348 0 Td (undesirable)Tj 4.8528 0 Td (outcomes.)Tj 4.3143 0 Td (In)Tj 1.1112 0 Td (contrast,)Tj 3.685 0 Td (we)Tj 1.3266 0 Td (might)Tj 2.6022 0 Td (ask)Tj -36.1641 -1.304 Td (to)Tj 1.0261 0 Td (what)Tj 2.16 0 Td (extent)Tj 2.6815 0 Td (is)Tj 0.8447 0 Td (a)Tj 0.6577 0 Td (research)Tj 3.5546 0 Td (project)Tj 3.016 0 Td (focused)Tj 3.2769 0 Td (on)Tj 1.2756 0 Td (enhancing)Tj 4.4106 0 Td (the)Tj 1.4684 0 Td (public)Tj 2.7099 0 Td (good)Tj 2.2223 0 Td (or)Tj 1.0999 0 Td (the)Tj 1.4683 0 Td (underserved)Tj -31.8725 -1.2982 Td (of)Tj 1.0147 0 Td (society?)Tj 3.3279 0 Td (Are)Tj 1.6951 0 Td (questions)Tj 4.0195 0 Td (about)Tj 2.4945 0 Td (equity)Tj 2.6985 0 Td (or)Tj 1.0999 0 Td (promoting)Tj 4.524 0 Td (other)Tj 2.3471 0 Td (public)Tj 2.7043 0 Td (values)Tj 2.6702 0 Td (being)Tj 2.4321 0 Td (addressed)Tj 4.1782 0 Td (in)Tj -35.206 -1.2983 Td (one's)Tj 2.2733 0 Td (data)Tj 1.9162 0 Td (streams,)Tj 3.5093 0 Td (or)Tj 1.0998 0 Td (is)Tj 0.8504 0 Td (a)Tj 0.652 0 Td (big)Tj 1.4627 0 Td (data)Tj 1.9105 0 Td (focus)Tj 2.3301 0 Td (rendering)Tj 4.1669 0 Td (them)Tj 2.2847 0 Td (invisible)Tj 3.5659 0 Td (or)Tj 1.0942 0 Td (irrelevant)Tj 4.0592 0 Td (to)Tj 1.0261 0 Td (your)Tj 2.0806 0 Td (analysis)Tj -34.2819 -1.3039 Td ([)Tj 0.83 0.64 0.02 0 k (37)Tj 0 g (]?)Tj 2.2223 0 Td (How)Tj 2.177 0 Td (can)Tj 1.6271 0 Td (increasingly)Tj 5.0286 0 Td (vulnerable)Tj 4.405 0 Td (yet)Tj 1.3946 0 Td (fundamentally)Tj 6.0208 0 Td (important)Tj 4.2803 0 Td (public)Tj 2.7099 0 Td (resourcesÐsuch)Tj 6.7521 0 Td (as)Tj -36.6177 -1.2983 Td (state-mandated)Tj 6.4005 0 Td (cancer)Tj 2.8403 0 Td (registriesÐbe)Tj 5.6863 0 Td (protected?)Tj 4.3823 0 Td (How)Tj 2.177 0 Td (might)Tj 2.5965 0 Td (research)Tj 3.5547 0 Td (aid)Tj 1.4513 0 Td (or)Tj 1.0998 0 Td (inhibit)Tj 2.9027 0 Td (different)Tj -33.0914 -1.2982 Td (business)Tj 3.5773 0 Td (and)Tj 1.7291 0 Td (political)Tj 3.4242 0 Td (actors?)Tj 2.9877 0 Td (While)Tj 2.6532 0 Td (all)Tj 1.1452 0 Td (big)Tj 1.457 0 Td (data)Tj 1.9162 0 Td (research)Tj 3.5546 0 Td (need)Tj 2.143 0 Td (not)Tj 1.5761 0 Td (take)Tj 1.8708 0 Td (up)Tj 1.2643 0 Td (social)Tj 2.4604 0 Td (and)Tj 1.7291 0 Td (cultural)Tj -33.4882 -1.2983 Td (questions,)Tj 4.2462 0 Td (a)Tj 0.652 0 Td (fundamental)Tj 5.3177 0 Td (aim)Tj 1.7405 0 Td (of)Tj 1.0148 0 Td (research)Tj 3.5546 0 Td (goes)Tj 1.9786 0 Td (beyond)Tj 3.1861 0 Td (understanding)Tj 6.0718 0 Td (the)Tj 1.4683 0 Td (world)Tj 2.5569 0 Td (to)Tj 1.0261 0 Td (considering)Tj -32.8136 -1.3039 Td (ways)Tj 2.1543 0 Td (to)Tj 1.0261 0 Td (improve)Tj 3.5773 0 Td (it.)Tj -5.5615 -1.2983 Td (Recognize)Tj 4.3143 0 Td (that)Tj 1.7801 0 Td (doing)Tj 2.5342 0 Td (big)Tj 1.4626 0 Td (data)Tj 1.9106 0 Td (research)Tj 3.5546 0 Td (has)Tj 1.542 0 Td (societal-wide)Tj 5.4369 0 Td (effects.)Tj ET Q q 1 j 1 J 0 w 11.9999 0 0 11.9999 200.0125 125.9716 cm BT /F2 1 Tf 1 TL -0.0033 Tc 0 0 Td (10.)Tj 1.5921 0 Td (Know)Tj 2.9339 0 Td (when)Tj 2.7685 0 Td (to)Tj 1.1575 0 Td (break)Tj 2.8583 0 Td (these)Tj 2.8063 0 Td (rules)Tj ET Q q 1 j 1 J 0 w 10 0 0 10 200.0125 109.0204 cm BT /F6 1 Tf 1 TL -0.004 Tc 0 0 Td (The)Tj 1.7801 0 Td (final)Tj 2.0069 0 Td (\(and)Tj 2.0693 0 Td (counterintuitive\))Tj 7.0129 0 Td (rule)Tj 1.7858 0 Td (is)Tj 0.8504 0 Td (the)Tj 1.4683 0 Td (charge)Tj 2.8686 0 Td (to)Tj 1.0205 0 Td (recognize)Tj 4.0649 0 Td (when)Tj 2.4037 0 Td (it)Tj 0.7881 0 Td (is)Tj 0.8447 0 Td (appropriate)Tj 4.8812 0 Td (to)Tj 1.0205 0 Td (stray)Tj -34.8659 -1.3039 Td (from)Tj 2.2053 0 Td (these)Tj 2.2507 0 Td (rules.)Tj 2.3754 0 Td (For)Tj 1.6271 0 Td (example,)Tj 3.7757 0 Td (in)Tj 1.0375 0 Td (times)Tj 2.3867 0 Td (of)Tj 1.0148 0 Td (natural)Tj 3.0841 0 Td (disaster)Tj 3.2598 0 Td (or)Tj 1.0999 0 Td (a)Tj 0.6519 0 Td (public)Tj 2.7099 0 Td (health)Tj 2.6759 0 Td (emergency,)Tj 4.8019 0 Td (it)Tj 0.7823 0 Td (may)Tj -35.7389 -1.2982 Td (be)Tj 1.1508 0 Td (important)Tj 4.2803 0 Td (to)Tj 1.0261 0 Td (temporarily)Tj 4.9266 0 Td (put)Tj 1.5647 0 Td (aside)Tj 2.2337 0 Td (questions)Tj 4.0195 0 Td (of)Tj 1.0148 0 Td (individual)Tj 4.2916 0 Td (privacy)Tj 3.1408 0 Td (in)Tj 1.0375 0 Td (order)Tj 2.4208 0 Td (to)Tj 1.0261 0 Td (serve)Tj 2.2507 0 Td (a)Tj 0.6576 0 Td (larger)Tj ET Q q 1 j 1 J 0 w 576 737.1 m 36 737.1 l 36 737.6 l 576 737.6 l f* 36 741.2598 169.285 23.6976 re W* n q 169.2283 0 0 23.6409 36 741.3165 cm q /I0 Do Q Q Q q 0 0 612 792 re W* n 1 j 1 J 0 w 7.9999 0 0 7.9999 391.9748 745.7952 cm BT /F0 1 Tf 1 TL -0.005 Tc 0 0 Td (Ten)Tj 1.9204 0 Td (Simple)Tj 3.2245 0 Td (Rules)Tj 2.7283 0 Td (for)Tj 1.3678 0 Td (Respons)Tj 3.8764 0 Td (ible)Tj 1.7362 0 Td (Big)Tj 1.637 0 Td (Data)Tj 2.3032 0 Td (Researc)Tj 3.6567 0 Td (h)Tj ET Q q 1 j 1 J 0 w 36 48.0002 m 576 48.0002 l 576 47.5002 l 36 47.5002 l f* 0.83 0.64 0.02 0 k 142.5827 34.9228 m 295.8236 34.9228 l h f* 0 g 7.9999 0 0 7.9999 36 36 cm BT /F0 1 Tf 1 TL -0.005 Tc 0 0 Td (PLOS)Tj 2.8559 0 Td (Computationa)Tj 6.1229 0 Td (l)Tj 0.4322 0 Td (Biology)Tj 3.4371 0 Td (|)Tj 0.83 0.64 0.02 0 k 0.4748 0 Td (https:/)Tj 2.6788 0 Td (/doi.org/10.13)Tj 6.0094 0 Td (71/journal.p)Tj 5.1166 0 Td (cbi.1005399)Tj 0 g 6.3497 0 Td (March)Tj 2.9551 0 Td (30,)Tj 1.5874 0 Td (2017)Tj 27.1207 0 Td (7)Tj 0.7654 0 Td (/)Tj ET endstream endobj 139 0 obj < >stream endstream endobj 140 0 obj < >stream endstream endobj 141 0 obj < >stream endstream endobj 142 0 obj < >stream endstream endobj 143 0 obj < >stream BT 66.402 0 Td (10)Tj ET Q endstream endobj 144 0 obj < >/XObject< >>>/CropBox[0 0 612 792]/MediaBox[0 0 612 792]/Parent 9 0 R/Annots 146 0 R/Contents 147 0 R/TrimBox[0 0 612 792]>> endobj 145 0 obj < > endobj 148 0 obj < > endobj 151 0 obj < >stream   MinionPro-Regular 6ø  à á âøûµûü ú)   ±í   H?   HDÎ   H¹\        ' - 3 > H N T Z ` j q t {    ¢ ¦ ± · ¹ À Ç Í Ø ä ê ð ü   +2=CIU[cnz~£ª±¶¼ÂÍÖÜçíóý "(.9CIOU[elov|¡¬²´»ÂÈÓßåë÷û&-8>DPV^iuy¥¬±·½ÈÑ×âèîø &+/49@DGLPU[]_ahjmru|  ¨±½ÉÕäîõü  &-4;BIPW^elsz¤«²¹ÀÇÎÕÜãêñøÿ ")07>ELSZahov} §®µ¼ÃÊÑØßæíôû  %,3:AHOV]dkry¡¨¬¯´¸½ÃÅÇÎÐÓØÛâåèëðú#/;JTey£ª±¸¿ÆÍÔÛâéð÷þ    ! (extracted from544.html)
o÷§ ¿´÷÷½ ­ q÷k ÷& ¥ø³ ¦ø)¦ M q ÷÷aÙ÷,§¦ ltthc] Y`o ~|{nOÃfÐ ¯­} vea ø ¸·µûû ¨÷t£  ÃWwîw Òû¸ q÷j¥ ÷% ¤¢ÐÄÑ ÷Å6eÚvû4 »Ë ®ûÌ ¶bØ ð  Ò÷ Ó TD==Ã]Ñm ¢ÿ ÿ   cs³j B\^Xa«p©| ~I 2©÷Î÷\ ÷M ÷I û¥ùBw  ¤û]r ÷¹÷P ¦÷Ü / ¾Ù÷ ®x¸ ÷»±}ûB ÷°}û. v«vø»Ô øvøtndR ÷¹Ç©÷cÞ v}yop ^ªø%÷B ÷Ü endstream endobj 149 0 obj < > endobj 150 0 obj < >stream /CIDInit/ProcSet findresource begin 12 dict begin begincmap /CMapName/Adobe-Identity-UCS def /CIDSystemInfo< >def /CMapType 2 def 1 begincodespacerange <00> endcodespacerange 1 beginbfchar <01><00EF> endbfchar endcmap CMapName currentdict /CMap defineresource pop end end endstream endobj 146 0 obj [152 0 R 153 0 R 154 0 R 155 0 R 156 0 R 157 0 R 158 0 R 159 0 R] endobj 152 0 obj < >/Border[0 0 0]/Dest(Rpcbi.1005399.ref038)>> endobj 153 0 obj < >/Border[0 0 0]/Dest(Rpcbi.1005399.ref018)>> endobj 154 0 obj < >/Border[0 0 0]/A 160 0 R>> endobj 160 0 obj < > endobj 155 0 obj < >/Border[0 0 0]/Dest(Lpcbi.1005399.ref001)>> endobj 156 0 obj < >/Border[0 0 0]/A 161 0 R>> endobj 161 0 obj < > endobj 157 0 obj < >/Border[0 0 0]/A 162 0 R>> endobj 162 0 obj < > endobj 158 0 obj < >/Border[0 0 0]/Dest(Lpcbi.1005399.ref002)>> endobj 159 0 obj < >/Border[0 0 0]/A 163 0 R>> endobj 163 0 obj < > endobj 147 0 obj [164 0 R 165 0 R 166 0 R 167 0 R 168 0 R 169 0 R] endobj 164 0 obj < >stream q 0.83 0.64 0.02 0 k 265.5496 423.0425 m 275.0173 423.0425 l h f* 417.1465 397.0205 m 426.6709 397.0205 l h f* 384.8315 267.2504 m 493.115 267.2504 l 493.115 266.7969 l 384.8315 266.7969 l f* 287.5465 123.4205 m 560.6362 123.4205 l h f* 221.4992 113.8961 m 248.2016 113.8961 l h f* 0 g 1 j 1 J 0 w 10 0 0 10 200.0125 707.4141 cm BT /F6 1 Tf 1 TL -0.004 Tc 0 0 Td (public)Tj 2.7099 0 Td (good.)Tj 2.4491 0 Td (Likewise,)Tj 3.9061 0 Td (the)Tj 1.4683 0 Td (use)Tj 1.5364 0 Td (of)Tj 1.0148 0 Td (genetic)Tj 3.0671 0 Td (or)Tj 1.0998 0 Td (other)Tj 2.3471 0 Td (biological)Tj 4.0875 0 Td (data)Tj 1.9162 0 Td (collected)Tj 3.7417 0 Td (without)Tj 3.3279 0 Td (informed)Tj -32.6719 -1.3039 Td (consent)Tj 3.3278 0 Td (might)Tj 2.6022 0 Td (be)Tj 1.1452 0 Td (vital)Tj 1.9219 0 Td (in)Tj 1.0374 0 Td (managing)Tj 4.201 0 Td (an)Tj 1.2018 0 Td (emerging)Tj 4.0025 0 Td (disease)Tj 3.0217 0 Td (epidemic.)Tj -21.2653 -1.2982 Td (Moreover,)Tj 4.3993 0 Td (be)Tj 1.1509 0 Td (sure)Tj 1.9048 0 Td (to)Tj 1.0205 0 Td (review)Tj 2.846 0 Td (the)Tj 1.4683 0 Td (regulatory)Tj 4.32 0 Td (expectations)Tj 5.1873 0 Td (and)Tj 1.7292 0 Td (legal)Tj 2.0352 0 Td (demands)Tj 3.8665 0 Td (associated)Tj 4.2519 0 Td (with)Tj -35.3761 -1.2983 Td (protection)Tj 4.3823 0 Td (of)Tj 1.0148 0 Td (privacy)Tj 3.1407 0 Td (within)Tj 2.812 0 Td (your)Tj 2.0806 0 Td (dataset.)Tj 3.2258 0 Td (After)Tj 2.2847 0 Td (all,)Tj 1.372 0 Td (this)Tj 1.6724 0 Td (is)Tj 0.8504 0 Td (an)Tj 1.2019 0 Td (exceedingly)Tj 4.8869 0 Td (slippery)Tj 3.3789 0 Td (slope,)Tj 2.5001 0 Td (so)Tj -34.8035 -1.2983 Td (before)Tj 2.7382 0 Td (following)Tj 3.9798 0 Td (this)Tj 1.6724 0 Td (rule)Tj 1.7915 0 Td (\(to)Tj 1.3663 0 Td (break)Tj 2.4491 0 Td (others\),)Tj 3.2769 0 Td (be)Tj 1.1508 0 Td (cautious)Tj 3.566 0 Td (that)Tj 1.7745 0 Td (the)Tj 1.474 0 Td (ªemergencyº)Tj 5.3574 0 Td (is)Tj 0.8447 0 Td (not)Tj 1.5761 0 Td (simply)Tj 2.8856 0 Td (a)Tj -35.9033 -1.3039 Td (convenient)Tj 4.6601 0 Td (justification.)Tj 5.193 0 Td (The)Tj 1.7802 0 Td (best)Tj 1.8085 0 Td (way)Tj 1.7971 0 Td (to)Tj 1.0205 0 Td (ensure)Tj 2.88 0 Td (this)Tj 1.6724 0 Td (is)Tj 0.8504 0 Td (to)Tj 1.0204 0 Td (build)Tj 2.2961 0 Td (experience)Tj 4.507 0 Td (in)Tj 1.0375 0 Td (engaging)Tj 3.8381 0 Td (in)Tj 1.0375 0 Td (the)Tj -35.3988 -1.2982 Td (tough)Tj 2.5454 0 Td (debates)Tj 3.1918 0 Td (\(Rule)Tj 2.3811 0 Td (6\),)Tj 1.2586 0 Td (constructing)Tj 5.2611 0 Td (codes)Tj 2.4661 0 Td (of)Tj 1.0148 0 Td (conduct)Tj 3.4752 0 Td (\(Rule)Tj 2.3811 0 Td (7\),)Tj 1.2586 0 Td (and)Tj 1.7291 0 Td (developing)Tj 4.6035 0 Td (systems)Tj 3.2995 0 Td (for)Tj -34.8659 -1.2983 Td (auditing)Tj 3.5546 0 Td (\(Rule)Tj 2.3811 0 Td (8\).)Tj 1.2585 0 Td (The)Tj 1.7802 0 Td (more)Tj 2.3414 0 Td (mature)Tj 3.0954 0 Td (the)Tj 1.474 0 Td (community)Tj 4.8812 0 Td (of)Tj 1.0148 0 Td (researchers)Tj 4.7169 0 Td (is)Tj 0.8447 0 Td (about)Tj 2.4944 0 Td (their)Tj 2.109 0 Td (processes,)Tj -31.9462 -1.3039 Td (checks,)Tj 3.101 0 Td (and)Tj 1.7292 0 Td (balances,)Tj 3.821 0 Td (the)Tj 1.474 0 Td (better)Tj 2.5399 0 Td (equipped)Tj 3.9288 0 Td (it)Tj 0.788 0 Td (is)Tj 0.8447 0 Td (to)Tj 1.0261 0 Td (assess)Tj 2.5229 0 Td (when)Tj 2.4037 0 Td (breaking)Tj 3.7304 0 Td (the)Tj 1.474 0 Td (rules)Tj 2.1486 0 Td (is)Tj 0.8448 0 Td (acceptable.)Tj 4.5807 0 Td (It)Tj -36.9578 -1.2983 Td (may)Tj 1.9275 0 Td (very)Tj 1.9219 0 Td (well)Tj 1.8198 0 Td (be)Tj 1.1509 0 Td (that)Tj 1.7801 0 Td (you)Tj 1.7121 0 Td (do)Tj 1.2529 0 Td (not)Tj 1.5704 0 Td (come)Tj 2.3981 0 Td (to)Tj 1.0261 0 Td (a)Tj 0.652 0 Td (final)Tj 2.0013 0 Td (clear)Tj 2.1203 0 Td (set)Tj 1.3039 0 Td (of)Tj 1.0148 0 Td (practices.)Tj 3.9628 0 Td (After)Tj 2.2847 0 Td (all,)Tj 1.372 0 Td (just)Tj 1.6667 0 Td (as)Tj 1.0148 0 Td (privacy)Tj -33.9531 -1.2983 Td (is)Tj 0.8447 0 Td (not)Tj 1.576 0 Td (binary)Tj 2.8006 0 Td (\(Rule)Tj 2.3811 0 Td (2\),)Tj 1.2586 0 Td (neither)Tj 3.0841 0 Td (is)Tj 0.8447 0 Td (responsible)Tj 4.7565 0 Td (research.)Tj 3.7814 0 Td (Ethics)Tj 2.6645 0 Td (is)Tj 0.8504 0 Td (often)Tj 2.2847 0 Td (about)Tj 2.4945 0 Td (finding)Tj 3.1294 0 Td (a)Tj 0.6577 0 Td (good)Tj 2.2223 0 Td (or)Tj -35.6312 -1.3039 Td (better,)Tj 2.7665 0 Td (but)Tj 1.5534 0 Td (not)Tj 1.5704 0 Td (perfect,)Tj 3.1918 0 Td (answer,)Tj 3.2712 0 Td (and)Tj 1.7291 0 Td (it)Tj 0.7823 0 Td (is)Tj 0.8504 0 Td (important)Tj 4.2803 0 Td (to)Tj 1.0261 0 Td (ask)Tj 1.5137 0 Td (\(and)Tj 2.0693 0 Td (try)Tj 1.3436 0 Td (to)Tj 1.0262 0 Td (answer\))Tj 3.3845 0 Td (the)Tj 1.4684 0 Td (challenging)Tj -31.8272 -1.2982 Td (questions.)Tj 4.2462 0 Td (Only)Tj 2.2167 0 Td (through)Tj 3.4412 0 Td (this)Tj 1.6781 0 Td (engagement)Tj 5.0684 0 Td (can)Tj 1.627 0 Td (a)Tj 0.652 0 Td (culture)Tj 3.0387 0 Td (of)Tj 1.0148 0 Td (responsible)Tj 4.7565 0 Td (big)Tj 1.457 0 Td (data)Tj 1.9162 0 Td (research)Tj -31.1128 -1.2983 Td (emerge.)Tj 1.1962 -1.3039 Td (Understand)Tj 5.0002 0 Td (that)Tj 1.7802 0 Td (responsible)Tj 4.7508 0 Td (big)Tj 1.457 0 Td (data)Tj 1.9162 0 Td (research)Tj 3.5547 0 Td (depends)Tj 3.5489 0 Td (on)Tj 1.2756 0 Td (more)Tj 2.3414 0 Td (than)Tj 2.0296 0 Td (meeting)Tj 3.4639 0 Td (checklists.)Tj ET Q q 1 j 1 J 0 w 11.9999 0 0 11.9999 200.0125 493.1149 cm BT /F2 1 Tf 1 TL -0.0033 Tc 0 0 Td (Conclusion)Tj ET Q q 1 j 1 J 0 w 10 0 0 10 200.0125 476.107 cm BT /F6 1 Tf 1 TL -0.004 Tc 0 0 Td (The)Tj 1.7801 0 Td (goal)Tj 1.8709 0 Td (of)Tj 1.0147 0 Td (this)Tj 1.6781 0 Td (set)Tj 1.304 0 Td (of)Tj 1.0148 0 Td (ten)Tj 1.4853 0 Td (rules)Tj 2.1543 0 Td (is)Tj 0.8448 0 Td (to)Tj 1.0261 0 Td (help)Tj 1.9332 0 Td (researchers)Tj 4.7112 0 Td (do)Tj 1.2585 0 Td (better)Tj 2.5399 0 Td (work)Tj 2.279 0 Td (and)Tj 1.7291 0 Td (ultimately)Tj 4.235 0 Td (become)Tj -32.859 -1.2982 Td (more)Tj 2.3414 0 Td (successful)Tj 4.1612 0 Td (while)Tj 2.3697 0 Td (avoiding)Tj 3.6907 0 Td (larger)Tj 2.5342 0 Td (complications,)Tj 6.0604 0 Td (including)Tj 4.0422 0 Td (public)Tj 2.7099 0 Td (mistrust.)Tj 3.753 0 Td (To)Tj 1.338 0 Td (achieve)Tj -33.0007 -1.3039 Td (this,)Tj 1.8992 0 Td (however,)Tj 3.838 0 Td (scholars)Tj 3.4526 0 Td (must)Tj 2.2224 0 Td (shift)Tj 1.9615 0 Td (from)Tj 2.2054 0 Td (a)Tj 0.6576 0 Td (mindset)Tj 3.4639 0 Td (that)Tj 1.7802 0 Td (is)Tj 0.8447 0 Td (rigorous)Tj 3.6 0 Td (when)Tj 2.4037 0 Td (focused)Tj 3.2769 0 Td (on)Tj 1.2755 0 Td (techniques)Tj -32.8816 -1.2983 Td (and)Tj 1.7291 0 Td (methodology)Tj 5.5048 0 Td (and)Tj 1.7291 0 Td (na)Tj /F12 1 Tf [()]TJ /F6 1 Tf (ve)Tj 2.3358 0 Td (when)Tj 2.4037 0 Td (it)Tj 0.7881 0 Td (comes)Tj 2.7552 0 Td (to)Tj 1.0262 0 Td (ethics.)Tj 2.7439 0 Td (Statements)Tj 4.6034 0 Td (to)Tj 1.0205 0 Td (the)Tj 1.474 0 Td (effect)Tj 2.3641 0 Td (that)Tj 1.7801 0 Td (ªData)Tj 2.5115 0 Td (is)Tj 0.8504 0 Td ([sic])Tj -35.6199 -1.2983 Td (already)Tj 3.1124 0 Td (publicº)Tj 3.1011 0 Td ([)Tj 0.83 0.64 0.02 0 k (38)Tj 0 g (])Tj 1.8481 0 Td (are)Tj 1.4514 0 Td (unjustified)Tj 4.5127 0 Td (simplifications)Tj 6.0944 0 Td (of)Tj 1.0148 0 Td (much)Tj 2.5172 0 Td (more)Tj 2.3414 0 Td (complex)Tj 3.6226 0 Td (data)Tj 1.9106 0 Td (ecosystems)Tj -31.5267 -1.3039 Td (embedded)Tj 4.3993 0 Td (in)Tj 1.0375 0 Td (even)Tj 2.0692 0 Td (more)Tj 2.3414 0 Td (complex)Tj 3.6227 0 Td (and)Tj 1.7291 0 Td (contingent)Tj 4.5468 0 Td (social)Tj 2.4604 0 Td (practices.)Tj 3.9628 0 Td (Data)Tj 2.126 0 Td (are)Tj 1.4457 0 Td (people,)Tj 3.084 0 Td (and)Tj 1.7292 0 Td (to)Tj -34.5541 -1.2982 Td (maintain)Tj 3.8324 0 Td (a)Tj 0.6463 0 Td (rigorously)Tj 4.2973 0 Td (na)Tj /F12 1 Tf [()]TJ /F6 1 Tf (ve)Tj 2.33 0 Td (definition)Tj 4.1613 0 Td (to)Tj 1.0204 0 Td (the)Tj 1.4627 0 Td (contrary)Tj 3.6226 0 Td ([)Tj 0.83 0.64 0.02 0 k (18)Tj 0 g (])Tj 1.8482 0 Td (will)Tj 1.6611 0 Td (end)Tj 1.7121 0 Td (up)Tj 1.2643 0 Td (harming)Tj 3.6453 0 Td (research)Tj 3.5546 0 Td (efforts)Tj -35.0586 -1.2983 Td (in)Tj 1.0318 0 Td (the)Tj 1.474 0 Td (long)Tj 1.9842 0 Td (run)Tj 1.6668 0 Td (as)Tj 1.0148 0 Td (pushback)Tj 4.0138 0 Td (comes)Tj 2.7552 0 Td (from)Tj 2.2054 0 Td (the)Tj 1.474 0 Td (people)Tj 2.8516 0 Td (whose)Tj 2.7269 0 Td (actions)Tj 3.0558 0 Td (and)Tj 1.7291 0 Td (utterances)Tj 4.3313 0 Td (are)Tj 1.4513 0 Td (subject)Tj 3.0161 0 Td (to)Tj -36.7821 -1.3039 Td (analysis.)Tj 1.1962 -1.2983 Td (In)Tj 1.1111 0 Td (short,)Tj 2.5115 0 Td (responsible)Tj 4.7509 0 Td (big)Tj 1.4626 0 Td (data)Tj 1.9106 0 Td (research)Tj 3.5546 0 Td (is)Tj 0.8504 0 Td (not)Tj 1.576 0 Td (about)Tj 2.4945 0 Td (preventing)Tj 4.5354 0 Td (research)Tj 3.5546 0 Td (but)Tj 1.5534 0 Td (making)Tj 3.2542 0 Td (sure)Tj -34.316 -1.2983 Td (that)Tj 1.7744 0 Td (the)Tj 1.474 0 Td (work)Tj 2.2791 0 Td (is)Tj 0.8447 0 Td (sound,)Tj 2.9197 0 Td (accurate,)Tj 3.7814 0 Td (and)Tj 1.7291 0 Td (maximizes)Tj 4.49 0 Td (the)Tj 1.4684 0 Td (good)Tj 2.228 0 Td (while)Tj 2.3697 0 Td (minimizing)Tj 4.9039 0 Td (harm.)Tj 2.5966 0 Td (The)Tj 1.7801 0 Td (prob-)Tj -34.6391 -1.2982 Td (lems)Tj 2.0692 0 Td (and)Tj 1.7292 0 Td (choices)Tj 3.1521 0 Td (researchers)Tj 4.7111 0 Td (face)Tj 1.7915 0 Td (are)Tj 1.4513 0 Td (real,)Tj 1.9162 0 Td (complex,)Tj 3.8495 0 Td (and)Tj 1.7291 0 Td (challenging)Tj 4.8188 0 Td (and)Tj 1.7292 0 Td (so)Tj 1.0885 0 Td (too)Tj 1.5307 0 Td (must)Tj 2.228 0 Td (be)Tj 1.1508 0 Td (our)Tj -34.9452 -1.3039 Td (response.)Tj 3.9571 0 Td (We)Tj 1.6101 0 Td (must)Tj 2.228 0 Td (treat)Tj 2.0409 0 Td (big)Tj 1.4627 0 Td (data)Tj 1.9105 0 Td (research)Tj 3.5603 0 Td (with)Tj 1.9956 0 Td (the)Tj 1.4683 0 Td (respect)Tj 3.0387 0 Td (that)Tj 1.7802 0 Td (it)Tj 0.7823 0 Td (deserves)Tj 3.566 0 Td (and)Tj 1.7348 0 Td (recognize)Tj 4.0592 0 Td (that)Tj -35.1947 -1.2983 Td (unethical)Tj 3.9174 0 Td (research)Tj 3.5546 0 Td (undermines)Tj 5.04 0 Td (the)Tj 1.4683 0 Td (production)Tj 4.7169 0 Td (of)Tj 1.0148 0 Td (knowledge.)Tj 4.7678 0 Td (Fantastic)Tj 3.8097 0 Td (opportunities)Tj 5.6296 0 Td (to)Tj 1.0261 0 Td (better)Tj -34.9452 -1.2983 Td (understand)Tj 4.7791 0 Td (society)Tj 2.948 0 Td (and)Tj 1.7178 0 Td (our)Tj 1.6158 0 Td (world)Tj 2.5511 0 Td (exist,)Tj 2.2507 0 Td (but)Tj 1.5421 0 Td (with)Tj 1.9899 0 Td (these)Tj 2.245 0 Td (opportunities)Tj 5.6239 0 Td (also)Tj 1.7575 0 Td (come)Tj 2.3867 0 Td (the)Tj 1.457 0 Td (responsibil-)Tj -32.8646 -1.3039 Td (ity)Tj 1.2415 0 Td (to)Tj 1.0205 0 Td (consider)Tj 3.651 0 Td (the)Tj 1.4683 0 Td (ethics)Tj 2.5228 0 Td (of)Tj 1.0148 0 Td (our)Tj 1.6271 0 Td (choices)Tj 3.1521 0 Td (in)Tj 1.0375 0 Td (the)Tj 1.4683 0 Td (everyday)Tj 3.7644 0 Td (practices)Tj 3.7417 0 Td (and)Tj 1.7291 0 Td (actions)Tj 3.0558 0 Td (of)Tj 1.0148 0 Td (our)Tj 1.627 0 Td (research.)Tj -33.1367 -1.2982 Td (The)Tj 1.7801 0 Td (Council)Tj 3.4015 0 Td (for)Tj 1.389 0 Td (Big)Tj 1.5307 0 Td (Data,)Tj 2.3471 0 Td (Ethics,)Tj 2.897 0 Td (and)Tj 1.7291 0 Td (Society)Tj 3.0671 0 Td (\()Tj 0.83 0.64 0.02 0 k (http://bdes.datasociet)Tj 8.8894 0 Td (y.net/)Tj 0 g (\))Tj 2.8403 0 Td (provides)Tj 3.6453 0 Td (an)Tj 1.2019 0 Td (initial)Tj -34.7185 -1.2983 Td (set)Tj 1.3039 0 Td (of)Tj 1.0148 0 Td (case)Tj 1.8595 0 Td (studies,)Tj 3.2145 0 Td (papers,)Tj 3.067 0 Td (and)Tj 1.7291 0 Td (even)Tj 2.0693 0 Td (ten)Tj 1.491 0 Td (simple)Tj 2.8573 0 Td (rules)Tj 2.1487 0 Td (for)Tj 1.389 0 Td (guiding)Tj 3.2881 0 Td (this)Tj 1.6725 0 Td (process;)Tj 3.4072 0 Td (it)Tj 0.788 0 Td (is)Tj 0.8447 0 Td (now)Tj 1.9616 0 Td (incum-)Tj -34.1062 -1.3039 Td (bent)Tj 1.9955 0 Td (on)Tj 1.2756 0 Td (you)Tj 1.7121 0 Td (to)Tj 1.0205 0 Td (use)Tj 1.5364 0 Td (and)Tj 1.7291 0 Td (improve)Tj 3.5773 0 Td (these)Tj 2.2563 0 Td (in)Tj 1.0375 0 Td (your)Tj 2.0806 0 Td (research.)Tj ET Q q 1 j 1 J 0 w 11.9999 0 0 11.9999 200.0125 196.7811 cm BT /F2 1 Tf 1 TL -0.0033 Tc 0 0 Td (Acknowledgmen)Tj 7.8945 0 Td (ts)Tj ET Q q 1 j 1 J 0 w 10 0 0 10 200.0125 179.7732 cm BT /F6 1 Tf 1 TL -0.004 Tc 0 0 Td (This)Tj 1.9842 0 Td (article)Tj 2.6872 0 Td (also)Tj 1.7688 0 Td (benefitted)Tj 4.2236 0 Td (from)Tj 2.2054 0 Td (the)Tj 1.4683 0 Td (input)Tj 2.3811 0 Td (of)Tj 1.0148 0 Td (Geoff)Tj 2.4434 0 Td (Bowker)Tj 3.2769 0 Td (and)Tj 1.7348 0 Td (Helen)Tj 2.6248 0 Td (Nissenbaum.)Tj ET Q q 1 j 1 J 0 w 11.9999 0 0 11.9999 200.0125 147.5149 cm BT /F2 1 Tf 1 TL -0.0033 Tc 0 0 Td (References)Tj ET Q q 1 j 1 J 0 w 7.9999 0 0 7.9999 204.8881 134.022 cm BT /F2 1 Tf 1 TL -0.005 Tc 0 0 Td (1.)Tj /F0 1 Tf 2.0763 0 Td (Metcalf)Tj 3.3946 0 Td (J,)Tj 0.985 0 Td (boyd)Tj 2.3528 0 Td (d,)Tj 1.0417 0 Td (Keller)Tj 2.7213 0 Td (E.)Tj 1.1551 0 Td (Perspecti)Tj 4.0961 0 Td (ves)Tj 1.7575 0 Td (on)Tj 1.3111 0 Td (Big)Tj 1.637 0 Td (Data,)Tj 2.5724 0 Td (Ethics,)Tj 3.1749 0 Td (and)Tj 1.8567 0 Td (Society.)Tj 3.7205 0 Td (Council)Tj 3.4866 0 Td (for)Tj 1.3677 0 Td (Big)Tj 1.6371 0 Td (Data,)Tj 2.5724 0 Td (Ethics,)Tj -40.8405 -1.1905 Td (and)Tj 1.8568 0 Td (Society.)Tj 3.7205 0 Td (2016.)Tj 0.83 0.64 0.02 0 k 2.6787 0 Td (http://bdes)Tj 4.5922 0 Td (.datasociety.n)Tj 6.0662 0 Td (et/council-ou)Tj 5.563 0 Td (tput/perspec)Tj 5.4142 0 Td (tives-on-big-)Tj 5.4001 0 Td (data-eth)Tj 3.6 0 Td (ics-and-)Tj -38.8917 -1.1906 Td (society/)Tj 0 g (.)Tj 3.8268 0 Td (Accessed)Tj 4.493 0 Td (31)Tj 1.311 0 Td (May)Tj 2.0835 0 Td (2016.)Tj /F2 1 Tf -13.7906 -1.6228 Td (2.)Tj /F0 1 Tf 2.0763 0 Td (Wu)Tj 1.7079 0 Td (HY,)Tj 1.8638 0 Td (Rubinstein)Tj 4.8544 0 Td (M,)Tj 1.3181 0 Td (Shih)Tj 2.1827 0 Td (E,)Tj 1.1551 0 Td (Guttag)Tj 3.182 0 Td (JV,)Tj 1.6511 0 Td (Durand)Tj 3.4371 0 Td (F,)Tj 1.1055 0 Td (Freeman)Tj 4.1599 0 Td (WT.)Tj 2.0409 0 Td (Eulerian)Tj 3.8198 0 Td (video)Tj 2.5582 0 Td (magnificatio)Tj 5.2371 0 Td (n)Tj 0.7654 0 Td (for)Tj -41.039 -1.1835 Td (revealing)Tj 4.1954 0 Td (subtle)Tj 2.8346 0 Td (changes)Tj 3.9331 0 Td (in)Tj 0.978 0 Td (the)Tj 1.5874 0 Td (world.)Tj 2.8418 0 Td (Eulerian)Tj 3.8126 0 Td (Video)Tj 2.7283 0 Td (Magnificatio)Tj 5.2371 0 Td (n)Tj 0.7654 0 Td (for)Tj 1.3677 0 Td (Revealing)Tj 4.578 0 Td (Subtle)Tj 3.0047 0 Td (Changes)Tj 4.1457 0 Td (in)Tj -42.0098 -1.1906 Td (the)Tj 1.5875 0 Td (World.)Tj 3.0685 0 Td (ACM)Tj 2.4165 0 Td (Transact)Tj 3.8268 0 Td (ions)Tj 2.0197 0 Td (on)Tj 1.3111 0 Td (Graphics.)Tj 4.4291 0 Td (2012;)Tj 2.6788 0 Td (31\(4\).)Tj ET Q q 1 j 1 J 0 w 576 737.1 m 36 737.1 l 36 737.6 l 576 737.6 l f* 36 741.2598 169.285 23.6976 re W* n q 169.2283 0 0 23.6409 36 741.3165 cm q /I0 Do Q Q Q q 0 0 612 792 re W* n 1 j 1 J 0 w 7.9999 0 0 7.9999 391.9748 745.7952 cm BT /F0 1 Tf 1 TL -0.005 Tc 0 0 Td (Ten)Tj 1.9204 0 Td (Simple)Tj 3.2245 0 Td (Rules)Tj 2.7283 0 Td (for)Tj 1.3678 0 Td (Respons)Tj 3.8764 0 Td (ible)Tj 1.7362 0 Td (Big)Tj 1.637 0 Td (Data)Tj 2.3032 0 Td (Researc)Tj 3.6567 0 Td (h)Tj ET Q q 1 j 1 J 0 w 36 48.0002 m 576 48.0002 l 576 47.5002 l 36 47.5002 l f* 0.83 0.64 0.02 0 k 142.5827 34.9228 m 295.8236 34.9228 l h f* 0 g 7.9999 0 0 7.9999 36 36 cm BT /F0 1 Tf 1 TL -0.005 Tc 0 0 Td (PLOS)Tj 2.8559 0 Td (Computationa)Tj 6.1229 0 Td (l)Tj 0.4322 0 Td (Biology)Tj 3.4371 0 Td (|)Tj 0.83 0.64 0.02 0 k 0.4748 0 Td (https:/)Tj 2.6788 0 Td (/doi.org/10.13)Tj 6.0094 0 Td (71/journal.p)Tj 5.1166 0 Td (cbi.1005399)Tj 0 g 6.3497 0 Td (March)Tj 2.9551 0 Td (30,)Tj 1.5874 0 Td (2017)Tj 27.1207 0 Td (8)Tj 0.7654 0 Td (/)Tj ET endstream endobj 165 0 obj < >stream endstream endobj 166 0 obj < >stream endstream endobj 167 0 obj < >stream endstream endobj 168 0 obj < >stream endstream endobj 169 0 obj < >stream BT 66.402 0 Td (10)Tj ET Q endstream endobj 170 0 obj < >/XObject< >>>/CropBox[0 0 612 792]/MediaBox[0 0 612 792]/Parent 9 0 R/Annots 171 0 R/Contents 172 0 R/TrimBox[0 0 612 792]>> endobj 171 0 obj [173 0 R 174 0 R 175 0 R 176 0 R 177 0 R 178 0 R 179 0 R 180 0 R 181 0 R 182 0 R 183 0 R 184 0 R 185 0 R 186 0 R 187 0 R 188 0 R 189 0 R 190 0 R 191 0 R 192 0 R 193 0 R 194 0 R 195 0 R 196 0 R 197 0 R 198 0 R 199 0 R 200 0 R 201 0 R 202 0 R 203 0 R 204 0 R 205 0 R 206 0 R 207 0 R 208 0 R 209 0 R 210 0 R 211 0 R 212 0 R] endobj 173 0 obj < >/Border[0 0 0]/Dest(Lpcbi.1005399.ref003)>> endobj 174 0 obj < >/Border[0 0 0]/Dest(Lpcbi.1005399.ref004)>> endobj 175 0 obj < >/Border[0 0 0]/Dest(Lpcbi.1005399.ref005)>> endobj 176 0 obj < >/Border[0 0 0]/Dest(Lpcbi.1005399.ref006)>> endobj 177 0 obj < >/Border[0 0 0]/A 213 0 R>> endobj 213 0 obj < > endobj 178 0 obj < >/Border[0 0 0]/A 214 0 R>> endobj 214 0 obj < > endobj 179 0 obj < >/Border[0 0 0]/Dest(Lpcbi.1005399.ref007)>> endobj 180 0 obj < >/Border[0 0 0]/A 215 0 R>> endobj 215 0 obj < > endobj 181 0 obj < >/Border[0 0 0]/Dest(Lpcbi.1005399.ref008)>> endobj 182 0 obj < >/Border[0 0 0]/Dest(Lpcbi.1005399.ref009)>> endobj 183 0 obj < >/Border[0 0 0]/Dest(Lpcbi.1005399.ref010)>> endobj 184 0 obj < >/Border[0 0 0]/Dest(Lpcbi.1005399.ref011)>> endobj 185 0 obj < >/Border[0 0 0]/Dest(Lpcbi.1005399.ref012)>> endobj 186 0 obj < >/Border[0 0 0]/Dest(Lpcbi.1005399.ref013)>> endobj 187 0 obj < >/Border[0 0 0]/Dest(Lpcbi.1005399.ref014)>> endobj 188 0 obj < >/Border[0 0 0]/Dest(Lpcbi.1005399.ref015)>> endobj 189 0 obj < >/Border[0 0 0]/A 216 0 R>> endobj 216 0 obj < > endobj 190 0 obj < >/Border[0 0 0]/A 217 0 R>> endobj 217 0 obj < > endobj 191 0 obj < >/Border[0 0 0]/Dest(Lpcbi.1005399.ref016)>> endobj 192 0 obj < >/Border[0 0 0]/Dest(Lpcbi.1005399.ref017)>> endobj 193 0 obj < >/Border[0 0 0]/Dest(Lpcbi.1005399.ref018)>> endobj 194 0 obj < >/Border[0 0 0]/A 218 0 R>> endobj 218 0 obj < > endobj 195 0 obj < >/Border[0 0 0]/A 219 0 R>> endobj 219 0 obj < > endobj 196 0 obj < >/Border[0 0 0]/Dest(Lpcbi.1005399.ref019)>> endobj 197 0 obj < >/Border[0 0 0]/A 220 0 R>> endobj 220 0 obj < > endobj 198 0 obj < >/Border[0 0 0]/Dest(Lpcbi.1005399.ref020)>> endobj 199 0 obj < >/Border[0 0 0]/Dest(Lpcbi.1005399.ref021)>> endobj 200 0 obj < >/Border[0 0 0]/Dest(Lpcbi.1005399.ref022)>> endobj 201 0 obj < >/Border[0 0 0]/Dest(Lpcbi.1005399.ref023)>> endobj 202 0 obj < >/Border[0 0 0]/Dest(Lpcbi.1005399.ref024)>> endobj 203 0 obj < >/Border[0 0 0]/Dest(Lpcbi.1005399.ref025)>> endobj 204 0 obj < >/Border[0 0 0]/Dest(Lpcbi.1005399.ref026)>> endobj 205 0 obj < >/Border[0 0 0]/Dest(Lpcbi.1005399.ref027)>> endobj 206 0 obj < >/Border[0 0 0]/A 221 0 R>> endobj 221 0 obj < > endobj 207 0 obj < >/Border[0 0 0]/A 222 0 R>> endobj 222 0 obj < > endobj 208 0 obj < >/Border[0 0 0]/Dest(Lpcbi.1005399.ref028)>> endobj 209 0 obj < >/Border[0 0 0]/A 223 0 R>> endobj 223 0 obj < > endobj 210 0 obj < >/Border[0 0 0]/A 224 0 R>> endobj 224 0 obj < > endobj 211 0 obj < >/Border[0 0 0]/A 225 0 R>> endobj 225 0 obj < > endobj 212 0 obj < >/Border[0 0 0]/A 226 0 R>> endobj 226 0 obj < > endobj 172 0 obj [227 0 R 228 0 R 229 0 R 230 0 R 231 0 R 232 0 R 233 0 R 234 0 R 235 0 R 236 0 R 237 0 R 238 0 R 239 0 R 240 0 R 241 0 R 242 0 R 243 0 R 244 0 R 245 0 R 246 0 R 247 0 R 248 0 R 249 0 R 250 0 R 251 0 R 252 0 R 253 0 R] endobj 227 0 obj < >stream q 0.83 0.64 0.02 0 k 502.7528 640.1197 m 545.7827 640.1197 l h f* 221.4992 630.652 m 485.5748 630.652 l h f* 270.822 598.6205 m 485.6315 598.6205 l h f* 465.052 437.6693 m 559.2189 437.6693 l h f* 221.4992 428.1449 m 444.189 428.1449 l h f* 531.6094 370.148 m 551.3953 370.148 l h f* 221.4992 360.6236 m 495.2693 360.6236 l h f* 274.9039 328.6488 m 503.6031 328.6488 l h f* 331.3701 120.1323 m 487.7291 120.1323 l h f* 513.1843 120.1323 m 548.1638 120.1323 l h f* 551.6787 107.1496 m 575.4331 107.1496 l h f* 221.4992 97.6252 m 330.2929 97.6252 l h f* 355.748 97.6252 m 390.6709 97.6252 l h f* 0 g 1 j 1 J 0 w 7.9999 0 0 7.9999 204.8881 708.7181 cm BT /F2 1 Tf 1 TL -0.005 Tc 0 0 Td (3.)Tj /F0 1 Tf 2.0763 0 Td (Crawford)Tj 4.2095 0 Td (K,)Tj 1.1552 0 Td (Schultz)Tj 3.4441 0 Td (J.)Tj 0.9921 0 Td (Big)Tj 1.637 0 Td (data)Tj 2.1331 0 Td (and)Tj 1.8496 0 Td (due)Tj 1.8567 0 Td (process:)Tj 3.9402 0 Td (Toward)Tj 3.5008 0 Td (a)Tj 0.7654 0 Td (framework)Tj 4.8189 0 Td (to)Tj 1.0418 0 Td (redress)Tj 3.4937 0 Td (predictive)Tj 4.4221 0 Td (privacy)Tj -39.2602 -1.1905 Td (harms.)Tj 3.2316 0 Td (BCL)Tj 2.1401 0 Td (Rev.)Tj 2.2394 0 Td (2014;)Tj 2.6788 0 Td (55:)Tj 1.5874 0 Td (93±128.)Tj /F2 1 Tf -13.9536 -1.6229 Td (4.)Tj /F0 1 Tf 2.0763 0 Td (Barocas)Tj 3.8268 0 Td (S,)Tj 1.1552 0 Td (Selbst)Tj 2.9551 0 Td (AD.)Tj 1.8638 0 Td (Big)Tj 1.637 0 Td (data's)Tj 2.8418 0 Td (disparat)Tj 3.4866 0 Td (e)Tj 0.7654 0 Td (impact.)Tj 3.3945 0 Td (California)Tj 4.3583 0 Td (Law)Tj 2.0197 0 Td (Review.)Tj 3.7134 0 Td (2016;)Tj 2.6788 0 Td (104\(3\):)Tj 3.3307 0 Td (671±732.)Tj /F2 1 Tf -40.1034 -1.6228 Td (5.)Tj /F0 1 Tf 2.0763 0 Td (Danyllo)Tj 3.4867 0 Td (WA,)Tj 2.0976 0 Td (Alisson)Tj 3.3804 0 Td (VB,)Tj 1.8142 0 Td (Alexandre)Tj 4.6417 0 Td (ND,)Tj 1.9205 0 Td (Moacir)Tj 3.1678 0 Td (LM,)Tj 1.8638 0 Td (Jansepetr)Tj 4.3157 0 Td (us)Tj 1.2615 0 Td (BP,)Tj 1.8142 0 Td (Oliveira)Tj 3.5433 0 Td (RF.)Tj 1.8142 0 Td (Identifying)Tj 4.6914 0 Td (relevant)Tj -39.813 -1.1906 Td (users)Tj 2.6221 0 Td (and)Tj 1.8567 0 Td (groups)Tj 3.2245 0 Td (in)Tj 0.9779 0 Td (the)Tj 1.5874 0 Td (context)Tj 3.3945 0 Td (of)Tj 1.0418 0 Td (credit)Tj 2.615 0 Td (analysis)Tj 3.763 0 Td (based)Tj 2.8984 0 Td (on)Tj 1.3111 0 Td (data)Tj 2.126 0 Td (from)Tj 2.1968 0 Td (Twitter.)Tj 3.4512 0 Td (InCloud)Tj 3.6001 0 Td (and)Tj 1.8567 0 Td (Green)Tj 2.9551 0 Td (Com-)Tj -41.4783 -1.1834 Td (puting)Tj 2.8914 0 Td (\(CGC\),)Tj 3.3449 0 Td (2013)Tj 2.4024 0 Td (Third)Tj 2.4591 0 Td (Internat)Tj 3.3307 0 Td (ional)Tj 2.2819 0 Td (Conference)Tj 5.3008 0 Td (on)Tj 1.3111 0 Td (2013)Tj 2.4024 0 Td (Sep)Tj 1.9701 0 Td (30)Tj 1.311 0 Td (\(pp.)Tj 1.9134 0 Td (587±592\).)Tj 4.6418 0 Td (IEEE.)Tj /F2 1 Tf -37.6373 -1.63 Td (6.)Tj /F0 1 Tf 2.0763 0 Td (Angwin)Tj 3.4371 0 Td (J,)Tj 0.9921 0 Td (Larson)Tj 3.2244 0 Td (J,)Tj 0.9851 0 Td (Mattu)Tj 2.6858 0 Td (S,)Tj 1.1552 0 Td (Kirchner)Tj 3.8764 0 Td (L.)Tj 1.0417 0 Td (Machine)Tj 3.9331 0 Td (bias.)Tj 2.2961 0 Td (Pro)Tj 1.7504 0 Td (Publica.)Tj 3.7134 0 Td (23)Tj 1.3111 0 Td (May)Tj 2.0834 0 Td (2016.)Tj 0.83 0.64 0.02 0 k 2.6717 0 Td (https://www)Tj 5.0953 0 Td (.)Tj -40.2523 -1.1834 Td (propublica.o)Tj 5.3363 0 Td (rg/article/mac)Tj 5.9032 0 Td (hine-bias-r)Tj 4.6276 0 Td (isk-assessme)Tj 5.967 0 Td (nts-in-)Tj 2.7213 0 Td (criminal-sente)Tj 6.1087 0 Td (ncing)Tj 0 g (.)Tj 2.8417 0 Td (Accesse)Tj 3.7205 0 Td (d)Tj 0.7654 0 Td (4)Tj 0.7653 0 Td (September)Tj -38.757 -1.1906 Td (2016.)Tj /F2 1 Tf -2.0763 -1.6229 Td (7.)Tj /F0 1 Tf 2.0763 0 Td (Ingold)Tj 2.8914 0 Td (D,)Tj 1.2047 0 Td (Spencer)Tj 3.8835 0 Td (S.)Tj 1.1552 0 Td (Amazon)Tj 3.8339 0 Td (Doesn't)Tj 3.5504 0 Td (Consider)Tj 4.1457 0 Td (the)Tj 1.5874 0 Td (Race)Tj 2.5158 0 Td (of)Tj 1.0417 0 Td (Its)Tj 1.2614 0 Td (Custome)Tj 3.9402 0 Td (rs.)Tj 1.3181 0 Td (Should)Tj 3.2741 0 Td (It?)Tj 1.3181 0 Td (Bloomb)Tj 3.3307 0 Td (erg.com)Tj -40.2523 -1.1905 Td (21)Tj 1.3111 0 Td (April)Tj 2.1756 0 Td (2016.)Tj 0.83 0.64 0.02 0 k 2.6787 0 Td (http://www)Tj 4.5993 0 Td (.bloomberg.c)Tj 5.6835 0 Td (om/graphic)Tj 4.8615 0 Td (s/2016-am)Tj 4.6488 0 Td (azon-same-)Tj 5.1945 0 Td (day/)Tj 0 g (.)Tj 2.3528 0 Td (Accessed)Tj 4.493 0 Td (12)Tj 1.311 0 Td (June)Tj 2.3457 0 Td (2016.)Tj /F2 1 Tf -43.7318 -1.6229 Td (8.)Tj /F0 1 Tf 2.0763 0 Td (Hauge)Tj 3.1111 0 Td (MV,)Tj 1.9843 0 Td (Stevenson)Tj 4.8685 0 Td (MD,)Tj 2.0339 0 Td (Rossmo)Tj 3.8339 0 Td (DK,)Tj 1.8638 0 Td (Le)Tj 1.311 0 Td (Comber)Tj 3.7205 0 Td (SC.)Tj 1.8638 0 Td (Tagging)Tj 3.7701 0 Td (Banksy:)Tj 3.7276 0 Td (using)Tj 2.5583 0 Td (geographic)Tj 5.074 0 Td (profiling)Tj 3.6426 0 Td (to)Tj -43.3634 -1.1834 Td (investigate)Tj 4.9111 0 Td (a)Tj 0.7654 0 Td (modern)Tj 3.5504 0 Td (art)Tj 1.3748 0 Td (myster)Tj 2.9551 0 Td (y.)Tj 0.9922 0 Td (Journal)Tj 3.437 0 Td (of)Tj 1.0347 0 Td (Spatial)Tj 3.2173 0 Td (Science.)Tj 3.9969 0 Td (2016;)Tj 2.6717 0 Td (61\(1\):185±)Tj 4.748 0 Td (90.)Tj /F2 1 Tf -35.7309 -1.63 Td (9.)Tj /F0 1 Tf 2.0763 0 Td (Metcalf)Tj 3.3946 0 Td (J,)Tj 0.985 0 Td (Crawford)Tj 4.2095 0 Td (K.)Tj 1.1551 0 Td (Where)Tj 3.1252 0 Td (are)Tj 1.6371 0 Td (Human)Tj 3.3945 0 Td (Subjects)Tj 3.9898 0 Td (in)Tj 0.9779 0 Td (Big)Tj 1.637 0 Td (Data)Tj 2.3032 0 Td (Resea)Tj 2.8347 0 Td (rch?)Tj 2.1331 0 Td (The)Tj 1.9205 0 Td (Emerging)Tj 4.4291 0 Td (Ethics)Tj 2.8985 0 Td (Divide.)Tj -41.0248 -1.1834 Td (The)Tj 1.9205 0 Td (Emergi)Tj 3.1111 0 Td (ng)Tj 1.311 0 Td (Ethics)Tj 2.9055 0 Td (Divide.)Tj 3.2174 0 Td (Big)Tj 1.637 0 Td (Data)Tj 2.2961 0 Td (and)Tj 1.8567 0 Td (Society,)Tj 3.7205 0 Td (2016.)Tj /F2 1 Tf -24.5977 -1.63 Td (10.)Tj /F0 1 Tf 2.6219 0 Td (Zwitter)Tj 3.1749 0 Td (A.)Tj 1.1551 0 Td (Big)Tj 1.637 0 Td (data)Tj 2.1331 0 Td (ethics.)Tj 3.0614 0 Td (Big)Tj 1.6371 0 Td (Data)Tj 2.2961 0 Td (&)Tj 0.8787 0 Td (Society.)Tj 3.7205 0 Td (2014;)Tj 2.6788 0 Td (1\(2\).)Tj /F2 1 Tf -24.9946 -1.6228 Td (11.)Tj /F0 1 Tf 2.6219 0 Td (Nissenbaum)Tj 5.6836 0 Td (H.)Tj 1.2047 0 Td (Privacy)Tj 3.4512 0 Td (in)Tj 0.978 0 Td (context)Tj 3.1677 0 Td (:)Tj 0.4961 0 Td (Technology)Tj 5.0811 0 Td (,)Tj 0.4961 0 Td (policy,)Tj 2.9976 0 Td (and)Tj 1.8567 0 Td (the)Tj 1.5875 0 Td (integrity)Tj 3.6567 0 Td (of)Tj 1.0346 0 Td (social)Tj 2.7284 0 Td (life.)Tj 1.963 0 Td (Stanfor)Tj 3.1677 0 Td (d)Tj 0.7654 0 Td (Univer-)Tj -40.3161 -1.1906 Td (sity)Tj 1.6938 0 Td (Press;)Tj 3.0189 0 Td (2009.)Tj /F2 1 Tf -7.3346 -1.6228 Td (12.)Tj /F0 1 Tf 2.6219 0 Td (Marwick)Tj 3.8268 0 Td (AE.)Tj 1.8142 0 Td (boyd)Tj 2.3528 0 Td (d.)Tj 1.0417 0 Td (Networke)Tj 4.1528 0 Td (d)Tj 0.7654 0 Td (privacy:)Tj 3.6071 0 Td (How)Tj 2.1898 0 Td (teenagers)Tj 4.5851 0 Td (negotiate)Tj 4.2591 0 Td (context)Tj 3.3945 0 Td (in)Tj 0.9779 0 Td (social)Tj 2.7213 0 Td (media.)Tj 3.1748 0 Td (New)Tj 2.1827 0 Td (Media)Tj -41.046 -1.1835 Td (&)Tj 0.8788 0 Td (Society.)Tj 3.7205 0 Td (2014:1461444)Tj 6.2788 0 Td (814543)Tj 3.2669 0 Td (995.)Tj /F2 1 Tf -16.7669 -1.6299 Td (13.)Tj /F0 1 Tf 2.6219 0 Td (Tene)Tj 2.4662 0 Td (O,)Tj 1.2685 0 Td (Polonets)Tj 3.8197 0 Td (ky)Tj 1.2048 0 Td (J.)Tj 0.9921 0 Td (Theory)Tj 3.2811 0 Td (of)Tj 1.0418 0 Td (Creepy:)Tj 3.6638 0 Td (Technolog)Tj 4.5851 0 Td (y,)Tj 0.9921 0 Td (Privacy)Tj 3.4441 0 Td (and)Tj 1.8567 0 Td (Shifting)Tj 3.4937 0 Td (Social)Tj 2.8914 0 Td (Norms,)Tj 3.3945 0 Td (A.)Tj 1.1551 0 Td (Yale)Tj 2.1827 0 Td (JL)Tj 1.2615 0 Td (&)Tj -42.9949 -1.1835 Td (Tech.)Tj 2.6859 0 Td (2013;)Tj 2.6788 0 Td (16:59.)Tj /F2 1 Tf -7.9866 -1.63 Td (14.)Tj /F0 1 Tf 2.6219 0 Td (Massey)Tj 3.6142 0 Td (DS,)Tj 1.8709 0 Td (Denton)Tj 3.3875 0 Td (NA.)Tj 1.8637 0 Td (American)Tj 4.3796 0 Td (aparth)Tj 2.778 0 Td (eid:)Tj 1.8 0 Td (Segregatio)Tj 4.7481 0 Td (n)Tj 0.7653 0 Td (and)Tj 1.8567 0 Td (the)Tj 1.5875 0 Td (making)Tj 3.3874 0 Td (of)Tj 1.0417 0 Td (the)Tj 1.5874 0 Td (undercl)Tj 3.2103 0 Td (ass.)Tj 2.2536 0 Td (Harvard)Tj -40.1319 -1.1834 Td (University)Tj 4.5284 0 Td (Press;)Tj 3.0189 0 Td (1993.)Tj /F2 1 Tf -10.1692 -1.6229 Td (15.)Tj /F0 1 Tf 2.6219 0 Td (Davidow)Tj 3.9828 0 Td (B.)Tj 1.1551 0 Td (Redlining)Tj 4.3016 0 Td (for)Tj 1.3677 0 Td (the)Tj 1.5874 0 Td (21st)Tj 2.0764 0 Td (Century.)Tj 3.9402 0 Td (The)Tj 1.9205 0 Td (Atlanti)Tj 2.7213 0 Td (c.)Tj 0.9921 0 Td (5)Tj 0.7654 0 Td (March)Tj 2.9551 0 Td (2014.)Tj 0.83 0.64 0.02 0 k 2.6788 0 Td (http://www)Tj 4.5992 0 Td (.theatlantic)Tj 4.7481 0 Td (.com/)Tj -39.7917 -1.1905 Td (business/ar)Tj 5.0245 0 Td (chive/2014)Tj 4.7481 0 Td (/03/red)Tj 3.0543 0 Td (lining-for-the-2)Tj 6.3213 0 Td (1st-centur)Tj 4.3725 0 Td (y/284235)Tj 4.0394 0 Td (/)Tj 0 g (.)Tj 0.7725 0 Td (Accesse)Tj 3.7205 0 Td (d)Tj 0.7653 0 Td (31)Tj 1.311 0 Td (May)Tj 2.0835 0 Td (2016.)Tj /F2 1 Tf -38.8348 -1.6229 Td (16.)Tj /F0 1 Tf 2.6219 0 Td (Young)Tj 3.0615 0 Td (JC,)Tj 1.7008 0 Td (Gilmore)Tj 3.6638 0 Td (MP.)Tj 1.9772 0 Td (Subaltern)Tj 4.4221 0 Td (empowermen)Tj 5.9669 0 Td (t)Tj 0.4961 0 Td (in)Tj 0.978 0 Td (the)Tj 1.5803 0 Td (Geoweb:)Tj 4.167 0 Td (Tensions)Tj 4.2094 0 Td (between)Tj 3.9331 0 Td (publicity)Tj 3.756 0 Td (and)Tj 1.8567 0 Td (pri-)Tj -41.7689 -1.1905 Td (vacy.)Tj 2.5229 0 Td (Antipode.)Tj 4.3654 0 Td (2014;)Tj 2.6787 0 Td (46\(2\):574±)Tj 4.7481 0 Td (91.)Tj /F2 1 Tf -16.937 -1.6229 Td (17.)Tj /F0 1 Tf 2.6219 0 Td (Barbaro)Tj 3.7135 0 Td (M,)Tj 1.3252 0 Td (Zeller)Tj 2.6716 0 Td (T,)Tj 1.0985 0 Td (Hansell)Tj 3.4866 0 Td (S.)Tj 1.1551 0 Td (A)Tj 0.8859 0 Td (face)Tj 2.0764 0 Td (is)Tj 0.9283 0 Td (exposed)Tj 3.9331 0 Td (for)Tj 1.3678 0 Td (AOL)Tj 2.1968 0 Td (searcher)Tj 4.0465 0 Td (no.)Tj 1.5803 0 Td (4417749.)Tj 4.3158 0 Td (New)Tj 2.1898 0 Td (York)Tj 2.2465 0 Td (Times.)Tj -39.2177 -1.1905 Td (2006)Tj 2.4024 0 Td (Aug)Tj 1.9701 0 Td (9;9.)Tj /F2 1 Tf -6.9944 -1.6229 Td (18.)Tj /F0 1 Tf 2.6219 0 Td (Cox)Tj 1.9701 0 Td (J.)Tj 0.9922 0 Td (70,000)Tj 3.2173 0 Td (OkCupid)Tj 4.0465 0 Td (Users)Tj 2.7922 0 Td (Just)Tj 2.0267 0 Td (Had)Tj 2.0268 0 Td (Their)Tj 2.4591 0 Td (Data)Tj 2.2961 0 Td (Published)Tj 4.3016 0 Td (.)Tj 0.4961 0 Td (Motherboard.)Tj 6.0732 0 Td (12)Tj 1.3111 0 Td (May)Tj 2.0835 0 Td (2016.)Tj 0.83 0.64 0.02 0 k 2.6716 0 Td (http://)Tj -38.7641 -1.1906 Td (motherbo)Tj 4.1528 0 Td (ard.vice.com/)Tj 5.8536 0 Td (read/70000-)Tj 5.2938 0 Td (okcupid-us)Tj 4.748 0 Td (ers-just-had-)Tj 5.5135 0 Td (their-data-)Tj 4.4717 0 Td (publishe)Tj 3.6425 0 Td (d)Tj 0 g (.)Tj 1.0417 0 Td (Accessed)Tj 4.493 0 Td (12)Tj 1.311 0 Td (June)Tj -40.5216 -1.1834 Td (2016.)Tj /F2 1 Tf -2.6219 -1.63 Td (19.)Tj /F0 1 Tf 2.6219 0 Td (Pandurang)Tj 4.8048 0 Td (an)Tj 1.311 0 Td (V.)Tj 1.1552 0 Td (On)Tj 1.5378 0 Td (Taxis)Tj 2.5724 0 Td (and)Tj 1.8567 0 Td (Rainbows)Tj 4.3087 0 Td (:)Tj 0.4961 0 Td (Lessons)Tj 3.8835 0 Td (from)Tj 2.1969 0 Td (NYC's)Tj 3.0047 0 Td (improperly)Tj 4.8048 0 Td (anonymiz)Tj 4.2024 0 Td (ed)Tj 1.311 0 Td (taxi)Tj 1.7504 0 Td (logs.)Tj -39.1964 -1.1834 Td (Medium.)Tj 3.9969 0 Td (2014.)Tj 0.83 0.64 0.02 0 k 2.6788 0 Td (https://)Tj 2.9551 0 Td (medium.com)Tj 5.641 0 Td (/@vijayp/of-tax)Tj 6.5198 0 Td (is-and-rain)Tj 4.6275 0 Td (bows-f6bc289)Tj 6.1229 0 Td (679a1)Tj 0 g (.)Tj 3.2174 0 Td (Accessed)Tj 4.4929 0 Td (10)Tj -40.2523 -1.1906 Td (Novemb)Tj 3.6639 0 Td (er)Tj 1.0984 0 Td (2015.)Tj /F2 1 Tf -7.3842 -1.6228 Td (20.)Tj /F0 1 Tf 2.6219 0 Td (Sweeney)Tj 4.2591 0 Td (L.)Tj 1.0347 0 Td (k-anonymity:)Tj 5.7969 0 Td (A)Tj 0.8716 0 Td (model)Tj 2.8914 0 Td (for)Tj 1.3606 0 Td (protectin)Tj 3.7631 0 Td (g)Tj 0.7653 0 Td (privacy.)Tj 3.6001 0 Td (Interna)Tj 3.0543 0 Td (tional)Tj 2.5583 0 Td (Journal)Tj 3.4299 0 Td (of)Tj 1.0347 0 Td (Uncertaint)Tj 4.4717 0 Td (y,)Tj 0.985 0 Td (Fuzziness)Tj -39.8767 -1.1906 Td (and)Tj 1.8568 0 Td (Knowled)Tj 3.763 0 Td (ge-Based)Tj 4.4291 0 Td (Systems.)Tj 4.2804 0 Td (2002;)Tj 2.6788 0 Td (10\(05\):557)Tj 4.748 0 Td (±70.)Tj /F2 1 Tf -24.378 -1.6229 Td (21.)Tj /F0 1 Tf 2.6219 0 Td (Zimmer)Tj 3.5646 0 Td (M.)Tj 1.3182 0 Td (ªBut)Tj 2.0338 0 Td (the)Tj 1.5804 0 Td (data)Tj 2.133 0 Td (is)Tj 0.9284 0 Td (already)Tj 3.437 0 Td (publicº)Tj 2.8772 0 Td (:)Tj 0.4961 0 Td (on)Tj 1.311 0 Td (the)Tj 1.5874 0 Td (ethics)Tj 2.7851 0 Td (of)Tj 1.0417 0 Td (research)Tj 4.0465 0 Td (in)Tj 0.978 0 Td (Facebook)Tj 4.3158 0 Td (.)Tj 0.496 0 Td (Ethics)Tj 2.8985 0 Td (and)Tj 1.8567 0 Td (informa-)Tj -39.6854 -1.1834 Td (tion)Tj 1.8001 0 Td (technolog)Tj 4.252 0 Td (y.)Tj 0.9921 0 Td (2010;)Tj 2.6717 0 Td (12\(4\):313±)Tj 4.748 0 Td (25.)Tj /F2 1 Tf -17.0858 -1.63 Td (22.)Tj /F0 1 Tf 2.6219 0 Td (Klouman)Tj 3.8765 0 Td (n)Tj 0.7653 0 Td (IM,)Tj 1.6016 0 Td (Kleinberg)Tj 4.3583 0 Td (JM.)Tj 1.8142 0 Td (Commun)Tj 3.9969 0 Td (ity)Tj 1.2047 0 Td (membership)Tj 5.6339 0 Td (identifi)Tj 2.8205 0 Td (cation)Tj 2.8418 0 Td (from)Tj 2.1897 0 Td (small)Tj 2.5158 0 Td (seed)Tj 2.3457 0 Td (sets.)Tj 2.3032 0 Td (InProceed-)Tj -38.2681 -1.1834 Td (ings)Tj 2.0197 0 Td (of)Tj 1.0418 0 Td (the)Tj 1.5803 0 Td (20th)Tj 2.1331 0 Td (ACM)Tj 2.4166 0 Td (SIGKDD)Tj 4.011 0 Td (internationa)Tj 5.1237 0 Td (l)Tj 0.4323 0 Td (conferen)Tj 3.8197 0 Td (ce)Tj 1.2614 0 Td (on)Tj 1.311 0 Td (Knowled)Tj 3.7631 0 Td (ge)Tj 1.311 0 Td (discovery)Tj 4.3725 0 Td (and)Tj 1.8567 0 Td (data)Tj 2.1331 0 Td (mining)Tj 3.1039 0 Td (2014)Tj -41.6909 -1.1906 Td (Aug)Tj 1.9701 0 Td (24)Tj 1.3111 0 Td (\(pp.)Tj 1.9134 0 Td (1366±1375\))Tj 5.237 0 Td (.)Tj 0.4961 0 Td (ACM.)Tj /F2 1 Tf -13.5496 -1.6228 Td (23.)Tj /F0 1 Tf 2.6219 0 Td (Narayanan)Tj 5.0245 0 Td (A,)Tj 1.1551 0 Td (Huey)Tj 2.5158 0 Td (J,)Tj 0.985 0 Td (Felten)Tj 2.9552 0 Td (EW.)Tj 2.0976 0 Td (A)Tj 0.8788 0 Td (precauti)Tj 3.4866 0 Td (onary)Tj 2.6788 0 Td (approach)Tj 4.3158 0 Td (to)Tj 1.0346 0 Td (big)Tj 1.5237 0 Td (data)Tj 2.133 0 Td (privacy.)Tj 3.6072 0 Td (In)Tj 1.0417 0 Td (Data)Tj 2.2961 0 Td (protection)Tj 4.5354 0 Td (on)Tj -42.2649 -1.1906 Td (the)Tj 1.5875 0 Td (move)Tj 2.6291 0 Td (2016)Tj 2.4024 0 Td (\(pp.)Tj 1.9134 0 Td (357±38)Tj 3.2669 0 Td (5\).)Tj 1.5946 0 Td (Springer)Tj 4.1457 0 Td (Netherlan)Tj 4.252 0 Td (ds.)Tj /F2 1 Tf -24.4135 -1.6229 Td (24.)Tj /F0 1 Tf 2.6219 0 Td (Michalevs)Tj 4.3654 0 Td (ky)Tj 1.2119 0 Td (Y,)Tj 1.1551 0 Td (Schulman)Tj 4.5921 0 Td (A,)Tj 1.1552 0 Td (Veerapand)Tj 4.8047 0 Td (ian)Tj 1.5237 0 Td (GA,)Tj 1.9275 0 Td (Boneh)Tj 3.0615 0 Td (D,)Tj 1.2047 0 Td (Nakibly)Tj 3.4371 0 Td (G.)Tj 1.2685 0 Td (Powerspy:)Tj 4.8189 0 Td (Location)Tj 3.926 0 Td (tracking)Tj -38.4523 -1.1905 Td (using)Tj 2.5654 0 Td (mobile)Tj 3.104 0 Td (device)Tj 3.0543 0 Td (power)Tj 2.8985 0 Td (analysis.)Tj 4.0394 0 Td (In24th)Tj 2.948 0 Td (USENIX)Tj 3.8977 0 Td (Security)Tj 3.7772 0 Td (Sympos)Tj 3.5575 0 Td (ium)Tj 1.8071 0 Td (\(USENIX)Tj 4.2237 0 Td (Security)Tj 3.7701 0 Td (15\))Tj 1.637 0 Td (2015)Tj -41.2799 -1.1835 Td (\(pp.)Tj 1.9134 0 Td (785±800\).)Tj /F2 1 Tf -4.5353 -1.6228 Td (25.)Tj /F0 1 Tf 2.6219 0 Td (Shelton)Tj 3.5505 0 Td (T,)Tj 1.1055 0 Td (Poorth)Tj 2.8913 0 Td (uis)Tj 1.4741 0 Td (A,)Tj 1.1551 0 Td (Zook)Tj 2.4095 0 Td (M.)Tj 1.3252 0 Td (Social)Tj 2.8913 0 Td (media)Tj 2.8914 0 Td (and)Tj 1.8567 0 Td (the)Tj 1.5874 0 Td (city:)Tj 1.9701 0 Td (Rethinking)Tj 4.8544 0 Td (urban)Tj 2.7284 0 Td (socio-spatia)Tj 5.237 0 Td (l)Tj 0.4323 0 Td (inequality)Tj -38.3602 -1.1906 Td (using)Tj 2.5654 0 Td (user-g)Tj 2.778 0 Td (enerated)Tj 4.0961 0 Td (geographic)Tj 5.074 0 Td (informati)Tj 3.7631 0 Td (on.)Tj 1.5874 0 Td (Landscape)Tj 5.0244 0 Td (and)Tj 1.8567 0 Td (Urban)Tj 2.8914 0 Td (Planning.)Tj 4.3087 0 Td (2015;)Tj 2.6788 0 Td (142:198±211)Tj 5.726 0 Td (.)Tj /F2 1 Tf -44.9719 -1.6228 Td (26.)Tj /F0 1 Tf 2.6219 0 Td (Acquisti)Tj 3.6568 0 Td (A,)Tj 1.1551 0 Td (Gross)Tj 2.8559 0 Td (R,)Tj 1.2048 0 Td (Stutzman)Tj 4.3866 0 Td (F.)Tj 1.1055 0 Td (Face)Tj 2.4095 0 Td (recognition)Tj 5.0174 0 Td (and)Tj 1.8496 0 Td (privacy)Tj 3.3378 0 Td (in)Tj 0.978 0 Td (the)Tj 1.5803 0 Td (age)Tj 1.8567 0 Td (of)Tj 1.0417 0 Td (augmente)Tj 4.3725 0 Td (d)Tj 0.7654 0 Td (reality.)Tj 3.111 0 Td (Journal)Tj -40.6846 -1.1906 Td (of)Tj 1.0418 0 Td (Privacy)Tj 3.4441 0 Td (and)Tj 1.8567 0 Td (Confidenti)Tj 4.415 0 Td (ality.)Tj 2.2323 0 Td (2014;)Tj 2.6788 0 Td (6\(2\):)Tj 2.2394 0 Td (1±20.)Tj /F2 1 Tf -20.53 -1.6229 Td (27.)Tj /F0 1 Tf 2.6219 0 Td (Homer)Tj 3.1749 0 Td (N,)Tj 1.2047 0 Td (Merriman)Tj 4.3796 0 Td (B,)Tj 1.1551 0 Td (Nelson)Tj 3.274 0 Td (SF.)Tj 1.7575 0 Td (BFAST:)Tj 3.6922 0 Td (an)Tj 1.311 0 Td (alignme)Tj 3.43 0 Td (nt)Tj 1.0417 0 Td (tool)Tj 1.8 0 Td (for)Tj 1.3678 0 Td (large)Tj 2.3952 0 Td (scale)Tj 2.5158 0 Td (genome)Tj 3.7701 0 Td (resequen)Tj 4.089 0 Td (cing.)Tj -40.3586 -1.1905 Td (PLoS)Tj 2.6292 0 Td (ONE.)Tj 2.6433 0 Td (2009;)Tj 2.6717 0 Td (4\(11\):e776)Tj 4.7481 0 Td (7.)Tj 0.83 0.64 0.02 0 k 1.0417 0 Td (https://doi.or)Tj 5.4142 0 Td (g/10.137)Tj 3.8197 0 Td (1/journal.po)Tj 5.1237 0 Td (ne.0007767)Tj 0 g 5.4071 0 Td (PMID:)Tj 0.83 0.64 0.02 0 k 2.9623 0 Td (19907642)Tj 0 g /F2 1 Tf -39.0829 -1.6229 Td (28.)Tj /F0 1 Tf 2.6219 0 Td (Lo)Tj 1.3111 0 Td (B.)Tj 1.1551 0 Td (Sharing)Tj 3.6 0 Td (clinical)Tj 3.1536 0 Td (trial)Tj 1.7929 0 Td (data:)Tj 2.4024 0 Td (maximizing)Tj 5.1378 0 Td (benefits,)Tj 3.9331 0 Td (minimizing)Tj 4.8544 0 Td (risk.)Tj 2.0197 0 Td (Jama.)Tj 2.9056 0 Td (2015;)Tj 2.6787 0 Td (313\(8\):793)Tj 4.7481 0 Td (±4.)Tj 0.83 0.64 0.02 0 k 1.5803 0 Td (https://)Tj -41.2728 -1.1905 Td (doi.org/10.10)Tj 5.7261 0 Td (01/jama.20)Tj 4.8614 0 Td (15.292)Tj 0 g 3.2245 0 Td (PMID:)Tj 0.83 0.64 0.02 0 k 2.9693 0 Td (25594500)Tj ET Q q 1 j 1 J 0 w 576 737.1 m 36 737.1 l 36 737.6 l 576 737.6 l f* 36 741.2598 169.285 23.6976 re W* n q 169.2283 0 0 23.6409 36 741.3165 cm q /I0 Do Q Q Q q 0 0 612 792 re W* n 1 j 1 J 0 w 7.9999 0 0 7.9999 391.9748 745.7952 cm BT /F0 1 Tf 1 TL -0.005 Tc 0 0 Td (Ten)Tj 1.9204 0 Td (Simple)Tj 3.2245 0 Td (Rules)Tj 2.7283 0 Td (for)Tj 1.3678 0 Td (Respons)Tj 3.8764 0 Td (ible)Tj 1.7362 0 Td (Big)Tj 1.637 0 Td (Data)Tj 2.3032 0 Td (Researc)Tj 3.6567 0 Td (h)Tj ET Q q 1 j 1 J 0 w 36 48.0002 m 576 48.0002 l 576 47.5002 l 36 47.5002 l f* 0.83 0.64 0.02 0 k 142.5827 34.9228 m 295.8236 34.9228 l h f* 0 g 7.9999 0 0 7.9999 36 36 cm BT /F0 1 Tf 1 TL -0.005 Tc 0 0 Td (PLOS)Tj 2.8559 0 Td (Computationa)Tj 6.1229 0 Td (l)Tj 0.4322 0 Td (Biology)Tj 3.4371 0 Td (|)Tj 0.83 0.64 0.02 0 k 0.4748 0 Td (https:/)Tj 2.6788 0 Td (/doi.org/10.13)Tj 6.0094 0 Td (71/journal.p)Tj 5.1166 0 Td (cbi.1005399)Tj 0 g 6.3497 0 Td (March)Tj 2.9551 0 Td (30,)Tj 1.5874 0 Td (2017)Tj 27.1207 0 Td (9)Tj 0.7654 0 Td (/)Tj ET endstream endobj 228 0 obj < >stream endstream endobj 229 0 obj < >stream endstream endobj 230 0 obj < >stream endstream endobj 231 0 obj < >stream endstream endobj 232 0 obj < >stream endstream endobj 233 0 obj < >stream endstream endobj 234 0 obj < >stream endstream endobj 235 0 obj < >stream endstream endobj 236 0 obj < >stream endstream endobj 237 0 obj < >stream endstream endobj 238 0 obj < >stream endstream endobj 239 0 obj < >stream endstream endobj 240 0 obj < >stream endstream endobj 241 0 obj < >stream endstream endobj 242 0 obj < >stream endstream endobj 243 0 obj < >stream endstream endobj 244 0 obj < >stream endstream endobj 245 0 obj < >stream endstream endobj 246 0 obj < >stream endstream endobj 247 0 obj < >stream endstream endobj 248 0 obj < >stream endstream endobj 249 0 obj < >stream endstream endobj 250 0 obj < >stream endstream endobj 251 0 obj < >stream endstream endobj 252 0 obj < >stream endstream endobj 253 0 obj < >stream BT 66.402 0 Td (10)Tj ET Q endstream endobj 254 0 obj < >/XObject< >>>/CropBox[0 0 612 792]/MediaBox[0 0 612 792]/Parent 9 0 R/Annots 255 0 R/Contents 256 0 R/TrimBox[0 0 612 792]>> endobj 255 0 obj [257 0 R 258 0 R 259 0 R 260 0 R 261 0 R 262 0 R 263 0 R 264 0 R 265 0 R 266 0 R 267 0 R 268 0 R 269 0 R 270 0 R 271 0 R 272 0 R 273 0 R] endobj 257 0 obj < >/Border[0 0 0]/Dest(Lpcbi.1005399.ref029)>> endobj 258 0 obj < >/Border[0 0 0]/Dest(Lpcbi.1005399.ref030)>> endobj 259 0 obj < >/Border[0 0 0]/A 274 0 R>> endobj 274 0 obj < > endobj 260 0 obj < >/Border[0 0 0]/Dest(Lpcbi.1005399.ref031)>> endobj 261 0 obj < >/Border[0 0 0]/Dest(Lpcbi.1005399.ref032)>> endobj 262 0 obj < >/Border[0 0 0]/Dest(Lpcbi.1005399.ref033)>> endobj 263 0 obj < >/Border[0 0 0]/Dest(Lpcbi.1005399.ref034)>> endobj 264 0 obj < >/Border[0 0 0]/A 275 0 R>> endobj 275 0 obj < > endobj 265 0 obj < >/Border[0 0 0]/Dest(Lpcbi.1005399.ref035)>> endobj 266 0 obj < >/Border[0 0 0]/Dest(Lpcbi.1005399.ref036)>> endobj 267 0 obj < >/Border[0 0 0]/A 276 0 R>> endobj 276 0 obj < > endobj 268 0 obj < >/Border[0 0 0]/A 277 0 R>> endobj 277 0 obj < > endobj 269 0 obj < >/Border[0 0 0]/Dest(Lpcbi.1005399.ref037)>> endobj 270 0 obj < >/Border[0 0 0]/Dest(Lpcbi.1005399.ref038)>> endobj 271 0 obj < >/Border[0 0 0]/A 278 0 R>> endobj 278 0 obj < > endobj 272 0 obj < >/Border[0 0 0]/A 279 0 R>> endobj 279 0 obj < > endobj 273 0 obj < >/Border[0 0 0]/A 280 0 R>> endobj 280 0 obj < > endobj 256 0 obj [281 0 R 282 0 R 283 0 R 284 0 R 285 0 R 286 0 R 287 0 R 288 0 R 289 0 R 290 0 R 291 0 R] endobj 281 0 obj < >stream q 0.83 0.64 0.02 0 k 242.9291 675.6661 m 534.7276 675.6661 l h f* 384.7748 585.6378 m 419.6976 585.6378 l h f* 380.4094 540.6236 m 561.8835 540.6236 l h f* 221.4992 531.1559 m 393.6756 531.1559 l h f* 524.8063 495.6661 m 567.7795 495.6661 l h f* 221.4992 486.1417 m 450.3118 486.1417 l h f* 0 g 1 j 1 J 0 w 7.9999 0 0 7.9999 200.5228 708.7181 cm BT /F2 1 Tf 1 TL -0.005 Tc 0 0 Td (29.)Tj /F0 1 Tf 2.622 0 Td (Goodman)Tj 4.5497 0 Td (A,)Tj 1.148 0 Td (Pepe)Tj 2.5158 0 Td (A,)Tj 1.1551 0 Td (Blocker)Tj 3.5008 0 Td (AW,)Tj 2.0977 0 Td (Borgma)Tj 3.4441 0 Td (n)Tj 0.7653 0 Td (CL,)Tj 1.7505 0 Td (Cranmer)Tj 4.0535 0 Td (K,)Tj 1.1552 0 Td (Crosas)Tj 3.3307 0 Td (M,)Tj 1.3252 0 Td (et)Tj 1.0417 0 Td (al.)Tj 1.2544 0 Td (Ten)Tj 1.9134 0 Td (simple)Tj 3.0543 0 Td (rules)Tj 2.3457 0 Td (for)Tj 1.3678 0 Td (the)Tj -41.7689 -1.1905 Td (care)Tj 2.1331 0 Td (and)Tj 1.8567 0 Td (feeding)Tj 3.43 0 Td (of)Tj 1.0417 0 Td (scientific)Tj 3.9827 0 Td (data.)Tj 2.4095 0 Td (PLoS)Tj 2.6291 0 Td (Compu)Tj 3.1678 0 Td (t)Tj 0.496 0 Td (Biol.;)Tj 2.4024 0 Td (10\(4\).)Tj /F2 1 Tf -26.171 -1.6229 Td (30.)Tj /F0 1 Tf 2.622 0 Td (Markham)Tj 4.33 0 Td (A.)Tj 1.1551 0 Td (OKCup)Tj 3.2315 0 Td (id)Tj 0.978 0 Td (data)Tj 2.1331 0 Td (release)Tj 3.437 0 Td (fiasco:)Tj 3.0614 0 Td (It's)Tj 1.4741 0 Td (time)Tj 2.0835 0 Td (to)Tj 1.0417 0 Td (rethink)Tj 3.1607 0 Td (ethics)Tj 2.785 0 Td (education.)Tj 4.7552 0 Td (Medium)Tj 3.4937 0 Td (.Points)Tj 3.2315 0 Td (18)Tj 1.3111 0 Td (May)Tj -41.6626 -1.1834 Td (2016.)Tj 0.83 0.64 0.02 0 k 2.6788 0 Td (https://p)Tj 3.5008 0 Td (oints.dataso)Tj 5.3008 0 Td (ciety.net/ok)Tj 4.9749 0 Td (cupid-data-r)Tj 5.237 0 Td (elease-fias)Tj 4.741 0 Td (co-ba0388348)Tj 6.2788 0 Td (cd#.g4ofb)Tj 4.3158 0 Td (pnc6)Tj 0 g (.)Tj 2.622 0 Td (Accessed)Tj -39.6499 -1.1906 Td (12)Tj 1.3111 0 Td (June)Tj 2.3456 0 Td (2016.)Tj /F2 1 Tf -6.2787 -1.6228 Td (31.)Tj /F0 1 Tf 2.622 0 Td (Ford)Tj 2.2465 0 Td (H.)Tj 1.2047 0 Td (Big)Tj 1.637 0 Td (Data)Tj 2.3032 0 Td (and)Tj 1.8496 0 Td (Small:)Tj 2.9552 0 Td (Collabora)Tj 4.1882 0 Td (tions)Tj 2.2961 0 Td (between)Tj 3.9331 0 Td (ethnographe)Tj 5.5134 0 Td (rs)Tj 1.0418 0 Td (and)Tj 1.8496 0 Td (data)Tj 2.1331 0 Td (scientists)Tj 4.0394 0 Td (.)Tj 0.496 0 Td (Big)Tj 1.6441 0 Td (Data)Tj 2.2961 0 Td (&)Tj -41.6271 -1.1906 Td (Society.)Tj 3.7205 0 Td (2014;)Tj 2.6788 0 Td (1\(2\):)Tj /F2 1 Tf -9.0213 -1.6228 Td (32.)Tj /F0 1 Tf 2.622 0 Td (Science)Tj 3.7134 0 Td (&)Tj 0.8788 0 Td (Justice)Tj 3.2811 0 Td (Research)Tj 4.4292 0 Td (Center)Tj 3.1677 0 Td (\(Collaboratio)Tj 5.556 0 Td (ns)Tj 1.2543 0 Td (Group.)Tj 3.2386 0 Td (Experi)Tj 2.778 0 Td (ments)Tj 2.9055 0 Td (in)Tj 0.978 0 Td (collaboration)Tj 5.5559 0 Td (:)Tj 0.4961 0 Td (interdisci)Tj 3.8622 0 Td (plin-)Tj -42.0948 -1.1906 Td (ary)Tj 1.5874 0 Td (graduate)Tj 4.0961 0 Td (education)Tj 4.4717 0 Td (in)Tj 0.978 0 Td (science)Tj 3.5504 0 Td (and)Tj 1.8567 0 Td (justice.)Tj 3.274 0 Td (PLoS)Tj 2.6292 0 Td (Biol.)Tj 2.126 0 Td (2013)Tj 2.4024 0 Td (Jul)Tj 1.474 0 Td (30;)Tj 1.5803 0 Td (11\(7\):e100)Tj 4.7481 0 Td (1619.)Tj /F2 1 Tf -37.3963 -1.6229 Td (33.)Tj /F0 1 Tf 2.622 0 Td (Knobel)Tj 3.2741 0 Td (C,)Tj 1.2047 0 Td (Bowker)Tj 3.5079 0 Td (GC.)Tj 1.9772 0 Td (Values)Tj 3.2244 0 Td (in)Tj 0.978 0 Td (design.)Tj 3.3803 0 Td (Commun)Tj 3.9969 0 Td (ications)Tj 3.5504 0 Td (of)Tj 1.0347 0 Td (the)Tj 1.5874 0 Td (ACM.)Tj 2.6929 0 Td (2011;)Tj 2.6788 0 Td (54\(7\):26±8)Tj 4.748 0 Td (.)Tj /F2 1 Tf -40.4577 -1.6228 Td (34.)Tj /F0 1 Tf 2.622 0 Td (Cho)Tj 2.0197 0 Td (MK,)Tj 1.9843 0 Td (Tobin)Tj 2.6787 0 Td (SL,)Tj 1.6938 0 Td (Greely)Tj 3.1181 0 Td (HT,)Tj 1.8213 0 Td (McCormi)Tj 3.9402 0 Td (ck)Tj 1.2047 0 Td (J,)Tj 0.9921 0 Td (Boyce)Tj 2.9552 0 Td (A,)Tj 1.1551 0 Td (Magnus)Tj 3.7205 0 Td (D.)Tj 1.2047 0 Td (Research)Tj 4.4292 0 Td (ethics)Tj 2.7851 0 Td (consultation)Tj 5.237 0 Td (:)Tj 0.4961 0 Td (The)Tj -41.4358 -1.1906 Td (Stanford)Tj 3.9402 0 Td (experien)Tj 3.7559 0 Td (ce.)Tj 1.5378 0 Td (IRB.)Tj 2.1402 0 Td (2008;\(6\):1±)Tj 5.0245 0 Td (6.)Tj 1.0417 0 Td (PMID:)Tj 0.83 0.64 0.02 0 k 2.9693 0 Td (191197)Tj 3.267 0 Td (57)Tj 0 g /F2 1 Tf -26.2986 -1.6228 Td (35.)Tj /F0 1 Tf 2.622 0 Td (boyd)Tj 2.3528 0 Td (d.)Tj 1.0346 0 Td (Untangling)Tj 4.9111 0 Td (resear)Tj 2.778 0 Td (ch)Tj 1.2614 0 Td (and)Tj 1.8567 0 Td (practice:)Tj 3.9331 0 Td (What)Tj 2.53 0 Td (Facebook)Tj 4.3157 0 Td ('s)Tj 0.9284 0 Td (ªemotion)Tj 3.8197 0 Td (al)Tj 0.9851 0 Td (contagion)Tj 4.252 0 Td (º)Tj 0.5456 0 Td (study)Tj 2.5725 0 Td (teaches)Tj 3.6638 0 Td (us.)Tj -41.7405 -1.1906 Td (Research)Tj 4.4292 0 Td (Ethics.)Tj 3.1748 0 Td (2016;)Tj 2.6788 0 Td (12\(1\):4±1)Tj 4.1953 0 Td (3.)Tj /F2 1 Tf -17.1001 -1.6228 Td (36.)Tj /F0 1 Tf 2.622 0 Td (Cook)Tj 2.5158 0 Td (G,)Tj 1.2685 0 Td (Dowdall)Tj 3.7063 0 Td (T,)Tj 1.1055 0 Td (Pomera)Tj 3.4442 0 Td (ntz)Tj 1.5378 0 Td (D,)Tj 1.2047 0 Td (Wang)Tj 2.7992 0 Td (Y.)Tj 1.1481 0 Td (Clicking)Tj 3.6496 0 Td (clean:)Tj 2.8418 0 Td (how)Tj 2.0197 0 Td (companies)Tj 4.9748 0 Td (are)Tj 1.637 0 Td (creating)Tj 3.7064 0 Td (the)Tj 1.5874 0 Td (green)Tj 2.7284 0 Td (inter-)Tj -41.8752 -1.1906 Td (net.)Tj 1.8638 0 Td (Greenpea)Tj 4.3725 0 Td (ce)Tj 1.2543 0 Td (Inc.,)Tj 2.0906 0 Td (Washingto)Tj 4.6489 0 Td (n,)Tj 1.0346 0 Td (DC.)Tj 1.9205 0 Td (2014.)Tj 0.83 0.64 0.02 0 k 2.6788 0 Td (http://w)Tj 3.1748 0 Td (ww.greenpea)Tj 5.8465 0 Td (ce.org/usa/w)Tj 5.5772 0 Td (p-content)Tj 4.0961 0 Td (/uploads/)Tj -38.5586 -1.1835 Td (legacy/Gl)Tj 4.0961 0 Td (obal/usa/p)Tj 4.5284 0 Td (lanet3/PDF)Tj 4.9252 0 Td (s/clickingc)Tj 4.4717 0 Td (lean.pdf)Tj 0 g /F2 1 Tf -20.6434 -1.6299 Td (37.)Tj /F0 1 Tf 2.622 0 Td (Zook)Tj 2.4095 0 Td (MA,)Tj 1.9701 0 Td (Graham)Tj 3.7772 0 Td (M.)Tj 1.3181 0 Td (Mapping)Tj 3.9756 0 Td (DigiPlace:)Tj 4.6347 0 Td (geocode)Tj 3.763 0 Td (d)Tj 0.7583 0 Td (Internet)Tj 3.5504 0 Td (data)Tj 2.126 0 Td (and)Tj 1.8496 0 Td (the)Tj 1.5804 0 Td (representat)Tj 4.9677 0 Td (ion)Tj 1.5237 0 Td (of)Tj 1.0346 0 Td (place.)Tj 2.8276 0 Td (Envi-)Tj -42.0665 -1.1835 Td (ronment)Tj 3.8268 0 Td (and)Tj 1.8567 0 Td (Planning)Tj 4.0323 0 Td (B:)Tj 1.1552 0 Td (Planning)Tj 4.0323 0 Td (and)Tj 1.8567 0 Td (Design.)Tj 3.5504 0 Td (2007)Tj 2.4024 0 Td (Jun)Tj 1.8071 0 Td (1;)Tj 1.0346 0 Td (34\(3\):466±)Tj 4.7481 0 Td (82.)Tj /F2 1 Tf -32.9246 -1.6228 Td (38.)Tj /F0 1 Tf 2.622 0 Td (Zimmer)Tj 3.5646 0 Td (M.)Tj 1.3181 0 Td (OkCupid)Tj 4.0536 0 Td (Study)Tj 2.7355 0 Td (Reveals)Tj 3.7701 0 Td (the)Tj 1.5874 0 Td (Perils)Tj 2.6717 0 Td (of)Tj 1.0346 0 Td (Big-Data)Tj 4.0465 0 Td (Science.)Tj 3.9898 0 Td (Wired.)Tj 3.0685 0 Td (14)Tj 1.3111 0 Td (May)Tj 2.0834 0 Td (2016.)Tj 0.83 0.64 0.02 0 k 2.6788 0 Td (https://)Tj 2.9551 0 Td (www.)Tj -40.8688 -1.1906 Td (wired.com/)Tj 4.7552 0 Td (2016/05/ok)Tj 4.8614 0 Td (cupid-study)Tj 5.0245 0 Td (-reveals-per)Tj 5.237 0 Td (ils-big-data-)Tj 5.1166 0 Td (science/)Tj 0 g (.)Tj 4.1032 0 Td (Accesse)Tj 3.7205 0 Td (d)Tj 0.7653 0 Td (12)Tj 1.3111 0 Td (June)Tj 2.3528 0 Td (2016.)Tj ET Q q 1 j 1 J 0 w 576 737.1 m 36 737.1 l 36 737.6 l 576 737.6 l f* 36 741.2598 169.285 23.6976 re W* n q 169.2283 0 0 23.6409 36 741.3165 cm q /I0 Do Q Q Q q 0 0 612 792 re W* n 1 j 1 J 0 w 7.9999 0 0 7.9999 391.9748 745.7952 cm BT /F0 1 Tf 1 TL -0.005 Tc 0 0 Td (Ten)Tj 1.9204 0 Td (Simple)Tj 3.2245 0 Td (Rules)Tj 2.7283 0 Td (for)Tj 1.3678 0 Td (Respons)Tj 3.8764 0 Td (ible)Tj 1.7362 0 Td (Big)Tj 1.637 0 Td (Data)Tj 2.3032 0 Td (Researc)Tj 3.6567 0 Td (h)Tj ET Q q 1 j 1 J 0 w 36 48.0002 m 576 48.0002 l 576 47.5002 l 36 47.5002 l f* 0.83 0.64 0.02 0 k 142.5827 34.9228 m 295.8236 34.9228 l h f* 0 g 7.9999 0 0 7.9999 36 36 cm BT /F0 1 Tf 1 TL -0.005 Tc 0 0 Td (PLOS)Tj 2.8559 0 Td (Computationa)Tj 6.1229 0 Td (l)Tj 0.4322 0 Td (Biology)Tj 3.4371 0 Td (|)Tj 0.83 0.64 0.02 0 k 0.4748 0 Td (https:/)Tj 2.6788 0 Td (/doi.org/10.13)Tj 6.0094 0 Td (71/journal.p)Tj 5.1166 0 Td (cbi.1005399)Tj 0 g 6.3497 0 Td (March)Tj 2.9551 0 Td (30,)Tj 1.5874 0 Td (2017)Tj 26.5821 0 Td (10)Tj 1.304 0 Td (/)Tj ET endstream endobj 282 0 obj < >stream endstream endobj 283 0 obj < >stream endstream endobj 284 0 obj < >stream endstream endobj 285 0 obj < >stream endstream endobj 286 0 obj < >stream endstream endobj 287 0 obj < >stream endstream endobj 288 0 obj < >stream endstream endobj 289 0 obj < >stream endstream endobj 290 0 obj < >stream endstream endobj 291 0 obj < >stream BT 66.402 0 Td (10)Tj ET Q endstream endobj 9 0 obj < > endobj 293 0 obj < > endobj 294 0 obj < > endobj 297 0 obj < > endobj 298 0 obj < > endobj 299 0 obj < > endobj 300 0 obj < > endobj 301 0 obj < > endobj 302 0 obj < > endobj 303 0 obj < > endobj 304 0 obj < > endobj 305 0 obj < > endobj 306 0 obj < > endobj 307 0 obj < > endobj 308 0 obj < > endobj 309 0 obj < > endobj 310 0 obj < > endobj 311 0 obj < > endobj 312 0 obj < > endobj 313 0 obj < > endobj 314 0 obj < > endobj 315 0 obj < > endobj 316 0 obj < > endobj 317 0 obj < > endobj 318 0 obj < > endobj 319 0 obj < > endobj 320 0 obj < > endobj 321 0 obj < > endobj 322 0 obj < > endobj 323 0 obj < > endobj 324 0 obj < > endobj 325 0 obj < > endobj 326 0 obj < > endobj 327 0 obj < > endobj 328 0 obj < > endobj 295 0 obj < > endobj 329 0 obj < > endobj 330 0 obj < > endobj 331 0 obj < > endobj 332 0 obj < > endobj 333 0 obj < > endobj 334 0 obj < > endobj 335 0 obj < > endobj 336 0 obj < > endobj 337 0 obj < > endobj 338 0 obj < > endobj 339 0 obj < > endobj 340 0 obj < > endobj 341 0 obj < > endobj 342 0 obj < > endobj 343 0 obj < > endobj 344 0 obj < > endobj 345 0 obj < > endobj 346 0 obj < > endobj 347 0 obj < > endobj 348 0 obj < > endobj 349 0 obj < > endobj 350 0 obj < > endobj 351 0 obj < > endobj 352 0 obj < > endobj 353 0 obj < > endobj 354 0 obj < > endobj 355 0 obj < > endobj 356 0 obj < > endobj 357 0 obj < > endobj 358 0 obj < > endobj 359 0 obj < > endobj 360 0 obj < > endobj 296 0 obj < > endobj 361 0 obj < > endobj 362 0 obj < > endobj 363 0 obj < > endobj 364 0 obj < > endobj 365 0 obj < > endobj 366 0 obj < > endobj 367 0 obj < > endobj 368 0 obj < > endobj 369 0 obj < > endobj 370 0 obj < > endobj 371 0 obj < > endobj 372 0 obj < > endobj 373 0 obj < > endobj 374 0 obj < > endobj 375 0 obj < > endobj 376 0 obj < > endobj 377 0 obj < > endobj 378 0 obj < > endobj 379 0 obj < > endobj 380 0 obj < > endobj 381 0 obj < > endobj 382 0 obj < > endobj 383 0 obj < > endobj 384 0 obj < > endobj 385 0 obj < >/Metadata 2 0 R>> endobj 292 0 obj < >/XObject< >>>/CropBox[0 0 612 792]/MediaBox[0 0 612 792]/Parent 9 0 R/Annots 396 0 R/Contents 397 0 R/TrimBox[0 0 612 792]>> endobj 4 0 obj [/PDF/ImageB/ImageC/ImageI/Text] endobj 5 0 obj < > endobj 398 0 obj < > endobj 399 0 obj < >stream %!PS-AdobeFont-1.0 %%This font is a conversion from Helvetica with the following copyright notice: %Copyright (c) 1985, 1987, 1989, 1990, 1997, 1998, 1999 Adobe Systems Incorporated. (extracted from544.html)
Accept additional cookies Reject additional cookies View cookies Hide cookie message Skip to main content GOV.UK Navigation menu Menu Menu Services and information Benefits Births, death, marriages and care Business and self-employed Childcare and parenting Citizenship and living in the UK Crime, justice and the law Disabled people Driving and transport Education and learning Employing people Environment and countryside Housing and local services Money and tax Passports, travel and living abroad Visas and immigration Working, jobs and pensions Government activity Departments Departments, agencies and public bodies News News stories, speeches, letters and notices Guidance and regulation Detailed guidance, regulations and rules Research and statistics Reports, analysis and official statistics Policy papers and consultations Consultations and strategy Transparency Data, Freedom of Information releases and corporate reports Search GOV.UK × Search GOV.UK Search Search GOV.UK Search Home Health and social care National Health Service Digital and data-driven health and care technology Department of Health & Social Care Guidance A guide to good practice for digital and data-driven health technologies Updated 19 January 2021 Contents Introduction 1. (extracted from114.html)
Cancel Services and information Benefits Births, death, marriages and care Business and self-employed Childcare and parenting Citizenship and living in the UK Crime, justice and the law Disabled people Driving and transport Education and learning Employing people Environment and countryside Housing and local services Money and tax Passports, travel and living abroad Visas and immigration Working, jobs and pensions Government activity Departments News Guidance and regulation Research and statistics Policy papers and consultations Transparency How government works Get involved Support links Help Privacy Cookies Accessibility statement Contact Terms and conditions Rhestr o Wasanaethau Cymraeg Government Digital Service All content is available under the Open Government Licence v3.0 , except where otherwise stated © Crown copyright (extracted from114.html)
This policy brief is based on the Dutch report Big Data in een vrije en veilige samenleving (Big Data in a Free and Secure Society), presented by wrr to Ard van der Steur, the Dutch Minister for Security and Justice, on 28 April 2016. (extracted from627.pdf)
The gdpr will not be applicable to the police and justice sector, whose work will be regulated by national legislation to be based on the new eu Police and Criminal Justice Data Protection Directive.56 Fundamental rights and security exceptions The regulation of data protection and privacy is founded on fundamental rights that are enshrined in treaties such as the European Convention on Human Rights (echr) and the Charter of Fundamental Rights of the European Union. (extracted from627.pdf)
Table 9.1 Legal frameworks Constitutional framework International Covenant on Civil and Political Rights (iccpr) European Convention for the Protection of Human Rights and Fundamental Freedoms (echr) Charter of Fundamental Rights of the European Union Constitution of the Netherlands Police and judiciary Intelligence and security services Government (other agencies) Code of Criminal Procedure (WvSv) Intelligence and Security Services Act Personal Data Protection Act (Wbp) Judicial Data and Criminal Records (Wiv 2002) Data Protection Directive (95/46/ec) Act (Wjsg) General Data Protection Regulation Police Data Act (Wpg) (eu) 2016/679 Special Investigative Services Act (Wet bod) Police and Criminal Justice Data Protection Directive (eu) 2016/680 Law enforcement and public prosecuting authorities operate under their own legal framework, often with specific laws regulating the collection, exchange and use of data within the police organisation and the wider law enforcement community. (extracted from627.pdf)
WRR-Policy Brief 6 | Big Data and Security Policies | p.27 process to shift to the European Court of Human Rights and the European Court of Justice. (extracted from627.pdf)
Regarding these rights, we focus on issues relating to the right to respect private life, human dignity, ownership, safety and liability, freedom of expression and the prohibition of discrimination as well as access to justice and the right to a fair trial. (extracted from633.pdf)
This report analyses the safety of robots and artificial intelligence artefacts and the respective responsibilities of the designer, the operator and the user, as well as the consequences for human dignity, freedom of expression, ownership, the security of robots and artificial intelligence artefacts, discrimination and access to justice. (extracted from633.pdf)
39 3.8 Access to justice and the right to a fair trial .................................................................... (extracted from633.pdf)
The next three sections are dedicated to how automated decisions may affect freedom of expression, the prohibition of discrimination, access to justice and the right to a fair trial (sections 3.6, 3.7 and 3.8). (extracted from633.pdf)
Intelligent artefacts may challenge different dimensions of human rights, for example in terms of freedoms (privacy and data protection, ownership, autonomy, personality), equality (more specifically, non-discrimination) and justice (fair trial, access to justice). (extracted from633.pdf)
Thus, the impact of intelligent artefacts is first discussed in the context of seven existing human rights: the right to respect for private and family life (3.2), the right to human dignity (3.3), the right to the peaceful enjoyment of possessions (3.4), tort rights and safety (3.5), the right to freedom of expression as well as the freedom of thought, conscience and religion (3.6), the prohibition of discrimination (3.7) and access to justice as well as the right to a fair trial (3.8). (extracted from633.pdf)
18 Court of Justice of the European Union 19 October 2016, C-582/14 (Breyer) and Court of Justice of the European Union 24 November 2011, C-70/10 (Scarlet/SABAM), paragraph 51. (extracted from633.pdf)
For instance, on an international level, both the European Court of Human Rights and the Court of Justice of the European Union have voiced their concerns on numerous occasions regarding state surveillance activities and the impact on privacy rights.25 For 21 The Dutch Data Protection Authority imposed penalty payments on a company that could not demonstrate that Wi-Fi tracking in public spaces was necessary for a legitimate purpose. (extracted from633.pdf)
21 instance when the EU Court of Justice declared the Data Retention Directive to be invalid.26 On a national level, the British Investigatory Powers Tribunal, for instance, ruled that during a time span of more than a decade, the British intelligence agencies illegally collected data about innocent citizens and illegally tracked their phone and web use.27 With regard to surveillance enacted by companies, the European Commission proposed the reform of EU data protection regulations, which ultimately led to the General Data Protection Regulation. (extracted from633.pdf)
Court of Justice of the European Union 8 April 2014, Joined Cases C-293/12 and C-594/12 (Digital Rights Ireland and Seitlinger and Others). (extracted from633.pdf)
See also: Court of Justice of the European Union 21 December 2016, Joined Cases C-203/15 (Tele2 Sverige AB v Post-och telestyrelsen) and C-698/15 (Secretary of State for the Home Department v Tom Watson and Others). (extracted from633.pdf)
28 For an overview of (a selection of) Court of Justice of the European Union and European Court of Human Rights cases, see Annex I of Article 29 Data Protection Working Party, Working Document 01/2016 on the justification of interferences with the fundamental rights to privacy and data protection through surveillance measures when transferring personal data (European Essential Guarantees), 13 April 2016. (extracted from633.pdf)
In addition, with regard to decisions of the Court of Justice of the European Union concerning data protection, see: L. (extracted from633.pdf)
These letters are available at: http://ec.europa.eu/justice/data-protection/article-29/documentation/other-document/index_en.htm. (extracted from633.pdf)
42 3.8 Access to justice and the right to a fair trial Access to justice and the right to a fair trial Article 6(2) ECHR plays an important role with regard to predictive AI. (extracted from633.pdf)
The increased use of risk-assessing algorithms in the American justice system raises accountability and transparency issues.100 It has been reported that software used to set bail, conditions for parole and sentencing decisions is biased against African Americans (Angwin et al. (extracted from633.pdf)
To give another example, in response to worries by consumers about Wi-Fi tracking by shop owners, the former Dutch minister of economic affairs and the state secretary of security and justice stated that people should just turn off their smartphone if they do not want to be tracked.102 On the basis of this response, it seems that tracking and tracing people is a right which is deemed more important than peoples’ (privacy) rights. (extracted from633.pdf)
Besides affecting the right to respect for private life in numerous ways, digitisation, virtualisation and robotisation influence human dignity, the right to the peaceful enjoyment of possessions, safety and tort rights, the right to freedom of expression, the prohibition of discrimination, access to justice and the right to a fair trial. (extracted from633.pdf)
Access to justice and the right to a fair trial (see subsection 3.8) The Council of Europe could establish a framework of minimum norms to be taken into account when a court uses AI. (extracted from633.pdf)
(2007) E-Justice, beginselen van behoorlijke elektronische rechtspraak, The Hague: SDu Uitgevers. (extracted from633.pdf)
responsibleai.com/ justice, privacy, knowledge, democracy and responsibility. (extracted from97.pdf)
European Parliament 2019-2024 Plenary sitting A9-0186/2020 8.10.2020 REPORT with recommendations to the Commission on a framework of ethical aspects of artificial intelligence, robotics and related technologies (2020/2012(INL)) Committee on Legal Affairs Rapporteur: Ibán García del Blanco Rapporteurs for the opinion (*): Urmas Paet, Committee on Foreign Affairs Alexandra Geese, Committee on Internal Market and Consumer Protection Valter Flego, Committee on Transport and Tourism Assita Kanko, Committee on Civil Liberties, Justice and Home Affairs (*) Associated committees – Rule 57 of the Rules of Procedure (Initiative – Rule 47 of the Rules of Procedure) RR\1215422EN.docx PE650.508v02-00 EN EN United in diversity PR_INL CONTENTS Page MOTION FOR A EUROPEAN PARLIAMENT RESOLUTION.............................................3 ANNEX TO THE MOTION FOR A RESOLUTION: DETAILED RECOMMENDATIONS AS TO THE CONTENT OF THE PROPOSAL REQUESTED..............................................33 A. (extracted from182.pdf)
TEXT OF THE LEGISLATIVE PROPOSAL REQUESTED............................................37 EXPLANATORY STATEMENT............................................................................................62 OPINION OF THE COMMITTEE ON FOREIGN AFFAIRS................................................65 OPINION OF THE COMMITTEE ON THE INTERNAL MARKET AND CONSUMER PROTECTION..........................................................................................................................76 OPINION OF THE COMMITTEE ON TRANSPORT AND TOURISM...............................84 OPINION OF THE COMMITTEE ON CIVIL LIBERTIES, JUSTICE AND HOME AFFAIRS..................................................................................................................................91 OPINION OF THE COMMITTEE ON EMPLOYMENT AND SOCIAL AFFAIRS..........100 OPINION OF THE COMMITTEE ON THE ENVIRONMENT, PUBLIC HEALTH AND FOOD SAFETY.....................................................................................................................108 OPINION OF THE COMMITTEE ON CULTURE AND EDUCATION............................123 INFORMATION ON ADOPTION IN COMMITTEE RESPONSIBLE...............................129 FINAL VOTE BY ROLL CALL IN COMMITTEE RESPONSIBLE..................................130 PE650.508v02-00 2/130 RR\1215422EN.docx EN MOTION FOR A EUROPEAN PARLIAMENT RESOLUTION with recommendations to the Commission on a framework of ethical aspects of artificial intelligence, robotics and related technologies (2020/2012(INL)) The European Parliament, – having regard to Article 225 of the Treaty on the Functioning of the European Union, – having regard to Article 114 of the Treaty on the Functioning of the European Union, – having regard to the Charter of Fundamental Rights of the European Union, – having regard to Council Regulation (EU) 2018/1488 of 28 September 2018 establishing the European High Performance Computing Joint Undertaking1, – having regard to Council Directive 2000/43/EC of 29 June 2000 implementing the principle of equal treatment between persons irrespective of racial or ethnic origin2 (Racial Equality Directive), – having regard to Council Directive 2000/78/EC of 27 November 2000 establishing a general framework for equal treatment in employment and occupation3 (Equal Treatment in Employment Directive), – having regard to Regulation (EU) 2016/679 of the European Parliament and of the Council of 27 April 2016 on the protection of natural persons with regard to the processing of personal data and on the free movement of such data, and repealing Directive 95/46/EC (General Data Protection Regulation)4 (GDPR), and to Directive (EU) 2016/680 of the European Parliament and of the Council of 27 April 2016 on the protection of natural persons with regard to the processing of personal data by competent authorities for the purposes of the prevention, investigation, detection or prosecution of criminal offences or the execution of criminal penalties, and on the free movement of such data, and repealing Council Framework Decision 2008/977/JHA5, – having regard to the Interinstitutional Agreement of 13 April 2016 on Better Law- Making6, – having regard to the proposal for a regulation of the European Parliament and of the Council of 6 June 2018 establishing the Digital Europe Programme for the period 2021- 2027 (COM(2018)0434), – having regard to the Communication from the Commission to the European Parliament, 1 OJ L 252, 8.10.2018, p. (extracted from182.pdf)
PE650.508v02-00 4/130 RR\1215422EN.docx EN Rights and Fundamental Freedoms, and the European Charter for Regional or Minority Languages, – having regard to the OECD Council Recommendation on Artificial Intelligence adopted on 22 May 2019, – having regard to Rules 47 and 54 of its Rules of Procedure, – having regard to the opinions of the Committee on Foreign Affairs, the Committee on the Internal Market and Consumer Protection, the Committee on Transport and Tourism, the Committee on Civil Liberties, Justice and Home Affairs, the Committee on Employment and Social Affairs, the Committee on the Environment, Public Health and Food Safety and the Committee on Culture and Education, – having regard to the report of the Committee on Legal Affairs (A9-0186/2020), Introduction A. (extracted from182.pdf)
Considers that technologies which can produce automated decisions, thus replacing decisions taken by public authorities, should be treated with the utmost precaution , notably in the area of justice and law enforcement; 68. (extracted from182.pdf)
https://www.sae.org/standards/content/j3016_201806/ PE650.508v02-00 38/130 RR\1215422EN.docx EN Rights of the European Union (the ‘Charter’), settled case-law of the Court of Justice of the European Union, and other European and international instruments which apply in the Union. (extracted from182.pdf)
Dalunde, Karima Delli, Anna Deparnay-Grunenberg, Tilly Metz 0 - 0 0 Key to symbols: + : in favour - : against 0 : abstention PE650.508v02-00 90/130 RR\1215422EN.docx EN 22.9.2020 OPINION OF THE COMMITTEE ON CIVIL LIBERTIES, JUSTICE AND HOME AFFAIRS for the Committee on Legal Affairs with recommendations to the Commission on the framework of ethical aspects of artificial intelligence, robotics and related technologies (2020/2012(INL)) Rapporteur for opinion (*): Assita Kanko (*) Associated committee – Rule 57 of the Rules of Procedure (Initiative – Rule 47 of the Rules of Procedure) SUGGESTIONS The Committee on Civil Liberties, Justice and Home Affairs calls on the Committee on Legal Affairs, as the committee responsible, to incorporate the following suggestions into its motion for a resolution: – having regard to Articles 2 and 3 of the Treaty on European Union (TEU), – having regard to Articles 10, 19, 21 and 167 of the Treaty on the Functioning of the European Union (TFEU), – having regard to the right to petition enshrined in Articles 20 and 227 of the TFEU and Article 44 of the Charter of Fundamental Rights of the European Union (EUCFR), – having regard to Articles 21 and 22 of the EUCFR, – having regard to the preamble to the TEU, – having regard to the Council of Europe’s Framework Convention for the Protection of National Minorities, Protocol No 12 to the Convention for the Protection of Human Rights and Fundamental Freedoms, and the European Charter for Regional or Minority Languages, – having regard to Council Directive 2000/43/EC of 29 June 2000 implementing the principle of equal treatment between persons irrespective of racial or ethnic origin1 1 OJ L 180, 19.7.2000, p. (extracted from182.pdf)
(2021) 28 Open letter by 61 organisations calling for legal limits on AI risk assessment systems in the criminal justice context 19 affects the right to the right to a fair trial. (extracted from40.pdf)
THE ADMINISTRATION OF JUSTICE AND DEMOCRATIC PROCESSES31 The use of AI in the administration of justice and democratic processes is particularly sensitive and should be approached with more nuance and scrutiny than it is done now. (extracted from40.pdf)
Administration of justice and democratic processes: (a) AI systems intended to be used for or assist a judicial authority in researching and interpreting facts and the law and in applying the las to a concrete set of facts. (extracted from40.pdf)
We do see areas for improvement though, in particular where it comes to inclusivity and multi- disciplinarity, complaints and redress, and the exclusion (for now) of the applicability of the AIA to ‘legacy high-risk AI’ and components of large scale European IT systems in the realm of “freedom, security and justice” already put into service before the application of the AIA. (extracted from40.pdf)
This issue has particular resonance for policy makers, because algorithmic systems increasingly play a role in determining outcomes in public sector realms like the welfare or criminal justice systems. (extracted from343.pdf)
Those the middle of a racial justice crisis and a pandemic, concerns keep getting louder from all elements of which are disproportionately affecting people of society. (extracted from431.pdf)
“Algorithms and Economic Justice: A Taxonomy of Harms and a Path Forward for the Federal Trade Commission.” Yale Journal of Law & Technology 23 (2020): S1-S1. (extracted from394.pdf)
Venice Commission :: Council of Europe EN FR Login Venice Commission Council of Europe Home The Council in brief Human Rights Democracy Rule of Law Organisation 46 Countries Topics Newsroom Council of Europe Human Rights and Rule of Law Venice Commission EN | FR | DE | IT | RU The Commission About us Types of activities Founding documents News Programme References Members Member states Individual members Map of member States Positions Statements Democratic institutions and fundamental rights Constitutional reforms Fundamental rights Democratic institutions Rule of law Judicial reforms Ombudsman Elections, referendums and political parties Council for Democratic Elections Conferences of Electoral Management Bodies Seminars and assistance Political parties VOTA database Council of Europe and elections Constitutional justice Regional co-operation Constitutional Courts CODICES E-Bulletin World Conference on Constitutional Justice Non-European Partners Southern Mediterranean Overview of activities Highlights by beneficiary Recent events UniDem Med Campus List of UniDem seminars Intercultural Workshops on Democracy Arabic Speaking EMBs Tunisia : support to independent bodies Central Asia Kyrgyzstan – support to electoral reform Central Asia Rule of Law Programme List of events Latin America Overview of activities Recent events Sub-commission on Latin America Main documents Statute Main reference documents Compilations of studies and opinions Annual reports Documents Opinions Ongoing opinions Follow-up Studies Recent documents access by: Countries Topics Series Language Search Web Resources Newsletter Publications Articles on the Commission Links Library Constitutions Contact us For the media Reset password CDL-AD(2019)016-e Joint Report of the Venice Commission and of the Directorate of Information Society and Action against Crime of the Directorate General of Human Rights and Rule of Law (DGI), on Digital Technologies and Elections, adopted by the Council of Democratic Elections at its 65th meeting (Venice, 20 June 2019) and by the Venice Commission at its 119th Plenary Session (Venice, 21-22 June 2019) Show related documents (1) Choose a year all 2019 CDL-AD(2019)016 French 07/02/2020 - Public Rapport conjoint de la Commission de Venise et de la Direction de la société de l’information et de la lutte contre la criminalité, Direction générale Droits de l’homme et État de droit (DGI) sur les technologies numériques et les élections, adopté par le Conseil des élections démocratiques lors de sa 65e réunion (Venise, 20 juin 2019) et par la Commission de Venise lors de sa 119e session plénière (Venise, 21-22 juin 2019) View in full screen Activities Human Rights and Rule of Law Democracy Who we are Human Rights Convention Council of Europe Treaties Press Multimedia Newsroom Web TV Photo galleries Campaigns Useful links Employment Call for tenders Archives Archived web pages Sitemap Amicale Administrative Tribunal E-cards Contact us Secretary General & Deputy Secretary General Media Contacts External Offices Visit us Newsletters Patronage Form close Disclaimer - © Council of Europe 2014 - © photo credit - Webmaster Bookmarks Print RSS © Council of Europe 2007-2025 (extracted from138.html)
Several witnesses highlighted the growing use of AI within the US justice system, in particular the Correctional Offender Management Profiling for Alternative Sanctions (COMPAS) system, developed by Northpointe, and used across several US states to assign risk ratings to defendants, which help to assist judges in sentences and parole decisions. (extracted from381.pdf)
Will Crosthwait, co-founder of AI start-up Kensai, highlighted investigations which found that the system commonly overestimated the recidivism risk of black defendants, and underestimated that of white defendants.143 Big Brother Watch observed that here in the UK, Durham Police have started to investigate the use of similar AI systems for determining whether suspects should be kept in custody, and described this and other developments as a “very worrying trend, particularly when the technology is being trialled when its abilities are far from accurate”.144 Evidence from Sheena Urwin, Head of Criminal Justice at Durham Constabulary, emphasised the considerable lengths that Durham Constabulary have taken to ensure their use of these tools is open, fair and ethical, in particular the development of their ‘ALGO-CARE’ framework for the ethical use of algorithms in policing.145 138 Written evidence from Dr Ansgar Koene (AIC0208) 139 Q 74 (Olivier Thereaux) 140 Written evidence from Research Councils UK (AIC0142) 141 Written evidence from Leverhulme Centre for the Future of Intelligence (AIC0182) 142 Tom Simonite, ‘When it comes to gorillas, Google Photos remains blind’, Wired (11 January 2018): https://www.wired.com/story/when-it-comes-to-gorillas-google-photos-remains-blind/ [accessed 31 January 2018] 143 Written evidence from Will Crosthwait (AIC0094) 144 Written evidence from Big Brother Watch (AIC0154) 145 Written evidence from Marion Oswald and Sheena Urwin (AIC0068) AI IN THE UK: READY, WILLING AND ABLE? (extracted from381.pdf)
* Libby Kinsey, Co-founder, Project Juno ** Dr Mercedes Bunz, Senior Lecturer, Communications QQ 55–64 and Media Research Institute, University of Westminster ** Elizabeth Denham, UK Information Commissioner, Information Commissioner’s Office * Dr Sandra Wachter, Postdoctoral Researcher in Data Ethics and Algorithms, Oxford Internet Institute * Olivier Thereaux, Head of Technology, The Open Data QQ 65–75 Institute * Javier Ruiz Diaz, Policy Director, The Open Rights Group ** Frederike Kaltheuner, Policy Officer, Privacy International ** Dr James Luke, Chief Technology Officer for the Public QQ 76–84 Sector, IBM ** Kriti Sharma, Vice President of Artificial Intelligence and Bots, Sage * Andrew de Rozairo, Vice President, Customer Innovation and Enterprise Platform, SAP * Colin Griffiths, Policy Manager, Citizens Advice QQ 85–94 ** Will Hayter, Project Director, Competition and Markets Authority ** Olly Buston, CEO and Founder, Future Advocacy QQ 95–104 * Professor Dame Henrietta Moore, Director, Institute for Global Prosperity, UCL ** Professor Richard Susskind OBE, IT Adviser to the Lord Chief Justice of England and Wales * Dr Mark Taylor, Global Strategy & Research Director, QQ 105–115 Dyson ** Dr Joseph Reger, Chief Technology Officer EMEIA, Fujitsu ** Paul Clarke, Chief Technology Officer, Ocado * Dr Julian Huppert, Chair, Independent Review Panel for QQ 116–127 DeepMind Health ** Dr Sobia Raza, Head of Science, PHG Foundation * Nicola Perrin, Head, Understanding Patient Data, Wellcome Trust ** Dr Hugh Harvey, Clinical Artificial Intelligence QQ 128–142 Researcher and Consultant Radiologist, Guy’s and St Thomas’ NHS Foundation Trust * Dame Fiona Caldicott, National Data Guardian for Health and Care, Office of the National Data Guardian AI IN THE UK: READY, WILLING AND ABLE? (extracted from381.pdf)
There were three overarching principles to this: • partnership with, not replacement of, humans; • putting human values at the centre of their applications; and • a strong focus on a wide-ranging set of ethical considerations, including the preservation of human autonomy, beneficence, non-maleficence, and justice. (extracted from381.pdf)
for use in the criminal justice system, so defendants and their lawyers can understand and challenge evidence used against them); and • Tools for investigators and auditors for use when things go wrong. (extracted from381.pdf)
How to use the Principles for Action The first version of these principles has been co-drafted through a multistakeholder process, while paying careful As stated, this first version of the Principles for Action will attention to the EU GDPR11 and the police and criminal be reviewed based on the practical findings of the policy justice directive,12 and has drawn inspiration from some of pilot. (extracted from430.pdf)
“EU Data Protection Rules”, EU website, https://ec.europa.eu/commission/priorities/justice-and-fundamental-rights/ data-protection/2018- reform-eu-data-protection-rules/eu-data-protection-rules_en (link as of 28/1/20). (extracted from430.pdf)
Also from Council of Europe, on 22 October, 2020, the Parliamentary Assembly of the Council of Europe (PACE) adopted seven reports concerning the impact of AI: the need for democratic governance of AI78 ​; the role of AI in policing and criminal justice systems79 ​; preventing ​ ​ discrimination caused by AI80 ​; ethical and legal frameworks for the research and development ​ 78 See Need for democratic governance of artificial intelligence, available at ​ ​ https://pace.coe.int/en/files/28742/html. (extracted from7.pdf)
​ 79 See Justice by algorithm - the role of artificial intelligence in policing and criminal justice systems, available at ​ ​ https://pace.coe.int/en/files/28723/html. (extracted from7.pdf)
She further noted that: The deaths of George Floyd and countless others have prompted a transnational uprising against systemic racism in law enforcement [...] Part of the human rights response must include greater scrutiny of how the design and use of digital technologies is further entrenching this systemic racism.113 ​ Achiume echoes much recent work on AI governance from a human rights perspective when she proclaims that ensuring racial justice and the protection of human rights will require prohibiting certain applications of AI. (extracted from7.pdf)
It was indicated that the question of red lines is under serious discussion on multiple sides, and that the possibility of a ban remains on the table, especially regarding so-called remote biometric identification systems, and the use of AI in sensitive domains such as criminal justice. (extracted from7.pdf)
​ 44 Europe’s Approach to Artificial Intelligence: How AI Strategy is Evolving We have seen how even simple algorithms, such as that used in the UK’s A-Level grading fiasco, can amplify unfair and discriminatory outcomes and mobilize people to demand justice amid chants of “fuck the algorithm”155 ​. (extracted from7.pdf)
Richard Benjamins, Ana Berenguer, Director General for the Observatorio del impacto social y ético de la President of the Valencian Region inteligencia artificial (ODISEIA) Carlos Castillo, Professor of Computer Science, Nuria Oliver, Commissioner for the President of Universitat Pompeu Fabra, Barcelona the Valencian Region on AI Strategy and Data Lorena Jaume-Palasí, founder and CEO of The Science to fight COVID-19, Spain Ethical Tech Society Amparo Alonso Betanzos, computer scientist and president of the Spanish Association for Brussels roundtable: Speakers Nuria Oliver, Commissioner for the President of Friederike Reinhold, senior policy advisor for the Valencian Region on AI Strategy and Data AlgorithmWatch, Germany Science to fight COVID-19, Spain Veronika Žolnerčíková, CyberSecurity & Krzysztof Izdebski, Policy Director of ePaństwo CyberCrime Center of Excellence at Masaryk Foundation, Poland University (C4E), Co-creator of Czech National Meeri Haataja, CEO & co-founder of Saidot, strategy on AI, Czech Republic Finland 47 Europe’s Approach to Artificial Intelligence: How AI Strategy is Evolving Karma Peiro, Data Journalist & Co-director of CommissionMarcel Kolaja, European the Visualization and Transparency Parliament Vice President and member of the Foundation, Spain Czech Pirate Party Sarah Chander, Senior Policy Advisor at Andreas Hartl, Head of Division, Strategy European Digital Rights (EDRi), Belgium Artificial Intelligence, Data Economy, Hanna Zinner, Artificial Intelligence and Digital Blockchain, Federal Ministry for Economic Industry, DG CNECT, European Affairs and Energy, Germany Audience Jim Dratwa, Team Leader, European Group on Despoina Riga, assistant to MEP Anna-Michelle Ethics, European Commission Assimakopoulou Killian McDonagh Dit, Directorate-General for Natalia Joanna Boniecka, assistant to MEP Justice and Consumers, European Commission Andrzej Halicki Anna Moscibroda, Directorate General for Georgios Theodotou, assistant to MEP Elena Justice and Consumers, European Commission Kountoura Zoi Kardasiadou, Directorate General for Matt Mahmoudi, Researcher/Advisor on Justice and Consumers, European Commission Artificial Intelligence & Human Rights at Aimilia Givropoulou, assistant to MEP Patrick Amnesty International Breyer Ella Jakubowska, Policy & Campaigns on Anne van Heijst, assistant to MEP Liesje van biometrics at European Digital Rights Schreinemacher 48 Europe’s Approach to Artificial Intelligence: How AI Strategy is Evolving Sample Agenda . (extracted from7.pdf)
www.nytimes.com/2017/06/13/opinion/how- computers-are-harming-criminal-justice.html. (extracted from96.pdf)
In addition to strong enforcement of the General Data Protection Regulation (GDPR) and safeguards such as human rights impacts assessments, software transparency and the availability of datasets for public scrutiny, it is vital that the upcoming regulatory proposal establishes in law clear limitations as to what can be considered lawful uses of AI, to unequivocally address the following issues: • the enabling of biometric mass surveillance and monitoring of public spaces; • the exacerbation of structural discrimination, exclusion and collective harms; • the restriction of and discriminatory access to vital services such as health-care and social security; • the surveillance of workers and infringement of workers’ fundamental rights; • the impeding of fair access to justice and procedural rights; • the use of systems which make inferences and predictions about our most sensitive characteristics, behaviours and thoughts; • and, crucially, the manipulation or control of human behaviour and associated threats to human dignity, agency, and collective democracy. (extracted from154.pdf)
Use of risk assessment tools in the criminal justice system and pre-trial context The use of algorithms in criminal justice matters to profile individuals within legal decision-making processes presents severe threats to fundamental rights. (extracted from154.pdf)
In addition, substantial evidence has shown that the introduction of such systems in criminal justice systems in Europe and elsewhere has resulted in unjust and discriminatory outcomes. (extracted from154.pdf)
We argue that legal limits must be imposed on AI risk assessment systems in the criminal justice context. (extracted from154.pdf)
Legal restrictions or legislative red-lines on the uses which contravene fundamental rights, including, but not limited to, uses of AI at the border, predictive policing, systems which restrict access to social rights and benefits, and risk-assessment tools in the criminal justice context; 3. (extracted from154.pdf)
Yours sincerely, European Digital Rights (EDRi), including: Access Now Bits of Freedom Chaos Computer Club D3 - Defesa dos Direitos Digitais Electronic Privacy Information Center (EPIC) Fitug Hermes Center Homo Digitalis IT-Pol Denmark Iuridicum Remedium Metamorphosis Foundation Panoptykon Foundation Privacy International Statewatch Other signatories: AI Now Institute, NYU Algorithm Watch Amnesty International App Drivers and Couriers Union (ADCU) Associazione Certi Diritti Associazione Luca Coscioni Associazione per gli Studi Giuridici sull'Immigrazione Big Brother Watch Center for Intersectional Justice (CIJ) Democratic Society Digitale Freiheit Dutch Section - International Commission of Jurists (NJCM) Each One Teach One (EOTO) e.V. (extracted from154.pdf)
Civil Rights Principles for the Era of Big Data - The Leadership Conference on Civil and Human Rights Mobile Menu Overlay Home About Our Work The Coalition Take Action Media & Resources Podcast Blog Library Contact Us Switch to: Education Fund Careers Donate Right Arrow An arrow pointing to the right Switch to: Education Fund Careers Donate About Us History Task Forces Board of Directors Staff Careers Internships Voting Record 75th Anniversary Ways to Give Our Work Democracy Census and Data Equity Courts Voting Rights Justice Immigrant Rights Justice Reform Policing Inclusion & Opportunity Center for Civil Rights and Technology Economic Justice Education Equity Fighting Hate & Bias Protect DEIA The Coalition Media & Resources Advocacy Letters Amicus Briefs Beyond 100 Days Biden Timeline Comments Fact Sheets Press Releases Project 2025 Reports Resources Testimony Trump Timeline Take Action Podcast Blog Civil Rights Principles for the Era of Big Data Center for Civil Rights and Technology News 02.27.14 Share Technological progress should bring greater safety, economic opportunity, and convenience to everyone. (extracted from250.html)
At the same time, as new technologies allow companies and government to gain greater insight into our lives, it is vitally important that these technologies be designed and used in ways that respect the values of equal opportunity and equal justice. (extracted from250.html)
This requires disclosure of the underlying data, and the right to correct it when inaccurate.Signatories: American Civil Liberties Union Asian Americans Advancing Justice — AAJC Center for Media Justice ColorOfChange Common Cause Free Press The Leadership Conference on Civil and Human Rights NAACP National Council of La Raza National Hispanic Media Coalition National Urban League NOW Foundation New America Foundation’s Open Technology Institute Public Knowledge Back to Press & Media Civil and Human Rights Coalition Applauds Priorities of Presidential Budget Join the fight for justice, inclusion, and fairness for all. (extracted from250.html)
Pah et al., How to Build a More Open Justice System, 369 Sci. (extracted from222.pdf)
An example is given by the US criminal Data Protection Regulation - GDPR).7 justice system, which is increasingly resorting to the use Article 8 of the EU Charter of Fundamental Rights states of artificial agents to ease the burden of managing such that “Everyone has the right to the protection of personal a large system. (extracted from236.pdf)
4 ProPublica (23/05/2016) Machine Bias 5 The Telegraph (24/03/2016) Microsoft deletes 'teen girl' AI after it became a Hitler-loving sex robot within 24 hours 6 The Guardian (03/08/2014) The death of privacy 7 http://ec.europa.eu/justice/data-protection/ 24 Greencoat Place, London SW1P 1BE • t +44 (0) 20 7798 6040 • e info@ibe.org.uk • www.ibe.org.uk • Charity No. (extracted from236.pdf)
1084014 Business Ethics and Artificial Intelligence Issue 58 | January 2018 Page 4 Fairness Binding clauses that define the use for which the technology is intended are helpful, however it is also Fairness and justice, which are core issues in the necessary to conduct appropriate due diligence on the stakeholder theory, remain paramount for ethical clients as well to minimise the risk of a potentially businesses when dealing with AI. (extracted from236.pdf)
Increasing the availability of open data - Continuing the development open MKM Ongoing activity --- data portal - Project to support both the open data demand and publishing 2 Abbreviations: EAS – Enterprise Estonia, HITSA – Information Technology Foundation for Education, HTM – Ministry of Education and Research, JM – Ministry of Justice, MKM – Ministry of Economic Affairs and Communications, STAT – Statistics Estonia 3 ’---’ marks action item which will not need additional or targeted budget, or it is not possible to distinguish such costs from rest of activity’s budget 4 See https://www.etag.ee/en/funding/programmes/rita/ 2 Estonia’s National AI Strategy – Government of the Republic of Estonia – July 2019 Expert group proposals and existing Action item Responsible Deadline Budget3 measures agency2 Additional activities and measures: 1.8. (extracted from220.pdf)
At a global level, the UNESCO Global Recommendations on the Ethics of AI in November 2021 forefronts the principles of human dignity, inclusive growth and social justice. (extracted from234.pdf)
justice, health, transport, etc). (extracted from620.html)
PAGE 4 STEP 1: DATA GENERATION AND ACQUISITION JUSTICE The recommendations based on justice are intended to ensure reciprocity, so that when data are collected, subjects are offered benefits that are proportionate to the data you request. (extracted from591.pdf)
With reference to the basic orientation of justice, the physician can convince Patient U. (extracted from591.pdf)
PAGE 10 STEP 2: DATA STORAGE AND MANAGEMENT JUSTICE Justice requires enabling customers to use their data for the benefit of third parties and the common good. (extracted from591.pdf)
Source: https:// deepmind.com/blog/article/specifying-ai-safety-problems PAGE 16 STEP 3: DATA ANALYSIS AND KNOWLEDGE GENERATION JUSTICE The recommendations on justice address the problem of indirect discrimination as it relates to data-based models and predictions. (extracted from591.pdf)
PAGE 22 STEP 4: USE OF DATA-BASED PRODUCTS AND SERVICES JUSTICE Justice requires ensuring that the use of data-based products and services does not lead to undesirable social effects such as indirect discrimination or the stigma- tisation or exclusion of groups of people. (extracted from591.pdf)
If the app were made compulsory, or even linked to significant social advantages (such as the right to use public transportation), not knowing what groups are notified and inequalities in the rate of false positives would imply ignoring important elements about justice in deployment. (extracted from591.pdf)
Thus, the app as designed protects privacy, but prevents the collection on information relevant to the justice implications of the app’s use and functionality. (extracted from591.pdf)
There is thus a tension between justice and privacy in the deployment of the app. (extracted from591.pdf)
justice, health, transport, etc). (extracted from362.html)
President: Pascal Pichonnaz First Vice-President: Lord John Thomas Second Vice-President: Anne Birgitte Gammeljord Treasurer: Pietro Sirena Speaker of the Senate: Reinhard Zimmermann Secretary-General: Vanessa Wilcox Scientific Director: Christiane Wendehorst European Law Institute Secretariat Schottenring 16/175 1010 Vienna Austria Tel.: + 43 1 4277 22101 Mail: secretariat@europeanlawinstitute.eu Website: www.europeanlawinstitute.eu ISBN: 978-3-9505192-7-3 © European Law Institute 2022 Cover image: Shutterstock This publication was co-funded by the European Union’s Justice Programme. (extracted from156.pdf)
Guiding Principle 8: No limitations to the exercise of rights and access to justice 21 Automation shall not prevent, limit, or render unfeasible the exercise of rights and access to justice by affected persons. (extracted from156.pdf)
The operator is the person dispute resolution as access to justice would thus in control of the risks connected with the ADM and be deprived of human intervention. (extracted from156.pdf)
The risk- No limitations to the allocation model proposed by this Guiding Principle is exclusively based on the operation of the ADM exercise of rights and under the parameters of control and benefit and has access to justice to be complemented by other liability regimes. (extracted from156.pdf)
A basic smart home system is installed in a house to control the heating, the unfeasible the exercise of rights and access shutters, the sunshades, and the sprinklers in to justice by affected persons. (extracted from156.pdf)
For unknown reasons, to justice may be prevented, hampered or limited the ADM instructs the system to unfurl the by the inadequate use of automation. (extracted from156.pdf)
garage are completely flooded, the sunshades Second, where the affected person is deprived of the collapse due to the weight of the water, and the possibility of exercising a right or access to justice water starts to seep through the windows of the solely on the grounds that the contested decision living room and the hall. (extracted from156.pdf)
In this regard, many countries (13) mentioned to have at least one department, unit or dedicated team working on stimulating the For example, in The Netherlands, the uptake of AI in the public sector, researching the effects of AI or Ministry of Justice and Safety now has a preparing new AI-specific regulations. (extracted from142.pdf)
Of these 13 countries, 9 number of people working in the Justice reported to have more than one of these units. (extracted from142.pdf)
2020 37 As presented at the Data Justice Lab in 2019 on AI Realism and structural alternatives 49 With that purpose in mind, one should consider how existing data governance regimes and national regulatory practices can be transforming and not just intensifying existing power asymmetries. (extracted from142.pdf)
As an example, the strategy highlights the use of hackathons in the justice domain to develop AI solutions for concrete policy issues. (extracted from142.pdf)
A number of policy domains where the Dutch government is exploring the use of AI or will stimulate other actors to use AI in their fields are mentioned and are listed below:  The use of AI in the field of security and justice. (extracted from142.pdf)
A good starting point can be Article 2 of the Treaty on the European Union, which defines the values on which the Union is founded and are common to the Member States as ‘a society in which pluralism, non- discrimination, tolerance, justice, solidarity and equality between women and men prevail’51 An additional set of shared European values (rights and freedoms) is defined by the Charter of Fundamental Rights of the European Union52 and only apply in cases where Member States implement EU regulation directly or transpose it into national legislation. (extracted from142.pdf)
In addition, there are also preventive administrative measures, such as the decision of the Belgian police regulator to forbid piloting the use of face recognition technology at the Zaventem airport57 ; or the negative advice by the French data protection regulator regarding two pilots using facial recognition technology in French schools58; or the cease and desist letter issued by the French data protection regulator to the French Ministry of the Interior regarding the use of Automatic number plate recognition (ANPR) systems 59 54 http://www.sigmaweb.org/publications/principles-public-administration.htm 55 https://www.europarl.europa.eu/charter/pdf/text_en.pdf 56 The French Justice Reform Act, Article 33, https://www.legifrance.gouv.fr/affichTexteArticle.do;jsessionid=98B09D0394DAE57F1618DC21F30405F6.tplgfr34s_1?idArticle=JORFARTI 000038261761&categorieLien=id&cidTexte=JORFTEXT000038261631&dateTexte= 57 https://www.vrt.be/vrtnws/nl/2019/09/20/politie-mag-geen-automatische-gezichtsherkenning-gebruiken-op-de 58 https://www.cnil.fr/fr/experimentation-de-la-reconnaissance-faciale-dans-deux-lycees-la-cnil-precise-sa-position 59 https://www.cnil.fr/fr/radars-troncons-mise-en-demeure-du-ministere-de-linterieur 73 In our initial framework it is thus proposed to consider the multiple elements of AI in public services that can be grouped into macro-areas labelled as: Digital Infrastructure, Organisational Resources, Digital Government Development and Digital Society Development, as described in Figure 16 below. (extracted from142.pdf)
But while like goes Page 6 Artificial Intelligence: Australia’s Ethics Framework (A Discussion Paper) with like, justice sometimes demands that different situations be treated differently. (extracted from57.pdf)
- Justice means that like situations should deliver like outcomes, but different situations can deliver different outcomes. (extracted from57.pdf)
It also highlighted the fact that ethics frameworks on their own are not enough, because concrete actions need to be taken to ensure accountability and justice. (extracted from57.pdf)
This technology could be especially useful in industries that require decision makers to generate frequent, accurate and replicable predictions and judgements such as the areas of justice, policing and medicine. (extracted from57.pdf)
Miscarriages of justice are frequently attributable to human error or misconduct. (extracted from57.pdf)
- Justice means that like situations should deliver like outcomes, but different situations can deliver different outcomes. (extracted from57.pdf)
Available from: https://ec.europa.eu/commission/priorities/justice-and-fundamental-rights/data-protection/2018- reform-eu-data-protection-rules_en. (extracted from57.pdf)
Hungry judges dispense rough justice. (extracted from57.pdf)
(2) Leverage graduate students and postdoctoral researchers from diverse student populations for curriculum creation, (3) Engage the undergraduate community in TA opportunities (i.e., UC Berkeley model) (4) Leverage existing resources currently serving a cross-functional role, e.g., MIT Libraries’ successful efforts to incorporate diversity, inclusion, and social justice into the Libraries' educational and research mission. (extracted from432.pdf)
h@²F+{²¼ËsM¶ \zÛÓ53, nß1Oq¯³Ú R[¤Î×¸Æ> 'ß»´ úÒ¢WôÕ¹Ùv×Ô6ùtZÏb,ÀãÙ=R$J1Ú k¸Ì5×ÿÏâ¿¿häs`ÝÁ.©öãÂ (É ÇD¢ÑÑÎÎ<&þêZ××Ù®Ìâ#»ÌÂ·Îß­`ðË ñË÷¾'´t´ß)i´#ûØ84ìeâ«)¼³5 ÆøÕb&}mêÜÙBÙåÙ­;lYwÂ;¯­TÇW!¯¾ r­£DeVçEZhVMxÃögOv9WÉZJæ¤·IÐO/×ml* wãò-»KP,ýÊ¥W£- 'È6Ët²'lg×gO¼òÍÓÖÜT÷Dá¡ ³ që|îå ¸Wobvgí Î'èè]ª¬RGôd?ÿæ¡ãpôòÜÆ±ÎÞEÍWÎë§lUv¹skë©-ñ¥z /¡Êë·y÷ ×÷\¦ uÐËÊ9pjÓ¦uEõ¶£P¨ÿÏD\ endstream endobj 75 0 obj [ 0[ 778] 3[ 250] 11[ 333 333] 15[ 250 333 250] 19[ 500 500 500 500 500 500 500 500 500 500 278 278] 36[ 722] 38[ 667 722 611 556 722 722 333] 48[ 889] 50[ 722 556] 53[ 667 556 611] 68[ 444 500 444 500 444 333 500 500 278 278 500 278 778 500 500 500 500 333 389 278 500 500 722 500 500 444] 182[ 333] ] endobj 76 0 obj [ 250 0 0 0 0 0 0 0 333 333 0 0 250 333 250 0 500 500 500 500 500 500 500 500 500 500 278 278 0 0 0 0 0 722 0 667 722 611 556 722 722 333 0 0 0 889 0 722 556 0 667 556 611 0 0 0 0 0 0 0 0 0 0 0 0 444 500 444 500 444 333 500 500 278 278 500 278 778 500 500 500 500 333 389 278 500 500 722 500 500 444] endobj 77 0 obj [ 250 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 500 0 444 500 0 0 0 500 0 0 0 0 0 0 500] endobj 78 0 obj [ 278] endobj 79 0 obj < > stream MicrosoftÂ® Word for Office 365 DÃ©claration de la PrÃ©sidence franÃ§aise du ComitÃ© des Ministres du Conseil de lâEurope [Ã lâoccasion de la confÃ©rence des ministres de la Justice sur Â« la Justice en Europe face dÃ©fis du numÃ©rique Â» ] Victoria LAFAGE-ROUX MicrosoftÂ® Word for Office 365 2019-10-15T11:32:44+02:00 2019-10-15T11:32:44+02:00 uuid:9CC3C3DE-C896-45A2-99B8-90C94449E669 uuid:9CC3C3DE-C896-45A2-99B8-90C94449E669 endstream endobj 80 0 obj < > endobj 81 0 obj < ] /Filter/FlateDecode/Length 246>> stream x5Ð¯OBQÆñs/("ÊÏ0 Ý¤tZ nfç? (extracted from323.html)
Veber 18 Big Data in Criminal Justice – Few Chances and Serious Risks 18 Uwe Ewald 18 Big Data, Data Protection and Citizen Empowerment: The Revival of Individual Participation Principle as a Response to New Technological Challenges 19 Wenlong Li 19 Big Data, Psychodiagnostics and Threats to Personal Autonomy 19 Friderik Klampfer, Bojan Musil, Nenad Čuš Babič and Domen Bajde 19 Big health data on social networking platforms: The legal and ethical questions 20 Maria Tzanou 20 Cross-border exchange of big data - innovative technology meets outdated legal framework 21 Stanislaw Tosza 21 Databases and Due Process with regard to European Court of Human Rights’ Case-Law 21 Begüm Bulak Uygun 21 Dispensable humans and indispensable machines in the context of class and social control 22 Zoran Kanduč 22 Economic Cyber Espionage and Regulation of Big Data Theft at the International Level 23 Maša Kovič Dine 23 Finding the right balance between security and privacy: NATO and the big data analyses 23 Mitko Bogdanoski, Metodi Hadji-Janev 23 Five Reasons Not to Personify AI 23 Joanna J. (extracted from397.pdf)
Current issues and future perspectives 31 Federico Costantini 31 State’s Due Diligence in Cyberspace in the Era of Big Data 31 Vasilka Sancin 31 The alluring promise of objectivity: Big data in criminal justice 32 Mojca Plesničar 32 We don’t know what the Questions are, but we know we're gonna find the Answers 32 Alexander Czadilek, Christof Tschohl and Walter Hötzendorfer 32 About the Venue 34 Map of the Building of the Faculty of Law 35 VENUE: Faculty of Law in Ljubljana 36 HOST CITY: Ljubljana 38 OTHER IMPORTANT INFORMATION 39 Notes 40 3 About the Conference “Big Data” is a phrase that has been used pervasively by the media and the lay public in the last several years. (extracted from397.pdf)
Criminal justice systems are using technological solution too, for instance, to predict future crimes of those applying for bail or those to be sent on a parole. (extracted from397.pdf)
• Which programmes and systems of algorithmic predictions are already in place in the criminal justice systems around the globe? (extracted from397.pdf)
Themes of interest include (tentative list): • big data and crime control • predictive policing • automated justice • big data and discrimination • big data and social sorting • ethical dilemmas and predictive analytics • big data and international law • big data and personal data protection law • big data and cyber espionage • big data and citizen empowerment 4 Keynote speakers • Dean Wilson, University of Sussex, Brighton, UK • Nadya Purtova, TILT, Tilburg University, The Netherlands • Joanna J. (extracted from397.pdf)
Dean’s key research interests are in surveillance and policing, and he has published widely in the areas of histories of urban policing, contemporary policing, surveillance and most recently pre-emption and criminal justice. (extracted from397.pdf)
Together with a preliminary request from Ireland this lead to the abolition of the Data Retention Directive by the European Court of Justice (CJEU) in April 2014. (extracted from397.pdf)
Maša Galič: Living labs Case-Law and big data in practice: Stratumseind 2.0 - A discussion of a living lab in the Netherlands 15:30 – 16:00 Coffee break 10 HUMAN RIGHTS, CRIMINAL JUSTICE BIG DATA POLICING AND BIG DATA Hour Session 3 Session 4 Seminar room 4 Seminar room 5 Chair: Primož Gorkič Chair: Miha Hafner 16:00 – 17:30 1. (extracted from397.pdf)
Plesničar: The exchange of big data - innovative alluring promise of technology meets outdated legal objectivity: Big data in framework criminal justice 3. (extracted from397.pdf)
Lydia Morgan: Reconfiguring freedom: Big data, the Criminal Justice – Few Investigatory Powers Act 2016 Chances and Serious Risks and the construction of liberty in the UK’s security state Hour 17:30 – WELCOME RECEPTION – FACULTY OF LAW Main Hall 11 TUESDAY 23rd MAY Hour Lecture hall 8:30 – 9:00 REGISTRATION Red Hour Keynote session 3 Lecture hall Chair: Mojca M. (extracted from397.pdf)
Algorithmic prediction in crime control Aleš Završnik Institute of Criminology at the Faculty of Law The paper will present several existent uses of big data in the criminal justice system, for example, for the prevention of payment card fraud by means of skimming; for the prediction of crime with predictive software; the use of algorithms to predict the recidivism of parolees. (extracted from397.pdf)
It will present the pitfalls of reliance on big data 16 predictions used by law enforcement and criminal justice agencies and the risks big data carries as regards encroachment on fundamental liberties. (extracted from397.pdf)
These are main reasons why Slovenian ministry of justice wants to publish all court decisions on the Internet. (extracted from397.pdf)
Big Data in Criminal Justice – Few Chances and Serious Risks Uwe Ewald Ruhr-Universität Bochum in Germany Starting from the Foucauldian concept of the “regime of truth” in criminal justice this paper will present findings of a case study analyzing a complex organized crime case in Germany were huge amounts of digital data have been introduced into evidence. (extracted from397.pdf)
As findings show Big Data Evidence (BDE) are about to alter the traditional way professionals in law enforcement and criminal justice act in the evidentiary process. (extracted from397.pdf)
Finally, some suggestions should be offered on how to conceptualize truth-finding and BDE and how to limit the risks for a fundamental human rights and rule of law centered approach in criminal justice. (extracted from397.pdf)
Big Data, Psychodiagnostics and Threats to Personal Autonomy Friderik Klampfer, Bojan Musil, Nenad Čuš Babič and Domen Bajde University of Maribor Experts and institutions have warned of the threat that big data analysis poses to our right to privacy, the challenge it raises to our traditional notions of criminal responsibility and justice, as well as concerns about the rising levels of invisible and unaccountable social control (EDPS 2015). (extracted from397.pdf)
Databases are in widespread use in the criminal justice field across the world. (extracted from397.pdf)
In particular, the privacy challenges associated with surveillance, primarily within the realm of the criminal justice databases will be highlighted. (extracted from397.pdf)
Examining the necessity and proportionality of the laws in accordance with the United Nations’ (UN) Universal Declaration of Human Rights (UDHR) (UN 1948), this paper sets out to establish how the average social media savvy citizen is likely to be affected by this restrictive and conservative turn in the Australian criminal justice and anti-terrorism landscape. (extracted from397.pdf)
Judicial oversight of (mass) collecting and processing of personal data Primož Gorkič Faculty of Law, University of Ljubljana The paper explores different approaches to securing judicial oversight of collecting and processing of personal data in a criminal justice system. (extracted from397.pdf)
The alluring promise of objectivity: Big data in criminal justice Mojca Plesničar Institute of Criminology at the Faculty of Law Criminal justice systems have long aimed at preventing judges’ subjectivity from having any impact on in the courtroom. (extracted from397.pdf)
Big data has so far entered criminal justice at three levels: bail, sentencing, and parole. (extracted from397.pdf)
Seen as more objective, such algorithms could instil the long-lost trust of the public in the fairness of the criminal justice system. (extracted from397.pdf)
However, there are some important considerations to be made before embarking on the big-data-saviour-of- justice wagon which we will discuss in detail. (extracted from397.pdf)
We will then delve into the new legislative acts of EU Data Protection Law, the General Data Protection Regulation (Regulation (EU) 2016/679, GDPR) and the Police and Criminal Justice Authorities Directive (Directive (EU) 2016/680) to analyse their implications on the use of Big Data. (extracted from397.pdf)
32 Another important issue is profiling and automated decision making for law enforcement, criminal justice and other purposes based on Big Data. (extracted from397.pdf)
An example is given by the US criminal Data Protection Regulation - GDPR).7 justice system, which is increasingly resorting to the use Article 8 of the EU Charter of Fundamental Rights states of artificial agents to ease the burden of managing such that “Everyone has the right to the protection of personal a large system. (extracted from396.pdf)
4 ProPublica (23/05/2016) Machine Bias 5 The Telegraph (24/03/2016) Microsoft deletes 'teen girl' AI after it became a Hitler-loving sex robot within 24 hours 6 The Guardian (03/08/2014) The death of privacy 7 http://ec.europa.eu/justice/data-protection/ 24 Greencoat Place, London SW1P 1BE • t +44 (0) 20 7798 6040 • e info@ibe.org.uk • www.ibe.org.uk • Charity No. (extracted from396.pdf)
1084014 Business Ethics and Artificial Intelligence Issue 58 | January 2018 Page 4 Fairness Binding clauses that define the use for which the technology is intended are helpful, however it is also Fairness and justice, which are core issues in the necessary to conduct appropriate due diligence on the stakeholder theory, remain paramount for ethical clients as well to minimise the risk of a potentially businesses when dealing with AI. (extracted from396.pdf)
Accept additional cookies Reject additional cookies View cookies Hide cookie message Skip to main content GOV.UK Navigation menu Menu Menu Services and information Benefits Births, death, marriages and care Business and self-employed Childcare and parenting Citizenship and living in the UK Crime, justice and the law Disabled people Driving and transport Education and learning Employing people Environment and countryside Housing and local services Money and tax Passports, travel and living abroad Visas and immigration Working, jobs and pensions Government activity Departments Departments, agencies and public bodies News News stories, speeches, letters and notices Guidance and regulation Detailed guidance, regulations and rules Research and statistics Reports, analysis and official statistics Policy papers and consultations Consultations and strategy Transparency Data, Freedom of Information releases and corporate reports Search GOV.UK × Search GOV.UK Search Search GOV.UK Search Home Government Government efficiency, transparency and accountability Data Ethics Framework Government Digital Service Central Digital & Data Office Guidance Data Ethics Framework Updated 16 September 2020 Contents How to use the Data Ethics Framework Overarching principles Specific actions Next steps Print this page © Crown copyright 2020 This publication is licensed under the terms of the Open Government Licence v3.0 except where otherwise stated. (extracted from83.html)
Cancel Services and information Benefits Births, death, marriages and care Business and self-employed Childcare and parenting Citizenship and living in the UK Crime, justice and the law Disabled people Driving and transport Education and learning Employing people Environment and countryside Housing and local services Money and tax Passports, travel and living abroad Visas and immigration Working, jobs and pensions Government activity Departments News Guidance and regulation Research and statistics Policy papers and consultations Transparency How government works Get involved Support links Help Privacy Cookies Accessibility statement Contact Terms and conditions Rhestr o Wasanaethau Cymraeg Government Digital Service All content is available under the Open Government Licence v3.0 , except where otherwise stated © Crown copyright (extracted from83.html)
PAGE 4 STEP 1: DATA GENERATION AND ACQUISITION JUSTICE The recommendations based on justice are intended to ensure reciprocity, so that when data are collected, subjects are offered benefits that are proportionate to the data you request. (extracted from341.pdf)
With reference to the basic orientation of justice, the physician can convince Patient U. (extracted from341.pdf)
PAGE 10 STEP 2: DATA STORAGE AND MANAGEMENT JUSTICE Justice requires enabling customers to use their data for the benefit of third parties and the common good. (extracted from341.pdf)
Source: https:// deepmind.com/blog/article/specifying-ai-safety-problems PAGE 16 STEP 3: DATA ANALYSIS AND KNOWLEDGE GENERATION JUSTICE The recommendations on justice address the problem of indirect discrimination as it relates to data-based models and predictions. (extracted from341.pdf)
PAGE 22 STEP 4: USE OF DATA-BASED PRODUCTS AND SERVICES JUSTICE Justice requires ensuring that the use of data-based products and services does not lead to undesirable social effects such as indirect discrimination or the stigma- tisation or exclusion of groups of people. (extracted from341.pdf)
If the app were made compulsory, or even linked to significant social advantages (such as the right to use public transportation), not knowing what groups are notified and inequalities in the rate of false positives would imply ignoring important elements about justice in deployment. (extracted from341.pdf)
Thus, the app as designed protects privacy, but prevents the collection on information relevant to the justice implications of the app’s use and functionality. (extracted from341.pdf)
There is thus a tension between justice and privacy in the deployment of the app. (extracted from341.pdf)
The rel- evant concepts include freedom and self-determination, privacy and intimacy, sovereignty and power, beneficence and non-maleficence, as well as justice, solidarity and responsibility. (extracted from355.pdf)
73) The collection and transmission of large volumes of health-relevant data touches on fundamental questions of justice. (extracted from355.pdf)
As a normalising principle of social relations, justice demands that the arbitrary privi- leging of certain persons or groups be avoided. (extracted from355.pdf)
2244 ExEcutIvE Summary 74) As regards big data applications in the healthcare sector, four sets of problems stand out as especially relevant to questions of justice: first, access to datasets for the research sector; second, the insidious con- solidation of monopolistic structures; third, the inclusion of health apps, as well as various devices that facilitate private self-tracking, in determining health insurance premiums; and fourth, aspects of social justice, understood in terms of the capabilities approach, as they con- cern the responsible handling of health-relevant data. (extracted from355.pdf)
Solidarity is frequent- ly understood as complimentary to – and often subsidiary to – the concept of justice. (extracted from355.pdf)
The shaping of such freedom is responsible when it also ori- ents itself towards the legal and societal demands of solidarity and justice. (extracted from355.pdf)
Ensure justice and solidarity C1. (extracted from355.pdf)
To do justice to the complexity and significance of this issue, for example, com- panies and institutions could expand their efforts to establish internal data science departments. (extracted from355.pdf)
If the data is not representative for minority populations then it could be potentially harmful.’ Berk Ustun, Postdoctoral Fellow, Center for Research in Computation and Society, Harvard University Racial bias in criminal justice algorithms The Correctional Offender Management Profiling for Alternative Sanctions (COMPAS) is an algorithmic risk score used to help judges in certain US states decide sentencing, by predicting a defendant’s risk of reoffending. (extracted from4.pdf)
4 Soraya Amrani Mekki, "Justice prédictive et accès au juge", La Justice Prédictive, Actes du Colloque of 12 February 2018 organised by Conseil d’Etat and Cour de cassation Lawyers Council for its bicentenary in partnership with the Paris-Dauphine PSL University, Paris, Dalloz, 2018. (extracted from180.pdf)
15-25 of 1 December 2015 on security in stations; Report titled "Lutte contre la fraude aux prestations sociales : à quel prix pour les droits des usagers ?", September 2017, Parcoursup decisions (2018-323 of 21 December 2018 and 2019-21 of 18 January 2019), Opinion 18-26 of 31 October 2018 on the Draft Programming and Reform Act for Justice, opinion 19-11 of 5 September 2019 on the Draft Act on Bioethics. (extracted from180.pdf)
Court of Appeals for the Second Circuit and Justice David H. (extracted from584.pdf)
Mariano-Florentino Cuéllar is a Justice on the Supreme Court of California, the Herman Phleger Visiting Professor of Law at Stanford University, and a faculty affiliate at the Stanford Center for AI Safety. (extracted from584.pdf)
We also appreciate superb editorial assistance and dedicated research support from three members of Justice Cuéllar’s staff: Ryan Azad, Alexandra Havrylyshyn, and Mikayla Hardisty. (extracted from584.pdf)
A large number of use cases fell under machine learning and were excluded.5 We attempted to health- and law-enforcement-focused subagencies such resolve boundary issues as well as we could through multiple as the Food and Drug Administration, the Office of Justice rounds of quality control.6 Programs, and the Transportation Safety Administration and Customs and Border Protection. (extracted from584.pdf)
As a result, the Department of Health and Human Services, the Department of Justice, Contrary to popular perceptions and the Department of Homeland Security account for a collective 51 use cases. (extracted from584.pdf)
TOP TEN AGENCIES AND SUBAGENCIES BY NUMBER OF USE CASES Number of Agency Name Use Cases Results Office of Justice Programs 12 The results of this survey shed significant light on the state of Securities and Exchange Commission 10 AI/ML in federal administrative agencies. (extracted from584.pdf)
For example, the Department of Health and Human Administration is exploring the use of image recognition to Services (19 use cases), the Department of Justice (16 use cases), screen passenger luggage for explosive devices. (extracted from584.pdf)
The Centers and the Department of Homeland Security (16 use cases) have been refactored into respective sub-agencies (e.g., the Food and for Medicare and Medicaid Services is developing AI-based Drug Administration, the Office of Justice Programs, and Customs tools to predict health care fraud. (extracted from584.pdf)
Others, such powered risk assessment scores for passengers75 and as the Federal Bureau of Investigation Terrorist Screening claims to instead compare each passenger’s personally Database and the Department of Justice National Crime identifiable information “against lookouts and patterns of Information Center, come from peer agencies. (extracted from584.pdf)
While the foregoing cannot possibly do justice to ongoing debate about the proper role of technology-enabled surveillance, it is a crucial debate to have. (extracted from584.pdf)
First, gender promotes accuracy that ultimately inures to the the emerging consensus within machine learning is that benefit of the justice system.”101 Due to the doctrinal “blinding” algorithms to protected characteristics is unlikely uncertainty, states and localities using criminal risk to be effective. (extracted from584.pdf)
Dep’t of Justice, Exec. (extracted from584.pdf)
public-justice-safety/gov-social-media-transforms-chicago-policing. (extracted from584.pdf)
sector-industry-solutions/justice-law-enforcement-and-border- Insider (Oct 12, 2019, 7:59 AM), https://www.businessinsider.com/ security-solutions/linesight (last visited Apr 7, 2019). (extracted from584.pdf)
Mashaw, Bureaucratic Justice: Managing Social Security the residual functional capacity determination, made using the grid Disability Claims (1985); David Ames, Cassandra Handan-Nader, Daniel E. (extracted from584.pdf)
Conference of the U.S., A Review in the Department of Justice and decide immigration cases. (extracted from584.pdf)
Immigration Judge, Dep’t of Justice (June 9, 2017), Rethinking Judicial Review of High Volume Agency Adjudication, 96 Tex. (extracted from584.pdf)
https://www.justice.gov/legal-careers/job/immigration-judge; 4 Executive L. (extracted from584.pdf)
Dep’t of Justice, Executive Off. (extracted from584.pdf)
One of the more interesting arguments MemorandaofUnderstandingMOUs/OtherMOUs/ucm622056.htm; for “internal” constraints on algorithmic decision-making is that, while Memorandum of Understanding Between the Health Information marquee uses of algorithmic decisions systems—e.g., the criminal justice Sharing & Analysis Center, Inc. (extracted from584.pdf)
Mashaw, Bureaucratic Justice: Algorithmic Support in High-Stakes Public Sector Decision-Making, CHI Managing Social Security (1985). (extracted from584.pdf)
101, 109 (2019); Rebecca Wexler, Life, Liberty, and Trade Secrets: Punish the Poor (2018); Cathy O’Neil, Weapons of Math Destruction Intellectual Property in the Criminal Justice System, 70 Stan. (extracted from584.pdf)
Tyler, What Is Procedural Justice?: Criteria Used by Citizens to 91 U.S. (extracted from584.pdf)
Mashaw, Bureaucratic Justice: Managing Social Security 138 Cf. (extracted from584.pdf)
124 Eugene Volokh, Chief Justice Robots, 68 Duke L.J. (extracted from584.pdf)
“Algorithms and Economic Justice: A Taxonomy of Harms and a Path Forward for the Federal Trade Commission.” Yale Journal of Law & Technology 23 (2020): S1-S1. (extracted from235.pdf)
4.2.6 Ethics and society It is necessary to guarantee that the uses of artificial intelligence are focused on humans’ well-being: artificial intelligence must be developed, applied and used with an ethical purpose based on fundamental rights, our social and cultural values, and the ethical principles of beneficence, autonomy of human beings, justice and the necessary explainability of their results. (extracted from221.pdf)
Artificial intelligence should be used in a responsible, sensible and secure way and must include ethical reasoning, in terms of following and maintaining tradition and the European differentiating fact for everything that affects people and their development, as well as guaranteeing justice, transparency and lawfulness. (extracted from221.pdf)
In areas like humanitarian response, where fundamental principles of The third principle, justice, considers who gets chosen to be a research privacy like accuracy are inherently challenged and consent is not subject and what population segments stand to benefit — or be harmed a panacea given that benefits are associated with risks, ethical and — by the research results. (extracted from225.pdf)
IBM’s researcher or non-profit with a social justice mission. (extracted from225.pdf)
It has litigated or intervened in cases implicating the right to privacy in the courts of the United States, the UK, and Europe, including the European Court of Human Rights and the European Court of Justice. (extracted from557.pdf)
on democracy, the rule of law and distributive justice, or on the human mind itself.) Adopt adequate measures to mitigate these risks when appropriate, and proportionately to the magnitude of the risk. (extracted from231.pdf)
9 by reference to dignity, freedoms, equality and solidarity, citizens’ rights and justice. (extracted from231.pdf)
Respect for democracy, justice and the rule of law. (extracted from231.pdf)
Additionally, fairness implies that AI practitioners should respect the principle of proportionality between means and ends, and consider carefully how to Fairness is closely linked to the rights to Non-discrimination, Solidarity and Justice (reflected in Articles 21 and following). (extracted from231.pdf)
Explicability and Responsibility are closely linked to the rights relating to Justice (as reflected in Article 47). (extracted from231.pdf)
on democracy, the rule of law and distributive justice, or on the human mind itself.) Adopt adequate measures to mitigate these risks when appropriate, and proportionately to the magnitude of the risk. (extracted from231.pdf)
In general terms, it deals with questions like “What is a good action?”, “What is the value of a human life?”, “What is justice?”, or “What is the good life?”. (extracted from231.pdf)
General ethical and legal principles)>> endobj 10683 0 obj <</Dest[1937 0 R/XYZ 96 669 null]/Next 10684 0 R/Parent 10682 0 R/SE 7595 0 R/Title(3.1 Human dignity)>> endobj 10684 0 obj <</Dest[1937 0 R/XYZ 322 422 null]/Next 10685 0 R/Parent 10682 0 R/Prev 10683 0 R/SE 7599 0 R/Title(3.2 Self-determination)>> endobj 10685 0 obj <</Dest[1944 0 R/XYZ 96 669 null]/Next 10686 0 R/Parent 10682 0 R/Prev 10684 0 R/SE 7608 0 R/Title(3.3 Privacy)>> endobj 10686 0 obj <</Dest[1944 0 R/XYZ 322 669 null]/Next 10687 0 R/Parent 10682 0 R/Prev 10685 0 R/SE 7612 0 R/Title(3.4 Security)>> endobj 10687 0 obj <</Dest[1949 0 R/XYZ 62 669 null]/Next 10688 0 R/Parent 10682 0 R/Prev 10686 0 R/SE 7618 0 R/Title(3.5 Democracy)>> endobj 10688 0 obj <</Dest[1949 0 R/XYZ 288 487 null]/Next 10689 0 R/Parent 10682 0 R/Prev 10687 0 R/SE 7622 0 R/Title(3.6 Justice and solidarity)>> endobj 10689 0 obj <</Dest[1952 0 R/XYZ 322 669 null]/Parent 10682 0 R/Prev 10688 0 R/SE 7628 0 R/Title(3.7 Sustainability)>> endobj 10690 0 obj <</Count -15/Dest[1959 0 R/XYZ 74 730 null]/First 10691 0 R/Last 10692 0 R/Next 10714 0 R/Parent 6342 0 R/Prev 10679 0 R/SE 7638 0 R/Title(Part C Technical foundations)>> endobj 10691 0 obj <</Dest[4199 0 R/XYZ 96 770 null]/Next 10692 0 R/Parent 10690 0 R/SE 7641 0 R/Title(1. (extracted from174.html)
Use of algorithmic systems by state bodies)>> endobj 10935 0 obj <</Dest[2546 0 R/XYZ 62 669 null]/Next 10936 0 R/Parent 10934 0 R/SE 9730 0 R/Title(7.1 Opportunities and risks involved in the use of algorithmic systems by state bodies)>> endobj 10936 0 obj <</Dest[2546 0 R/XYZ 288 344 null]/Next 10937 0 R/Parent 10934 0 R/Prev 10935 0 R/SE 9734 0 R/Title( 7.2 Algorithmic systems in law-making)>> endobj 10937 0 obj <</Dest[2563 0 R/XYZ 96 500 null]/Next 10938 0 R/Parent 10934 0 R/Prev 10936 0 R/SE 9737 0 R/Title(7.3 Algorithmic systems in the dispensation of justice)>> endobj 10938 0 obj <</Dest[2566 0 R/XYZ 62 669 null]/Next 10939 0 R/Parent 10934 0 R/Prev 10937 0 R/SE 9743 0 R/Title(7.4 Algorithmic systems in public administration)>> endobj 10939 0 obj <</Dest[2566 0 R/XYZ 288 318 null]/Next 10940 0 R/Parent 10934 0 R/Prev 10938 0 R/SE 9749 0 R/Title(7.5 Algorithmic systems in public security law)>> endobj 10940 0 obj <</Dest[2569 0 R/XYZ 322 669 null]/Next 10941 0 R/Parent 10934 0 R/Prev 10939 0 R/SE 9753 0 R/Title(7.6 Transparency requirements for the use of algorithmic systems by state actors)>> endobj 10941 0 obj <</Dest[2645 0 R/XYZ 96 669 null]/Next 10942 0 R/Parent 10934 0 R/Prev 10940 0 R/SE 9767 0 R/Title(7.7 The risk involved in automated total enforcement)>> endobj 10942 0 obj <</Dest[2648 0 R/XYZ 62 666 null]/Parent 10934 0 R/Prev 10941 0 R/SE 9770 0 R/Title(Summary of the most important recommendations for action)>> endobj 10943 0 obj <</Count 8/Dest[2662 0 R/XYZ 96 770 null]/First 10944 0 R/Last 10955 0 R/Parent 10865 0 R/Prev 10934 0 R/SE 9780 0 R/Title(8. (extracted from174.html)
Khoo said that racial justice activists, whom she and her colleagues talked to in the context of the research conducted for the Citizen Lab report, consider the use of algorithmic technologies by police to be 21st-century state violence: before it was done with pen and paper, now it is done with computers and algorithms.20 Ms. (extracted from91.pdf)
The RCMP also confirmed that no ethics review was done before using Clearview AI’s FRT.26 Roch Séguin, director of the Strategic Services Branch, Technical Operations, said that the RCMP had approached the Department of Justice regarding the use of FRTs in its investigations only once, but it was internal to the RCMP. (extracted from91.pdf)
17 (Justice La Forest). (extracted from91.pdf)
In April 2022, the Scottish Commissioner published a draft code of practice on the acquisition, retention, use and destruction of biometric data for criminal justice and police purposes in Scotland. (extracted from91.pdf)
throughout history, rarely more so than in Society may reasonably conclude that justice requires decision-making processes to be designed so that human recent months. (extracted from85.pdf)
individuals faced bias, including overt discrimination, in parts of the justice system. (extracted from85.pdf)
human biases and prejudices, in particular against BAME individuals, in the criminal justice system. (extracted from85.pdf)
94 CDEI have published a research paper on facial recognition technology, which covers police use of live facial recognition technology, along with other uses; https://www.gov.uk/ government/publications/cdei-publishes-briefing-paper-on-facial-recognition-technology/snapshot-paper-facial-recognition-technology 95 See for example: Richardson, Rashida and Schultz, Jason and Crawford, Kate, ‘Dirty Data, Bad Predictions: How Civil Rights Violations Impact Police Data, Predictive Policing Systems, and Justice’, AI Now Institute, https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3333423; Kearns, Ian and Rick Muir, ‘Data Driven Policing and Public Value’, The Police Foundation, 2019; http://www.police-foundation.org.uk/2017/wp-content/uploads/2010/10/data_driven_policing_final.pdf; ‘Policing By Machine’, Liberty, 2019; https://www.libertyhumanrights.org.uk/issue/ policing-by-machine/ 96 RUSI sent Freedom of Information requests to all police forces in England and Wales, interviewed over 60 people from police forces, technology providers, academia, civil society, government, and regulation, and ran roundtables, jointly with CDEI and TechUK. (extracted from85.pdf)
For example, the Offender Assessment System (OASys) and the Offender Group Reconviction Scale (OGRS), routinely used by HM Prison and Probation Service (HMPPS) to measure individuals’ likelihood of reoffending and to develop individual risk management plans.101 99 CDEI ‘Call for evidence summary of responses - Review into bias in algorithmic decision-making’, 2019; https://assets.publishing.service.gov.uk/government/uploads/system/uploads/attachment_data/ file/838426/CDEI-Call-for-Evidence-Bias-Summary-of-responses-October2019.pdf 100 Couchman, Hannah; ‘Policing by machine’, Liberty, 2019; https://www.gov.uk/government/publications/responses-to-cdei-call-for-evidence/cdei-bias-review-call-for-evidence-summary-of-responses 101 Robin Moore, ‘A Compendium of Research and Analysis on the Offender Assessment System (OASys) 2009–2013’, National Offender Management Service, Ministry of Justice Analytical Series, 2015; https://assets.publishing.service.gov.uk/government/uploads/system/uploads/attachment_data/file/449357/research-analysis-offender-assessment-system.pdf 102 The Guardian, ‘Met uses software that can be deployed to see if ethnic groups ‘specialise’ in areas of crime’, 2020; https://www.theguardian.com/uk-news/2020/jul/27/met-police-use-software-ethnic- groups-specialise-profile Centre for Data Ethics and Innovation 67 Review into bias in algorithmic decision-making: Policing Mitigating bias and ensuring fairness requires looking at the entire decision-making process As set out earlier in the report, we think it influence how high or low risk certain crimes or areas are is crucial to take a broad view of the whole deemed by a data analytics tool and potentially perpetuate or exacerbate biased criminal justice outcomes for certain decision-making process when considering groups or individuals. (extracted from85.pdf)
It could also 103 See for example: Richardson, Rashida and Schultz, Jason and Crawford, Kate, ‘Dirty Data, Bad Predictions: How Civil Rights Violations Impact Police Data, Predictive Policing Systems, and Justice’, AI Now Institute, https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3333423 104 Home Office, ‘Police powers and procedures, England and Wales, year ending 31 March 2019’; https://assets.publishing.service.gov.uk/government/uploads/system/uploads/attachment_ data/file/841408/police-powers-procedures-mar19-hosb2519.pdf 105 Nesta, ‘Decision-making in the Age of the Algorithm’, 2019; https://media.nesta.org.uk/documents/Decision-making_in_the_age_of_the_algorithm.pdf Centre for Data Ethics and Innovation 68 Review into bias in algorithmic decision-making: Policing There is a need for strong national leadership on the ethical use of data analytics tools The key finding from the RUSI research was There is work underway at a national level, led by the National Police Chiefs’ Council, in order to develop a a “widespread concern across the UK law coordinated approach to data analytics in policing. (extracted from85.pdf)
more time investigating sexual crime and responding to mental health incidents.111 Research by the RSA and DeepMind113 highlights that people feel more strongly against the use of automated decision systems in the criminal justice system (60 60 percent of people oppose or strongly percent of people oppose or strongly oppose its use in this domain) than other sectors such as financial services. (extracted from85.pdf)
oppose the use of automated decision Moreover, people are least familiar with the use of systems in the criminal justice system.113 automated decision-making systems in the criminal justice system; 83 percent were either not very familiar or not at all familiar with its use. (extracted from85.pdf)
local government Given the rising interest in the use of predictive There have been multiple attempts to map the usage of tools in local government, local authorities are algorithmic decision-making tools across local authorities but many researchers have found this challenging.120 An keen to emphasize that their algorithms support investigation by The Guardian found that, at a minimum, 140 councils out of 408 have invested in software contracts rather than replace decision-makers, that cover identifying benefit fraud, identifying children at risk and allocating school places.121 However this did not particularly in sensitive areas such as children’s include additional use cases found in a report by the Data Justice Lab, a research group based in Cardiff University. (extracted from85.pdf)
The Data Justice Lab used Freedom of Information requests to learn which tools are being used and how frequently. (extracted from85.pdf)
et al., ‘Data Scores as Governance: Investigating uses of citizen scoring in public services’, Data Justice Lab, Cardiff University, 2018; https://datajustice.files.wordpress. (extracted from85.pdf)
et al., ‘Data Scores as Governance: Investigating uses of citizen scoring in public services’, Data Justice Lab, Cardiff University, 2018; https://datajustice.files.wordpress. (extracted from85.pdf)
et al., ‘Data Scores as Governance: Investigating uses of citizen scoring in public services’, Data Justice Lab, Cardiff University, 2018; https://datajustice.files.wordpress.com/2018/12/data-scores-as-governance-project-report2.pdf Centre for Data Ethics and Innovation 76 Review into bias in algorithmic decision-making: Local government Third-party providers offer specialist data science expertise training data results in a poor quality algorithm. (extracted from85.pdf)
et al., ‘Data Scores as Governance: Investigating uses of citizen scoring in public services’, Data Justice Lab, Cardiff University, 2018, p34; https://datajustice.files.wordpress. (extracted from85.pdf)
P.; and Weller, A.; ‘Blind Justice: Fairness with Encrypted Sensitive Attributes’. (extracted from85.pdf)
P., and Weller, A., ‘Blind Justice: Fairness with Encrypted Sensitive Attributes’. (extracted from85.pdf)
159 Open Banking, ‘Open Banking - A Consumer Perspective’, 2017; https://www.openbanking.org.uk/wp-content/uploads/Open-Banking-A-Consumer-Perspective.pdf CCeennttrree ffoorr DDaattaa EEtthhiiccss aanndd IInnnnoovvaattiioonn 9955 Review into bias in algorithmic decision-making: Enabling fair innovation Case study: Monitoring for bias in digital transformation of the courts Accessing protected characteristic data to academic researchers to rebuild their data architecture to monitor outcomes is not only necessary support this.163 The resulting information is intended to both be valuable to the Ministry of Justice for designing fair when introducing algorithmic decision- interventions in the functioning of the courts, but also making, but also when making other major eventually to be made available for independent academic changes to significant decision-making research (via Administrative Data Research UK and the processes. (extracted from85.pdf)
They have worked with the Government Statistical Service’s Harmonisation Team and 160 H ouse of Commons Justice Committee, ‘Court and Tribunal reforms, Second Report of Session 2019’; https://publications.parliament.uk/pa/cm201919/cmselect/cmjust/190/190.pdf 161 Lord Chancellor; Lord Chief Justice; Senior President of Tribunals, ‘Transforming Our Justice System’, 2016; https://assets.publishing.service.gov.uk/government/uploads/system/uploads/ attachment_data/file/553261/joint-vision-statement.pdf 162 Administrative Data Research UK, ‘Data First, Harnessing the Potential of linked administrative data for the justice system’, ongoing; https://www.adruk.org/our-work/browse-all-projects/ data-first-harnessing-the-potential-of-linked-administrative-data-for-the-justice-system-169/ 163 Ibid. (extracted from85.pdf)
An example of how these different definitions play out in practice can be seen in the US criminal justice system, as per the following case study. (extracted from85.pdf)
186 Lord Sales, Justice of the UK Supreme Court; ‘Algorithms, Artificial Intelligence and the Law’, The Sir Henry Brooke Lecture for BAILI, 2019; https://www.supremecourt.uk/ docsspeech-191112.pdf Centre for Data Ethics and Innovation 113 Review into bias in algorithmic decision-making: The regulatory environment 8.3 Legal background Equality Act The Equality Act 2010 (the Act) legally Recent controversies over exam results have highlighted broad public concern about socio-economic disparities. (extracted from85.pdf)
For example, the ruling by the European They must draw from existing statute and case law, Court of Justice in the Test-Achats case made it unlawful and focus on how to apply the existing law to particular for insurers to charge different rates based on sex or situations. (extracted from85.pdf)
Instead the implications of new technologies increasingly across sectors and industries, for the justice system, transport provision and decision- and in novel ways. (extracted from85.pdf)
It also recently (EHRC) is a statutory body responsible for completed an inquiry into the experiences of people with enforcing the Equality Act 2010, as well as disabilities in the criminal justice system, including the responsibilities as a National Human Rights challenges arising from a move towards digital justice, Institution. (extracted from85.pdf)
For example, an investigation by The Guardian last year (https://www.theguardian.com/society/2019/oct/15/councils-using-algorithms-make-welfare- decisions-benefits) showed some 140 of 408 councils in the UK are using privately-developed algorithmic ‘risk assessment’ tools, particularly to determine eligibility for benefits and to calculate entitlements; the New Statesman (https://www.newstatesman.com/science-tech/technology/2019/07/revealed-how-citizen-scoring-algorithms-are-being-used-local) revealed that Experian secured £2m from British councils in 2018; and Data Justice Lab research in late 2018 (https://datajustice.files.wordpress.com/2018/12/data-scores-as-governance-project-report2.pdf) showed 53 out of 96 local authorities and about a quarter of police authorities are now using algorithms for prediction, risk assessment and assistance in decision-making. (extracted from85.pdf)
These could include social care, criminal justice or benefits allocation. (extracted from85.pdf)
It is likely that a collection of Law Society ‘Algorithms in the Criminal Justice System’254 sector-specific registers might be the best approach, with any public sector organisations out of scope of any sector CDEI agrees that there are some significant advantages register remaining responsible for publishing equivalent both to government and citizens in some central transparency data themselves. (extracted from85.pdf)
253 House of Commons Science and Technology Committee, ‘Algorithms in decision-making, Fourth Report of Session 2017-19’; https://publications.parliament.uk/pa/cm201719/cmselect/ cmsctech/351/351.pdf 254 The Law Society, ‘Algorithms in the criminal justice system’, 2019; https://www.lawsociety.org.uk/support-services/research-trends/algorithm-use-in-the-criminal-justice-system-report/ 255 Guidance: Gender pay gap reporting: overview; https://www.gov.uk/guidance/gender-pay-gap-reporting-overview Centre for Data Ethics and Innovation 139 Review into bias in algorithmic decision-making: Transparency in the public sector The relationship between transparency and explainability As mentioned in Chapter 4 the ICO and ATI have jointly developed guidance for organisations on how to explain To uphold accountability, public sector decisions made with AI. (extracted from85.pdf)
This could help society are often dealing with issues that cut identify those who are in particular need of across public services, including housing, tailored cross-departmental, coordinated health, and justice. (extracted from635.pdf)
To further ensure trustworthy AI, most governments are adopting new legislative frameworks for AI technologies, which are often sector- specific and cover areas such as autonomous driving, healthcare and e-justice. (extracted from147.pdf)
The Federal Ministry also highlights the use of AI for evaluating judicial proceedings, as mentioned in its National e-justice strategy. (extracted from147.pdf)
ensuring that citizens can make informed and independent decisions) and to human dignity, equality and justice (i.e. (extracted from147.pdf)
In addition, public sector operators should be secured sufficient resources and incentives to engage in such development, paying particular attention to sort out the rights of the outcomes of co-development;  The Ministry of Justice and the Ministry of Finance are currently examining national regulation of automated decision-making. (extracted from147.pdf)
5.13.4 Regulation Hungary’s national AI strategy aims to ensure a responsible, reliable and human-centred utilisation of AI technologies by means of the following policies:  Creating an ethical framework: developing an AI code of conduct by the first half of 2021 in collaboration between the Ministry of Justice, the Ministry for Innovation and Technology, AI Innovation Hub and the Central Statistical Office;  Setting up an Artificial Intelligence Regulation and Ethics Knowledge Centre: the aim is to create and coordinate an extensive pool of experts to help resolve legal issues and matters of ethics relating to the regulation of AI and the implementation of the strategy;  Establishing a regulatory framework for AI: the objective is to amend the current regulatory system to suit AI and to align it to EU regulations;  Building data management regulation: the objective is to set up regulations for the use and exchange of public and private data and to define rules regarding data monetisation. (extracted from147.pdf)
The Latvian strategy identifies priority sectors with a high potential for AI applications in the country, such as transport (Intelligent transport systems), culture, justice (AI as support for decision making and drafting legislation), agriculture (automated control), and translation. (extracted from147.pdf)
Regarding ethics, the government adopts the European ethical guidelines outlined by the European Commission for the Efficiency of Justice. (extracted from147.pdf)
66 1) International bodies and AI, 2) Swiss intelligence of interests in the European AI (Digital Europe Programme) activities, 3) Changes in the world of work, 4) AI in industry and services, 5) AI in education, 6) Application of AI in science and research, 7) AI in cybersecurity and security policy, 8) AI, Media & Public, 9) Automated mobility and AI, 10) AI in healthcare, 11) AI in the financial sector, 12) AI in agriculture, 13) Energy, climate, environment and AI, 14) AI in administration, 15) Further development of the general legal framework on AI, 16) AI in justice, 17) AI, data and intellectual property law. (extracted from147.pdf)
While the establishment of the legal basis is ensured by a wide range of institutions, the FDFA will specifically focus on the following policies to further develop the general legal framework on AI:  Examining the emergence of AI-specific international law and its impact on Switzerland;  Following-up developments with regard to the visibility of AI systems in interaction with consumers;  Monitoring developments in AI-based decision-making in the justice system (predictive justice). (extracted from147.pdf)
Empowerment TRUST: How can Reliability and safety: Throughout their Data Agency: A/IS EQUITABLE: DoD should Fairness and non- Justice and AI be trusted? (extracted from190.pdf)
States that in line with strict liability systems of the Member States, the proposed Regulation should cover violations of the important legally protected rights to life, health, physical integrity and property, and should set out the amounts and extent of compensation, as well as the limitation period; is of the opinion that the proposed Regulation should also incorporate significant immaterial harm that results in a verifiable economic loss above a threshold harmonised in Union liability law, that balances the access to justice of affected persons and the interests of other involved persons; urges the Commission to re-evaluate and to align the thresholds for damages in Union law; is of the opinion that the Commission should analyse in depth the legal traditions in all Member States and their existing national laws that grant compensation for immaterial harm, in order to evaluate if the inclusion of immaterial harm in AI- specific legislative acts is necessary and if it contradicts the existing Union legal framework or undermines the national law of the Member States; 20. (extracted from184.pdf)
This follows from general and widely accepted liability concepts of justice, according to which the person that creates or maintains a risk for the public is liable if that risk causes harm or damage, and thus should ex-ante minimise or ex-post compensate that risk. (extracted from184.pdf)
(16) This Regulation should cover harm or damage to life, health, physical integrity, property and significant immaterial harm that results in a verifiable economic loss above a threshold, harmonised in Union liability law, that balances the access to justice of affected persons with the interests of other involved persons. (extracted from184.pdf)
The specific human rights implications for AI systems can be viewed through provisions of the European Convention of Human Rights (ECHR) and the European Social Charter (ESC), including its specific guarantees regarding liberty and justice, privacy, freedom of expression, equality and non-discrimination, and social and economic rights. (extracted from351.pdf)
14 Liberty and Justice: AI can adversely affect the liberty A system that supports and justice of individuals, particularly when implemented criminal sentencing decisions in high impact contexts such as criminal justice. (extracted from351.pdf)
AI systems in sensitive public policy areas, including but not limited to law enforcement, -AI systems can also give rise to unjust justice, asylum and migration, health, social categorisation based on new types of security, and employment. (extracted from351.pdf)
This should also include the possibility used in the field of justice and law enforcement of receiving insight into and challenging AI- are in line with the essential requirements of informed decisions in the context of law the right to a fair trial. (extracted from351.pdf)
To this end, they should enforcement or justice, including the right to ensure the quality and security of judicial review of such decision by a human. (extracted from351.pdf)
Such information must especially be provided when AI systems are used in the field of justice and law enforcement, both as concerns the role of AI systems within the process, and the right to challenge the decisions informed or made thereby. (extracted from351.pdf)
We hope that, taken together, this material can function as a kind of launching pad for meaningful reflection on the prospects for a principles-based legal framework for governing AI research and innovation in accordance with the Council of Europe's stewardship of fundamental rights and freedoms, justice, and democratic values. (extracted from351.pdf)
Policy makers, scholars, and activists are tasked with proposing and critiquing strategies and actions aimed at promoting general well-being and social justice. (extracted from351.pdf)
Work in the field of justice European Ethical Charter on the use of AI in judicial systems and their environment (2018) Five key principles are outlined in this charter including respect for fundamental rights, non- discrimination, quality and security, transparency, impartiality and fairness, and “under user control.” Most applications of AI in the judicial field have been found to be in the private sector – “commercial initiatives aimed at insurance companies, legal departments, lawyers, and individuals.” Some potential uses of AI in a judicial setting include case-law enhancement, access to law, and the creation of new strategic tools Other considerations that require considerable methodological precautions include the creation of scales, support for alternative dispute settlement measures in civil matters, pre- litigation resolution of disputes online (when a later appeal to the judge remains possible), or identification of where criminal offences are being committed 4.5. (extracted from351.pdf)
Importance of careful management of AI challenges 'How important is it for companies and governments to carefully manage this challenge?' % Low importance %Moderate importance % High importance Cyber Attack 3 9 88 Autonomous Vehicles 4 10 86 Misaligned with Human Values 4 10 86 Data Privacy 3 12 85 Disease Misdiagnosis 3 12 85 Fake Online Content 3 13 84 Critical AI Failures 4 12 84 Surveillance 4 12 84 Criminal Justice Bias 4 12 84 Autonomous Weapons 5 12 83 HR Bias 5 12 83 Technological Unemployment 4 13 83 Low importance = 'Not at all important' or 'Slightly important' Moderate importance = 'Moderately important' High importance = 'Very important' or 'Extremely important' ©2021 The University of Queensland ©2021 KPMG, an Australian partnership and a member firm of the KPMG global organisation of independent member firms affiliated with KPMG International Limited, a private English company limited by guarantee. (extracted from423.pdf)
Likelihood of AI challenges impacting large numbers of citizens 'In the next 10 years, how likely do you think it is that this challenge will impact large % Unlikely numbers of the people in your country?' % Equally likely as unlikely % Likely Surveillance 17 22 61 Fake Online Content 17 23 60 Cyber Attack 19 21 60 Data Privacy 19 22 59 Disease Misdiagnosis 25 25 50 HR Bias 26 25 49 Technological 30 25 45 Unemployment Critical AI Failures 29 26 45 Misaligned with Human 30 25 45 Values Autonomous Vehicles 31 24 45 Criminal Justice Bias 29 28 43 Autonomous Weapons 42 22 36 Unlikely = 'Very unlikely (<5% chance'), 'Unlikely (5-20% chance)' or 'Somewhat unlikely (20-40% chance)' Equally likely as unlikely = 40-60% chance Likely = 'Somewhat likely (60-80% chance)', 'Likely (80-95% chance)' or 'Very likely (>95% chance)' ©2021 The University of Queensland ©2021 KPMG, an Australian partnership and a member firm of the KPMG global organisation of independent member firms affiliated with KPMG International Limited, a private English company limited by guarantee. (extracted from423.pdf)
criminal justice bias, where Australia was higher on AI. (extracted from423.pdf)
https://www.justice.gc.ca/eng/csj-sjc/pl/charter-charte/c27_1.html#:~:text=In%20 addition%2C%20the%20Artificial%20Intelligence,with%20other%20government%20 entities%20specified 12. (extracted from345.pdf)
Copyright © United Nations Interregional Crime and Justice Research Institute (UNICRI), 2020 Viale Maestri del Lavoro,10, 10127 Torino – Italy Tel: +39 011-6537 111 / Fax: +39 011-6313 368 Website: www.unicri.it E-mail: unicri.publicinfo@un.org © The International Criminal Police Organization (INTERPOL), 2020 200, Quai Charles de Gaulle, 69006 Lyon – France Tel: +33 4 72 44 70 00 / Fax: +33 4 72 44 71 63 Website: www.interpol.int E-mail: edgci-ic@interpol.int 2 FOREWORD Crime is not stagnant. (extracted from387.pdf)
We have strived to shape this forum, giving it meaning and purpose, and positioning it to grow into a global platform for cooperation and collaboration amongst law enforcement on AI This report on AI for law enforcement is the most recent product of the collaboration on AI between the Innovation Centre of the International Criminal Police Organization (INTERPOL) and the United Nations Interregional Crime and Justice Research Institute’s (UNICRI) Centre for AI and Robotics. (extracted from387.pdf)
The increasing interest and attention these meetings are receiving is both a reward for INTERPOL and UNICRI and reveals the growing relevance of AI for the criminal justice community. (extracted from387.pdf)
Human rights, civil liberties and even the fundamental principles of law upon which our criminal justice system is based may be unacceptably exposed, or even irreparably compromised, if we do not navigate this route with extreme caution. (extracted from387.pdf)
The chapter be- gins by presenting the general principles that law enforcement should endeavour to adhere to, namely the respect for human rights, democracy, justice and rule of law, as well as the related requirements of fairness, accountability, transparency and explainability that should be adopted in order for law en- forcement to meet these principles. (extracted from387.pdf)
Equally, it is a valuable exercise for policy- and deci- sion-makers in the broader criminal justice community to, from a legal and ethical perspective, prepare frameworks for the eventual integration of such technologies into law enforcement. (extracted from387.pdf)
The seminal 2019 white paper AI and Ethics at the Police by Leiden University and TU Delft16 suggests that, from a legal perspective, to act responsibly means “to accept moral integrity and authenticity as ideals and to deploy reasonable effort toward achieving them.”17 Striving for moral integrity, in turn, implies “adhering to the values of freedom, equality, and solidarity.”18 For the purposes of this report, however, a more straightforward understanding will be adopted and the term ‘responsible’ will be framed in line with the Oxford Dictionary, which defines ‘responsibly’ as acting “in a sensible or trustworthy manner.”19 In this context, the responsible use of AI by law enforcement should be understood as use that enshrines the general principles of respect for human rights, democracy, justice and the rule of law. (extracted from387.pdf)
Justice for Hedgehogs. (extracted from387.pdf)
It has been heralded for its role in building “an area of freedom, security and justice with a high level of data protection, in accordance with the EU Charter of Fundamental Rights.” Aiming at protecting individuals’ personal data, while guaran- teeing a high level of public security, the LED provides rights for data subjects, as well as obligations for “competent authorities” when processing data for “law enforcement purposes”, i.e., prevention, investi- gation, detection, prosecution of criminal offences or the execution of criminal penalties, including the safeguarding against and the prevention of threats to public security. (extracted from387.pdf)
The development of trustworthy AI systems should be based upon established fundamental values, such as the respect for human dignity, democracy, justice and rule of law, while, at the same time, guaranteeing the freedom of the individual and citizens’ rights in order to ensure equality and non-dis- crimination. (extracted from387.pdf)
45 RECOMMENDED ACTIONS Define a statement of principles on the use of AI in law enforcement that will guide law enforcement to ensure respect for human rights, democracy, justice and the rule of law and support it to prioritize the key requirements of fairness, accountability, transparen- cy and explainability, as well as safety and robustness; › Develop guidance for law enforcement on the implementation of new technology to support and encourage law enforcement agencies to explore and invest in new AI opportunities and to develop training in new AI applications and disseminate best practices; › Create a knowledge-base with the law enforcement community on the requiremen- ts for the adoption of AI, such as what kinds of problems AI is capable of tackling, the current or inherent limitations and the resources (tools, data, expertise, com- puting power) required to implement AI solutions; › Develop guidance for law enforcement on the admissibility of AI in court that as- sesses the impact and results of the specific use of AI in courts, while ensuring the respect for human rights and rule of law; › Create an expert advisory committee that can provide guidance to law enforcement in terms of legislation and serve as a forum for discussing appropriate legislative models with legal experts and other key stakeholders; › Identify an external global body to provide advisory support to law enforcement on ethical issues and to provide support in carrying out audits to check whether a system is responsible and complies with legal requirements; › Foster a community and organize training courses and workshops to attract and connect different stakeholders from law enforcement, industry, academia, civil society and international bodies with the diverse backgrounds and essential per- spectives to gather and synthesize views from cross-sections of society, in order to provide a balanced and facts-based picture of the opportunities and challenges of the use of AI and to highlight the application of AI to law enforcement and provide hands-on support. (extracted from387.pdf)
52 ANNEX II LIST OF ABBREVIATIONS ADM Automated Decision-Making AFP Australian Federal Police AI Artificial intelligence AI-HLEG High-Level Expert Group on AI AiLECS Artificial Intelligence for Law Enforcement of Community Safety CCTV Closed-Circuit Television EU European Union FATE Fairness, Accountability, Transparency and Explainability GDPR General Data Protection Regulation GPS Global Positioning Services IC INTERPOL’s Innovation Centre IEDs Improvised Explosive Devices IGCI INTERPOL’s Global Complex for Innovation INTERPOL International Criminal Police Organization LED Law Enforcement Directive 2016/680 MAS Monetary Authority of Singapore NLP Natural Language Processing non-POI non-Person of Interest NPA Japan National Police Agency OECD Organisation for Economic Co-operation and Development R&D Research and Development UAV Unmanned Aerial Vehicles UNICRI United Nations Interregional Crime and Justice Research Institute ZITiS Central Office for Information Technology in the Security Sector 53 ABOUT INTERPOL INTERPOL is the world’s largest international police organiza- tion. (extracted from387.pdf)
ABOUT UNICRI The United Nations Interregional Crime and Justice Research Institute was established in 1968. (extracted from387.pdf)
Within the broad scope of its mandate, the Institute contributes, through research, training, field activities and the collection, exchange and dissemination of information, to the formulation and implementation of improved policies in the field of crime prevention, justice and emerging security threats, due regard being paid to the integration of such policies within broader policies for socio-economic change and development, and to the protection of human rights. (extracted from387.pdf)
In 2017, UNICRI opened its Centre for Artificial Intelligence and Robotics in The Hague, the Netherlands, with a view towards advancing understanding of artificial intelligence, robotics and related technologies vis-à-vis crime prevention, criminal justice, the rule of law and security. (extracted from387.pdf)
At a global level, the UNESCO Global Recommendations on the Ethics of AI in November 2021 forefronts the principles of human dignity, inclusive growth and social justice. (extracted from393.pdf)
• Utilise AI innovations pro-socially so as to enable bonds of interpersonal solidarity to form and individuals to be socialised and recognised by each other • Use AI technologies to foster this capacity to connect so as to reinforce the edifice of trust, empathy, reciprocal responsibility, and mutual understanding upon which all ethically well- founded social orders rest → CARE for the wellbeing of each and all: • Design and deploy AI systems to foster and to cultivate the welfare of all stakeholders whose interests are affected by their use • Do no harm with these technologies and minimise the risks of their misuse or abuse Understanding Artificial Intelligence Ethics and Safety 10 • Prioritise the safety and the mental and physical integrity of people when scanning horizons of technological possibility and when conceiving of and deploying AI applications → PROTECT the priorities of social values, justice, and the public interest: • Treat all individuals equally and protect social equity • Use digital technologies as an essential support for the protection of fair and equal treatment under the law • Prioritise social welfare, public interest, and the consideration of the social and ethical impacts of innovation in determining the legitimacy and desirability of AI technologies • Use AI to empower and to advance the interests and well-being of as many individuals as possible • Think big-picture about the wider impacts of the AI technologies you are conceiving and developing. (extracted from350.pdf)
Ethical considerations about looking after patient wellbeing and clinical safety are paramount and wider justice concerns about improving healthcare for all and health equity factor in as well. (extracted from350.pdf)
I am also incredibly grateful for the impact that our interactions with the Ministry of Justice (MoJ)’s Data Science Hub has had on developing the framing for this guide. (extracted from350.pdf)
'It's reducing a human being to a percentage': Perceptions of justice in algorithmic decisions. (extracted from350.pdf)
Optimized scoring systems: Toward trust in machine learning for healthcare and criminal justice. (extracted from350.pdf)
Criminal Justice and Behavior, 46(2), 185-209. (extracted from350.pdf)
Data Justice Lab. (extracted from350.pdf)
Dirty data, bad predictions: How civil rights violations impact police data, predictive policing systems, and justice. (extracted from350.pdf)
WFEO-CEIT-Big Data and AI Principles in Engineering - 大数据与人工智能工程原则 - WFEO Home Search this website Who we are About us Members President Executive Board Executive Council Board Committees Strategic Documents Partners and Sponsors Becoming a member Membership payment What we do WFEO Academy With the United Nations & other International Organisations WFEO and Small Island Developing States (SIDS) WFEO in Africa Declarations and Statements Events Publications Biennial Reports Awards The Code of Ethics The Code of Practice on Principles of Climate Change Adaptation for Engineers The Code of Practice for Sustainable Development and Environmental Stewardship WFEO in Action World Engineering Day World Engineers Convention 2023 – WEC 2023 50th anniversary celebrations News & Reports Flash-Infos & Newsletters Sustainable Development Goals WFEO Engineering 2030 Plan (PDF) Young Engineers Advancing the UN SDGs (PDF) Goal 1 – No poverty Goal 2 – Zero hunger Goal 3 – Good health and well-being Goal 4 – Quality education Goal 5 – Gender equality Goal 6 – Clean water and sanitation Goal 7 – Affordable and clean energy Goal 8 – Decent work and economic growth Goal 9 – Industry, innovation and infrastructure Goal 10 – Reduced inequalities Goal 11 – Sustainable cities and communities Goal 12 – Responsible consumption and production Goal 13 – Climate action Goal 14 – Life Below Water Goal 15 – Life on land Goal 16 – Peace, justice and strong institutions Goal 17 – Partnerships for the goals Committees & WGs Disaster Risk Management Engineering for Innovative Technologies Engineering and the Environment Energy Young Engineers / Future Leaders Education in Engineering Anti Corruption Women in Engineering Engineering Capacity Building Information and Communication Water Working Group on Engineering and Climate Change WFEO-UN Relations Committee WFEO-CEIT - Big Data and AI Principles in Engineering Overview Themes Chair Sustainable Development Goals Big Data and AI Principles Online Open Courses Members section WFEO-CEIT-Big Data and AI Principles in Engineering – 大数据与人工智能工程原则 Promoting responsible conduct of Big Data and AI innovation and application in Engineering In order to promote responsible conduct of Big Data and Artificial Intelligence (AI) application and innovation in engineering, World Federation of Engineering Organizations (WFEO) has formulated the following 7 Principles and releases it now on the first World Engineering Day Celebration. (extracted from725.html)
To further ensure trustworthy AI, most governments are adopting new legislative frameworks for AI technologies, which are often sector- specific and cover areas such as autonomous driving, healthcare and e-justice. (extracted from146.pdf)
The Federal Ministry also highlights the use of AI for evaluating judicial proceedings, as mentioned in its National e-justice strategy. (extracted from146.pdf)
ensuring that citizens can make informed and independent decisions) and to human dignity, equality and justice (i.e. (extracted from146.pdf)
In addition, public sector operators should be secured sufficient resources and incentives to engage in such development, paying particular attention to sort out the rights of the outcomes of co-development;  The Ministry of Justice and the Ministry of Finance are currently examining national regulation of automated decision-making. (extracted from146.pdf)
5.13.4 Regulation Hungary’s national AI strategy aims to ensure a responsible, reliable and human-centred utilisation of AI technologies by means of the following policies:  Creating an ethical framework: developing an AI code of conduct by the first half of 2021 in collaboration between the Ministry of Justice, the Ministry for Innovation and Technology, AI Innovation Hub and the Central Statistical Office;  Setting up an Artificial Intelligence Regulation and Ethics Knowledge Centre: the aim is to create and coordinate an extensive pool of experts to help resolve legal issues and matters of ethics relating to the regulation of AI and the implementation of the strategy;  Establishing a regulatory framework for AI: the objective is to amend the current regulatory system to suit AI and to align it to EU regulations;  Building data management regulation: the objective is to set up regulations for the use and exchange of public and private data and to define rules regarding data monetisation. (extracted from146.pdf)
The Latvian strategy identifies priority sectors with a high potential for AI applications in the country, such as transport (Intelligent transport systems), culture, justice (AI as support for decision making and drafting legislation), agriculture (automated control), and translation. (extracted from146.pdf)
Regarding ethics, the government adopts the European ethical guidelines outlined by the European Commission for the Efficiency of Justice. (extracted from146.pdf)
66 1) International bodies and AI, 2) Swiss intelligence of interests in the European AI (Digital Europe Programme) activities, 3) Changes in the world of work, 4) AI in industry and services, 5) AI in education, 6) Application of AI in science and research, 7) AI in cybersecurity and security policy, 8) AI, Media & Public, 9) Automated mobility and AI, 10) AI in healthcare, 11) AI in the financial sector, 12) AI in agriculture, 13) Energy, climate, environment and AI, 14) AI in administration, 15) Further development of the general legal framework on AI, 16) AI in justice, 17) AI, data and intellectual property law. (extracted from146.pdf)
While the establishment of the legal basis is ensured by a wide range of institutions, the FDFA will specifically focus on the following policies to further develop the general legal framework on AI:  Examining the emergence of AI-specific international law and its impact on Switzerland;  Following-up developments with regard to the visibility of AI systems in interaction with consumers;  Monitoring developments in AI-based decision-making in the justice system (predictive justice). (extracted from146.pdf)
Regarding these rights, we focus on issues relating to the right to respect private life, human dignity, ownership, safety and liability, freedom of expression and the prohibition of discrimination as well as access to justice and the right to a fair trial. (extracted from634.pdf)
This report analyses the safety of robots and artificial intelligence artefacts and the respective responsibilities of the designer, the operator and the user, as well as the consequences for human dignity, freedom of expression, ownership, the security of robots and artificial intelligence artefacts, discrimination and access to justice. (extracted from634.pdf)
39 3.8 Access to justice and the right to a fair trial .................................................................... (extracted from634.pdf)
The next three sections are dedicated to how automated decisions may affect freedom of expression, the prohibition of discrimination, access to justice and the right to a fair trial (sections 3.6, 3.7 and 3.8). (extracted from634.pdf)
Intelligent artefacts may challenge different dimensions of human rights, for example in terms of freedoms (privacy and data protection, ownership, autonomy, personality), equality (more specifically, non-discrimination) and justice (fair trial, access to justice). (extracted from634.pdf)
Thus, the impact of intelligent artefacts is first discussed in the context of seven existing human rights: the right to respect for private and family life (3.2), the right to human dignity (3.3), the right to the peaceful enjoyment of possessions (3.4), tort rights and safety (3.5), the right to freedom of expression as well as the freedom of thought, conscience and religion (3.6), the prohibition of discrimination (3.7) and access to justice as well as the right to a fair trial (3.8). (extracted from634.pdf)
18 Court of Justice of the European Union 19 October 2016, C-582/14 (Breyer) and Court of Justice of the European Union 24 November 2011, C-70/10 (Scarlet/SABAM), paragraph 51. (extracted from634.pdf)
For instance, on an international level, both the European Court of Human Rights and the Court of Justice of the European Union have voiced their concerns on numerous occasions regarding state surveillance activities and the impact on privacy rights.25 For 21 The Dutch Data Protection Authority imposed penalty payments on a company that could not demonstrate that Wi-Fi tracking in public spaces was necessary for a legitimate purpose. (extracted from634.pdf)
21 instance when the EU Court of Justice declared the Data Retention Directive to be invalid.26 On a national level, the British Investigatory Powers Tribunal, for instance, ruled that during a time span of more than a decade, the British intelligence agencies illegally collected data about innocent citizens and illegally tracked their phone and web use.27 With regard to surveillance enacted by companies, the European Commission proposed the reform of EU data protection regulations, which ultimately led to the General Data Protection Regulation. (extracted from634.pdf)
Court of Justice of the European Union 8 April 2014, Joined Cases C-293/12 and C-594/12 (Digital Rights Ireland and Seitlinger and Others). (extracted from634.pdf)
See also: Court of Justice of the European Union 21 December 2016, Joined Cases C-203/15 (Tele2 Sverige AB v Post-och telestyrelsen) and C-698/15 (Secretary of State for the Home Department v Tom Watson and Others). (extracted from634.pdf)
28 For an overview of (a selection of) Court of Justice of the European Union and European Court of Human Rights cases, see Annex I of Article 29 Data Protection Working Party, Working Document 01/2016 on the justification of interferences with the fundamental rights to privacy and data protection through surveillance measures when transferring personal data (European Essential Guarantees), 13 April 2016. (extracted from634.pdf)
In addition, with regard to decisions of the Court of Justice of the European Union concerning data protection, see: L. (extracted from634.pdf)
These letters are available at: http://ec.europa.eu/justice/data-protection/article-29/documentation/other-document/index_en.htm. (extracted from634.pdf)
42 3.8 Access to justice and the right to a fair trial Access to justice and the right to a fair trial Article 6(2) ECHR plays an important role with regard to predictive AI. (extracted from634.pdf)
The increased use of risk-assessing algorithms in the American justice system raises accountability and transparency issues.100 It has been reported that software used to set bail, conditions for parole and sentencing decisions is biased against African Americans (Angwin et al. (extracted from634.pdf)
To give another example, in response to worries by consumers about Wi-Fi tracking by shop owners, the former Dutch minister of economic affairs and the state secretary of security and justice stated that people should just turn off their smartphone if they do not want to be tracked.102 On the basis of this response, it seems that tracking and tracing people is a right which is deemed more important than peoples’ (privacy) rights. (extracted from634.pdf)
Besides affecting the right to respect for private life in numerous ways, digitisation, virtualisation and robotisation influence human dignity, the right to the peaceful enjoyment of possessions, safety and tort rights, the right to freedom of expression, the prohibition of discrimination, access to justice and the right to a fair trial. (extracted from634.pdf)
Access to justice and the right to a fair trial (see subsection 3.8) The Council of Europe could establish a framework of minimum norms to be taken into account when a court uses AI. (extracted from634.pdf)
(2007) E-Justice, beginselen van behoorlijke elektronische rechtspraak, The Hague: SDu Uitgevers. (extracted from634.pdf)
Mobility, DG GROW Roberto Viola, Director-General, Communications Networks, Content and Technology Paul Nemitz, Principal Advisor in the Directorate General for Justice and Consumers GETTING IN TOUCH WITH THE EU In person All over the European Union there are hundreds of Europe Direct information centres. (extracted from152.pdf)
For instance, even if a machine's impact is only ever good, the distribution of that good (concerns for justice) might be in question”. (extracted from224.pdf)
Our results reveal a global convergence emerging around five ethical principles (transparency, justice and fairness, non-maleficence, responsibility and privacy), with substantive divergence in relation to how these principles are interpreted, why they are deemed important, what issue, domain or actors they pertain to, and how they should be implemented. (extracted from206.html)
Justice and Justification: Reflective Equilibrium in Theory and Practice (Cambridge Univ. (extracted from206.html)
Human rights: A path for solutions Topics Instruments and mechanisms Publications News UN Human Rights Office Press releases 29 September 2025 Madagascar: UN Human Rights Chief shocked by violent response to electricity and water protests Press releases 26 September 2025 South Sudan South Sudan: Türk alarmed by deteriorating human rights situation amid rising violence and political tensions Press releases 24 September 2025 Attacks on Gaza-bound flotilla defy belief, accountability a must View more news View speeches and statements Independent Experts and Committees Special Procedures Press releases 01 October 2025 UN experts call for reparatory justice for enslavement, the trade in enslaved Africans and colonialism Press releases 01 October 2025 China UN experts urge China to end repression of Uyghur and cultural expression of minorities Press releases 30 September 2025 Democratic Republic of Congo: UN expert warns of abuses against women human rights defenders and their families in South Kivu View all Special Procedures News Treaty Bodies Press releases 30 September 2025 Economic, social and cultural rights Development cannot be achieved on dying planet, UN committee issues new guidance Press releases 29 September 2025 UN Committee on Economic, Social and Cultural Rights publishes findings on Chile, Colombia, Laos, Netherlands, Russia, and Zimbabwe Media advisories 19 September 2025 Enforced and involuntary disappearances UN Committee on Enforced Disappearances to review Montenegro, Benin, and Sri Lanka View all Treaty Bodies News Human Rights Council Press releases 24 September 2025 People of African descent UN experts: Systemic racism against Africans and people of African descent in criminal justice systems is pervasive Press releases 24 September 2025 Sudan: UN Fact-Finding Mission urges immediate action after deadly mosque strike in El Fasher Statements and speeches 23 September 2025 Oral Update by Mr. (extracted from656.html)
Advisory statement on human ethics in artificial intelligence and big data research (2017) - National Research Council Canada Skip to main content Skip to "About government" Language selection Language selection Français fr / Gouvernement du Canada Search Search Menu Main Menu Home Jobs and the workplace Immigration and citizenship Travel and tourism Business and industry Benefits Health Taxes Environment and natural resources National security and defence Culture, history and sport Policing, justice and emergencies Transport and infrastructure Canada and the world Money and finances Science and innovation You are here Canada.ca National Research Council Canada Corporate Values and ethics Research involving human participants Advisory statement on human ethics in artificial intelligence and big data research (2017) 1. (extracted from482.html)
Related links Research involving human participants From: National Research Council Canada Date modified: 2019-03-26 About this site National Research Council Canada Contact the NRC Order products Directory of science professionals Government of Canada All Contacts Departments and agencies About government Themes and topics Jobs Immigration and citizenship Travel and tourism Business Benefits Health Taxes Environment and natural resources National security and defence Culture, history and sport Policing, justice and emergencies Transport and infrastructure Canada and the world Money and finance Science and innovation Indigenous peoples Veterans and military Youth Government of Canada Corporate Social media Mobile applications About Canada.ca Terms and conditions Privacy Top of Page (extracted from482.html)
Peace, inclusiveness and justice, equity and interconnectedness should be promoted throughout the lifecycle of AI systems. (extracted from178.pdf)
These include freedom, dignity and autonomy, privacy and data protection, non- discrimination and equality, diversity, fairness, social justice, and internationally recognised labour rights'. (extracted from178.pdf)
Moreover, a few specific cases, mentioned as running at the time of first gathering, were later found to have been discontinued because of various reasons, including significant criticism received from the general public, pressure from adversarial political forces or even executive orders from local courts of criminal justice. (extracted from144.pdf)
— Fairness and justice: The development of AI should promote fairness and justice, the rights and interests of stakeholders and promote equality of opportunity. (extracted from144.pdf)
Previous research had analysed 84 ethical AI documents published by various businesses, NGOs and (international) governmental organizations, highlighting that some principles such as transparency, fairness, justice, and responsibility were quite common in all. (extracted from144.pdf)
The impact of using algorithms for managerial decisions on public employees’ procedural justice. (extracted from144.pdf)
include human oversight, explainability or interpretability, legal status of AI systems, and the equitable economic effect of AI.31 A separate analysis of 84 AI ethics documents done in 2019 found that there has been a global convergence around “transparency, justice and fairness, non-maleficence, responsibility and privacy.”32 While much progress has been made aligning on responsible AI, there remain differences—even among FCAI participants. (extracted from45.pdf)
is other values.211 Work to advance data free with trust has continued through the handicapped in World Economic Forum.212 advocating for free flow of data so long The “Schrems II” judgment by the Court of Justice of the European Union (CJEU) in 2020, however, has been a seismic event for international transfers as it remains an of personal information, the aftershocks of which are still reverberating and outlier on privacy magnify the impact of the EU regime. (extracted from45.pdf)
Kerry, ”The oracle at Luxembourg: The EU Court of Justice judges the world on surveillance and privacy,” The Brookings Institution, January 11, 2021, https://www.brookings.edu/ research/the-oracle-at-luxembourg-the-eu-court-of-justice-judges-the-world-on-surveillance-and-privacy/. (extracted from45.pdf)
Kerry, ”The oracle at Luxembourg: The EU Court of Justice judges the world on surveillance and privacy,” The Brookings Institution, January 11, 2021, https://www.brookings.edu/research/the-oracle-at-luxembourg-the-eu-court-of-justice-judges-the-world-on- surveillance-and-privacy/. (extracted from45.pdf)
AI tightly developed under human oversight and control), embed the principles of fairness and justice in algorithms, applied in all phases of AI systems’ design, implementation and testing, while following through bias complaints and other undesired effects reporting. (extracted from187.pdf)
During the 71st session of the United Nations General Assembly, on 29 September 2016, UNICRI (United Nations International Crime and Justice research Institute) announced the opening of the first Centre on AI and Robotics 30 PE 626.074 European Artificial Intelligence (AI) leadership, the path for an integrated vision within the United Nations system. (extracted from187.pdf)
AI tightly developed under human oversight and control), embed the principles of fairness and justice in algorithms, applied in all phases of AI systems’ design, implementation and testing, while following through bias complaints and other undesired effects reporting. (extracted from187.pdf)
Apply EUR-Lex Access to European Union law This document is an excerpt from the EUR-Lex website You are here EUROPA EUR-Lex home EUR-Lex - 52021DC0118 - EN Help Print Menu EU law Treaties Treaties currently in force Founding Treaties Accession Treaties Other treaties and protocols Chronological overview Legal acts Consolidated texts International agreements Preparatory documents EFTA documents Lawmaking procedures Summaries of EU legislation Browse by EU institutions European Parliament European Council Council of the European Union European Commission Court of Justice of the European Union European Central Bank European Court of Auditors European Economic and Social Committee European Committee of the Regions Browse by EuroVoc EU case-law Case-law Reports of cases Directory of case-law Official Journal Access the Official Journal Official Journal L series daily view Official Journal C series daily view Browse the Official Journal Legally binding printed editions Special editions National law and case-law National transposition National case-law JURE case-law Information Themes in focus EUR-Lex developments Statistics ELI register About ELI Technical information ELI implementation overview Resources for implementing ELI ELI highlights ELI testimonials Legislation in schema.org EU budget online Quick search Use quotation marks to search for an "exact phrase". (extracted from210.html)
Using Green Public Procurement criteria 34 can boost demand for a green digital transformation The digital transformation should also enable modern and efficient justice systems 35 , enforcement of consumer rights and an increased effectiveness of public action including law enforcement and investigation capacities 36 – what is illegal offline is also illegal online, and law enfor cement must be best equipped to deal with more and more sophisticated digital crimes. (extracted from210.html)
The digital principles are rooted in primary EU law, notably the Treaty on European Union (TEU), the Treaty on the Functioning of the European Union (TFEU), the Charter of Fundament al Rights and the case-law of the Court of Justice of the European Union, as well as in secondary legislation 37 . (extracted from210.html)
(34) https://ec.europa.eu/environment/gpp/eu_gpp_criteria_en.htm (35) Communication from the Commission on the Digitalisation of justice in the European Union A toolbox of opportunities, COM(2020) 710 final. (extracted from210.html)
Help pages Contact Sitemap Follow us X Legal Legal notice Cookies policy Accessibility Privacy statement Information About EUR-Lex Newsletter Useful links Other services European Data EU tenders EU research results EU Whoiswho EU publications N-Lex EU Law Tracker Discover more on europa.eu Contact the EU Call us 00 800 6 7 8 9 10 11 Use other telephone options Write to us via our contact form Meet us at one of the EU centres Social media Search for EU social media channels Legal Languages on our websites Privacy policy Legal notice Cookies EU institutions European Parliament European Council Council of the European Union European Commission Court of Justice of the European Union (CJEU) European Central Bank (ECB) European Court of Auditors European External Action Service (EEAS) European Economic and Social Committee European Committee of Regions (CoR) European Investment Bank European Ombudsman European Data Protection Supervisor (EDPS) European Data Protection Board European Personnel Selection Office Publications Office of the European Union Agencies Switch to mobile Switch to desktop (extracted from210.html)
Hollywood, “Evaluation of the Shreveport Predictive Policing Experiment,” National Institute of Justice, 2014,https://nij.ojp.gov/topics/articles/evaluation-shreveport- predictive-policing-experiment. (extracted from420.pdf)
4.2.6 Ethics and society It is necessary to guarantee that the uses of artificial intelligence are focused on humans’ well-being: artificial intelligence must be developed, applied and used with an ethical purpose based on fundamental rights, our social and cultural values, and the ethical principles of beneficence, autonomy of human beings, justice and the necessary explainability of their results. (extracted from352.pdf)
Artificial intelligence should be used in a responsible, sensible and secure way and must include ethical reasoning, in terms of following and maintaining tradition and the European differentiating fact for everything that affects people and their development, as well as guaranteeing justice, transparency and lawfulness. (extracted from352.pdf)
For example, the use of artificial intelligence tools by law enforcement and the criminal justice system could have an impact on an individual’s right to be free from arbitrary arrest or to equality before the law; surveillance technologies could impact on the right to peaceful assembly; the use of social media platforms could impact the right to mental health; and property rental platforms could alter housing markets, possibly impacting the right to an adequate standard of living. (extracted from385.pdf)
IMPA (European Commission Impact Assessment) , Direction générale des politiques internes de l’Union ( Parlement européen ) Maintenant connu sous… Maintenant connu sous… Direction générale des affaires budgétaires ( Parlement européen ) Maintenant connu sous… Direction générale de l’économie, de la transformation et de l’industrie ( Parlement européen ) Maintenant connu sous… Direction générale des droits des citoyens, de la justice et des affaires institutionnelles ( Parlement européen ) Maintenant connu sous… Direction générale de la cohésion, de l’agriculture et des politiques sociales ( Parlement européen ) Auteurs personnel(s): Ballon, Elke Thèmes: Politique et protection de l'environnement Sujet: adaptation au changement climatique , développement durable , gestion de l'espace , mer Méditerranée , protection du littoral , région côtière , écosystème marin , étude d'impact PDF ISSN ISBN 978-92-823-4634-1 DOI 10.2861/29776 Numéro de catalogue BA-03-13-317-EN-N PDF ISBN 978-92-823-4634-1 DOI 10.2861/29776 Numéro de catalogue BA-03-13-317-EN-N Released on EU publications website: 2017-09-15 Afficher plus Afficher moins Cette publication peut être téléchargée au format web (PDF) et au format de qualité «impression» (PDF/X). (extracted from284.html)
Consider an appropriate mechanism for ensuring a coordinated approach by Irish regulators to Digital, including AI, as part of the forthcoming National Digital Strategy [DOT / DETE / DECC D/ Justice / DTCAGSM / DCEDIY] iii. (extracted from353.pdf)
For example, the Company EXPERIMENTAL REGULATION – REGULATORY Law Review Group submitted a report on AI to the SANDBOXES FOR AI Tánaiste and Minister for Enterprise, Trade and In its proposed new regulation for AI, the EU Employment in December 2020, which analysed encourages Member States to establish regulatory possible impacts of the increased use of AI in the sandboxes to give small and medium enterprises context of company law and corporate governance the opportunity to experiment with innovative AI matters.20 approaches and provides a specific framework to do so.19 Such sandboxes provide a structured context for This work to address regulatory gaps spans a wide experimentation, enabling the testing of innovative range of legal and regulatory regimes including data technologies, products, services or approaches in a protection; justice; policing; intellectual property; real-world environment and for a limited time, under transport and haulage; finance; health; human rights, regulatory supervision and ensuring that appropriate export controls; consumer protection; competition law safeguards are in place. (extracted from353.pdf)
31 In 2021, the ADAPT Centre, Science Gallery Dublin and other partners engaged in an innovative multidisciplinary collaboration between artists and technologists on the theme of bias, exploring AI, Ethics, Trust and Justice.32 In a Europe-wide first, a cutting-edge law and technology module has been rolled out for undergraduate students at Maynooth University. (extracted from353.pdf)
Consider an appropriate mechanism for ensuring a coordinated approach by Irish regulators to Digital, including AI, as part of the forthcoming National Digital Strategy [DOT/ DETE / DECC / D/Justice/ DTCAGSM / DCEDIY] iii. (extracted from353.pdf)
There is also a need to ensure easy technical and number of industry sectors, including video production; administrative access to AI infrastructure by SMEs criminal justice and security; patient empowerment; and and larger businesses, and to remove barriers to decision support systems in medical care. (extracted from353.pdf)
However, e-Government domain; Justice and law enforcement; this will be dependent on the availability of good quality and the Healthcare sector.49 The European Commission data. (extracted from353.pdf)
43 AI – HERE FOR GOOD A National Artificial Intelligence Strategy for Ireland HEALTH SECTOR JUSTICE SECTOR Health is an area in which substantial potential The EU e-Justice Strategy and Action Plan 2019-2023 benefits can be realised through the application of AI, identifies the use of AI as a priority area in the justice for example in imaging procedures, decision-making, field. (extracted from353.pdf)
However, Ireland’s Competitiveness However, the use of AI within the justice sector also has Challenge 2019 report identified medical sector considerable implications for ethics, human rights and digitalisation as particularly low in Ireland, compared to the rule of law. (extracted from353.pdf)
E uropean Commission for the Efficiency of Justice (CEPEJ), European Ethical Charter on the Use of Artificial Intelligence in Judicial Systems and their environment, December 2018, accessed at: https://rm.coe.int/ethical-charter-en-for-publication-4-december-2018/16808f699c 55. (extracted from353.pdf)
69 AI – HERE FOR GOOD A National Artificial Intelligence Strategy for Ireland Glossary of Acronyms AI Artificial Intelligence ALTAI Assessment List for Trustworthy Artificial Intelligence CAHAI Ad Hoc Committee on Artificial Intelligence CCPC Competition and Consumer Protection Commission CCI Cybersecurity and Cybercrime Investigation CeADAR Ireland’s Centre for Applied Artificial Intelligence CEN European Committee for Standardization CENELEC European Committee for Electrotechnical Standardization CEPEJ European Commission for the Efficiency of Justice CLRG Company Law Review Group CRT Centres for Research Training CSO Central Statistics Office DAFM Department of Agriculture, Food and the Marine DFA Department of Foreign Affairs DCEDIY Department of Children, Equality, Disability, Integration and Youth DCU Dublin City University DETE Department of Enterprise, Trade and Employment DECC Department of the Environment, Climate and Communications DESI Digital Economy and Society Index DFHERIS Department of Further and Higher Education, Research, Innovation and Science DFKI German Research Centre for Artificial Intelligence DIH Digital Innovation Hub DoE Department of Education DoJ Department of Justice DoH Department of Health DPAI Data Protection Impact Assessment DPER Department of Public Expenditure and Reform DTCAGSM Department of Tourism, Culture, Arts, Gaeltacht, Sport and Media DTIF Disruptive Technologies Innovation Fund EDIHs European Digital Innovation Hubs EGFSN Expert Group on Future Skills Needs EI Enterprise Ireland EPA Environmental Protection Agency ESO European Standards Organizations ETSI European Telecommunications Standards Institute EU European Union FET Further Education and Training GCID Grand Canal Innovation District GDP Gross Domestic Product GDPR General Data Protection Regulation 70 AI – HERE FOR GOOD A National Artificial Intelligence Strategy for Ireland GGE on LAWS UN Group of Governmental Experts on Lethal Autonomous Weapons Systems GPAI Global Partnership on AI GSI Geological Survey Ireland HE Higher Education HEA Higher Education Authority HEI Higher Education Institutes HLEG EU AI High Level Expert Group HPC High Performance Computing HSE Health Service Executive IA Impact assessments ICHEC Irish Centre for High-end Computing ICT Information and Communications IEC International Electrotechnical Commission IF SFI Industry Fellowship IMR Irish Manufacturing Research IP Intellectual Property IPCEI Important Projects of Common European Interest IRC Irish Research Council ISO International Organization for Standardization ITI InterTrade Ireland MOOC Massive Open Online Course MNE Multinational Enterprise NSAI National Standards Authority of Ireland OECD Organisation for Economic Cooperation and Development OGP Office of Government Procurement ONE-AI OECD on Network of Experts on AI RCSI Royal College of Surgeons Ireland R&I Research and innovation REC Research Ethics Committee RTEF Reference Testing and Experimentation Facilities SEAI Sustainable Energy Authority of Ireland SFI Science Foundation Ireland SME Small and Medium-Sized Enterprises STEM Science, technology, engineering, and mathematics TUD Technological University Dublin UCC University College Cork UCD University College Dublin UN United Nations UNGP UN Guiding Principles on Business and Human Rights WAI Women in AI WIPO World Intellectual Property Organisation 71 AI – HERE FOR GOOD A National Artificial Intelligence Strategy for Ireland List of Organisations Consulted The list below reflects the organisations that took part in a range of stakeholder engagements throughout the development of this Strategy. (extracted from353.pdf)
The written submissions to the strategy are available on the Department of Enterprise, Trade and Employment’s website at: https://enterprise.gov.ie/en/Consultations/Public-Consultation-Development-of-a-National-Strategy-on-Artifi- cial-Intelligence.html  30% Club  Department of Tourism, Culture, Arts, Gaeltacht,  Accenture Sport and Media  AJH Emerging Technology Intelligence  Department of Transport  Allied Irish Banks  Digital Skills Global  American Chamber of Commerce  Dublin Chamber of Commerce  Arvoia  Dublin City University, Business School  Cainthus  Dublin City University, School of Computing  CarTrawler  Dublin City University, School of Electronic  CeADAR Ireland’s Centre for Applied Artificial Engineering Intelligence  Edgetier  Central Bank  Enable Ireland  Chambers Ireland  Enterprise Ireland  Concern Worldwide  Fotonation / Xperi  CONFIRM Centre (AIT)  FourThereom  CONSUS (Crop Optimisation through Sensing,  Freedomtech Understanding & viSualisation)  FTI Consulting  CR Robotics  Genesys  Data Protection Commission  Health and Safety Authority  Department of Agriculture, Food and the Marine  Ibec  Department of Children, Equality, Disability,  IBM Integration and Youth  ICT Skillnet  Department of Defence  IDA Ireland  Department of Education  I-Form Advanced Manufacturing Research Centre  Department of Enterprise, Trade and Employment  Industry Research and Development Group  Department of Environment, Climate and  Insight Centre for Data Analytics Communication  Institute of Chartered Accountants Ireland  Department of Finance  Insurance Ireland  Department of Foreign Affairs  Intel  Department of Further and Higher Education,  InterTrade Ireland Research, Innovation and Science  Irish Centre for High-End Computing  Department of Health  Irish Computer Society/ICS Foundation  Department of Housing, Local Government and  Irish Congress of Trade Unions Heritage  Irish Human Rights and Equality Commission  Department of Justice  Irish Institute of Digital Business  Department of Public Expenditure and Reform  Irish Manufacturing Research  Department of Rural and Community  Irish Marie Skłodowska-Curie Office Development  Irish Small and Medium Enterprises Association  Department of Social Protection  Irish Universities Association  Department of the Taoiseach  Jaguar Land Rover 72 AI – HERE FOR GOOD A National Artificial Intelligence Strategy for Ireland  Kerry Group  SOLAS  Law Society of Ireland  Swrve  Learnovate  Talent Garden  Letterkenny Institute of Technology, Department  Tech Ireland of Computing  Technological University Dublin, School of  Lincoln Recruitment Computer Science  Live tiles  The ADAPT Centre  Mason Hayes Curran  The Competition and Consumer Protection  Mastercard Labs Commission  Met Éireann  Trinity College Dublin, School of Computer Science  Microsoft and Statistics  National Archives  Trinity College Dublin, School of Creative Arts  National Standards Authority of Ireland  Truata  National University of Ireland Galway, School of  Ubotica Computer Science  University College Cork, School of Computer  National University of Ireland Maynooth, Science & IT Department of Computer Science  University College Dublin, School of Computer  National University of Ireland Maynooth, School of Science Business  University College Dublin, School of Information  Nokia Bell Labs and Communication Studies  Office of the Government Chief Information  University of Limerick LERO, Science Foundation Officer Ireland Research Centre for Software  Office of the Revenue Commissioners  Valeo  Science Foundation Ireland  Version1  Science Foundation Ireland Centre for Research  Vodafone Training in Machine Learning  Waterford Institute of Technology,  ServisBot Telecommunications Software & Systems Group  Skillnet Ireland  Webio  Small Firms Association 73 AI – HERE FOR GOOD A National Artificial Intelligence Strategy for Ireland 74 3 1 (extracted from353.pdf)
Department of Commerce The following Federal departments and agencies are represented on the Subcommittee on Machine Learning and Artificial Intelligence and through it, work together to monitor the state of the art in machine learning (ML) and AI (within the Federal Government, in the private sector, and internationally), to watch for the arrival of important technology milestones in the development of AI, to coordinate the use of and foster the sharing of knowledge and best practices about ML and AI by the Federal Government, and to consult in the development of Federal research and development priorities in AI: Department of Commerce (Co-Chair) Department of Veterans Affairs Department of Defense United States Agency for International Department of Education Development Department of Energy Central Intelligence Agency Department of Health and Human Services General Services Administration Department of Homeland Security National Science Foundation Department of Justice National Security Agency Department of Labor National Aeronautics and Space Administration Department of State Office of the Director of National Intelligence Department of Transportation Social Security Administration Department of Treasury The following offices of the Executive Office of the President are also represented on the Subcommittee: Council of Economic Advisers Office of the Vice President Domestic Policy Council National Economic Council Office of Management and Budget National Security Council Office of Science and Technology Policy (Co- Chair) PREPARING FOR THE FUTURE OF ARTIFICIAL INTELLIGENCE PREPARING FOR THE FUTURE OF ARTIFICIAL INTELLIGENCE Contents Executive Summary ..................................................................................................................................................... (extracted from192.pdf)
30 Justice, Fairness, and Accountability ................................................................................................................. (extracted from192.pdf)
Public- and private- sector investments in basic and applied R&D on AI have already begun reaping major benefits to the public in fields as diverse as health care, transportation, the environment, criminal justice, and economic inclusion. (extracted from192.pdf)
Use of AI to make consequential decisions about people, often replacing decisions made by human-driven bureaucratic processes, leads to concerns about how to ensure justice, fairness, and accountability—the same concerns voiced previously in the Administration’s Big Data: Seizing Opportunities, Preserving Values report of 2014,1 as well as the Report to the President on Big Data and Privacy: A Technological Perspective published by the President’s Council of Advisors on Science and Technology in 2014.2 Transparency concerns focus not only on the data and algorithms involved, but also on the potential to have some form of explanation for any AI-based determination. (extracted from192.pdf)
Many areas of public policy, from education and the economic safety net, to defense, environmental preservation, and criminal justice, will see new opportunities and new challenges driven by the continued progress of AI. (extracted from192.pdf)
The promise of AI has been compared to the transformative impacts of advances in mobile computing.21 Public- and private-sector investments in basic and applied R&D on AI have already begun reaping major benefits for the public in fields as diverse as health care, transportation, the environment, criminal justice, and economic inclusion.22 At Walter Reed Medical Center, the Department of Veteran Affairs is using AI to better predict medical complications and improve treatment of severe combat wounds, leading to better patient outcomes, faster healing, and lower costs.23 The same general approach—predicting complications to enable preventive treatment—has also reduced hospital-acquired infections at Johns Hopkins University.24 Given the current transition to electronic health records, predictive analysis of health data may play a key role across many health domains like precision medicine and cancer research. (extracted from192.pdf)
Autonomous watercraft may be much cheaper to operate than manned ships, and may someday be used for enhanced weather prediction, climate monitoring, or policing illegal fishing.27 AI also has the potential to improve aspects of the criminal justice system, including crime reporting, policing, bail, sentencing, and parole decisions. (extracted from192.pdf)
The Administration is exploring how AI can responsibly benefit current initiatives such as Data Driven Justice and the Police Data Initiative that seek to provide law enforcement and the public with data that can better inform decision-making in the criminal justice system, while also taking care to minimize the possibility that AI might introduce bias or inaccuracies due to deficiencies in the available data. (extracted from192.pdf)
Social justice and public policy institutions that do not typically engage with advanced technologies and data science in their work should consider partnerships with AI researchers and practitioners that can help apply AI tactics to the broad social problems these institutions already address in other ways. (extracted from192.pdf)
The use of AI to make consequential decisions about people, often replacing decisions made by human actors and institutions, leads to concerns about how to ensure justice, fairness, and accountability—the same concerns voiced previously in the “Big Data” context.62 The use of AI to control physical-world equipment leads to concerns about safety, especially as systems are exposed to the full complexity of the human environment. (extracted from192.pdf)
Justice, Fairness, and Accountability A common theme in the Law and Governance, AI for Social Good, and Social and Economic Impacts workshops was the need to ensure that AI promotes justice and fairness, and that AI-based processes are accountable to stakeholders. (extracted from192.pdf)
In the criminal justice system, some of the biggest concerns with Big Data are the lack of data and the lack of quality data.65 AI needs good data. (extracted from192.pdf)
It is important that anyone using AI in the criminal justice context is aware of the limitations of current data. (extracted from192.pdf)
65 Matt Ford, “The Missing Statistics of Criminal Justice,” The Atlantic, May 31, 2015, http://www.theatlantic.com/politics/archive/2015/05/what-we-dont-know-about-mass-incarceration/394520/ 66 Julia Angwin, Jeff Larson, Surya Mattu, and Lauren Kirchner, “Machine Bias,” ProPublica, May 23, 2016, https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing. (extracted from192.pdf)
Many areas of public policy, from education and the economic safety net, to defense, environmental preservation, and criminal justice, will see new opportunities and new challenges driven by the continued progress of AI. (extracted from192.pdf)
Social justice and public policy institutions that do not typically engage with advanced technologies and data science in their work should consider partnerships with AI researchers and practitioners that can help apply AI tactics to the broad social problems these institutions already address in other ways. (extracted from192.pdf)
Matt Ford, “The Missing Statistics of Criminal Justice,” The Atlantic, May 31, 2015, http://www.theatlantic.com/politics/archive/2015/05/what-we-dont-know-about-mass-incarceration/394520/ Jason Furman, “Is This Time Different? (extracted from192.pdf)
Such tasks concern low-skilled as well as highly-skilled personnel, for example in sectors such as banking, insurance or justice. (extracted from186.pdf)
For the deployment of ADS, certification can be on either a voluntary basis (as encouraged by the GDPR), or mandatory in certain areas such as justice and healthcare. (extracted from186.pdf)
Decision-making algorithms are increasingly used in areas such as access to information, e-commerce, recommendation systems, employment, health, justice, policing, banking and insurance. (extracted from186.pdf)
Users Individuals Private sector Public sector Objectives Improvement of N/A Drugs discovery Climate general Weather forecast Knowledge Environment Healthcare Quantified-self Risk scoring Predictive justice Finance Payment systems Predictive policing Digital services Note taking Targeting Hazard prediction Smart home Personalised services Infrastructure Recommendations development planning Autonomous Cars Autonomous robots Autonomous Home Robots weapons Physical systems Security Defence Personal assistants in Transport the home Smart cities Smart grids 6 Understanding algorithmic decision-making: Opportunities and challenges 3. (extracted from186.pdf)
For example, Directive 2000/78/EC9 lays down: 'a general framework for combating discrimination on the grounds of religion or belief, disability, age or sexual orientation as regards employment and occupation, with a view to putting into effect in the Member States the principle of equal treatment.' In a similar vein, the Convention for the Protection of Human Rights and Fundamental Freedoms10 provides that: 'the enjoyment of the rights and freedoms set forth in this Convention shall be secured without discrimination on any ground such as sex, race, colour, language, religion, political or other opinion, national or social origin, association with a national minority, property, birth or other status.' The fact that ADS can lead to discrimination has been documented in many areas, such as the justice system, targeted advertisements and employment. (extracted from186.pdf)
Discrimination in justice: Another area that has raised much concern is the increasing reliance on ADS in the criminal justice system. (extracted from186.pdf)
COMPAS scores can be used at different stages of the criminal justice system, e.g. (extracted from186.pdf)
Several occurrences of this process have already been observed, not only in the field of justice with COMPAS, but also in education with the public debate raised by an algorithm called APB32 in France. (extracted from186.pdf)
47 https://gobo.social 14 Understanding algorithmic decision-making: Opportunities and challenges authors, 'perhaps even more problematic is the theory of justice implicitly embedded in the algorithms'.48 The point is that most ADS used in this context are risk-assessment tools: based on a number of factors about the defendants' criminal history, sociological data or demographic features, they provide an estimation of their risk of recidivism. (extracted from186.pdf)
ADS are already in use in the medical sector and can potentially contribute to improve the decisions taken by practitioners and specialists in many ways: 48 Angèle Christin, Alex Rosenblat, Danah Boyd; Courts and predictive algorithms; Data & Civil Rights: A new era of policing and justice; 2015. (extracted from186.pdf)
Public services ADS are currently being used by government and public agencies to provide new services or improve existing ones in many areas, such as energy, education, healthcare, transportation, justice systems and security. (extracted from186.pdf)
These tasks concern low-skilled as well as highly- skilled personnel, for example in sectors like banking, insurance or justice. (extracted from186.pdf)
ADS Fairness As ADS replace or support human decision-makers in a number of sensitive domains such as justice, health or education, it is important to ensure that they do not result in decisions that are considered unfair or discriminatory. (extracted from186.pdf)
For example, it is well known that, in certain cities, there is a strong correlation between the religion or 141 Richard Berk, Hoda Heidari, Shahin Jabbari, Michael Kearns, Aaron Roth; Fairness in Criminal Justice Risk Assessments: The State of the Art; Sociological Methods & Research; 2018. (extracted from186.pdf)
186 Richard Berk, Hoda Heidari, Shahin Jabbari, Michael Kearns, Aaron Roth; Fairness in Criminal Justice Risk Assessments: The State of the Art; Sociological Methods & Research; 2018. (extracted from186.pdf)
187 Richard Berk, Hoda Heidari, Shahin Jabbari, Michael Kearns, Aaron Roth; Fairness in Criminal Justice Risk Assessments: The State of the Art; Sociological Methods & Research; 2018. (extracted from186.pdf)
Ethical and political debate As illustrated in Chapter 3, ADS raise far reaching issues in many areas such as justice, policing, healthcare, democratic life, etc. (extracted from186.pdf)
Another example of the systematic analysis of ethical issues that can be useful in this context is the EDPS Ethics Advisory Group Report,204 which proposes a list of 'foundational values to digital ethics': dignity, freedom, autonomy, solidarity, equality, democracy, justice and trust. (extracted from186.pdf)
As seen previously, different approaches can be taken to 212 Rebecca Wexler; Life, liberty, and trade secrets : intellectual property in the criminal justice system; (70); Standford Law Review; (1343); 2018; https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2920883. (extracted from186.pdf)
[…] The trend towards using risk instruments in all sectors of the criminal justice system, therefore, merits further theoretical deliberation and empirical study.'221 • In the same vein, Chelsea Barabas and her colleagues argue: 'for a shift away from predictive technologies, towards diagnostic methods that will help us to understand the criminogenic effects of the criminal justice system itself, as well as evaluate the effectiveness of interventions designed to interrupt cycles of crime. (extracted from186.pdf)
221 Kelly Hannah-Moffat; Actuarial sentencing: an 'unsettled' proposition; Justice Quarterly; (30,2); 2013. (extracted from186.pdf)
Dillon Reisman and his colleagues have already advocated AIA as a 'practical framework for public agency accountability' in a recent AINow Institute report.226 Beyond a 'self-assessment of existing and proposed automated decision systems, evaluating potential impacts on fairness, justice, bias, or other concerns across affected communities', they emphasise the need for 'researcher review processes before the system has been acquired'. (extracted from186.pdf)
For example, several studies have been conducted about the use of ADS in the area of justice, some of them focusing on the risks of discrimination,232 others on the benefits in improving judges' decisions.233 In addition, the benefit risk balance applies to both the primary functionalities of the ADS and to its transparency and explainability features. (extracted from186.pdf)
ADS certification can be either on a voluntary basis (as encouraged by the GDPR) or mandatory in certain areas such as justice and healthcare. (extracted from186.pdf)
Berk R., Heidari H., Jabbari S., Kearns M., Roth A.; Fairness in Criminal Justice Risk Assessments: The State of the Art; Sociological Methods & Research; 2018. (extracted from186.pdf)
Christin A., Rosenblat A., Boyd D.; Courts and predictive algorithms; Workshop on Data & Civil Rights: A new era of policing and justice; 2015. (extracted from186.pdf)
Hannah-Moffat K.; Actuarial sentencing: an 'unsettled' proposition; Justice Quarterly (30); 2013. (extracted from186.pdf)
The majority of services underpinned by AI, from education, to strategies recognised that developing high quality, in- healthcare, to justice, as well as AI having a key role country AI expertise is vital for a country to remain at in policy-making itself.15 Other countries are creating the forefront of the AI revolution. (extracted from44.pdf)
Digitalization, Anticorruption, Rule of Ariana British Embassy in Mexico City International Cooperation Law and Competition Policy Carla Vazquez RUCAM Ministry of Economy Government Carlos Gershenson Researcher Metro CDMX/ UNAM Academia CDMX Carlos López Franco Researcher UDG/CUCEI Jal Academia Head of Human Rights, Security and Justice at the Chris Wall British Embassy in Mexico City International Cooperation British Embassy in Mexico City Coordinator of Development of Industrial Ministry of Innovation, Science and Claudia Araujo Gálvez Jal Govt Technological Platforms Technology Claudia Pando Programme Manager, Future Cities British Embassy in Mexico City International Cooperation Cristina Cardenas General Coordinator @prende.mx SEP Government Ministry of Innovation, Science and David Bates Social Innovation Programmes Coordinator Jal Govt Technology Edgar Nelson Sánchez Researcher CINVESTAV Jal Academia Camperos Eduardo Barbosa Emerging Technologies Director Ciudad Creativa Digital Jal Govt Eduardo Clark Public Innovation Deputy Director CEDN Government Eduardo Farina CEO Bluemessaging Start-up Bluemessaging Eduardo Morales AI Phd INAOE Puebla Academia Elsa Ayala General Director of Normatividad Mercantil Ministry of Economy Government Enrique Jaime Herrera Researcher CIATEJ Jal Academia López Enrique Sucar Senior Research Scientist INAOEP Academia Enrique Zapata General Director for Open Data CEDN Government Technical Secretary of the Urban Development Fernando Cota Senate Government Commission Member of the Science and Francisco Búrquez Senator Legislative branch Technology Commission Gabriella Gomez Mont Director Laboratorio de la Ciudad CDMX Govt Director of Technological Platforms Development Ministry of Innovation, Science and Gerardo Rodríguez Barba Jal Govt and Promotion Technology Gustavo Carreon Researcher Metro CDMX/ UNAM Academia CDMX Gustavo Pares Nearshore CEO Nearshore Start-up Isaac Avila Coordinator CANIETI Occidente GDL Industry Ivan Millan General Director Jalisco Talent Land GDL Javier Mata Yalo CEO Yalo Start-up Jessica Paola Avila Open Data Director SEPAF Gobierno Jalisco Jal Govt 39 TOWARDS AN AI STRATEGY IN MEXICO: Harnessing the AI Revolution Name Job Title Organisation Sector Jesus Cepeda CEO One Smart City MTY Civil Society/Consultancy José Cantoral Researcher CIATEQ Jal Academia Foro Consultivo Científico y Jose Franco General Coordinator Academia / Civil Society Tecnológico Juan Pablo Escobar Director Civica Digital MTY Civil Society/Consultancy Katie Allan Associate Oxford Insights Consultancy Laura Caccia Consultant Oxford Insights Consultancy Director - Center for Business Development in IT Lorenzo Valle TEC Academia and Big Data Initiative Lorenzo Valle Garcilazo Big Data Center Coordinator ITESM Academia ITESM Luis Cadena General Administrator of Communications and ICT SAT, SHCP Government Luis Valtierra President IJALTI Cluster Manuel Avalos IBM Storage Cloud and Solutions for IBM WW IBM/ Watson Jal Industry General Director of Social Innovation and Ministry of Innovation, Science and Margarita Solis Jal Govt Entrepreneurship Technology María de Lourdes Martínez Vocal SMIA Academia Villaseñor, SMIA Future Cities Programme and Policy Officer– Marian Urizar British Embassy in Mexico City International Cooperation Programme Team Mario Angel Siller Researcher CINVESTAV Jal Academia Gonzalez Martha González Pérez- Director Cognitive Solutions, IBM Mexico Industry Sandi Matt Pasiensky VP of International Operations Wizeline GDL Industry Miguel Gonzalez President & Researcher SMIA, ITESM Academia Miguel González Mendoza President Mexico's AI Society Academia / Civil Society Miguel Salazar Director Ejecutivo Codeando Civil Society GDL Miriam Díaz Rodríguez Researcher/ Professor Tecnológico Mario Molina Jal Academia General Director of Science and Technological Ministry of Innovation, Science and Morris Schwarzblat y Katz Jal Govt Development Technology Nancy Guadalupe Arana Director, Systems Control & AI Center UDG/CUCEI Jal Academia Daniel Neil Hernández Gress Researcher ITESM Academia ITESM Olivia Barron AI PhD UDEM MTY Academia Oliver Rice Associate Oxford Insights Consultancy Omar Gonzalez KYSE Agritech UNAM CDMX Startup Raymundo Vazquez SW Engineering Manager INTEL Jal Industry Ricardo Reyes Data Wuki & CEO Data Wuki and Quantum Labs Start-up Quantum Labs Head of Anti-corruption, Competition, Digitalisation Rodrigo Félix British Embassy in Mexico City International Cooperation and Rule Of Law Policy Saiph Savage Coordinator BanFakeNews Bot CDMX Sebastian Sposito Public Policy and Government Affairs Advisor Google Industry Sissi de la Peña Regional Markets CEDN Government Sophie Marment Head of Prosperity Fund FCO Government Tania Cruz Digital Government Services CEDN Government Victor Gutierrez Industry CCE Industry Yamin Ruiz Global Proteus CEO Global Proteus Start-up Yolanda Martinez Coordinator National Digital Strategy Government 40 TOWARDS AN AI STRATEGY IN MEXICO: Harnessing the AI Revolution Appendix 2: Innovation in Mexico’s Regions and major economic sectors. (extracted from44.pdf)
Initiate and support measures that make systemic addressing diversity, equality, and social justice). (extracted from637.pdf)
systematically take into account and build on the orientation towards rights, equality, and social justice for all capabilities, desires, aspirations and needs of all families children and families. (extracted from637.pdf)
UNESCO, Forward World Bank) planning Implementation Own historiy, culture, values individual and group identity Better understanding Local knowledge(s) Interpretation and practices Resistance Evaluation Creativity Creative Invention Research and critical inquiry ‘Grassroots’ Competent Systems: for social justice, diversity and equality The Future of Work and Education for the Digital Age 1. (extracted from637.pdf)
He works and publishes widely on questions of diversity and equality, social justice, evaluation and professionalism in working with young children, families and communities in Rita Flórez-Romero diverse socio-cultural contexts. (extracted from637.pdf)
———.“TheEUCodeofConductonCounteringIllegalHateSpeechOnline:TheRobustResponseProvided bytheEuropeanUnion.”AccessedJuly11,2022.https://ec.europa.eu/info/policies/justice-and-funda- THEBRUSSELSEFFECTANDARTIFICIALINTELLIGENCE•86 mental-rights/combatting-discrimination/racism-and-xenophobia/eu-code-conduct-countering-illegal- hate-speech-online_en. (extracted from87.pdf)
At EU level, the processing of biometric data has been actively encouraged and directly supported over the past years in the context of EU-level large-scale information technology (IT) systems in the area of freedom, security and justice (AFSJ). (extracted from179.pdf)
A review of this architecture and of the most relevant rules on biometrics and on automated decision-making in EU data protection law, as well as of the most important case law in this area emanating from the Court of Justice of the EU (CJEU) and the European Court of Human Rights (ECtHR), shows that ongoing technological developments are taking place amid – and possibly also somehow despite – existing rights and principles, which might thus possibly need to be reinforced, clarified, or at least fine-tuned. (extracted from179.pdf)
The processing of biometric data has been actively supported at EU level23 in the context of EU-level large-scale IT systems in the Area of Freedom, Security and Justice (AFSJ). (extracted from179.pdf)
It also presents the most important case law in this area emanating from the Court of Justice of the EU (CJEU) and the European Court of Human Rights (ECtHR). (extracted from179.pdf)
Legal aid shall be made available to those who lack sufficient resources in so far as such aid is necessary to ensure effective access to justice.' 51 In this sense, see for instance EDPB and EDPS, Joint Opinion 5/2021 12. (extracted from179.pdf)
Fundamental rights case law Analysing the case law of the Court of Justice of the EU (CJEU) on biometrics, a series of points stand out. (extracted from179.pdf)
As underlined in a report for the Committee on Equality and Non-Discrimination of the Council of Europe's Parliamentary Assembly, certain flaws in a the criminal justice system can have 'far-reaching human rights consequences' (Lacroix 2020 12). (extracted from179.pdf)
Other areas mentioned as involving the qualification of an AI system as high risk are management and operation of critical infrastructure, educational and vocational training; employment, workers management and access to self-employment; access to and enjoyment of essential private services and public services and benefits; law enforcement; migration, asylum and border control management; administration of justice and democratic processes. (extracted from179.pdf)
This could relate to 'emotion recognition.' '(e) AI systems intended to be used by law enforcement authorities for predicting the occurrence or reoccurrence of an actual or potential criminal offence based on profiling of 42 Person identification, human rights and ethical principles natural persons as referred to in Article 3(4) of Directive (EU) 2016/680 or assessing personality traits and characteristics or past criminal behaviour of natural persons or groups.' '(f) AI systems intended to be used by law enforcement authorities for profiling of natural persons as referred to in Article 3(4) of Directive (EU) 2016/680 in the course of detection, investigation or prosecution of criminal offences.' '(g) AI systems intended to be used for crime analytics regarding natural persons, allowing law enforcement authorities to search complex related and unrelated large data sets available in different data sources or in different data formats in order to identify unknown patterns or discover hidden relationships in the data.' Under heading 7, on 'Migration, asylum and border control management', stand out: '(a) AI systems intended to be used by competent public authorities as polygraphs and similar tools or to detect the emotional state of a natural person;' 'd) AI systems intended to assist competent public authorities for the examination of applications for asylum, visa and residence permits and associated complaints with regard to the eligibility of the natural persons applying for a status.' Under heading 8, on 'Administration of justice and democratic processes', are mentioned: '(a) AI systems intended to assist a judicial authority in researching and interpreting facts and the law and in applying the law to a concrete set of facts.' The proposal foresees that Annex III might be amended by the European Commission via delegated acts (proposed Article 7(1) AIA). (extracted from179.pdf)
European Union Agency for the Operational Management of Large-Scale IT Systems in the Area of Freedom, Security and Justice (eu-LISA). (extracted from179.pdf)
Accept additional cookies Reject additional cookies View cookies Hide cookie message Skip to main content GOV.UK Navigation menu Menu Menu Services and information Benefits Births, death, marriages and care Business and self-employed Childcare and parenting Citizenship and living in the UK Crime, justice and the law Disabled people Driving and transport Education and learning Employing people Environment and countryside Housing and local services Money and tax Passports, travel and living abroad Visas and immigration Working, jobs and pensions Government activity Departments Departments, agencies and public bodies News News stories, speeches, letters and notices Guidance and regulation Detailed guidance, regulations and rules Research and statistics Reports, analysis and official statistics Policy papers and consultations Consultations and strategy Transparency Data, Freedom of Information releases and corporate reports Search GOV.UK × Search GOV.UK Search Search GOV.UK Search Home Health and social care National Health Service Digital and data-driven health and care technology Department of Health & Social Care Guidance A guide to good practice for digital and data-driven health technologies Updated 19 January 2021 Contents Introduction 1. (extracted from380.html)
Cancel Services and information Benefits Births, death, marriages and care Business and self-employed Childcare and parenting Citizenship and living in the UK Crime, justice and the law Disabled people Driving and transport Education and learning Employing people Environment and countryside Housing and local services Money and tax Passports, travel and living abroad Visas and immigration Working, jobs and pensions Government activity Departments News Guidance and regulation Research and statistics Policy papers and consultations Transparency How government works Get involved Support links Help Privacy Cookies Accessibility statement Contact Terms and conditions Rhestr o Wasanaethau Cymraeg Government Digital Service All content is available under the Open Government Licence v3.0 , except where otherwise stated © Crown copyright (extracted from380.html)
General ethical and legal principles)>> endobj 10683 0 obj <</Dest[1937 0 R/XYZ 96 669 null]/Next 10684 0 R/Parent 10682 0 R/SE 7595 0 R/Title(3.1 Human dignity)>> endobj 10684 0 obj <</Dest[1937 0 R/XYZ 322 422 null]/Next 10685 0 R/Parent 10682 0 R/Prev 10683 0 R/SE 7599 0 R/Title(3.2 Self-determination)>> endobj 10685 0 obj <</Dest[1944 0 R/XYZ 96 669 null]/Next 10686 0 R/Parent 10682 0 R/Prev 10684 0 R/SE 7608 0 R/Title(3.3 Privacy)>> endobj 10686 0 obj <</Dest[1944 0 R/XYZ 322 669 null]/Next 10687 0 R/Parent 10682 0 R/Prev 10685 0 R/SE 7612 0 R/Title(3.4 Security)>> endobj 10687 0 obj <</Dest[1949 0 R/XYZ 62 669 null]/Next 10688 0 R/Parent 10682 0 R/Prev 10686 0 R/SE 7618 0 R/Title(3.5 Democracy)>> endobj 10688 0 obj <</Dest[1949 0 R/XYZ 288 487 null]/Next 10689 0 R/Parent 10682 0 R/Prev 10687 0 R/SE 7622 0 R/Title(3.6 Justice and solidarity)>> endobj 10689 0 obj <</Dest[1952 0 R/XYZ 322 669 null]/Parent 10682 0 R/Prev 10688 0 R/SE 7628 0 R/Title(3.7 Sustainability)>> endobj 10690 0 obj <</Count -15/Dest[1959 0 R/XYZ 74 730 null]/First 10691 0 R/Last 10692 0 R/Next 10714 0 R/Parent 6342 0 R/Prev 10679 0 R/SE 7638 0 R/Title(Part C Technical foundations)>> endobj 10691 0 obj <</Dest[4199 0 R/XYZ 96 770 null]/Next 10692 0 R/Parent 10690 0 R/SE 7641 0 R/Title(1. (extracted from103.html)
Use of algorithmic systems by state bodies)>> endobj 10935 0 obj <</Dest[2546 0 R/XYZ 62 669 null]/Next 10936 0 R/Parent 10934 0 R/SE 9730 0 R/Title(7.1 Opportunities and risks involved in the use of algorithmic systems by state bodies)>> endobj 10936 0 obj <</Dest[2546 0 R/XYZ 288 344 null]/Next 10937 0 R/Parent 10934 0 R/Prev 10935 0 R/SE 9734 0 R/Title( 7.2 Algorithmic systems in law-making)>> endobj 10937 0 obj <</Dest[2563 0 R/XYZ 96 500 null]/Next 10938 0 R/Parent 10934 0 R/Prev 10936 0 R/SE 9737 0 R/Title(7.3 Algorithmic systems in the dispensation of justice)>> endobj 10938 0 obj <</Dest[2566 0 R/XYZ 62 669 null]/Next 10939 0 R/Parent 10934 0 R/Prev 10937 0 R/SE 9743 0 R/Title(7.4 Algorithmic systems in public administration)>> endobj 10939 0 obj <</Dest[2566 0 R/XYZ 288 318 null]/Next 10940 0 R/Parent 10934 0 R/Prev 10938 0 R/SE 9749 0 R/Title(7.5 Algorithmic systems in public security law)>> endobj 10940 0 obj <</Dest[2569 0 R/XYZ 322 669 null]/Next 10941 0 R/Parent 10934 0 R/Prev 10939 0 R/SE 9753 0 R/Title(7.6 Transparency requirements for the use of algorithmic systems by state actors)>> endobj 10941 0 obj <</Dest[2645 0 R/XYZ 96 669 null]/Next 10942 0 R/Parent 10934 0 R/Prev 10940 0 R/SE 9767 0 R/Title(7.7 The risk involved in automated total enforcement)>> endobj 10942 0 obj <</Dest[2648 0 R/XYZ 62 666 null]/Parent 10934 0 R/Prev 10941 0 R/SE 9770 0 R/Title(Summary of the most important recommendations for action)>> endobj 10943 0 obj <</Count 8/Dest[2662 0 R/XYZ 96 770 null]/First 10944 0 R/Last 10955 0 R/Parent 10865 0 R/Prev 10934 0 R/SE 9780 0 R/Title(8. (extracted from103.html)
Texts adopted - Artificial intelligence in criminal law and its use by the police and judicial authorities in criminal matters - Wednesday, 6 October 2021 Choisissez la langue de votre document : bg - български es - español cs - čeština da - dansk de - Deutsch et - eesti keel el - ελληνικά en - English fr - français ga - Gaeilge hr - hrvatski it - italiano lv - latviešu valoda lt - lietuvių kalba hu - magyar mt - Malti nl - Nederlands pl - polski pt - português ro - română sk - slovenčina sl - slovenščina fi - suomi sv - svenska Index Previous Next Full text Procedure : 2020/2016(INI) Document stages in plenary Document selected : A9-0232/2021 Texts tabled : A9-0232/2021 Debates : PV 04/10/2021 - 13 CRE 04/10/2021 - 13 Votes : PV 05/10/2021 - 9 PV 06/10/2021 - 2 Texts adopted : P9_TA(2021)0405 Texts adopted 168k 65k Wednesday, 6 October 2021 - Strasbourg Artificial intelligence in criminal law and its use by the police and judicial authorities in criminal matters P9_TA(2021)0405 A9-0232/2021 European Parliament resolution of 6 October 2021 on artificial intelligence in criminal law and its use by the police and judicial authorities in criminal matters ( 2020/2016(INI) ) The European Parliament, – having regard to the Treaty on European Union, in particular Articles 2 and 6 thereof, and to the Treaty on the Functioning of the European Union, in particular Article 16 thereof, – having regard to the Charter of Fundamental Rights of the European Union (the “Charter”), in particular Articles 6, 7, 8, 11, 12, 13, 20, 21, 24 and 47 thereof, – having regard to the Convention for the Protection of Human Rights and Fundamental Freedoms, – having regard to the Council of Europe Convention for the Protection of Individuals with regard to Automatic Processing of Personal Data (ETS 108), and its amending protocol (Convention 108+), – having regard to the European Ethical Charter on the use of artificial intelligence in judicial systems and their environment of the European Commission for the Efficiency of Justice (CEPEJ) of the Council of Europe, – having regard to the Commission communication of 8 April 2019 entitled ‘Building Trust in Human-Centric Artificial Intelligence’ ( COM(2019)0168 ), – having regard to the Ethics Guidelines for Trustworthy AI published by the Commission’s High-Level Expert Group on Artificial Intelligence on 8 April 2019, – having regard to the Commission white paper of 19 February 2020 entitled ‘Artificial Intelligence – A European approach to excellence and trust’ ( COM(2020)0065 ), – having regard to the Commission communication of 19 February 2020 entitled ‘A European strategy for data’ ( COM(2020)0066 ), – having regard to Regulation (EU) 2016/679 of the European Parliament and of the Council of 27 April 2016 on the protection of natural persons with regard to the processing of personal data and on the free movement of such data, and repealing Directive 95/46/EC (General Data Protection Regulation) (1) , – having regard to Directive (EU) 2016/680 of the European Parliament and of the Council of 27 April 2016 on the protection of natural persons with regard to the processing of personal data by competent authorities for the purposes of the prevention, investigation, detection or prosecution of criminal offences or the execution of criminal penalties, and on the free movement of such data, and repealing Council Framework Decision 2008/977/JHA (2) , – having regard to Regulation (EU) 2018/1725 of the European Parliament and of the Council of 23 October 2018 on the protection of natural persons with regard to the processing of personal data by the Union institutions, bodies, offices and agencies and on the free movement of such data, and repealing Regulation (EC) No 45/2001 and Decision No 1247/2002/EC (3) , – having regard to Directive 2002/58/EC of the European Parliament and of the Council of 12 July 2002 concerning the processing of personal data and the protection of privacy in the electronic communications sector (Directive on privacy and electronic communications) (4) , – having regard to Regulation (EU) 2016/794 of the European Parliament and of the Council of 11 May 2016 on the European Union Agency for Law Enforcement Cooperation (Europol) and replacing and repealing Council Decisions 2009/371/JHA, 2009/934/JHA, 2009/935/JHA, 2009/936/JHA and 2009/968/JHA (5) , – having regard to its resolution of 19 June 2020 on the anti-racism protests following the death of George Floyd (6) , – having regard to its resolution of 14 March 2017 on fundamental rights implications of big data: privacy, data protection, non-discrimination, security and law-enforcement (7) , – having regard to the hearing in the Committee on Civil Liberties, Justice and Home Affairs (LIBE) on 20 February 2020 on artificial intelligence in criminal law and its use by the police and judicial authorities in criminal matters, – having regard to the report of the LIBE mission to the United States in February 2020, – having regard to Rule 54 of its Rules of Procedure, – having regard to the opinions of the Committee on the Internal Market and Consumer Protection and the Committee on Legal Affairs, – having regard to the report of the Committee on Civil Liberties, Justice and Home Affairs ( A9-0232/2021 ), A. (extracted from292.html)
whereas the Union together with the Member States bears a critical responsibility for ensuring that decisions surrounding the life cycle and use of AI applications in the field of the judiciary and law enforcement are made in a transparent manner, fully safeguard fundamental rights, and in particular do not perpetuate discrimination, biases or prejudices where they exist; whereas the relevant policy choices should respect the principles of necessity and proportionality in order to guarantee constitutionality and a fair and humane justice system; H. (extracted from292.html)
whereas the relationship between protecting fundamental rights and effective policing must always be an essential element in the discussions on whether and how AI should be used by the law enforcement sector, where decisions may have long-lasting consequences on the life and freedom of individuals; whereas this is particularly important as AI has the potential to be a permanent part of our criminal justice ecosystem providing investigative analysis and assistance; M. (extracted from292.html)
gunshot detection algorithms), autonomous research and analysis of identified databases, forecasting (predictive policing and crime hotspot analytics), behaviour detection tools, advanced virtual autopsy tools to help determine cause of death, autonomous tools to identify financial fraud and terrorist financing, social media monitoring (scraping and data harvesting for mining connections), and automated surveillance systems incorporating different detection capabilities (such as heartbeat detection and thermal cameras); whereas the aforementioned applications, alongside other potential or future applications of AI technology in law enforcement, can have vastly varying degrees of reliability and accuracy and impact on the protection of fundamental rights and on the dynamics of criminal justice systems; whereas many of these tools are used in non-EU countries but would be illegal under the Union data protection aquis and case law; whereas the routine deployment of algorithms, even with a small false positive rate, can result in false alerts outnumbering correct alerts by far; N. (extracted from292.html)
whereas, the deployment of AI in the field of law enforcement and the judiciary should not be seen as a mere technical feasibility, but rather a political decision concerning the design and the objectives of law enforcement and of criminal justice systems; whereas modern criminal law is based on the idea that authorities react to an offence after it has been committed, without assuming that all people are dangerous and need to be constantly monitored in order to prevent potential wrongdoing; whereas AI-based surveillance techniques deeply challenge this approach and render it urgent that legislators worldwide thoroughly assess the consequences of allowing the deployment of technologies that diminish the role of human beings in law enforcement and adjudication; 1. (extracted from292.html)
Considers it essential, both for the effectiveness of the exercise of defence rights and for the transparency of national criminal justice systems, that a specific, clear and precise legal framework regulates the conditions, modalities and consequences of the use of AI tools in the field of law enforcement and the judiciary, as well as the rights of targeted persons, and effective and easily available complaint and redress procedures, including judicial redress; underlines the right of the parties to a criminal proceeding to have access to the data collection process and the related assessments made by or obtained through the use of AI applications; underlines the need for executing authorities involved in judicial cooperation, when deciding on a request for extradition (or surrender) to another Member State or non-EU country, to assess whether the use of AI tools in the requesting country might manifestly compromise the fundamental right to a fair trial; calls on the Commission to issue guidelines on how to conduct such an assessment in the context of judicial cooperation in criminal matters; insists that Member States, in accordance with applicable laws, should ensure that individuals are informed when they are subject to the use of AI applications by law enforcement authorities or the judiciary; 15. (extracted from292.html)
Points out that if humans only rely on the data, profiles and recommendations generated by machines, they will not be able to conduct an independent assessment; highlights the potentially grave adverse consequences, specifically in the area of law enforcement and justice, when individuals overly trust in the seemingly objective and scientific nature of AI tools and fail to consider the possibility of their results being incorrect, incomplete, irrelevant or discriminatory; emphasises that over-reliance on the results provided by AI systems should be avoided, and stresses the need for authorities to build confidence and knowledge to question or override an algorithmic recommendation; considers it important to have realistic expectations on such technological solutions and not to promise perfect law enforcement solutions and detection of all offences committed; 16. (extracted from292.html)
Stresses that only robust European AI governance with independent evaluation can enable the necessary operationalisation of fundamental rights principles; calls for periodic mandatory auditing of all AI systems used by law enforcement and the judiciary where there is the potential to significantly affect the lives of individuals, by an independent authority, to test and evaluate algorithmic systems, their context, purpose, accuracy, performance and scale, and, once they are in operation, in order to detect, investigate, diagnose and rectify any unwanted and adverse effects and to ensure the AI systems are performing as intended; calls therefore for a clear institutional framework for this purpose, including proper regulatory and supervisory oversight, to ensure full implementation and to guarantee a fully informed democratic debate on the necessity and proportionality of AI in the field of criminal justice; underlines that the results of these audits should be made available in public registers so that citizens know the AI systems being deployed and which measures are taken to remedy any violation of fundamental rights; 22. (extracted from292.html)
Highlights further that adequate accountability, responsibility, and liability require significant specialised training with regard to the ethical provisions, potential dangers, limitations, and proper use of AI technology, especially for police and judiciary personnel; emphasises that suitable professional training and qualifications should ensure that decision-makers are trained about the potential for bias, as the data sets may be based on discriminatory and prejudiced data; supports the establishment of awareness-raising and educational initiatives to ensure that individuals working in law enforcement and the judiciary are aware of and understand the limitations, capabilities and risks that the use of AI systems entails, including the risk of automation bias; recalls that the inclusion in AI training data sets of instances of racism by police forces in fulfilling their duties will inevitably lead to racist bias in AI-generated findings, scores, and recommendations; reiterates its call on Member States, therefore, to promote anti-discrimination policies and to develop national action plans against racism in the field of policing and the justice system; 24. (extracted from292.html)
Conclusions __________________________________________________________________ 70 AI and digital tools in workplace management and evaluation IX List of a bbreviations AGI Artificial general intelligence AI Artificial intelligence AIaaS AI as a service AI act Regulation of the European Parliament and of the Council laying down harmonised rules on artificial intelligence (Artificial Intelligence act) And amending certain union legislative acts AIDA Special Committee on Artificial Intelligence in a Digital Age BCI Brain -computer interface CDEI Centre for Data Ethics and Innovation Charter EU Charter of Fundamental Rights CIPD Chartered Institute of Personnel and Development CJEU Court of Justice of the European Union CNIL Commission Nationale de l 'Informatique et des Libertés DPA Data protection authority DPIA Data Protection Impact Assessment DPO Data protection officer ECHR European Convention on Human Rights ECtHR European Court of Human Rights EDPB European Data Protection Body EDPS European Data Protection Supervisor EESC European Economic and Social Committee ETUC European Trade Union Confederation EU-OSHA European Occupational Safety and Health at Work Authority FLI Future of Life Institute GDPR General Data Protection Regulation HBS Harvard Business School HRM Human resource management ICO Information Commissioner 's Office ILO International Labour Organization IP Internet protocol STOA | Panel for the Future of Science and Technology X ML Machine learning NFC Near -field communication OECD Organisation for Economic Co -operation and Development OSH Occupational safety and health PwC PricewaterhouseCoopers SMS Social media screening TUC Trade s Union Congress WEF World Economic Forum AI and digital tools in workplace management and evaluation 1 1. (extracted from262.txt)
To that extent, as the Court of Justice of the European Union (CJEU) ruled, ' an employer that does not allow a w orker to exercise his right to paid annual leave must bear the consequences. (extracted from262.txt)
72 European Court of Justice 29 November 2017, Case No. (extracted from262.txt)
73 European Court of Justice 21 February 2 018, Case No. (extracted from262.txt)
Rudy Matzak; European Court of Justice 9 March 2021, Case No. (extracted from262.txt)
Radiotelevizija Slovenija; European Court of Justice 9 March 2021, Case No. (extracted from262.txt)
' European Court of Justice 17 October 1989, Case No. (extracted from262.txt)
86 'AI is pervasive and will be used in diverse fields – such as consultancy, consumer products and services, mobility, online connectivity, energy production and distribution, police and justice administration – , where EU and MS liability rules are already sector -specific. (extracted from262.txt)
Van der Mei, Anne Pieter, 'Fixed -Term work: Recent developments in the case law of the Court of Justice of the European Union' , European Labour Law Journal , 2020. (extracted from262.txt)
Vecchione, Briana, Barocas, Solon and Karen Levy, ' Algorithmic Auditing and Social Justice: Lessons from the History of Audit Studies , Equity and Access ' in Algorithms, Mechanisms, and Optimization , 2021. (extracted from262.txt)
Concludes that it is the EU’s responsibility to quickly set up a favourable regulatory environment for AI that provides for swift digital law-making, effective governance and balanced ethical standards, while at the same time preventing overregulation and giving enough leeway for innovation; urges that the adequate development and training of AI will require better access to high-quality data, common standards and incentives for voluntary data sharing; calls on its Committees on Legal Affairs (JURI), Internal PE680.928v01-00 34/38 PR\1224166EN.docx ENMarket and Consumer Protection (IMCO), Industry, Research and Energy (ITRE), Civil Liberties, Justice and Home Affairs (LIBE), and Constitutional Affairs (AFCO) to ensure that these goals are met; 160. (extracted from276.txt)
”2 Implementations of AIAs are gaining momentum as a v iable AI governance strategy, finding their way into binding regulation and legislation.3Corporate policies are also requiring implementation of AIAs as a mechanism to reduce legal risks stemmi ng from liability and negligence.4The European Commissionʼs Artificial Intelligence Act s uggests a risk-based approach to AI governance, prohibiting certain harmful applications of AI and calling for developers to go through a form of impact assessment (called a “conformity assessment” ) for high-risk applications to identify necessary oversight mechanisms.5The Algorithmic Accountability Act proposed in the U nited States Congress in 2019 would have required compani es with large user bases to conduct impact assessments of highly sensitive ADS (the Act is exp ected to be reintroduced).6In 2021, the National Institute of Standards and Technology (NIST) was ta sked by Congress to develop an “AI risk management framework” to guide the “reliability, ro bustness, and trustworthiness of AI systems” used in the federal government.7In 2021, the National Security Commission on Artific ial Intelligence issued a report recommending that gove rnment agencies deploying AI systems conduct ex ante risk assessments and ex post impact assessments to “increase public transparency 7"Commerce, Justice, Science And Related Agencies Ap propriations Bill, 2021 - Report Together With Mino rity Views," House Committee on Appropriations, July 2020, https://appropriations.house.gov/sites/democrats.ap propriations.house.gov/files/July%209th%20report%20 for%20circulation_0.pdf , 23.6Algorithmic Accountability Act, 116th Cong. (extracted from15.txt)
”34Typical sources of risk to be identified include the presence of bias in datasets used to train an AI system, as well as the fairness and explainability of the model; identification of potential impacts can include con textual considerations related to equity and justice, as well as the economic interests, health, and well-being of users or populations potentially aﬀected by the proposed system. (extracted from15.txt)
Toronto Waterfront Revitalization Corporation, et a l., (Ontario Superior Court of Justice File No. (extracted from15.txt)
This list is non-exhaustive, and many important ethical issues -- including justice, economic development, poverty reduction, and inequality, are missing. (extracted from699.txt)
There are also a number of AI use cases that could support SDG 16, “Promote peaceful and inclusive societies for sustainable development, provide access to justice for all and build effective, accountable and inclusive institutions at all levels.” The use cases cover domains ranging from helping individuals verify and validate information, providing improved security through detection and prediction of violence, addressing bias to ensure fair and equal access to justice, to optimizing the management of public and social sector institutions. (extracted from699.txt)
List of Organizational Documents Document Categorization Official Government/Regulation Official Government/Regulation Official Government/RegulationOfficial Government/Regulation Think Tanks / Policy InstitutesOfficial Government/RegulationOfficial Government/RegulationIndustry & ConsultancyOfficial Government/Regulation Tech Companies Industry & ConsultancyThink Tanks / Policy Institutes Industry & Consultancy Tech CompaniesOfficial Government/Regulation Associations & Consortiums Tech Companies Industry & ConsultancyThink Tanks / Policy Institutes Official Government/RegulationAssociations & ConsortiumsIssuer EUROPEAN COMMISSION FOR THEEFFICIENCY OF JUSTICE (CEPEJ) AI HLEG Australian Government - Department of Industry, Innovation & Science Smart DubaiOECDG20Singapore PDPCDeloitteFinland - Ministry of Economic Affairs and Employment TietoOP Financial GroupFrance - Commission Nationale de l’Informatique et des Libertés Deutsche TelekomSAPAgenzia per l’Italia Digitale Japan - Conference toward AI Network Society SonyTelefonicaInstitute of Business Ethics UK - Department of Health and Social Care The Information Accountability Foundation 273Artificial Intelligence Index Report 2019Technical Appendix 8 - Societal Considerations [Societal_Considerations]_[Appendix_Start] [Access_Data]Return to Societal Considerations - Ethical Challenges Acronym AMA UNTGWGSIIITIWEFICD TPV FAT MAS VOD DNB INDDEKDocument Title Policy Recommendations on Augmented Intelligence in Health Care H-480.940 Introducing Unity’s Guiding Principles for Ethical AI – Unity Blog Position on Robotics and Artificial Intelligence Ethical Principles for Artificial Intelligence and Data AnalyticsITI AI Policy PrinciplesWhite Paper: How to Prevent Discriminatory Outcomes in Machine Learning Declaration on ethics and data protection in Artificial Intelligence Universal Guidelines for Artificial Intelligence Principles for Accountable Algorithms and a Social Impact Statement for Algorithms Principles to Promote Fairness, Ethics, Accountability and Transparency (FEAT) in the Use of Artificial Intelligence and Data Analytics in Singapore’s Financial Sector Artificial Intelligence frameworkGeneral Principles for the use of Artificial Intelligence in the Financial Sector Artificial Intelligence in the Governance Sector in India Opinion of the Data Ethics CommissionTable A8.2. (extracted from699.txt)
Osborne China State CouncilSecretary of State for Business, Energy and Industrial Strategy United Nations Interregional Crime and Justice Research Institute Future of Humanity Institute, Oxford University,Centre for the Study of Existential Risk, University of Cambridge FTI Consulting Table A9.1. (extracted from699.txt)
Accept additional cookies Reject additional cookies View cookies Hide cookie message Skip to main content GOV.UK Navigation menu Menu Menu Services and information Benefits Births, death, marriages and care Business and self-employed Childcare and parenting Citizenship and living in the UK Crime, justice and the law Disabled people Driving and transport Education and learning Employing people Environment and countryside Housing and local services Money and tax Passports, travel and living abroad Visas and immigration Working, jobs and pensions Government activity Departments Departments, agencies and public bodies News News stories, speeches, letters and notices Guidance and regulation Detailed guidance, regulations and rules Research and statistics Reports, analysis and official statistics Policy papers and consultations Consultations and strategy Transparency Data, Freedom of Information releases and corporate reports Search GOV.UK × Search GOV.UK Search Search GOV.UK Search Home Health and social care National Health Service Digital and data-driven health and care technology Departmentof Health &Social Care Guidance A guide to good practice for digital and data-driven health technologies Updated 19 January 2021 Contents Introduction 1. (extracted from114.txt)
Cancel Services and information Benefits Births, death, marriages and care Business and self-employed Childcare and parenting Citizenship and living in the UK Crime, justice and the law Disabled people Driving and transport Education and learning Employing people Environment and countryside Housing and local services Money and tax Passports, travel and living abroad Visas and immigration Working, jobs and pensions Government activity Departments News Guidance and regulation Research and statistics Policy papers and consultations Transparency How government works Get involved Support links Help Privacy Cookies Accessibility statement Contact Terms and conditions Rhestr o Wasanaethau Cymraeg Government Digital Service All content is available under the Open Government Licence v3.0, except where otherwise stated © Crown copyright (extracted from114.txt)
Wecontribut etothisconversationaspeopleconcerned,firstandforemos t,withthelearninganddevelopmen tofananti-racist,intersectionalracecritic alframe workforall.Benjamin(2019)iden tifiestheneedforourcontributionaseduc atorsinthetechnologyspacewhenshesays, “Justice …isnotastaticvaluebutanongoingmethodologythatcanandshouldbeincorpor atedintotechdesign.Forthisreason,too,itisvitalthatpeopleengagedintechdevelopmen tpartnerwiththosewhodoimport antsociocultur alworkhoningnarrativetoolsthroughthearts,humanities,andsocialjusticeorganizing ”(pp.192-193).Ourpersonalpoliticsasauthor sareimport antasourbelie fscomposetheman ysidesofourpersonalabilitiestofunctionlikequbits. (extracted from470.txt)
(2020).# Hash tagActivism: Ne twork s of rac e and gender justice. (extracted from470.txt)
“P ods and P od Mapping W orkshee t.” Weblog.BAY ARE A TRANSF ORMA TIVE JUSTICE C OLLE CTIVE Building T ransf ormative Jus ticeResponses t o Child Se xual Abuse (blog). (extracted from470.txt)
Katlyn T urner Dr.KatlynTurnerisaResearchScientistwithintheSpaceEnabledresearchgroup.Inthatrole,herprimaryresearchincludesworkoninclusiveinnovationpractices,andonprinciplesofanti-racisttechnologydesign.Sheadditionallymentorsstudents,worksonproposalwritingefforts,andhelpstocommunicatetheteam'swork.Dr.TurnerearnedherPhDinGeologicalSciencesfromStanfordUniversity,wheresheresearchednovelnuclearwasteforms.From2017-2019,KatlynwasapostdoctoralfellowattheProjectonManagingtheAtom&theInternationalSecurityProgramatHarvardKennedySchool'sBelferCenterforScience&InternationalAffairs,wheresheresearchedenvironmentalandsocio-politicalimpactsofnuclearenergy.Dr.TurneradditionallyholdsanM.S.inEarth&EnvironmentalSciencesfromtheUniversityofMichigan,andaB.S.inChemical&BiomolecularEngineeringfromtheUniversityofNotreDame.Dr.Turnerispassionateaboutissuesofdiversity,justice,inclusion,andaccessibilitywithinsociety--particularlyinhigher education and within STEM employment sectors. (extracted from470.txt)
I teach gener al concep ts and trends in Digit al Ethics, which covers a wide range of ways technology impacts individuals (such as privacy, accessibility , financial health and opportunity , men tal well-being , personal dignity , and legal status), socie ty (such as health care, educ ation, the econom y, criminal justice, and law enforcemen t), and the environmen t (such as ener gy use, material use, waste, pollution, and impact on biodiv ersity). (extracted from471.txt)
The team used manual coding to iden tify unifying themes and came up with 11 of them: transpar ency (appear ed in 87% of the documen ts), justice and Fairness (81%), non-male ficence (71%), responsibility (71%), Privacy (56%), bene ficence (49%), freedom and autonom y (40%), Trust (33%), sus tainability (17%), dignity (15%), and solidarity (7%). (extracted from471.txt)
In sear ching for unifying themes in AI ethics principles, the author s draw from the four ethical principles commonly used in bioe thics: bene ficence, non-male ficence, autonom y, and justice. (extracted from471.txt)
Hagendorf f iden tified eigh t themes: privacy protection (appear ed in 82% of documen ts), fairness, non-discrimina tion, justice (82%), accountability (77%), transpar ency/ openness (73%), safety, cyber -security (73%), common good, sustainability , well-being (73%), human oversight, control, auditing (54%), and solidarity , inclusion, social c ohesion (50%). (extracted from471.txt)
Other moral values like autonom y, justice, and respect for people were also noticeably absent. (extracted from471.txt)
Such values are likely to include democr acy, justice, privacy, the protection of human rights, and a commitmen t to environmen tal protection. (extracted from471.txt)
These are by frequency of the number of sour ces in which they appear ed: transpar ency , justice, and fairness, non-male ficence, responsibility , privacy, bene ficence, freedom and autonom y, trust, dignity , sustainability , and solidarity . (extracted from471.txt)
● The resear cher s found that no single ethical principle was found common to the entire corpus of documen t, however, an emer ging convergence was found around the following principles: transpar ency , justice and fairness, non-male ficence, responsibility , and priv acy. (extracted from471.txt)
” ● Ther e is an emer gence of a cross-s takeholder convergence on promoting the ethical principles of transpar ency , justice, non-male ficence, responsibility , and privacy. (extracted from471.txt)
Some of the other values emphasiz ed in the documen t include impr oving human well-being , promoting fairness and justice, protecting priv acy and sa fety, and r aising e thical lit eracy. (extracted from471.txt)
Madaio , Jennif er Wortman Vaughan, Luke Stark and Hanna Wallach] [Resear ch Summar y by Anne Boily] Overview : Among the burgeoning literature on AI ethics and the values that would be import ant to respect in the developmen t and use of artificial intelligence systems (AIS), fairness comes up a few times, perhap s as an echo of the very current notion of social justice. (extracted from471.txt)
In this sense, there should be a lack of cheap and subversive techniques to avoid complic ated issues like justice, with the social good and social infrastructur e over inno vation and the good of the governmen ts. (extracted from471.txt)
In Informania, machine learning systems “optimiz e on outcomes for millions (or billions) of user s, with little regard for individual rights within the c ollectiv e.” Such a futur e is becoming more likely, the author s state, poin ting to China’ s social credit system and the US Justice Departmen t’s COMP AS algorithm. (extracted from471.txt)
Getting from Commitmen t to Content in AI and Data Ethics: Justice and Explainability [ Original paper by John Basl, R onald Sandler and St even Tiell] [Resear ch Summar y by Angshuman K aushik] Overview : AI or data ethics principles or frame works mean t to demons trate a commitmen t to addr essing the challeng es posed by AI are ubiquit ous and are an ‘easy first step’. (extracted from471.txt)
According to it, much of this comple xity arises fr om thr ee key factors: ● ethical concep ts such as justice and transpar ency that often have man y senses and meaning; ● which senses of e thical concep ts ar e oper ative or appr opria te is oft en contextual; and ● ethical concep ts are multidimensional e.g., in terms of wha t needs to be transpar ent, to whom, and in wha t form. (extracted from471.txt)
T h e S t a t e o f A I E t h i c s R e p o r t , V o l u m e 6 ( J a nu a r y 2 0 2 2 ) 1 9 0 Further , the objectiv es of the r eport ar e to: ● demons trate the import ance and comple xity of moving from gener al ethical concep ts and principles t o action-guiding sub stantive content, i.e., norma tive content; ● provide detailed analy sis of two widely discussed and interconnect ed ethical concep ts, justice and tr anspar ency; and ● indic ate strategies for moving from gener al ethical concep ts and principles to more specific norma tive content and ultima tely t o oper ationalizing tha t content. (extracted from471.txt)
Meaning of jus tice in AI The report men tions that the concep t of justice is a comple x one, and can mean different things in different contexts. (extracted from471.txt)
To determine wha t justice in AI and data use requir es in a particular context, it is imper ative to clarif y the norma tive content and underlying values. (extracted from471.txt)
Only then it is possible to specif y wha t is requir ed in specific cases, and in turn how or to wha t extent justice can be oper ationaliz ed in technic al systems. (extracted from471.txt)
According to the report, the gener al principle of justice is that all people should be equally respect ed and valued in social, economic and politic al systems and processes. (extracted from471.txt)
However, there are man y ways this very gener al principle of justice intersects with social structur es and systems. (extracted from471.txt)
As a result, there is a diverse set of more specific justice-orien ted principles such as pr ocedur al, dis tributiv e and r ecognition jus tice. (extracted from471.txt)
The resear cher s consider context to be critic ally import ant in determining which justice-orien ted principles take precedence. (extracted from471.txt)
Ther efore, the first step in specif ying the norma tive content is to iden tify the justice-orien ted principles that are crucial to the work that the AI system does. (extracted from471.txt)
Only then can a commitmen t to justice be effectiv ely put into practice. (extracted from471.txt)
Articula ting the relevant justice-orien ted principles will also requir e considering organizational missions, the types of products and services involved, how those products and services could impact communities and individuals etc. (extracted from471.txt)
Further , the report states that the diversity of the justice-orien ted principles and the need to mak e context-specific determina tions about which are relevant and which to prioritiz e expose the limits of a strictly algorithmic manner in incorpor ating justice in AI systems. (extracted from471.txt)
The reason being , firstly, there is no singular , gener al justice-orien ted constraint, optimiz ation or utility function and secondly , there will not be a strictly algorithmic way to fully incorpor ate justice into decision-making , even once the relevant justice consider ations have been iden tified. (extracted from471.txt)
The report then goes on to ask the ques tion as to how and to wha t extent can the salien t aspects of justice be achie ved algorithmic ally. (extracted from471.txt)
According to the resear cher s, accomplishing justice in AI will requir e developing justice-in formed, techno-social or human-alg orithm systems. (extracted from471.txt)
According to the resear cher s, a commitmen t to justice in AI involves remaining open to the possibility that some times an AI-orien ted appr oach migh t not be a just one. (extracted from471.txt)
They stress on the fact that, organizations that are commit ted to justice in AI will requir e signific ant organizational capacity and processes to oper ationaliz e and implemen t their commitmen t, in addition to technic al capacity and expertise. (extracted from471.txt)
Transpar ency in AI In the view of the resear cher s, in spite of the role that transpar ency plays in helping to achie ve justice, it can also play an import ant role in realizing other concep ts and values. (extracted from471.txt)
T h e S t a t e o f A I E t h i c s R e p o r t , V o l u m e 6 ( J a nu a r y 2 0 2 2 ) 2 0 7 Risk and unaccep table uses The central oper ating mechanism of the Act is to look at high-risk AI uses-c ases which include biome tric iden tification, critic al infrastructur e that can signific antly impact human lives, determining access to educ ation and emplo ymen t, worker manag emen t, access to private and public services (e.g., finance), law enforcemen t, migr ation and immigr ation, and adminis tration of justice and democr atic processes. (extracted from471.txt)
Each of the appr oaches come with their own pros and cons, in the case of civil offenses, the burden of proof remains lighter making it perhap s easier to obtain justice but criminal of fenses c arry a higher punitiv e bur den of fering a s tronger de terrent. (extracted from471.txt)
● Promot e fairness and justice – It talks about adher ence to inclusiv eness and inclusiv eness, which again does not convey any meaning , wha tsoe ver. (extracted from471.txt)
The other ethical norms clubbed under this broad heading effectiv ely protect the legitima te rights and interests of all relevant subjects, promot e social fairness and justice and equal opportunities. (extracted from471.txt)
Figur es such as Archbishop Desmond Tutu, Professor Gessler Mux e Nkondo and Justice Yvonne Mokg oro have commen ted on Ubun tu. (extracted from471.txt)
Alarming reports have detailed how discriminatory algorithms are already deployed in the justice system, wherein judges use these tools for sentencing that purport to predict the likelihood a criminal defendant will reoffend.21 In Automating Inequality , Virginia Eubanks details how government actors implement automated and surveillance technologies that harm marginalized groups.22 Eubanks studied automated systems in the US that discriminated against the poor’s receipt of DATA & SOCIETY 11 GOVERNING ARTIFICIAL INTELLIGENCEgovernment assistance. (extracted from101.txt)
Baluarte and Christian De Vos, From Judgment to Justice: Implementing International and Regional Human Rights Decisions , Open Society Justice Initiative (November 2010), https:/ / www.opensocietyfoundations.org/sites/default/files/from-judgment-to-justice-20101122.pdf . (extracted from101.txt)
Also from Council of Europe, on 22 October, 2020, the Parliamentary Assembly of the Council of Europe (PACE) adopted seven reports concerning the impact of AI: the need for democratic governance of AI ​78 ​; the role of AI in policing and criminal justice systems ​79 ​; preventing discrimination caused by AI ​80 ​; ethical and legal frameworks for the research and development 78 See ​Need for democratic governance of artificial intelligence ​, available at https://pace.coe.int/en/files/28742/html ​. (extracted from14.txt)
79 See ​Justice by algorithm - the role of artificial intelligence in policing and criminal justice systems ​, available at https://pace.coe.int/en/files/28723/html ​. (extracted from14.txt)
​113 Achiume echoes much recent work on AI governance from a human rights perspective when she proclaims that ensuring racial justice and the protection of human rights will require prohibiting certain applications of AI. (extracted from14.txt)
It was indicated that the question of red lines is under serious discussion on multiple sides, and that the possibility of a ban remains on the table, especially regarding so-called remote biometric identification systems, and the use of AI in sensitive domains such as criminal justice. (extracted from14.txt)
44 Europe’s Approach to Artificial Intelligence: How AI Strategy is Evolving We have seen how even simple algorithms, such as that used in the UK’s A-Level grading fiasco, can amplify unfair and discriminatory outcomes and mobilize people to demand justice amid chants of “fuck the algorithm”​155 ​. (extracted from14.txt)
Richard Benjamins, Observatorio del impacto social y ético de la inteligencia artificial (ODISEIA) Nuria Oliver, Commissioner for the President of the Valencian Region on AI Strategy and Data Science to fight COVID-19, Spain Brussels roundtable: Speakers Nuria Oliver, Commissioner for the President of the Valencian Region on AI Strategy and Data Science to fight COVID-19, Spain Krzysztof Izdebski, Policy Director of ePaństwo Foundation, Poland Meeri Haataja, CEO & co-founder of Saidot, Finland Friederike Reinhold, senior policy advisor for AlgorithmWatch, Germany Veronika Žolnerčíková, CyberSecurity & CyberCrime Center of Excellence at Masaryk University (C4E), Co-creator of Czech National strategy on AI, Czech Republic 47 Europe’s Approach to Artificial Intelligence: How AI Strategy is Evolving Karma Peiro, Data Journalist & Co-director of the Visualization and Transparency Foundation, Spain Sarah Chander, Senior Policy Advisor at European Digital Rights (EDRi), Belgium Hanna Zinner, Artificial Intelligence and Digital Industry, DG CNECT, European CommissionMarcel Kolaja, European Parliament Vice President and member of the Czech Pirate Party Andreas Hartl, Head of Division, Strategy Artificial Intelligence, Data Economy, Blockchain, Federal Ministry for Economic Affairs and Energy, Germany Audience Jim Dratwa, Team Leader, European Group on Ethics, European Commission Killian McDonagh Dit, Directorate-General for Justice and Consumers, European Commission Anna Moscibroda, Directorate General for Justice and Consumers, European Commission Zoi Kardasiadou, Directorate General for Justice and Consumers, European Commission Aimilia Givropoulou, assistant to MEP Patrick Breyer Anne van Heijst, assistant to MEP Liesje van Schreinemacher Despoina Riga, assistant to MEP Anna-Michelle Assimakopoulou Natalia Joanna Boniecka, assistant to MEP Andrzej Halicki Georgios Theodotou, assistant to MEP Elena Kountoura Matt Mahmoudi, Researcher/Advisor on Artificial Intelligence & Human Rights at Amnesty International Ella Jakubowska, Policy & Campaigns on biometrics at European Digital Rights 48 Europe’s Approach to Artificial Intelligence: How AI Strategy is Evolving Sample Agenda ​. (extracted from14.txt)
Criminal justice, public housing, welfare and health provision are examples of areas where “black box” systems should not be developed or used. (extracted from713.txt)
29 In “Fairness in Criminal Justice Risk Assessments: The State of the Art” Berk et al, 2017 provide a through review of the technical pathways towards promoting fairness in machine learning. (extracted from713.txt)
Berk et al, 2017, Fairness in Criminal Justice Risk Assessments: The State of the Art https://arxiv.org/ abs/1703.0920730 Ai Now Institute 2017 Report 13 How to Prevent Discriminatory Outcomes in Machine LearningBringing principles of non-discrimination to life: Human rights due diligence for machine learning Companies developing and using ML systems must integrate these principles of non-discrimination into their human rights due diligence – a process by which businesses take ongoing, proactive, and reactive steps to ensure that they uphold people’s dignity and do not cause or contribute to human rights abuses. (extracted from713.txt)
For instance, AI poses risks to the right to personal data protection and priva cy, and equally so a risk of discrimination when algorithm s are used for purposes such as to profile people or to resolve situations in criminal justice.3 There are also some concerns about the impact of AI technologies and robotics on the labour market (e.g. (extracted from277.txt)
The eight principles with regard to AI are: harmony and friendliness; fairness and justice; inclusivity and sharing; respect for privacy; secure/safe and controllable; shared responsibility; open collabo ration; and agile governance. (extracted from277.txt)
• Non -economic services , such as the police, justice and statutory social security schemes, are not subject to specific European legislation or to intern al market and competition rules. (extracted from263.txt)
Responsible innovation, anticipation and responsiveness: case studies of algorithms in decision support in justice and security, and an exploration of potential, unintended, undesirable, higher -order effects. (extracted from263.txt)
F airness is discussed thr ough the lens of social justice , highlighting the STOA | Panel for the Future of Science and Technology II potential for algorithmic systems to systematically disadvantage, or even discriminate against, different social groups and demographics. (extracted from261.txt)
The second step included a review of the types and degrees of impact that algorithmic systems have on social justice, fair decision -making and the associated technological and societal need/limits for algorithmic literacy, transparency, oversight and information symmetry. (extracted from261.txt)
We discuss fairness through the lens of social justice and highlight the potential for algorithmic systems to systematically disadvantage, or even discriminate against, different social groups and demographics. (extracted from261.txt)
Therefore, we also understand fairness within the lens of social justice, as opposed to individual cases in which there is a perceived imbalance of goods or penalties (' Why did she get more cookies than me?), an uneven applications of a rule (' You let him throw the ball out of turn '), a case of discrimination based on irrelevant factors that are not subject to rights claims ('You didn’t pick me for the team even though I’m faster than the person you did pick '), etc. (extracted from261.txt)
Social justice is another complex term with many potential definitions [38, 39, 40, 41, 42]. (extracted from261.txt)
Discussions of social justice (in academic , policy and public discourses) typically recognise that ensuring a fair distribution is complicated by inherent inequalities in contemporary society; there are various differences of perspective over the extent to which a fair distribution should accommod ate for, or attempt to address, such inequalities [44,45]. (extracted from261.txt)
One high- profile campaigner is Joy Buolamwini, computer scientist at MIT and founder of the Algorithmic Justice League [54]. (extracted from261.txt)
Algorithm based decision -making in the US criminal justice system In the early 2000s the US criminal justice system began using risk assessmen ts to assist decision making [91, 92]. (extracted from261.txt)
In written evidence submitted to the UK government’s inquiry into Algori thms Used in decision -making [101], the Head of Criminal Justice at Durham Constabulary reported that i t was too early to make conclusions about the accuracy of HART, but research into it is being conducted in order to support evidence based good practice, and that the results of this research would be made available. (extracted from261.txt)
● Justice: where citizens feel that algorithms are biased or even discriminatory, this can compromise their feeling that they live in a just society. (extracted from261.txt)
Each of these values is closely entwined with understandings of fairness and social justice. (extracted from261.txt)
Various means to achieve fairness and social justice in algorithms have been suggested. (extracted from261.txt)
[111] considered is the use of algorithmic decision making in the criminal justice system and they note the controversy surrounding the use of C OMPAS in the US court system. (extracted from261.txt)
As Rawls [42] describes, justice encompasses an overall acceptability that existing institutions generate mutual benefit and cooperation in society. (extracted from261.txt)
Accoun tability Measures for Algorithmic System use by Public Authorities Algorithmic systems are currently being used in government, reshaping how criminal justice systems work via risk assessment algorithms and predictive policing [ 412, 413], optimizing energy use in critical infrastructure through AI -driven resource allocation [414, 415] and changing government resource allocation and monitoring practices [412, 413]. (extracted from261.txt)
It also revealed a lack of general knowledge about the systems among the authorities, leading to situations where the students had to explain what ‘criminal justice algorithms’ were to the public servants in charge of providing the records on their use. (extracted from261.txt)
4 87], restrictions of due process in criminal justice proceedings [4 88] and more. (extracted from261.txt)
The Times ' investigation led to broad media coverage and a Department of Justice inquiry into potential criminal behaviour by the company [5 68]. (extracted from261.txt)
Justice and care: Essential readings in feminist ethics. (extracted from261.txt)
Sex and social justice. (extracted from261.txt)
Unveiling the meaning of social justice in Colombia. (extracted from261.txt)
Justice as fairness: A restatement. (extracted from261.txt)
Defining social justice in a socially unjust world. (extracted from261.txt)
Defining social justice. (extracted from261.txt)
Va lue differences underlying public views about social justice. (extracted from261.txt)
[46] Michael Walzer, Spheres of Justice, (NY: Basic Books, 1983) [47] Foster, A. (extracted from261.txt)
[54] Algorithmic Justice League https://www.ajlunited.org/ Accessed on: 28 September 2018 [55] Puri, R. (extracted from261.txt)
Algorithms in the Criminal Justice System: Assessing the Use of Risk Assessments in Sentencing. (extracted from261.txt)
Criminal Justice and Behavior 31(3), 306– 323. (extracted from261.txt)
' Fairness in criminal justice risk assessments: the state of the art. (extracted from261.txt)
[416] Kade Crockford, ' Risk assessment tools in the criminal justice system: inaccurate, unfair, and unjust?,' ACLU of Massachusetts , March 8, 2018, https://privacysos.org/blog/risk -assessment -tools -criminal -justice system -inaccurate -unfair -unjust [417] Virginia Eubank s, Automating Inequality: How High -Tech Tools Profile, Police, and Punish the Poor, (New York: St. (extracted from261.txt)
Martin’s Press, 2018); Nazgol Ghandnoosh, Black Lives Matter: Eliminating Racial Inequity in the Criminal Justice System (Washington DC: The Sentencing Project, 2015), http://sentencingproject.org/wp content/uploads/2015/11/Black -Lives -Matter.pdf [418] Insha Rahman, 'The State of Bail: A Breakthrough Year for Bai l Reform, ' Vera Institute of Justice, 2017, https://www.vera.org/state -of-justice -reform/2017/bail- pretrial [419] Dillon Reisman, Jason Schultz, Kate Crawford, Meredith Whi ttaker, 'Algorithmic Impact Assessments: A Practical Framework for Public Agency Accountability ', AI Now, April 2018 https://ainowinstitute.org/aiareport2018.pdf [420] Ali Winston, ' Transparency Advocates Win Release of NYPD ‘Predictive Policing’ Documents, ' The Intercept, Jan. (extracted from261.txt)
29, 2017, https://www.axios.com/lawmakers -are-trying -to-understand -how- tech -giants -algorithms -work 1513307255- b4109efc -9566- 4e69- 8922- f37d9e829f1f.html [441] Eric Holder, ' Speech at the National Association of Criminal Defense Lawyers 57th Annual Meeting and 13th State Criminal Justice Network Conference' (Philadelphia, PA, Aug. (extracted from261.txt)
1, 2014), Department of Justice, https://www.justice.gov/opa/speech/attorney -general- eric-holder-speaks -national -association -criminal defense -lawyers -57th A governance framework for algorithmic accountability and transparency 101 [442] John Fry, Anne Maxwell, Sarah Apere, Paddy McAweeney, Luke McSharry, and Ainhoa Gonz a�lez, 'Non Technical Summaries -Due Care and Attention, ' In 34th IAIA Annual Conference, http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.567.8444&rep=rep1&type=pdf . (extracted from261.txt)
Times, July 29, 2016, https://www.nytimes.com/roomfordeb ate/2014/08/06/is -big-data -spreading -inequality/big -datashould -be-regulated -by-technological- due -process [461] Wexler, ' Life, Liberty, and Trade Secrets '; Ram, ' Innovating Criminal Justice '. (extracted from261.txt)
Department of Justice, Office of the Inspector General (2018), https://oig.justice.gov . (extracted from261.txt)
[475] Rebecca Wexler, ' Life, Liberty, and Trade Secrets: Intellectual Property in the Criminal Justice System, ' 70 Stan. (extracted from261.txt)
Rev., (forthcoming 2018), https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2920883 [476] Natalie Ram, ' Innovating Criminal Justice, ' Northwestern L. (extracted from261.txt)
3, 2017 [568] Mike Isaac, ' Justice Department Expands Its Inquiry Into Uber’s Greyball Tool, ' The New York Times, Mar. (extracted from261.txt)
AI Now produces interdisciplinary research to help ensure that AI systems are accountable to the communities and contexts they are meant to serve, and that they are applied in ways that promote justice and equity. (extracted from16.txt)
Our workshop on Immigration, Data, and Automation in the Trump Era , co-hosted with the Brennan Center for Justice and the Center for Privacy and Technology at Georgetown Law, focused on the Trump Administration’s use of data harvesting, predictive analytics, and machine learning to target immigrant communities. (extracted from16.txt)
Domains like health, education, criminal justice, and welfare all have their own histories, regulatory frameworks, and hazards. (extracted from16.txt)
Facial recognition technology poses its own dangers, reinforcing skewed and potentially discriminatory practices, from criminal justice to education to employment, and presents risks to human rights and civil liberties in multiple countries. (extracted from16.txt)
Yet, without a framework that accounts for social and political contexts and histories, these mathematical formulas for fairness will almost inevitably miss key factors, and can serve to paper over deeper problems in ways that ultimately increase harm or ignore justice. (extracted from16.txt)
Broadening perspectives and expanding research into AI fairness and bias beyond the merely mathematical is critical to ensuring we are capable of addressing the core issues and moving the focus from parity to justice. (extracted from16.txt)
The following report develops these themes in detail, reﬂecting on the latest academic research, and outlines seven strategies for moving forward: 1.Expanding AI fairness research beyond a focus on mathematical parity and statistical fairness toward issues of justice 2.Studying and tracking the full stack of infrastructure needed to create AI, including accounting for material supply chains 3.Accounting for the many forms of labor required to create and maintain AI systems 4.Committing to deeper interdisciplinarity in AI 5.Analyzing race, gender, and power in AI 6.Developing new policy interventions and strategic litigation 7.Building coalitions between researchers, civil society, and organizers within the technology sector These approaches are designed to positively recast the AI ﬁeld and address the growing power imbalance that currently favors those who develop and proﬁt from AI systems at the expense of the populations most likely to be harmed. (extracted from16.txt)
1 There have been major movements from both inside and outside technology companies pushing for greater accountability and justice. (extracted from16.txt)
Facial recognition ampliﬁes civil rights concerns Concerns are intensifying that facial recognition increases racial discrimination and other biases in the criminal justice system. (extracted from16.txt)
53 In its response to the ACLU, Amazon acknowledged that “the Rekognition results can be signiﬁcantly skewed by using a facial database that is not appropriately representative.” 54 Given the deep and historical racial biases in the criminal justice system, most law enforcement databases are unlikely to be “appropriately representative.” 55 Despite these serious ﬂaws, ongoing pressure from civil rights groups, and protests from Amazon employees over the potential for misuse of these technologies, Amazon Web Services CEO Andrew Jassy recently told employees that “we feel really great and really strongly about the value that Amazon Rekognition is providing our customers of all sizes and all types of industries in law enforcement and out of law enforcement.” 56 Nor is Amazon alone in implementing facial recognition technologies in unaccountable ways. (extracted from16.txt)
17 1.2 The Risks of Automated Decision Systems in Government Over the past year, we have seen a substantial increase in the adoption of Automated Decision Systems (ADS) across government domains, including criminal justice, child welfare, education, and immigration. (extracted from16.txt)
For years, criminal justice advocates and researchers have pushed for the elimination of cash bail, which has been shown to disproportionately harm individuals based on race and socioeconomic status while at the same time failing to enhance public safety. (extracted from16.txt)
90 The shift from policies such as cash bail to automated systems and risk assessment scoring is still relatively new, and is proceeding even without substantial research examining the potential to amplify discrimination within the criminal justice system. (extracted from16.txt)
91 Similarly, when California’s legislation passed earlier this year, many of the criminal justice advocates who pushed for the end of cash bail, and supported an earlier version of the bill, opposed its ﬁnal version due to the risk assessment requirement. (extracted from16.txt)
The National Association for the Advancement of Colored People (NAACP) and the Lawyers’ Committee for Civil Rights and 21 Economic Justice opposed the plan because of the school district’s failure to appreciate that parents of color and lower-income parents often rely on jobs that lack work schedule ﬂexibility and may not be able to afford additional child care. (extracted from16.txt)
3.1 From Fairness to Justice Any debate about bias and fairness should approach issues of power and hierarchy, looking at who is in a position to produce and proﬁt from these systems, whose values are embedded in these systems, who sets their “objective functions,” and which contexts they are intended to work within. (extracted from16.txt)
For example, making a facial recognition system perform equally on people with light and dark skin may be a type of technical progress in terms of parity, but if that technology is disproportionately used on people of color and low-income communities, is it really “fair?” This is why deﬁnitions of fairness face a hard limit if they remain purely contained within the technical domain: in short, “parity is not justice.” 169 32 3.2 Infrastructural Thinking In order to better understand and track the complexities of AI systems, we need to look beyond the technology and the hype to account for the broader context of how AI is shaping and shaped by social and material forces. (extracted from16.txt)
192 Such initiatives are critical: as AI becomes more deeply embedded in areas like healthcare, criminal justice, hiring, housing, and educational systems, experts from these domains are essential if we are to ensure AI works as envisioned. (extracted from16.txt)
234 The last year revealed many of the hardest challenges for accountability and justice as AI systems moved deeper into the social world. (extracted from16.txt)
45 21.Natalie Ram, “Innovating Criminal Justice,” Northwestern University Law Review 112, no. (extracted from16.txt)
165.For a more general description of justice as fairness, see: John Rawls, Justice as Fairness: A Restatement , ed. (extracted from16.txt)
167.Ben Green, “‘Fair’ Risk Assessments: A Precarious Approach for Criminal Justice Reform” (5th Workshop on Fairness, Accountability, and Transparency in Machine Learning, Stockholm, 2018), https://scholar.harvard.edu/ﬁles/bgreen/ﬁles/18-fatml.pdf . (extracted from16.txt)
46 3.6 Justice and solidarity .......................................................................................... (extracted from103.txt)
212 7.3 Algorithmic systems in the dispensation of justice ....................................................... (extracted from103.txt)
Justice and Solidarity In view of the vast amounts of power being accumulated using data and technologies, and the new threats of exclusion and discrimination, the safeguarding of equitable access and distributive justice is an urgent task. (extracted from103.txt)
Regardless of the position that data protection authorities and the European Court of Justice will ultimately take with regard to the prohibition under the GDPR of “tying” or “bundling” consent with the provision of a service, the Data Ethics Commission believes that consumers must be offered reasonable alternatives to releasing their data for commercial use (e. (extracted from103.txt)
8 The Data Ethics Commission advises the Federal Government not to consider the issues falling under the heading of “digital inheritance” as having been settled by the Federal Court of Justice’s 2018 ruling. (extracted from103.txt)
69 In the areas of law-making and the dispensation of justice , algorithmic systems may at most be used for peripheral tasks. (extracted from103.txt)
The first took place on 7 February 2019 at the Federal Ministry of Justice and Consumer Protection ( Bundesministerium der Justiz und für Verbraucherschutz ), and centred around the issue of “Selfdetermination and external determination in the age of artificial intelligence”. (extracted from103.txt)
Another vital aspect of self-determination is that people must not only be allowed to assume responsibility , but must do so and do justice to the task. (extracted from103.txt)
3.6 Justice and solidarity Observance of the principles of justice by society and its institutions is another fundamental factor that allows us to live together in peace, prosperity, freedom and democracy. (extracted from103.txt)
in the workplace and the healthcare sector raises other questions relating to equitable access and distributive justice , however, for example in relation to income and the provision of healthcare; these developments may mean that scarce resources can be distributed more fairly, but they may also mean that individual groups of people suffer disadvantage or discrimination. (extracted from103.txt)
There is also a close link between justice and opportunities for participation. (extracted from103.txt)
Finally, questions of justice arise in connection with situations where the use of algorithmic systems – in particular self-learning algorithmic systems – means that individuals or groups of people suffer discrimination for no justifying reason. (extracted from103.txt)
This raises questions with regard to sustainable economic and ecological development, and also questions of international justice concerning the use of natural resources and global responsibility for future generations. (extracted from103.txt)
To further this aim, in October 2018, the Federal Ministry of Justice and Consumer Protection launched an initiative to clarify the principles and concepts of corporate digital responsibility (www.bmjv.de/cdr ). (extracted from103.txt)
The issue raises fundamental questions about distributive and participatory justice, and about what a just economic system looks like. (extracted from103.txt)
eu/transparency/regdoc/rep/1/2017/EN/COM-2017-9-F1-EN-MAIN-PART-1.PDF ); Arbeitsgruppe “Digitaler Neustart” der Konferenz der Justizministerinnen und Justizminister der Länder [Working Group “Digital New Start” of the Conference of Ministers of Justice of the Länder]: Report of 15 May 2017, pp. (extracted from103.txt)
However, given the huge number of individuals that contribute to the generation and processing of data, the level of complexity of a fair remuneration system and the 24/7 monitoring that would be required to measure data flows would be out of all proportion to any potential gains in terms of justice. (extracted from103.txt)
Personal data are therefore often referred to in shorthand terms as “counter-performance” for digital content or services, for example in the original draft of Article 3(1) of the Digital Content Directive (although the term was removed at a later point in the legislative procedure).11 The extent to which the economic model described above is, in fact, compatible with the prohibition under Article 7(4) GDPR of “tying” or “bundling” consent with the provision of a service12 must ultimately be clarified by the European Court of Justice. (extracted from103.txt)
16 Judgment by the German Federal Court of Justice of 12 July 2018, ref. (extracted from103.txt)
III ZR 183/17.The principle set forth by the Federal Court of Justice – that an estate should be transferred to the deceased’s heirs – is linked to the existence of a contractual relationship. (extracted from103.txt)
Regardless of the position that data protection authorities and the European Court of Justice will ultimately take with regard to the prohibition under the GDPR of “tying” or “bundling” consent with the provision of a service, the Data Ethics Commission believes that consumers must be offered reasonable alternatives to releasing their data for commercial use (e. (extracted from103.txt)
8 The Data Ethics Commission advises the Federal Government not to consider the issues falling under the heading of “digital inheritance” as having been settled by the Federal Court of Justice’s 2018 ruling. (extracted from103.txt)
Systems already exist which can relieve state bodies of repetitive tasks (thereby expediting processes and freeing up human resources for complex cases) and which, in certain set-ups, improve the consistency and quality of state activity or, in the form of chatbots or voice assistants, for example, can facilitate citizens’ access to justice. (extracted from103.txt)
7.3 Algorithmic systems in the dispensation of justice The Data Ethics Commission is of the view that the use of algorithmic systems in the dispensation of justice is permissible only for peripheral tasks . (extracted from103.txt)
Justice is administered “in the name of the people”, and that means, at least in contentious proceedings as well as in administrative court proceedings and criminal proceedings, always administered by human judges. (extracted from103.txt)
Due to the often high level of trust placed in the supposed “infallibility” of technical systems (automation bias) as well as the low level of willingness to make divergent decisions, in particular if this is associated with an additional burden of reasoning and proof and the risk of a “miscarriage of justice” (default effects), even legally non-binding proposals for decisions for judgments by algorithmic systems are generally highly problematic from the perspective of the parties concerned. (extracted from103.txt)
Such systems could, for example, work out whether decisions were influenced by external factors and, if so, which ones in order to provide judges in future with ways to prevent such distortions themselves and thus contribute to better and more consistent dispensation of justice. (extracted from103.txt)
69 In the areas of law-making and the dispensation of justice , algorithmic systems may at most be used for peripheral tasks. (extracted from103.txt)
Dr Mario Martini ●Professor of Public Administration, Public Law, Administrative Law and European Law at the German University of Administrative Sciences Speyer (DUV Speyer) ●Head of the Programme Area “Transfor mation of the State in the Digital Age” and Deputy Director of the German Research Institute for Public Administration (FÖV) Klaus Müller ●Executive Director of the Federation of German Consumer Organisations (vzbv) ●Lecturer at Heinrich Heine University Düsseldorf (HHU) Paul Nemitz ●Principle Advisor at the European Commis sion, Directorate-General for Justice and Consumers Prof. (extracted from103.txt)
Dr Thomas Wischmeyer ●Assistant Professor (Tenure Track) for Public Law and Information Law at the University of Bielefeld Current as of: 10 October 2019 Imprint Berlin, December 2019 Opinion of the Data Ethics CommissionPublisher Data Ethics Commission of the Federal GovernmentFederal Ministry of the Interior, Building and CommunityAlt-Moabit 140, 10557 BerlinFederal Ministry of Justice and Consumer ProtectionMohrenstraße 37, 10117 Berlin E-mail datenethikkommission_gs@bmi.bund.dedatenethikkommission_gs@bmjv.bund.de Website www.datenethikkommission.de Design Atelier Hauer + Dörfler GmbH, Berlin Photo credits p. (extracted from103.txt)
162 5.2 Impact on law enforcement and criminal justice ................................ (extracted from301.txt)
Christian Archambeau Executive Director EUIPO 8 Acronyms AAAI American Association for the Advancement of Artificial Intelligence ACR Automatic content recognition AGI Artificial general intelligence AI Artificial intelligence A.L.I.C.E Artificial Linguistic Internet Computer Entity ANI Artificial narrow intelligence ASI Artificial superintelligence ASIMO Advanced Step in Innovative M obility APT Advanced Persistent Threat BEC Business email compromise CaaS Cybercrime -as-a-Service CAPTCHA Completely Automated Public Turing test to tell Computers and Humans Apart CoE Council of Europe CJEU Court of Justice of the European Union DARPA Defense Advanced Research Projects Agency DDoS Distributed denial of service EC European Commission EG Impact of Technology Expert Group EIPPN European Intellectual Property Prosecutors Network ETL ENISA Threat Landscape ENISA European Union Agency for Cyber security EU European Union FGCS Fifth-generation computer systems FORTRAN Formula T ranslator HNN Hopfield neural network IoE Internet of Everything IOCTA Internet Organised Crime Threat Assessment IoT Internet of Things IP Intellectual property IPR Intellectual property right IPTV Internet protocol television 9 ISP Internet service provider LEA Law enforcement agenc y LSTM Long Short -Term Memory MANIAC I Mathematical Analyzer, Numerical Integrator, and Computer MASP General Multi -Annual Strategic Plan MITI Ministry of International Trade and Industry of Japan MS Member State (s) (of the European Union) NLP Natural language processing OCR Optical character recognition OAP Operational Action Pla n PSP Payment service provider SIENA Secure Informatio n Exchange Network Application SP2025 EUIPO Strategic Plan 2025 TLD Top-level domain UBA User behavio ur analytics UNICRI United Nations Interregional Crime and Justice Research Institute WIPO World Intellectual Property Organization 10 Definitions The following terminology and definitions will be used in this study. (extracted from301.txt)
STUDY ON THE IMPACT O F ARTIFICIAL INTELLIGENCE ON THE INFRINGEMENT AND ENFORCEMENT OF COPYRIGHT AND DESIGN 17 The ‘Intellectual Property Tech Chain ’ In 2021, the EUIPO commissioned the United Nations Interregional Crime and Justice Research Institute (UNICRI) to carry out the first deep -dive research project applying this methodology in cooper ation with the Impact of Technology Expert Group. (extracted from301.txt)
In 2021, the EUIPO commissioned the United Nations Interregional Crime and Justice Research Institute (UNICRI) (15) to carry out the first deep -dive research project applying th is methodology in cooperation with the Impact of Technology Expert Group. (extracted from301.txt)
(23) Trend Micro, the United Nations Interregional Crime and Justice Research Institute (UNICRI), and Europol. (extracted from301.txt)
(24) Trend Micro, the United Nations Interregional Crime and Justice Research Institute (UNICRI), and Europol. (extracted from301.txt)
(25) Trend Micro, the United Nations Interregional Crime and Justice Research Institute (UNICRI), and Europol. (extracted from301.txt)
The present study will focus on AI technologies ’ impact on crime, law enforcement and criminal justice, taking into account the relevant ethical and fundamental rights -related issues (including data protection and privacy concerns). (extracted from301.txt)
Nevertheless, the experts highlight ed that use of AI in the justice system also presents many opportunities to figure out how to effectively use it without violating individual privacy and affecting fundamental rights. (extracted from301.txt)
https://eur -lex.europa.eu/legal -content/EN/TXT/?uri=COM%3A2018%3A237%3AFIN Euro pean Commission for the Efficiency of Justice (CEPEJ) (2019). (extracted from301.txt)
Artificial Intelligence in the Context of Crime and Criminal Justice . (extracted from301.txt)
Dirty Data, Bad Predictions: How Civil Rights Violations Impact Police Data, Predictive Policing Systems, and Justice. (extracted from301.txt)
New York University Law STUDY ON THE IMPACT O F ARTIFICIAL INTELLIGENCE ON THE INFRINGEMENT AND ENFORCEMENT OF COPYRIGHT AND DESIGN 138 Review Online, https://www.nyulawreview.org/online -features/dirty -data-bad-predictions -how-civilrights -violations -impact -police -data-predictive -policing -systems -and-justice/ Russell S. (extracted from301.txt)
https://www.usenix.org/conference/usenixsecurity16/technical -sessions/presentation/tramer Trend Micr o, the United Nations Interregional Crime and Justice Research Institute (UNICRI), and Europol. (extracted from301.txt)
5 Overview of the main AI-affected a reas relevant for the study For the purpose of this study, three areas in which AI technologies have a significant impact are particularly relevant: crime, law enforcement and criminal justice. (extracted from301.txt)
Artificial Intelligence in the Context of Crime and Criminal Justice A Report For The Korean Institute Of Criminology. (extracted from301.txt)
STUDY ON THE IMPACT O F ARTIFICIAL INTELLIGENCE ON THE INFRINGEMENT AND ENFORCEMENT OF COPYRIGHT AND DESIGN 164 5.2 Impact on law enforcement and criminal justice The wide range of existing legitimate AI applications include s systems for crime prevention and detection. (extracted from301.txt)
As AI and related technologies are used to make determinations and predictions in high -stakes domains such as criminal justice and law e nforcement, they have the potential to impact basic fundamental rights and liberties in profound ways. (extracted from301.txt)
Moreover , as is widely discussed at international level, law enforcement authorities and the criminal justice system should ensure fairness, accountability, transparency and that the use of AI is effectively communicated to the publ ic. (extracted from301.txt)
Experts highlight that the use of AI in the justice system also presents many opportunities to figure out how to effectively use it without violating an individual’s privacy (293). (extracted from301.txt)
With regards to criminal justice, numerous examples can be found of judi cial systems making use of AI tools in criminal proceedings. (extracted from301.txt)
Some countries also use automated risk assessment tools in the criminal justice system, though their use may be questioned. (extracted from301.txt)
In this context , in December 2018 , the European Commission for the Efficiency of Justice (CEPEJ) of the Council of Europe adopted the ‘Ethical Charter on the Use of Artificial Intelligence in Judicial Systems and their environment ’ (296), which encompasses the following five principles: 1. (extracted from301.txt)
Di rty Data, Bad Predictions: How Civil Rights Violations Impact Police Data, Predictive Policing Systems, and Justice. (extracted from301.txt)
New York University Law Review Online, https://www.nyulawreview.org/online -features/dirty -data-bad-predictions -how-civil-rights violations -impact -police -data-predictive -policing -systems -and-justice/ (296) European Commis sion for the Efficiency of Justice (CEPEJ) (2019). (extracted from301.txt)
(297) European Commission for the Efficiency of Justice (CEPEJ) (2019). (extracted from301.txt)
They are less aware that other rights – such as human dignity, access to justice and consumer protection, among others – can also be at risk. (extracted from328.txt)
ACCESS TO JUSTICE . (extracted from328.txt)
These include, but also go beyond, privacy and data protection, non-discrimination and access to justice. (extracted from328.txt)
In addition to rights concerning privacy and data protection, equality and non-discrimination, and access to justice, other rights could be considered. (extracted from328.txt)
Using AI systems engages a wide range of fundamental rights, regardless of the field of application� These include – but also go beyond – privacy, data protection, non-discrimination and access to justice � FRA OPINION 1 When introducing new policies and adopting new legislation on AI, the EU legislator and the Member States, acting within the scope of EU law, must ensure that respect for the full spectrum of fundamental rights, as enshrined in the Charter and the EU Treaties, is taken into account � Specific fundamental rights safeguards need to accompany relevant policies and laws � In doing so, the EU and its Member States should rely on robust evidence concerning AI’s impact on fundamental rights to ensure that any restrictions of certain fundamental rights respect the principles of necessity and proportionality � Relevant safeguards need to be provided for by law to effectively protect against arbitrary interference with fundamental rights and to give legal certainty to both AI developers and users � Voluntary schemes for observing and safeguarding fundamental rights in the development and use of AI can further help mitigate rights violations � In line with the minimum requirements of legal clarity – as a basic principle of the rule of law and a prerequisite for securing fundamental rights – the legislator has to take due care when defining the scope of any such AI law � Given the variety of technology subsumed under the term AI and the lack of knowledge about the full scope of its potential fundamental rights impact, the legal definition of AI-related terms might need to be assessed on a regular basis � 8 Using effective impact assessments to prevent negative effects Deploying AI systems engages a wide spectrum of fundamental rights, regardless of the field of application. (extracted from328.txt)
However, other rights, such as non-discrimination or access to justice-related rights, are less well known among business representatives who work with AI. (extracted from328.txt)
NON-DISCRIMINATION, DATA PROTECTION AND ACCESS TO JUSTICE: THREE HORIZONTAL THEMES The research shows that the use of AI affects various fundamental rights. (extracted from328.txt)
While interviewees disagree as to whether or not the existing legislation is sufficient, many called for more concrete interpretation of the existing data protection rules with respect to automated decision making, as enshrined in Article 22 of the GDPR.FRA OPINION 5 The European Data Protection Board (EDPB) and the European Data Protection Supervisor (EDPS) should consider providing further guidance and support to effectively implement GDPR provisions that directly apply to the use of AI for safeguarding fundamental rights, in particular as regards the meaning of personal data and its use in AI, including in AI training datasets � There is a high level of uncertainty concerning the meaning of automated decision making and the right to human review linked to the use of AI and automated decision making � Thus, the EDPB and the EDPS should also consider further clarifying the concepts of ‘automated decision making’ and ‘human review’ , where they are mentioned in EU law � In addition, national data protection bodies should provide practical guidance on how data protection provisions apply to the use of AI� Such guidance could include recommendations and checklists, based on concrete use cases of AI, to support compliance with data protection provisions �More clarity is needed on the scope and meaning of legal provisions regarding automated decision making � 6 13 Effective access to justice in cases involving AI-based decisions Access to justice is both a process and a goal, and is crucial for individuals seeking to benefit from other procedural and substantive rights. (extracted from328.txt)
Accordingly, the notion of access to justice obliges states to guarantee each individual’s right to go to court – or, in some circumstances, an alternative dispute resolution body – to obtain a remedy if it is found that the individual’s rights have been violated. (extracted from328.txt)
To effectively contest decisions based on the use of AI, people need to know that AI is used, and how and where to complain� Organisations using AI need to be able to explain their AI system and decisions based on AI�FRA OPINION 6 The EU legislator and Member States should ensure effective access to justice for individuals in cases involving AI-based decisions � To ensure that available remedies are accessible in practice, the EU legislator and Member States could consider introducing a legal duty for public administration and private companies using AI systems to provide those seeking redress information about the operation of their AI systems � This includes information on how these AI systems arrive at automated decisions � This obligation would help achieve equality of arms in cases of individuals seeking justice � It would also support the effectiveness of external monitoring and human rights oversight of AI systems (see FRA opinion 3) � In view of the difficulty of explaining complex AI systems, the EU, jointly with the Member States, should consider developing guidelines to support transparency efforts in this area� In so doing, they should draw on the expertise of national human rights bodies and civil society organisations active in this field� 14 15 1 AI AND FUNDAMENTAL RIGHTS – WHY IT IS RELEVANT FOR POLICYMAKING Artificial intelligence (AI) is increasingly used in the private and public sectors, affecting daily life. (extracted from328.txt)
In October 2020, the European Parliament adopted resolutions with recommendations to the European Commission on a framework of ethical aspects of AI, robotics and related technologies,28 and a civil liability regime for AI.29 It also adopted a resolution on intellectual property rights for the development of artificial intelligence technologies,30 and continues to work on resolutions on AI in criminal law and its use by the police and judicial authorities in criminal matters,31 and AI in education, culture and the audio-visual sector.32 It also established a special committee on artificial intelligence in the digital age.33 Following their meeting on 1-2 October 2020, the heads of state and government of the EU Member States declared that the “EU needs to be a global leader in the development of secure, trustworthy and ethical Artificial Intelligence” and invited the Commission to “provide a clear, objective definition of high-risk Artificial Intelligence systems.34 In addition, the Council of the EU adopted Conclusions on shaping Europe’s digital future35 and on seizing the opportunities of digitalisation for access to justice, which included a dedicated section on deploying AI systems in the justice sector.36 The German Presidency of the Council of the EU published conclusions on the Charter of Fundamental Rights in the context of artificial intelligence and digital change; the text was supported, or not objected to, by 26 Member States.37 23 The growing reference to fundamental rights in these discussions indicates that a fundamental rights framework alongside other legal frameworks38 is necessary for an effective and human rights compliant evaluation of the many opportunities and challenges brought by new technologies. (extracted from328.txt)
36 Council of the European Union, Council Conclusions “ Access to Justice – Seizing the Opportunities of Digitalisation” , 13 October 2020. (extracted from328.txt)
This is then combined with other criminal and environmental data.19 The EU and its Member States have shared competence in the area of freedom, security, and justice (Article 4 (2) (j) of the TFEU). (extracted from328.txt)
18 The Law Society Commission on the Use of Algorithm in the Justice System (2019), Algorithms in the criminal justice system , p. (extracted from328.txt)
Interferences with such fundamental rights can only be justified if they respect the requirements of the Charter and of the ECHR, in case of Charter rights corresponding to rights guaranteed in the ECHR (Article 52 (3) of the Charter).36 Pursuant to Article 52 (1) of the Charter, any limitation on fundamental rights must: ―be provided for by law, ―genuinely meet objectives of general interest recognised by the Union or the need to protect the rights and freedoms of others, ―respect the essence of the right, ―be necessary, and ―be proportionate.37 The Court of Justice of the EU (CJEU) has also emphasised that any limitation on the exercise of the rights and freedoms recognised by in the Charter must respect “the essence” of those rights and freedoms.38 This means that fundamental rights can be limited to a certain extent, but not completely disregarded. (extracted from328.txt)
22 Estonia, National Audit Office and Chancellor of Justice (2018), Everyone’s Rights in e-State: The e-State Charter . (extracted from328.txt)
(2017), Data Protection Directive (EU) 2016/680 for Police and Criminal Justice Authorities , available at SSRN . (extracted from328.txt)
Orla Lynskey (2019), Criminal justice profiling and EU data protection law: Precarious protection from predictive policing , p. (extracted from328.txt)
75 4�6� ACCESS TO JUSTICE The right to an effective remedy before a tribunal and to a fair trial (Article 47 of the Charter) is one of the most often used Charter right in legal proceedings. (extracted from328.txt)
Opportunities to successfully complain about the use of AI and challenge decisions based on AI are essential for providing access to justice. (extracted from328.txt)
Other rights linked to access to justice set out in the Charter are also impacted, most notably by the use of AI in law enforcement. (extracted from328.txt)
As a result, the requirements for good administration are directly linked to the discussion and analysis above with respect to the legal processing of data (under data protection), fair decisions (linked to the discussion about non-discrimination), alongside transparency and ways to challenge and explain decisions (with respect to access to justice). (extracted from328.txt)
33 Aleš Završnik (2019), ‘Algorithmic justice: Algorithms and big data in criminal justice settings’, European Journal of Criminology , p. (extracted from328.txt)
and Castillo C (2019), ‘Why Machine Learning May Lead to Unfairness: Evidence from Risk Assessment for Juvenile Justice in Catalonia’, Best Paper Award, International Conference on AI and Law, 2019; Richardson R., Schultz J. (extracted from328.txt)
(2019), Dirty Data, Bad Predictions: How Civil Rights Violations Impact Police Data, Predictive Policing Systems, and Justice, 94 N.Y .U. (extracted from328.txt)
47 Aleš Završnik (2019), ‘Algorithmic justice: Algorithms and big data in criminal justice settings’, European Journal of Criminology , pp. (extracted from328.txt)
See also: FRA and CoE (2016 ), Handbook on European law relating to access to justice , Luxembourg, Publications Office, June 2016, p. (extracted from328.txt)
(eds.) (2016), The Right to Social Security in the Constitutions of the World: Broadening the moral and legal space for social justice , Geneva, ILO Global Study, Vol. (extracted from328.txt)
Minister for Justice , Equality and Law Reform, Ireland, Attorney General , 8 May 2014, para. (extracted from328.txt)
Based on a risk management approach, it supports fair automated decisions and minimising unintentional harm to individuals in the field of the criminal justice, higher-education, social media and other areas. (extracted from328.txt)
For example, in the field of criminal justice, the ALGO CARE framework36 introduced a step-by-step assessment to evaluate the key legal and practical concerns that should be considered in relation to police using algorithmic risk assessment tools. (extracted from328.txt)
Some examples include: ―the right to social protection, when working with social benefits; ―the right to freedom of expression and information, when using AI to support online content moderation; ―the right of assembly and of association, when considering the use of facial recognition technology in public spaces; ―the right to education, when using AI in the education sector; ―the right to asylum, when using AI to support migration management; ―the right of collective bargaining and action, when using AI in the ‘gigeconomy’; ―the right to fair and just working conditions, when using AI at the workplace; ―the right to access preventive health care, when using AI in health services; ―and the right to the presumption of innocence and the right to defence, when using AI in the justice sector or for law-enforcement purposes. (extracted from328.txt)
The resources in the programme will be used in areas where they are expected to be most effective, such as health, justice, consumer protection and publi c administration. (extracted from466.txt)
Norway, represented by the Ministry of Justice and Public Security, participates in this work. (extracted from466.txt)
The Ministry of Justice and Public Security and the Ministry of Defence have overarching responsibility for following up the National Cyber Security Strategy for Norway. (extracted from466.txt)
46 Ministries (2019): National Cyber Security Strategy 47 Ministry of Justice and Public Security (2019): National Strategy for Cyber Security Competence 65 Artificial intelligence in law enforcement The Norwegian Police University College and NTNU in Gjøvik are cooperating on a project that examines the use of different forms of artificial intelligence for analysing big data, aimed at detecting, preventing a nd investigating economic crime. (extracted from466.txt)
The Commissioner for Human Rights11, the Consultative Commit tee of the Convention for the Protection of Individuals with regard to Automatic Processing of Personal Data (T -PD12) and the European Commission for the Efficiency of Justice (CEPEJ13) use a relatively similar generic definition referring to a set of scienc es, theories and techniques. (extracted from116.txt)
the reports of the Parliamentary Assembly of the Council of Europe, in particu lar on the need for democratic governance of AI ; the role of AI in policing and criminal justice systems ; preventing discrimination caused by AI ; ethical and legal frameworks for the research and development of neurotechnology ; AI and health care ; consequences of AI on labour markets ; and legal aspects of ‘autonomous vehicles’ . (extracted from116.txt)
5, 6, 7 ECHR) when these systems are used in situations where physical freedom or personal security is at stake (such as justice and law enforcement). (extracted from116.txt)
If applied responsibly and with prudence, however, certain AI applications can also make the work of justice and law enforcement professionals more efficient and hence have a positive impact on these rights. (extracted from116.txt)
4.4 Work in the field of justice 50. (extracted from116.txt)
The European Commission for the Efficiency of Justice (CEPEJ) adopted in December 2018 the European Ethical Charter for the use of artificial intelligence in judicial systems75 which sets five key principles (respect of fundamental rights, non -discriminatio n, quality and security, transparency, impartiality and fairness, "under the control" of the user) for the use of AI systems in this field. (extracted from116.txt)
On 22 October 2020, the PACE adopted 7 reports, focusing on: the need for democratic governance of AI; the role of AI in policing and criminal justice systems; discrimination caused by AI; threats to fundamental freedoms; medical, legal and ethical challenges in the fie ld of health care; consequences on labour markets; and legal aspects of ‘autonomous vehicles’. (extracted from116.txt)
19 non-binding instruments in four core areas (data protection, health, de mocracy and justice) and was complemented by an overview of the Council of Europe’s instruments in other fields. (extracted from116.txt)
The principles of privacy, justice and fairness showed the least variation across Council of Europe’s member States, observers and the rest of the world, and hence the highest degree of cross -geographical and cross -cultural stability. (extracted from116.txt)
The development and use of AI systems has also been considered in sectorial strategies on agriculture, e -justice, public services, health, environment, education, security and defence, mobility and data. (extracted from116.txt)
❖ Key obligations: o Member States are obliged to ensure that the AI systems they deploy do not result in unlawful discrimination, harmful stereotypes (including but not limited to gender stereotypes) and wider social inequality, and should therefore apply the highest level of scrutiny when using or promoting the use of AI systems in sensitive public policy areas, including but not limited to law enforcement, justice, asylum and migration, health, social security and employment. (extracted from116.txt)
This should also include the possibility to get insight into and challenge a n AI-informed decision in the context of law enforcement or justice, including the right to review of such decision by a human. (extracted from116.txt)
❖ Key obligations o Member States must ensure that AI systems used in the field of justice and law enforcement are in line with the essential requirements of the right to a fair trial. (extracted from116.txt)
Moreover, this would secure access to justice should they fail to meet these obligations.173 125. (extracted from116.txt)
The Foundation funds research that informs social policy, primarily in education, welfare and justice. (extracted from17.txt)
In addition to the Ada Lovelace Institute, the Foundation is also the founder and co-funder of the Nuffield Council on Bioethics and the Nuffield Family Justice Observatory. (extracted from17.txt)
117IIHetNederlandsgrondrechtelijkkader –Persoonlijke onafhankelijkheid –ermoeten voldoende waarborgen worden geboden rondom benoeming, ambtsduur enontslag, zodat andere staatsmachten niet alteveel invloed kunnen hebben ophetsoort rechter datwordt benoemd engeen druk kunnen uitoefenen oprechters omopeenbepaalde manier tebeslissen.514 –Zakelijke offunctionele onafhankelijkheid –ermoeten voldoende garanties zijntegen druk vanbuitenaf omzaken opeenbepaalde manier tebeslechten enderechter moet zelfstandig, zonder inmenging vandeandere staatsmachten, totzijnbeslissing kunnen komen.515 –Institutionele onafhankelijkheid –deoverheidsorganisatie waarvan rechters deel uitmaken moet eenonafhankelijke positie kunnen innemen tenaanzien van deandere staatsmachten, bijvoorbeeld alshetgaat omfinanciering, organisatie enwerkwijze.516 –Onpartijdigheid –derechter hoort onpartijdig tezijn, watvooral betekent dathijniet vooringenomen istenopzichte vandezaak oftenopzichte van(een van) departijen.517 Vooral indeEHRM-rechtspraak worden hierbij twee aspecten onderscheiden: –Subjectieve onpartijdigheid –derechter mag geen zodanige verbondenheid hebben met dezaak ofmet (een van) debeide partijen diemaakt dathijniet meer objectief over dezaak kanoordelen; hetgaat hier dusomzijnpersoonlijke instelling enovertuiging.518 –Objectieve onpartijdigheid –deprocedure moet zozijningericht datergeen legitieme twijfel mogelijk isover deobjectiviteit vanderechter (ofeenrechterlijk college) bijde beoordeling vaneenzaak, onder hetmotto ‘justice must notonly bedone, itmust also seen tobedone ’.519 Derechtspraak over deze twee aspecten issterk casuïstisch vanaard, maar hetelement vanontbrekende ‘bias’–hetzij subjectief, hetzij indeogen vandebuitenstaander –komt steeds weer terug. (extracted from704.txt)
Safety and Justice Program 2013, online via;https://www. (extracted from704.txt)
23In considering the applications of AI in areas such as criminal justice and health care, organizations should design, build and deploy AI systems that leverage human judgment and responsibility where they are needed. (extracted from274.txt)
Suggested lead: Department of Commerce, Department of State, Department of Justice. (extracted from274.txt)
Suggested lead: Department of Commerce, Department of State, Department of Justice. (extracted from274.txt)
Other Legislation Convention on Access to Information, Public Participation in Decision-Making and Access to Justice in Environmental Matters (Aarhus Convention). (extracted from258.txt)
What constitutes a sufficient interest and impairment of a right shall be determined consistently with the objective of giving the public concerned wide access to justice. (extracted from258.txt)
Decision (2.6.) The term ‘decision’ is broader than typical definitions of administrative decision in national administrative procedure or administrative justice legislation (compare, eg, Article III-2 (1) of the ReNEUAL Model Rules). (extracted from258.txt)
(16.5.) Paragraph 5 recalls some of the characteristics necessary for effective access to justice. (extracted from258.txt)
Indeed, several notions of fairness exist that are not only technically defined but also entangled with concepts of social justice, specifically the concept of privilege , held by virtue of belonging to certain social identity groups 34. (extracted from270.txt)
These values are common to the Member S tates in a society in which pluralism, non -discrimination, tolerance, justice, solidarity and equality between women and men prevail. (extracted from270.txt)
' Furthermore, its article 3(3) states that the Uni on 'shall combat social exclusion and discrimination, and shall promote social justice and protection, equality between women and men, solidarity between generations and protection of the rights of the child. (extracted from270.txt)
Even worse, this contextually limited interpretation of the concept of discrimination has been endorsed by the Court of Justice of the EU43. (extracted from270.txt)
There is a nice argument that supports such additional use of the concept: even though the European Court of Justice has never defined the notion of fairness in data protection law, it has used this notion of fairness in two different contexts: f air balance and transparency.48 If we consider that fairness has to do with the reasonable expectations of data subjects, then it should help us to avoid some of the discriminatory results that are not so easy to uncover : a data subject would hardl y allow t he type of processing that would cause him/her to suffer a damage that other people do not suffer. (extracted from270.txt)
' Fairness in Criminal Justice Risk Assessments: The State of the Art. (extracted from270.txt)
Gerards, J., Xenidis, R., Algorithmic discrimination in Europe: challenges and opportunities for gender equality and non -discrimination law, European Commission, Directorate -General for Justice and Consumers, Publications Office, 2021, https://data.europa.eu/doi/10.2838/77444 Gianclaudio Malgieri and Vincenzo Tiani, How the EU Council is rewriting the AI Act, REPORT - 6 December 2021, December 2021, at: https://brusselsprivacyhub.eu/publications/how -the-eu-council -isrewriting -the-ai-act, last accessed 24/03/2022. (extracted from270.txt)
The jurisprudence of the Court of Justice of the EU on the use of AI 35 2.3.1. (extracted from264.txt)
Policy recommendations for the best use of AI in the fisheries and its value chain 74 REFERENCES 76 Artificial Intelligence and the fisheries sector 5 LIST OF ABBREVIATIONS AFMA AI Australian Fisheries Management Authority Artificial Intelligence AIA proposal Proposal for a Regulation of the European Parliament and of the Council laying down harmonised rules on artificial intelligence (A rtificial Intelligence Act) AIS ANN BN Automatic Identification System Artificial Neural Network Bayesian Network CAP CCTV Common Agricultural Policy Closed -Circuit Television CFP Common Fisheries Policy CJEU CNN COM Court of Justice of the European Union Convolutional neural network Common Organi sation of the Markets DCF DL EGD EM EMFAF Data Collection Framework Deep Learning European Green Deal Electronic Monitoring European Maritime, Fisheries and Aquaculture Fund EU EUMOFA FAO European Union European Market Observatory for Fisheries and Aquaculture Food and Agriculture Organisation of the United Nations FCR Fisheries Control Regulation IPOL | Policy Department for Structural and Cohesion Policies 6 FPS Frontal protection systems FROODS Fishing Route Optimization Decision Support System GDP Gross Domestic Product GDPR GES GVA H2020 IATTC ICCAT General Data Protection Regulation Good Environmental Status Gross Value Added Horizon 2020 EU research and innovation funding programme Inter American Tropical Tuna Commission International Commission for the Conservation of Atlantic Tunas ICES IMO IOTC IUU LO MCRS ML International Council for the Exploitation of the Sea International Maritime Organization Indian Ocean Tuna Commission Illegal, Unreported and Unregulated Landing obligation Minimum Conservation Reference Size Machine Learning MPI MSFD MSY NMFS NOAA Ministry of Primary Industries of New Zealand Marine Strategy Framework Directive Maximum Sustainable Yield National Marine and Fisheries Services North Oceanic and Atmospheric Administration OJ PECH Committee PET Official Journal European Parliament’s Committee on Fisheries Protected Endangered and Threatened species Artificial Intelligence and the fisheries sector 7 RFMO SDG SME STECF SVM TAC TEU TFEU UN Regional fisheries management organi sation Sustainable Development Goal Small and Medium Enterprise Scientific Technical Economic Committee on Fisheries Support Vector Machine Total Allowable Catches Treaty on European Union Treaty on the Functioning of the European Union United Nations US VMS WCPFC United States of America Vessel Monitoring System Western and Central Pacific Fisheries Commission IPOL | Policy Department for Structural and Cohesion Policies 8 LIST OF FIGURES Figure 1: Conceptual diagram showing the role of AI in relation with other digitalisation activities 15 Figure 2: Classification of AI techniques and approaches in the AIA proposal expanded with further subcategories used in this study 42 Figure 3: Simplified diagram of an expert system 47 Figure 4: Simplified schema of the Fish Value Chain 55 Figure 5: Traceability aspects improvable by AI systems 56 Figure 6: Published papers related to fisheries, AI and traceability keywords 57 LIST OF TABLES Table 1: Non-exhaustive summary of the recitals and articles of the most relevant EU fisheries legislation containing elements making the use of AI systems possible 31 Table 2: Similarities, differences and keywords associated to specific ML methods 43 Artificial Intelligence and the fisheries sector 9 EXECUTIVE SUMMARY This study reviews the main applications of Artificial Intelligence (AI) systems in fisheries and identifies current challenges for fisheries that have the potenti al to be dealt with through AI. (extracted from264.txt)
The third section (2.3) include s a reference to the scarce jurisprudence of the Court of Justice of the Europ ean Union (CJEU) that has so far, directly , or indirectly, considered AI related issues with projection on the fisheries sector. (extracted from264.txt)
58 European Commission, Directorate -General for Justice and Consumers, Liability for artificial intelligence and other emerging digital technologies, Publications Office, 2019. (extracted from264.txt)
The jurisprudence of the Court of Justice of the EU on the use of AI The CJEU as the judicial authority of the EU by virtue of Article 19 TEU, in cooperation with the judicial bodies of the EU Member States, ensures the uniform application and interpretation of EU law. (extracted from264.txt)
A n appeal has now been brought against this judgment before the Court of Justice 61. (extracted from264.txt)
Case on illegal mechanical device for fish classification A Court of Justice case was found where an illegal mechanical fish classification device was used . (extracted from264.txt)
In its judgment of 11 February 2021 in Case K.M.62, the Court of Justice ruled on the reference for a preliminary ruling received from the Irish Court of Appeal under Article 267 TFEU. (extracted from264.txt)
62 Judgment of the Court of Justice of 11 February 2021, K.M., C -77/20, ECLI:EU:C:2021:112. (extracted from264.txt)
The Court of Justice was asked whether Article 89 and Article 90 of FCR , read in the light of the principle of proportionality enshrined in Article 49(3) of the Charter, were to be interpreted as precluding a national provision which, in order to penalise a n infringement of Article 32 of Regulation No 850/98, provided for the imposition of a fine and the mandatory confiscation of prohibited or non -compliant catches and fishing gear found on board the vessel concerned (paragraph 25). (extracted from264.txt)
The Cou rt of Justice found that the mandatory confiscation of prohibited or non -compliant catches and fishing gear may deter the persons concerned from infringing the prohibition on sorting equipment, laid down in Article 32(1) of Regulation No 850/98, by deprivi ng them of the illegally obtained benefits which they could otherwise enjoy, and of the possibility of continuing to use such equipment (paragraph 44). (extracted from264.txt)
65 Among other judgments, see: judgment of the Court of Justice of 16 July 2015, Chmielewski, C -178/03, ECLI:EU:C:2015:475, paragraph 21. (extracted from264.txt)
ISP DIGITAL FUTURE WHITEPAPER & YJoLT SPECIAL PUBLICATION Algorithms and Economic Justice: A Taxonomy of Harms and a Path Forward for the Federal Trade Commission Rebecca Kelly Slaughter with Janice Kopec and Mohamad Batal August 2021 Contents Algorithms and Economic Justice .................................................................................... (extracted from728.txt)
59 Algorithms and Economic Justice | Rebecca Kelly Slaughter 1 Algorithms and Economic Justice: A Taxonomy of Harms and a Path Forward for the Federal Trade Commission The proliferation of artificial intelligence and algorithmic decision-making has helped shape myriad aspects of our society: from facial recognition to deepfake technology to criminal justice and health care, their applications are seemingly endless. (extracted from728.txt)
As an FTC Commissioner, I aim to promote economic and social justice through consumer protection and competition law and policy. (extracted from728.txt)
The applications of these technologies are innumerable, from facial recognition to deepfake technology, criminal justice, and health care. (extracted from728.txt)
Algorithmic decision-making, and the AI that fuels it, could realize its promise of promoting economic justice by distributing opportunities more broadly, resources more efficiently, and benefits more effectively. (extracted from728.txt)
Algorithms and Economic Justice | Rebecca Kelly Slaughter 3 transformed access to educational opportunities3 and improved health outcomes through improved diagnostic rates and care adjustments.4 But the potentially transformative power of algorithmic decision-making also risks serious harm if misused. (extracted from728.txt)
In the criminal justice system, for example, commentators note that algorithms and AI contribute to over-surveillance,5 wrongful detainment and arrest,6 and biased risk assessments used to determine pre-trial status and even sentencing.7 Mounting evidence reveals that algorithmic decisions can produce biased, discriminatory, and unfair outcomes in a variety of high-stakes economic spheres including employment, credit, health care, and housing.8 3 See, e.g., Matt Kasman & Jon Valant, The Opportunities and Risks of K-12 Student Placement Algorithms, BROOKINGS INST. (extracted from728.txt)
7 See, e.g., Algorithms in the Criminal Justice System: Pre-Trial Risk Assessment Tools, ELEC. (extracted from728.txt)
CTR., https://epic.org/algorithmic-transparency/crim-justice (last visited Jan. (extracted from728.txt)
Algorithms and Economic Justice | Rebecca Kelly Slaughter 5 of some of the algorithmic harms that threaten to undermine economic and civil justice.13 I identify three ways in which flaws in algorithm design can produce harmful results: faulty inputs, faulty conclusions, and failure to adequately test. (extracted from728.txt)
I hope to draw the attention and ingenuity of the interested public to the challenges posed by algorithms so that we can work together on creating an enforcement regime that advances economic justice and equity. (extracted from728.txt)
David Edelman, “AI is not magic; it is math and code.”15 As we consider the threats that algorithms pose to justice, we must remember that just as the technology is not magic, neither is any cure to its shortcomings. (extracted from728.txt)
Algorithms and Economic Justice | Rebecca Kelly Slaughter 7 two subparts. (extracted from728.txt)
The second subpart describes three ways in which even sophisticated algorithms still systemically undermine civil and economic justice. (extracted from728.txt)
Algorithms and Economic Justice | Rebecca Kelly Slaughter 9 thousands of dollars in tuition.19 According to the IB, 60 percent of US public schools that offer IB classes are Title I schools,20 and numerous IB students reportedly saw their college scholarships or admissions offers rescinded because the algorithm assigned them unexpectedly low test scores.21 In a similar case, the United Kingdom used an algorithm to replace its A-Level exams—which play a pivotal role in university admissions there—before ultimately retracting the scores in response to widespread protests. (extracted from728.txt)
Algorithms and Economic Justice | Rebecca Kelly Slaughter 11 products can accurately detect an individual’s emotional state by analyzing her facial expressions, eye movements, tone of voice, or even gait.26 The underlying algorithms attempt to find patterns in, and reach conclusions based on, certain types of physical presentations and mannerisms. (extracted from728.txt)
Algorithms and Economic Justice | Rebecca Kelly Slaughter 13 suggests that other products in this space can suffer from major structural deficiencies.33 Indeed, Princeton’s Arvind Narayanan, a computer scientist, has criticized AI tools that claim to predict job performance based on body language and speech patterns as “fundamentally dubious.”34 Such algorithmic hiring products merit skepticism in any application, and recent studies suggest they might systematically disadvantage applicants with disabilities because they present differently than the majority of a company’s applicants or employees.35 These reports should trouble any employer using an algorithmic hiring product to screen applicants. (extracted from728.txt)
For example, companies can use a five-second clip of a person’s Algorithms and Economic Justice | Rebecca Kelly Slaughter 15 phenomenon with which we are quite familiar at the FTC: new technology, same old lack of substantiation for claims.40 Failure to Test Even if an algorithm is designed with care and good intentions, it can still produce biased or harmful outcomes that are unanticipated. (extracted from728.txt)
24, 2019), https://www.sciencenews.org/article/bias-common-health-care-algorithm- Algorithms and Economic Justice | Rebecca Kelly Slaughter 17 of Black patients identified for extra care was reduced by more than half. (extracted from728.txt)
Since we weren’t Algorithms and Economic Justice | Rebecca Kelly Slaughter 19 This kind of bias can have meaningful real-world consequences: in this case, that profiles with female-identified names turned up less frequently, potentially resulting in fewer employment opportunities for women. (extracted from728.txt)
Algorithms and Economic Justice | Rebecca Kelly Slaughter 21 Facebook’s use of Lookalike Audiences that facilitated housing discrimination presents one of the clearest illustrations of proxy discrimination. (extracted from728.txt)
Algorithms and Economic Justice | Rebecca Kelly Slaughter 23 into both the inputs and the formulae used to make those decisions. (extracted from728.txt)
The proliferation of other AI-driven technologies, such as deepfakes and rapidly improving algorithmic text generation, can further exacerbate the Algorithms and Economic Justice | Rebecca Kelly Slaughter 25 radicalization,74 undermines consumers’ mental health,75 and reduces or eliminates consumers’ choices.76 disinformation problem. (extracted from728.txt)
Algorithms and Economic Justice | Rebecca Kelly Slaughter 27 platform starting from benign videos.”78 While most of the toddler-oriented content on YouTube is innocuous, the authors highlight an influx of disturbing or inappropriate content that targets young children, as in the infamous “Elsagate” controversy. (extracted from728.txt)
Algorithms and Economic Justice | Rebecca Kelly Slaughter 29 Despite this requirement, I voted against the settlement. (extracted from728.txt)
Algorithms and Economic Justice | Rebecca Kelly Slaughter 31 The FTC recently announced a timely and important section 6(b) study90 of nine social media and video-streaming services—an industry where the potential for this subtle, data-driven manipulation is clear and obvious. (extracted from728.txt)
But the FTC is also responsible for promoting competition, and the threats posed by algorithms profoundly affect that mission as well; moreover, these two missions are not actually distinct, and problems—including those related to algorithms and economic justice—need to be considered with both competition and consumer protection lenses. (extracted from728.txt)
Algorithms and Economic Justice | Rebecca Kelly Slaughter 33 algorithms to execute a price-fixing agreement has even given rise to criminal antitrust charges.97 Algorithms may enhance the ability of firms to collude, either tacitly or explicitly.98 While there have been limited cases of enforcement against collusion facilitated by algorithms, it is unclear whether the conduct is in fact not occurring or whether it is simply very difficult for enforcers to detect. (extracted from728.txt)
Moving forward, competition enforcers may deploy 97 In 2015, the US Department of Justice brought criminal charges against two e-commerce companies in United States v. (extracted from728.txt)
Dep’t of Justice, Former E-Commerce Executive Charged with Price Fixing in the Antitrust Division’s First Online Marketplace Prosecution (Apr. (extracted from728.txt)
6, 2015), https://www.justice.gov/opa/pr/former-e-commerce-executive-charged-price-fixing-antitrust-divisions-first-online-marketplace. (extracted from728.txt)
Algorithms and Economic Justice | Rebecca Kelly Slaughter 35 $99.103 This practice does not always result in price increase, but it can. (extracted from728.txt)
Algorithms and Economic Justice | Rebecca Kelly Slaughter 37 rivals. (extracted from728.txt)
Using the FTC’s Current Authorities to Better Protect Consumers There is no question that the critical algorithmic problems identified—faulty inputs, faulty conclusions, failure to adequately test, proxy discrimination, surveillance capitalism, and threats to competition—undermine rather than advance economic justice. (extracted from728.txt)
The FTC has four types of enforcement authority that provide the agency with some ability to protect consumers and promote economic justice in the face of algorithmic harms: our general authority under the FTC Act; sector-specific rules and statutes, such as FCRA, ECOA, and COPPA; the study authority of section 6(b); and the rulemaking authority of section 18. (extracted from728.txt)
Algorithms and Economic Justice | Rebecca Kelly Slaughter 39 the agency has been able to apply the statute’s general language to meet new enforcement challenges. (extracted from728.txt)
Algorithms and Economic Justice | Rebecca Kelly Slaughter 41 authority cannot be used to combat the fundamentally unfair phenomenon of unlawful discrimination, as well as some of the other algorithmic harms discussed above. (extracted from728.txt)
Algorithms and Economic Justice | Rebecca Kelly Slaughter 43 should encourage, non-mortgage creditors to collect demographic data on most borrowers and use it to test algorithmic systems to reduce disparities.128 Vanishingly few creditors take advantage of this exception. (extracted from728.txt)
Algorithms and Economic Justice | Rebecca Kelly Slaughter 45 decision-making poses unique challenges in this area.135 Expanding data-reporting requirements under FCRA—for example, broader reporting on the existence and correction of errors, the rates of adverse action notices, and the volume and nature of error complaints—could also help mitigate problems that arise in algorithmic decision-making by providing visibility into the effects of those decisions. (extracted from728.txt)
(L 330) (requiring companies meeting certain criteria to publish information on their social and environmental practices Algorithms and Economic Justice | Rebecca Kelly Slaughter 47 about specific businesses, the FTC can also collect information about industry-wide phenomena.144 The study of social media and video-streaming services discussed above is one exciting use of this important tool.145 The FTC should continue to use its 6(b) authority to deepen its expertise on the use and impact of algorithms in our modern economy, focusing on the potential harms to consumers and competition.146 Whether in enforcement or with industry studies, the agency always strives to keep pace with emerging technologies and changing markets.147 But particularly given the scale, opacity, and rapid proliferation of algorithmic decision-making in our economy, there is room for improvement. (extracted from728.txt)
Algorithms and Economic Justice | Rebecca Kelly Slaughter 49 complicated and facially neutral technology provides a false sense of security in the objectivity of algorithmic decision-making. (extracted from728.txt)
Algorithms and Economic Justice | Rebecca Kelly Slaughter 51 the increase of transparency, equity, and accountability in its work—including through Executive Orders.159 Prioritizing transparency and fairness is necessary, but not sufficient; regulation of algorithmic decision-making must also involve real accountability and appropriate remedies. (extracted from728.txt)
Algorithms and Economic Justice | Rebecca Kelly Slaughter 53 promulgating rules under section 18, resulting in a variety of rules that protect consumers.166 In recent years, however, the Commission has shied away from extensive section 18 rulemaking.167 The new Democratic majority at the Commission has already taken action to make section 18 rulemaking more viable by bringing Commission procedures in line with statutory requirements and congressional intent. (extracted from728.txt)
Algorithms and Economic Justice | Rebecca Kelly Slaughter 55 In the area of algorithmic justice, a section 18 rule might affirmatively impose requirements of transparency, fairness, and accountability. (extracted from728.txt)
While privacy legislation may not seem directly applicable to the problems we are discussing today, it can in fact play an important role in addressing algorithmic justice—and it is worth noting that the algorithmic-justice requirements imposed in Europe were done as a part of its privacy law, the GDPR. (extracted from728.txt)
The Algorithmic Justice and Online Platform Transparency Act also provides strong civil rights provisions, building upon the transparency requirements of the Algorithmic Accountability Act and adding the civil rights protections Algorithms and Economic Justice | Rebecca Kelly Slaughter 57 of data on the basis of an individual’s actual or perceived protected status for the purpose of marketing in a manner that unlawfully discriminates or otherwise makes the opportunity unavailable to the individual or class of individuals.178 The proposed bill also prohibits the processing or transfer of data in a manner that unlawfully segregates, discriminates against, or otherwise makes unavailable the goods, services, or facilities of any place of public accommodations. (extracted from728.txt)
See Algorithmic Justice and Online Platform Transparency Act S. (extracted from728.txt)
Algorithms could promote economic justice by helping distribute opportunities more broadly, resources more efficiently, and benefits more effectively. (extracted from728.txt)
Algorithms and Economic Justice | Rebecca Kelly Slaughter 59 Acknowledgements I would like to thank the many colleagues and experts who generously helped this article take shape through their insights and contributions. (extracted from728.txt)
Copyright © United Nations Interregional Crime and Justice Research Institute (UNICRI), 2020 Viale Maestri del Lavoro,10, 10127 Torino – Italy Tel: +39 011-6537 111 / Fax: +39 011-6313 368 Website: www.unicri.it E-mail: unicri.publicinfo@un.org © The International Criminal Police Organization (INTERPOL), 2020 200, Quai Charles de Gaulle, 69006 Lyon – France Tel: +33 4 72 44 70 00 / Fax: +33 4 72 44 71 63 Website: www.interpol.int E-mail: edgci-ic@interpol.int 2 FOREWORD Crime is not stagnant. (extracted from660.txt)
We have strived to shape this forum, giving it meaning and purpose, and positioning it to grow into a global platform for cooperation and collaboration amongst law enforcement on AI This report on AI for law enforcement is the most recent product of the collaboration on AI between the Innovation Centre of the International Criminal Police Organization (INTERPOL) and the United Nations Interregional Crime and Justice Research Institute’s (UNICRI) Centre for AI and Robotics. (extracted from660.txt)
The increasing interest and attention these meetings are receiving is both a reward for INTERPOL and UNICRI and reveals the growing relevance of AI for the criminal justice community. (extracted from660.txt)
Human rights, civil liberties and even the fundamental principles of law upon which our criminal justice system is based may be unacceptably exposed, or even irreparably compromised, if we do not navigate this route with extreme caution. (extracted from660.txt)
The chapter be gins by presenting the general principles that law enforcement should endeavour to adhere to, namely the respect for human rights, democracy, justice and rule of law, as well as the related requirements of fairness, accountability, transparency and explainability that should be adopted in order for law en forcement to meet these principles. (extracted from660.txt)
Equally, it is a valuable exercise for policy- and deci sion-makers in the broader criminal justice community to, from a legal and ethical perspective, prepare frameworks for the eventual integration of such technologies into law enforcement. (extracted from660.txt)
The seminal 2019 white paper AI and Ethics at the Police by Leiden University and TU Delft16 suggests that, from a legal perspective, to act responsibly means “to accept moral integrity and authenticity as ideals and to deploy reasonable effort toward achieving them.”17 Striving for moral integrity, in turn, implies “adhering to the values of freedom, equality, and solidarity.”18 For the purposes of this report, however, a more straightforward understanding will be adopted and the term ‘responsible’ will be framed in line with the Oxford Dictionary, which defines ‘responsibly’ as acting “in a sensible or trustworthy manner.”19 In this context, the responsible use of AI by law enforcement should be understood as use that enshrines the general principles of respect for human rights, democracy, justice and the rule of law. (extracted from660.txt)
Justice for Hedgehogs . (extracted from660.txt)
It has been heralded for its role in building “an area of freedom, security and justice with a high level of data protection, in accordance with the EU Charter of Fundamental Rights.” Aiming at protecting individuals’ personal data, while guaran teeing a high level of public security, the LED provides rights for data subjects, as well as obligations for “competent authorities” when processing data for “law enforcement purposes”, i.e., prevention, investi gation, detection, prosecution of criminal offences or the execution of criminal penalties, including the safeguarding against and the prevention of threats to public security. (extracted from660.txt)
The development of trustworthy AI systems should be based upon established fundamental values, such as the respect for human dignity, democracy, justice and rule of law, while, at the same time, guaranteeing the freedom of the individual and citizens’ rights in order to ensure equality and non-dis crimination. (extracted from660.txt)
45 RECOMMENDED ACTIONS Define a statement of principles on the use of AI in law enforcement that will guide law enforcement to ensure respect for human rights, democracy, justice and the rule of law and support it to prioritize the key requirements of fairness, accountability, transparen cy and explainability, as well as safety and robustness; ›Develop guidance for law enforcement on the implementation of new technology to support and encourage law enforcement agencies to explore and invest in new AI opportunities and to develop training in new AI applications and disseminate best practices; ›Create a knowledge-base with the law enforcement community on the requiremen ts for the adoption of AI, such as what kinds of problems AI is capable of tackling, the current or inherent limitations and the resources (tools, data, expertise, com puting power) required to implement AI solutions; ›Develop guidance for law enforcement on the admissibility of AI in court that as sesses the impact and results of the specific use of AI in courts, while ensuring the respect for human rights and rule of law; ›Create an expert advisory committee that can provide guidance to law enforcement in terms of legislation and serve as a forum for discussing appropriate legislative models with legal experts and other key stakeholders; ›Identify an external global body to provide advisory support to law enforcement on ethical issues and to provide support in carrying out audits to check whether a system is responsible and complies with legal requirements; ›Foster a community and organize training courses and workshops to attract and connect different stakeholders from law enforcement, industry, academia, civil society and international bodies with the diverse backgrounds and essential per spectives to gather and synthesize views from cross-sections of society, in order to provide a balanced and facts-based picture of the opportunities and challenges of the use of AI and to highlight the application of AI to law enforcement and provide hands-on support. (extracted from660.txt)
52 ANNEX II LIST OF ABBREVIATIONS ADM Automated Decision-Making AFP Australian Federal Police AI Artificial intelligence AI-HLEG High-Level Expert Group on AI AiLECS Artificial Intelligence for Law Enforcement of Community Safety CCTV Closed-Circuit Television EU European Union FATE Fairness, Accountability, Transparency and Explainability GDPR General Data Protection Regulation GPS Global Positioning Services IC INTERPOL’s Innovation Centre IEDs Improvised Explosive Devices IGCI INTERPOL’s Global Complex for Innovation INTERPOL International Criminal Police Organization LED Law Enforcement Directive 2016/680 MAS Monetary Authority of Singapore NLP Natural Language Processing non-POI non-Person of Interest NPA Japan National Police Agency OECD Organisation for Economic Co-operation and Development R&D Research and Development UAV Unmanned Aerial Vehicles UNICRI United Nations Interregional Crime and Justice Research Institute ZITiS Central Office for Information Technology in the Security Sector 53 ABOUT INTERPOL INTERPOL is the world’s largest international police organiza tion. (extracted from660.txt)
ABOUT UNICRI The United Nations Interregional Crime and Justice Research Institute was established in 1968. (extracted from660.txt)
Within the broad scope of its mandate, the Institute contributes, through research, training, field activities and the collection, exchange and dissemination of information, to the formulation and implementation of improved policies in the field of crime prevention, justice and emerging security threats, due regard being paid to the integration of such policies within broader policies for socio-economic change and development, and to the protection of human rights. (extracted from660.txt)
In 2017, UNICRI opened its Centre for Artificial Intelligence and Robotics in The Hague, the Netherlands, with a view towards advancing understanding of artificial intelligence, robotics and related technologies vis-à-vis crime prevention, criminal justice, the rule of law and security. (extracted from660.txt)
And it was in this mindset that the CNIL set a collective approach in motion, for several months overseeing a public debate with the help of partners from various sectorial fields (health, justice...). (extracted from106.txt)
Among them, we could mention the “Ligue de l’Enseignement” (associa tion that focused on education concerns), French Insu rance Federation (FFA), French Ministry of Culture (DG-MIC), Open Law (association that reflects on the justice system) as well as trade unions such as CFE-CFC and FO Cadres (for recruitment and HR), etc.An innovative approach to crafting a collective and pluralist ethical thought process Ethical thinking concerns decisive societal choices. (extracted from106.txt)
The views of the diverse stakeholders (trade unions, associations, businesses, researchers, citizens, etc.) across a wide range of sectors (from insurance to education, justice and healthcare) thus informed the writing of this report, which provides an overview of the ethical matters raised by algorithms and artificial intel ligence in their current applications and their potential uses in the relatively short term. (extracted from106.txt)
THE ETHICAL MATTERS RAISED BY ALGORITHMS AND ARTIFICIAL INTELLIGENCE ALGORITHMS AND ARTIFICIAL INTELLIGENCE TODAY 22 Education Justice Health Security Work, HR Culture Other Generating knowledgeBetter identify learners’ abilitiesReveal the different ways judgments are handed down between regionsTap into the vast amount of scientific publicationsIdentify unsuspected links for solving gendarmerie-led investigations Understand social phenomena in the workplaceCreate cultural showpieces (painting, music)Fine-tune an insurance company customer’s risk profile MatchingAllocate higher education places to candidates (APB)Allocate patients for participation in a clinical trialMatch a list of applicants to a job vacancyMatch “compatible” profiles on dating apps, etc. (extracted from106.txt)
The next step of what some refer to as “predictive justice” would involve entrusting systems with the task of making decisions based on a cross-analysis of the data pertaining to a certain case, with case-law data.Delegating tasks to algorithms: contrasting situations What immediately becomes clear is that concern over the potential ethical and social implications of automated systems varies depending on the tasks being delegated to the latter and the very conditions shaping this delegation. (extracted from106.txt)
In the report it submitted to the CNIL, the Conseil National des Barreaux, the national institution that represents all practising lawyers in France, highlighted that “care must be taken to ensure that the obsession for effectiveness and predictability behind the use of algorithms does not lead to us designing legal rules and categories no longer on the grounds of our ideal of justice, but so that they are more readily ‘codable’”. (extracted from106.txt)
A question of scale: the massive delegation of non-critical decisions Should ethical thinking on algorithms and artificial intel-ligence be limited to crucial decisions, sectors where the impact on humans is undeniable, such as medicine, justice, educational guidance, and even the automotive sector with its implications in terms of safety? (extracted from106.txt)
Practical examples of algorithms being used by the authorities as well as the example of predictive justice give a clearer idea of this ambivalence, between the optimising and diminishing of processes stripped of their spatial dimension. (extracted from106.txt)
Similarly, at the symposium on predictive justice orga nised on 19 May 2017 by the Lille Bar, Law Department of Université catholique de Lille and the Douai Court of Appeal, certain participants stressed that “knowledge of judgments given by the other neighbouring jurisdictions or by the other magistrates would contribute towards a certain consistency and prevent that the outcome of a dispute depends on knowing whether it is heard in a city or another”. (extracted from106.txt)
The same line of thinking could be applied to the idea of a predictive justice. (extracted from106.txt)
Predictive justice applications are being subjected to particularly close public scrutiny as regards their Management Profile for Alternative Sanction) tool designed to come up with a recidivism risk score for prisoners or defendants on trial. (extracted from106.txt)
Medicine and justice are other sectors where this question might be asked. (extracted from106.txt)
IIt is crucial to guard against excessive trust by raising awar eness of the ethical dimensions of a decision-making process that must not exclude human intervention and by honing critical thinking in some particularly sensitive sec tors, such as medicine, recruitment, justice and perhaps now marketing above all, where the antisemitic categories recently generated by Facebook’s machine learning algorithms are a stark wakeup call to the sharpness of the risks. (extracted from106.txt)
The partners in the public debate • Allistene’s research committee on ethics (CERNA) • Bordeaux’s Cognitique Institute (ENSC) • Bordeaux University • Caisse des dépôts et consignations (CDC) • Club des Juristes (thinktank) • Collège des Bernardins • Complex Systems Institute of Paris Ile-de-France (ISC-PIF) • Confédération française de l’encadrement – Confédération générale des cadres (CFE-CGC, trade union) • Communication Publique • Conseil National des Barreaux (national institution that represents all practising lawyers in France/CNB) • Conseil Supérieur de l’Audiovisuel (independent authority to protect audiovisual communication freedom/CSA) • Conservatoire National des Arts et Métiers (leading higher education and research institution dedicated to adult continuing education/CNAM) • Douai court of appeal • ESCP Europe, IoT Chair • Etalab(body that works in France on data sharing in the public sector) • “Familles rurales” association • Federal University of Toulouse • French Association for Artificial intelligence (AFIA) • French Association for Employment Law (AFDT) • French Development Agency(AFD) • French governmental advisory council on bioethics issues (CCNE) • French Insurance Federation (FFA) • French National Center for Scientific Research (CNRS)’s ethics committee (COMETS) • FO-Cadres (trade union) • Fondation Internet Nouvelle Génération (FING) • Fotonower • Génotoul societal (bioscience and ethics platform) • Groupe VYV (MGEN – ISTYA – Harmonie) • Imagine Institute on genetic diseases • INNOvation Ouverte par Ordinateur (INNOOO)• Institut Mines-Télécom (IMT), Research Chair “Values and Politics of Personal Information” • Laboratory for Collective and Artificial Intelligence (LICA) • Law Department of Université Catholique de Lille, Centre of research on relations between risk and law • Law Department of Université Catholique de Lyon • Ligue des Droits de l’Homme (Human Rights League/LDH) • Ligue de l’Enseignement (Education League) • Lille 2 University • Lille Association of Lawyers • Lyon’s administrative court of appeal • Microsoft • Ministry of Culture, via the General Directorate of Media and Cultural Industries (DGMIC) • Ministry of National Education, via the Directorate of Digital Technology for Education (DNE) and its Numéri’lab • National Academy of Technologies of France • National Institute of Higher Studies on Defence (IHEDN) • National Institute of Higher Studies on Security and Justice (INHESJ) • National Institute of Applied Sciences (INSA) • Necker Hospital • OpenLaw (association) • Paris II University • Randstad • Research Centre of the National Gendarmerie School of Officers (CREOGN) • Rhône Département -level Council of the Medical Association • Renaissance Numérique (thinktank) • School of Advanced Studies in the Social Sciences (EHESS) • Sciences Po Lille • Sciences Po Paris • Société informatique de France (association devoted to computer science/SIF) • The Future Society at Harvard Kennedy School, AI Initiative • Universcience • Visions d’Europe (association) The other contributors • Arbre des connaissances (association) • Autorité de contrôle prudentiel et de résolution (French authority responsible for the supervision of the banking and insurance sectors/ACPR) • Autorité des marchés financiers (authority which regulates participants and products in France’s financial markets/AMF) • Montpellier Méditerranée Métropole and its President, Philippe Saurel • City of MontpellierThe 37 citizens who took part in the public consultation organised in Montpellier on 14 October 2017. (extracted from106.txt)
LAUNCH EVENT ROUNDTABLE SESSIONS “Algorithms and humans” and “Fairness, transparency and plurality of algorithms” > CNIL SYMPOSIUM “Towards new forms of humanity?” > Universcience CONFERENCE “Algorithms and law” > Lille II University CONFERENCE “The choice in the age of Big Data” > Sciences Po Lille and Visions d’Europe DEBATE “The governance of emerging technosciences” > German American Conference at Harvard University DEBATE “Transatlantic perspectives on: AI in the age of social media; privacy, security and the future of political campaigning” > The Future Society at Harvard Kennedy School ROUNDTABLE SESSIONS “Big Data, human resources: algorithms on the agenda” > FO-Cadres CONFERENCE “Fairness of algorithmic decision-making” > Toulouse III – Paul Sabatier University DEBATE “Will digital technology spell the end of the rule of law?” > Collège des Bernadins SYMPOSIUM “Predictive justice” > Douai Court of Appeal, Lille Association of Lawyers and Law Department of Université Catholique de Lille WORKSHOPS “Fairness of algorithmic decision-making and processing” > LabEx International Centre for Mathematics and Computer Science in Toulouse23/01/2017 23/03/2017 25/03/2017 31/03/2017 06/04/201708/04/201718/04/2017 18/04/2017 04/05/201716/05/201719/05/201702/06/2017 DEBATE “Algorithms in healthcare: what ethics?” > Groupe VYV (MGEN – ISTYA – Harmonie) ROUNDTABLE SESSION “Artificial intelligence: ethics, at the intersection of HR and Big Data” > Confédération française de l’encadrement – Confédération générale des cadres (CFE-CGC) DEBATE “Algorithms, employment and ethics” > French Association for Employment Law (AFDT) DAY “Ethical algorithms, a moral requirement and competitive advantage” > Allistene’s CERNA and Société Informatique de France (SIF) SYMPOSIUM “Human, non-human in the age of artificial intelligence” > Paris II University SYMPOSIUM “Artificial intelligence: autonomy, delegation and accountability” > Bordeaux’s Cognitique Institute (ENSC) WORKSHOP “Ethics of algorithms: implications for healthcare” > Genotoul (bioscience and ethics platform) CROWDSOURCING WORKSHOP “Artificial intelligence and law” > OpenLaw SYMPOSIUM “ The many dimensions of data ” > Institut Mines-Télécom, Values and Politics of Personal Information Research Chair SYMPOSIUM “Security and justice, the challenge of the algorithm” > National Institute of Higher Studies of Security and Justice (INHESJ) MOCK TRIAL AND ROUNDTABLE SESSION “ Ethique, algorithmes and justice ” > Law Department of Université Catholique de Lyon and Lyon’s Administrative Court of Appeal STUDY DAY “Admission Post-bac, textbook case of public algorithms” > Fondation Internet Nouvelle Génération (FING) and Etalab DAY “Algorithms and digital sovereignty” > Allistene’s CERNA DAY “Ethics and artificial intelligence” > French National Center for Scientific Research (CNRS)’s ethics committee (COMETS) and French Association for AI (AFIA) DEBATES on algorithms in education. (extracted from106.txt)
40 Privacy Commissioner, Office of the Privacy Commissioner Briefing for the Incoming Minister of Justice: Hon Andrew Little, October 2017, para 2.1 https://privacy.org.nz/assets/Uploads/Briefing-for-Incoming-Min-ister-October-2017.pdf 41 See https://www.eugdpr.org/key-changes.html.1.3 Data Privacy Guidelines and Principles Essential to the right to privacy is the right to protection of personal data. (extracted from489.txt)
45 Comprehensive and Progressive Agreement for the Trans-Pacific Partnership Agreement (TPP) 42 Privacy Commissioner, Office of the Privacy Commissioner Briefing for the Incoming Minister of Justice: Hon Andrew Little, October 2017, para 4.14 https://privacy.org.nz/assets/Uploads/Briefing-for-Incom-ing-Minister-October-2017.pdf Ibid para 4.14. (extracted from489.txt)
Jurisprudence arising from the judgments of the European Court of Human Rights (ECtHR) and the Court of Justice of the European Union on the application of the right to privacy under the European Convention on Human Rights (ECHR) 52 Hon Christopher Finlayson, Ministerial Policy Statement: Cooperation of New Zealand intelligence and security agencies (GCSB and NZIS) with overseas public authorities, Appendix One, p 15, September 2017 available: https://www.nzic.govt.nz/assets/MPSs/Ministerial-Policy-State-ment-Cooperation-with-overseas-public-authorities.pdf. (extracted from489.txt)
56 See para 11 of Winkelmann J’s judgment in TV3 v N (name suppression) (unreported, HC Auckland, 7 July 2006) where she finds at paras 10 and 11 that public interest factors as to reporting of evidence must be balanced with “the State’s particular obligations to young offenders, and in particular Article 8 of the United Nations Standard Minimum Rules for the administration of juvenile justice (“the Beijing Rules”). (extracted from489.txt)
That same month, the Minister of Justice announced that Cabinet had agreed in principle to allow courts to make a declaration of inconsistency and that the Bill of Rights Act will be amended to give the Courts this power. (extracted from489.txt)
It is notable that in 2016 the UN Committee on the Rights of the Child recommended that the New Zealand Government ensure “that the Privacy, Human Rights and Ethics framework governing predictive risk modelling takes in to consideration the potentially discriminatory impacts of this practice, is made public and is referenced in all relevant legislation.” 85 The advent of this approach has coincided with extensive reforms to the legislation governing New Zealand’s child protection and youth justice jurisdictions. (extracted from489.txt)
• Have you consulted the Privacy Commissioner, the Ministry of Justice and the GCPO? (extracted from489.txt)
without technical jargon: the place for the 149 Privacy Commissioner, Office of the Privacy Commissioner Briefing for the Incoming Minister of Justice: Hon Andrew Little, October 2017, para 4.2 https://privacy.org.nz/assets/Uploads/Briefing-for-Incoming-Min ister-October-2017.pdf 150 Report of Special Rapporteur for freedom of expression, Frank La Rue, (17 April 2013) para 91. (extracted from489.txt)
174 • Human rights obligations: Agencies must not 172 See https://www.privacy.org.nz/blog/providing-an-adequate-lev-el-of-data-protection/ 173 See http://ec.europa.eu/justice/data-protection/international-trans fers/adequacy/index_en.htm174 Ibid. (extracted from489.txt)
235 As discussed earlier in the paper, AI is increasingly used in the criminal justice system. (extracted from489.txt)
For example, by the police to target resources or high-risk individuals, by the courts to predict the likelihood of re-offending and prisons in targeting restorative justice. (extracted from489.txt)
238 Concerns regarding the inherent risk of bias that arises from algorithmic risk assessments in the criminal justice sector were raised by U.S. (extracted from489.txt)
Attorney General Eric Holder raised concern about algorithms that produce risk assessments that seek to assign the probability of individual’s likelihood of committing future crimes: “Although these measures were crafted with the best of intentions, I am concerned that they inadvertently undermine our efforts to ensure individualized and equal justice… they may exacerbate unwarranted and unjust disparities that are already far too common in our criminal justice system and in our society. (extracted from489.txt)
See https://www.justice.gov/opa/speech/attorney-general-eric-holder-speaks-national-association-crimi nal-defense-lawyers-57th. (extracted from489.txt)
2 On behalf of the German Presidency of the Committee of Ministers of the Council of Europe, the German Fede ral Foreign Office and the Federal Ministry of Justice and Consumer Protection are proud to have hosted a virt ual highlevel Conference on this issue on 20 January 2021, with the support of the Council of Europe. (extracted from338.txt)
Department of Commerce The following Federal departments and agencies are represented on the Subcommittee on Machine Learning and Artificial Intelligence and thro ugh it, work together to monitor the state of the art in machine learning (ML) and AI (within the Federal Government, in the private sector, and internationally), to watch for the arrival of important technology milestones in the development of AI, to coor dinate the use of and foster the sharing of knowledge and best practices about ML and AI by the Federal Government, and to consult in the development of Federal research and development priorities in AI: Department of Commerce (Co -Chair ) Depa rtment of Defense Department of Education Department of Energy Department of Health and Human Services Department of Homeland Security Department of Justice Department of Labor Department of State Department of Transportation Department of Treasury Department of Veterans Affairs United States Agency for International Development Central Intelligence Agency General Services Administration National Science Foundation National Security Agency National Aeronautics and Space Administration Office of the Director of National Intelligence Social Security Administration The following offices of the Executive Office of the President are also represented on the Subcommittee: Council of Economic Advisers Domestic Policy Council Office of Management and Budget Office of Science and Technology Policy (Co Chair) Office of the Vice President National Economic Council National Security Council PREPARING FOR THE FUTURE OF ARTIFICIAL INTELLIGENCE PREPARING FOR THE FUTURE OF ARTIFICIAL INTELLIGENCE Contents Executive Summary ................................ (extracted from304.txt)
30 Justice, Fairness, and Accountability ................................ (extracted from304.txt)
Public - and private sector investments in basic and applied R&D on AI have already begun reaping major benefits to the public in fields as diverse as health care, transportation, the environment, criminal justice, and economic inclusion . (extracted from304.txt)
Use of AI to make consequential decisions about people, often replacing decisions made by human -driven bureaucra tic processes , leads to concerns about how to ensure justice, fairness, and accountability —the same concerns voiced previously in the Administration’s Big Data: Seizing Opportunities, Preserving Values report of 2014,1 as well as the Report to the President on Big Data and Privacy: A Technological Perspective published by the President’s Council of Advisors on Science and Technology in 2014 .2 Transparency concerns focus not only on the data and algorithms involved, but also on the potential to have some form of explanat ion for any AI -based determination. (extracted from304.txt)
Many areas of public policy, from education and the economic safety net, to defense, envi ronmental preservation, and criminal justice, will see new opportunities and new challenges driven by the continued progress of AI. (extracted from304.txt)
The promise of AI has been compared to the transformative impacts of advances in mobile computing.21 Public - and private -sector investments in basic and applied R&D on AI have already begun reaping major benefits for the public in fields as diverse as health care, transportation, the environment, criminal justice, and economic inclusion .22 At Walter Reed Medical Center, the Department of Veteran Affairs is using AI to better predict medical complications and improve treatment of severe combat wounds, leading to better patient outcomes, faster healing, and lower costs.23 The same general approac h—predicting complications to enable preventive treatment —has also reduced hospital -acquired infections at Johns Hopkins University .24 Given the current transition to electronic health records, predictive analysis of health data may play a key role across m any health domains like precision medicine and cancer research. (extracted from304.txt)
Autonomous watercraft may be much cheaper to operate than manned ships, and may some day be used for enhanced weather prediction, climate monitoring, or policing illegal fishing.27 AI also has the potential to improve aspects of the criminal justice system, including crime reporting, policing, bail, sentencing, and parole decisions. (extracted from304.txt)
The Admini stration is exploring how AI can responsibly benefit current initiatives such as Data Driven Justice and the Police Data Initiative that seek to provide law enforcement and the public with data that can better inform decision -making in the criminal justice system, while also taking care to minimize the possibility that AI might introduce bias or inaccuracies due to deficiencies in the available data. (extracted from304.txt)
Social justice and public policy institutions that do not typically engage with advanced technologies and data science in their work should consider partnerships with AI researchers and practitioners that can help apply AI tactics to the broad social problems these institutions already address in other ways. (extracted from304.txt)
The u se of AI to make consequential decisions about people, often replacing decisions made by human actors and institutions , leads to concerns about how to ensure justice, fairness, and accountability —the same concerns voice d previously in the “ Big Data” context.62 The u se of AI to control physical -world equipment leads to concerns about safety, especially as systems are exposed to the full complexity of the human environment. (extracted from304.txt)
Justice, Fairness, and Accountability A common theme in the Law and Governance, AI for Social Good, and Social and Economic Impac ts workshops was the need to ensure that AI promotes justice and fairness, and that AI -based processes are accountab le to stakeholders . (extracted from304.txt)
In the criminal justice system, some of the biggest concerns with Big Data are the lack of data and the lack of quality data.65 AI needs good data. (extracted from304.txt)
It is important that anyone using AI in the criminal justice context is aware of the limitations of current data. (extracted from304.txt)
65 Matt Ford, “The Missing Statistics of Criminal Justice,” The Atlantic, May 31, 2015, http://www.theatlantic.com/politics/archive/2015/05/what -we-dont-know -about -mass -incarceration/394520/ 66 Julia Angwin, Jeff Larson, Surya Mattu, and Lauren Kirchner, “Machine Bias,” ProPublica, May 23, 2016, https://www.propublica.org/article/machine -bias-risk-assessments -in-criminal -sentencing. (extracted from304.txt)
Many areas of public policy, from education and the economic safety net, to defense , environmental prese rvation, and criminal justice, will see new opportunities and new challenges driven by the continued progress of AI. (extracted from304.txt)
Social justice and public policy institutions that do not typically engage with advanced t echnologies and data science in their work should consider partnerships with AI researchers and practitioners that can help apply AI tactics to the broad social problems these institutions already address in other ways. (extracted from304.txt)
Matt Ford, “The Missing Statistics of Criminal Justice,” The Atlantic, May 31, 2015, http://www.theatlantic.com/politics/archive/2015/05/what -we-dont-know-about -mass -incarceration/394520/ Jason Furman, “Is This Time Different? (extracted from304.txt)
An inaccurate explanation of how the algorithm 9 ______________________________________________________________________________________________________ This publication is available free of charge from: https://doi.org/10.6028/NIST.IR.8312 arrived at its outcome could result in a miscarriage of justice. (extracted from476.txt)
Science and Justice, 57(2):144–154, 2017. (extracted from476.txt)
There is no shortage of examples where bias in some aspect of AI technology and its use has caused harm and negatively impacted lives, such as in hiring, [2–7] health care, [8–17] and criminal justice [18–30]. (extracted from477.txt)
Department of Justice, and the Office Federal Contract Compliance Programs are responsible for enforcement and interpretation of these laws. (extracted from477.txt)
for racial and ethnic minorities in areas such as criminal justice) [18–30, 214]. (extracted from477.txt)
These perspectives have led to the deployment of automated and predictive modeling tools within trusted institutions and high-stakes settings such as hiring or criminal justice. (extracted from477.txt)
Available: http://www.liebertpub.com/doi/10.1089/big.2016.0047 [23] EPIC, “Algorithms in the Criminal Justice System Risk Assessment Tools,” Electronic Privacy Information Center (EPIC), Tech. (extracted from477.txt)
Available: https://epic.org/algorithmic-transparency/crim-justice/ [24] K. (extracted from477.txt)
Available: https://www.nytimes.com/2017/06/13/opinion/how -computers-are-harming-criminal-justice.html [30] “State v. (extracted from477.txt)
Crawford, “Dirty Data, Bad Predictions: How Civil Rights Violations Impact Police Data, Predictive Policing Systems, And Justice,” New York University Law Review , vol. (extracted from477.txt)
Costanza-Chock, “Design Justice, A.I., and Escape from the Matrix of Domination,” Journal of Design and Science , Jul. (extracted from477.txt)
Shadbolt, “’It’s Reducing a Human Being to a Percentage’; Perceptions of Justice in Algorithmic Decisions,” Montreal QC, Canada, Jan. (extracted from477.txt)
123 “The Federal government should therefore emphasize AI investments in areas of strong societal impor tance that are not aimed at consumer markets —areas such as AI for public health, urban systems and smart communities, social welfare, criminal justice, environmental sust ainability, and national security, as well as long -term research that accelerates the production of AI knowledge and technologies.” 124 The following subareas are described in the plan: Data analysis; Perception; Theoretical limitations for AI; General AI; Scalable AI; Human -like AI; Robotics; AI Hardware 125 Sub -areas: Human -AI communication; Strengthening of human ability; Natural language processing; Interface and visualisation. (extracted from339.txt)
Accept additional cookies Reject additional cookies View cookies Hide cookie message Skip to main content GOV.UK Navigation menu Menu Menu Services and information Benefits Births, death, marriages and care Business and self-employed Childcare and parenting Citizenship and living in the UK Crime, justice and the law Disabled people Driving and transport Education and learning Employing people Environment and countryside Housing and local services Money and tax Passports, travel and living abroad Visas and immigration Working, jobs and pensions Government activity Departments Departments, agencies and public bodies News News stories, speeches, letters and notices Guidance and regulation Detailed guidance, regulations and rules Research and statistics Reports, analysis and official statistics Policy papers and consultations Consultations and strategy Transparency Data, Freedom of Information releases and corporate reports Search GOV.UK × Search GOV.UK Search Search GOV.UK Search Home Health and social care National Health Service Digital and data-driven health and care technology Departmentof Health &Social Care Guidance A guide to good practice for digital and data-driven health technologies Updated 19 January 2021 Contents Introduction 1. (extracted from649.txt)
Cancel Services and information Benefits Births, death, marriages and care Business and self-employed Childcare and parenting Citizenship and living in the UK Crime, justice and the law Disabled people Driving and transport Education and learning Employing people Environment and countryside Housing and local services Money and tax Passports, travel and living abroad Visas and immigration Working, jobs and pensions Government activity Departments News Guidance and regulation Research and statistics Policy papers and consultations Transparency How government works Get involved Support links Help Privacy Cookies Accessibility statement Contact Terms and conditions Rhestr o Wasanaethau Cymraeg Government Digital Service All content is available under the Open Government Licence v3.0, except where otherwise stated © Crown copyright (extracted from649.txt)
Making government services more efficient and accessible: Despite often being slow to adopt new technologies, governments around the world are using AI, from the local to the national levels, to make public services more efficient and accessible, with an emphasis on developing “smart cities.” AI is also being used to allocate government resources and optimize budgets.37 HARMFUL AI Perpetuating bias in criminal justice: There are many documented cases of AI gone wrong in the criminal justice system. (extracted from12.txt)
criminal justice system, was inaccurate at forecasting future crime and heavily biased against black defendants. (extracted from12.txt)
Considering ethical concepts such as justice, fairness, transparency, and accountability allows for valuable debate about the societal impacts of AI, and the role of AI in our lives.52 There is also an academic research community devoted to addressing ethical issues.53 Ethics have helped those researching and developing AI to define boundaries for themselves. (extracted from12.txt)
” - Article 6 of the ICCPR The growing use of AI in the criminal justice system risks interfering with rights to be free from interferences with personal liberty. (extracted from12.txt)
criminal justice system to inform detainment decisions at nearly every stage, from assigning bail to criminal sentencing.62 The software has led to more black defendants falsely labeled as high risk and given higher bail conditions, kept in pre-trial detention, and sentenced to longer prison terms. (extracted from12.txt)
In criminal justice, this discrimination is often the result of forms of bias. (extracted from12.txt)
accessnow.org33 HUMAN RIGHTS IN THE AGE OF ARTIFICIAL INTELLIGENCEthe uses of AI for these purposes.118 Any assessment process should include: • Testing and audits by independent experts • Identifying measures to mitigate identified risks and prevent any rights violations from occurring, and measuring compliance and efficacy • A failsafe to terminate acquisition, deployment, or any continued use if at any point an identified human rights violation is too high or unable to be mitigated • Identification of any new legal safeguards needed to protect human rights in specific applications of AI tools • Special determinations of bias, particularly in the criminal justice sector due to the risks to fair trial, right to liberty, and non-discrimination • If a third party is used to develop and/or implement the system, a requirement for the third party to participate in the human rights assessment process 3. (extracted from12.txt)
There should always be a human in the loop, and for high-risk areas, including criminal justice, significant human oversight is necessary. (extracted from12.txt)
This is particularly important in areas such as in law enforcement and the justice system. (extracted from12.txt)
Case Law 35 European Court of Human Rights (ECtHR) 35 Court of Justice of the European Union (CJEU) 37 National courts/data protection authorities 38 ETHICAL ASPECTS OF B IOMETRIC IDENTIFICAT ION 42 3.1. (extracted from271.txt)
Recommendations with regard to consent management 87 REFERENCES 89 ANNEX: PROPOSED WORD ING OF TITLE II AND TITLE IIA 96 Biometric Recognition and Behavioural D etection PE 696.968 5 LIST OF ABBREVIATIONS AI Artificial Intelligence AIA Artificial Intelligence Act AFIS Automated Fingerprint Identification System Art(s) Article (s) BCI Brain -Computer -Interface BDSG Bundesdatenschutzgesetz - German Federal Data Protection Act BIPA BVerfG Illinois Biometric Information Privacy Act Bundesverfassungsgericht - German Constitutional Court BVerwG Bundesverwaltungsgericht – German Federal Administrative Court CCPA CCTV California Consumer Privacy Act Closed -Circuit Television CFR Charter of Fundamental Rights of the European Union CJEU Court of Justice of the European Union DNA Deoxyribonucleic acid DSA Digital Services Act DSG Datenschutzgesetz - Austrian Data Protection Act EC European Commission ECG Electrocardiography ECHR European Convention on Human Rights ECtHR European Court of Human Rights Ed(s) Editor (s) Edn Edition IPOL | Policy Department for Citizens’ Rights and Constitutional Affairs 6 PE 696.968 EEG Electroencephalography EES Entry -Exit-System e.g. (extracted from271.txt)
id est (that is) IoT Internet of Things LED Law Enforcement Directive – Directive (EU) 2016/680 OGH Oberster Gerichtshof - Austrian Supreme Court of Justice OJ Official Journal of the European Union Para (s) Paragraph (s) PIPL SIS Personal Information Protection Law of the People’s Republic of China Schengen Information System SPG Sicherheitspolizeigesetz - Federal Security Police Act TEU TTDSG Treaty of the European Union (TEU) German Act on Data Protection and Privacy in T elecommunica tions and T elemedia UDHR Universal Declaration of Human Rights UK-DPA Data Protection Act of the United Kingdom UN United Nations Biometric Recognition and Behavioural D etection PE 696.968 7 LIST OF BOXES WITH ILLUSTRATIONS Illustration 1: Differentiating biometrics- based data and other personal data 68 Illustration 2: Differentiating ‘real -time’ and ‘post’ remote identification 70 Illustration 3: Relationship between ‘biometric categorisation’ and ‘biometric inferences’ 71 Illustration 4: Undesirable remote biometric identification beyond law enforcement 77 Illustration 5: Remote biometric identification in grey zones around law enforcement 78 Illustration 6: Data collection and storage in the context of biometric identification 80 Illustration 7: Justification of emotion recognition or biometric categorisation 82 Illustration 8: Emotion recognition or biometric categorisation used as legal evidence 83 Illustration 9: Personality profiles created with the help of a video game 85 Illustration 10: Biometric inferences drawn with regard to third parties 86 LIST OF FIGURES Figure 1: Authentication/identification, categorisation, and detection 21 Figure 2: Steps involved in biometric identification 43 Figure 3: Steps involved in biometric categorisation 54 Figure 4: Steps involved in biometric detection 58 Figure 5: Risk -based approach of the AIA Proposal 63 Figure 6: Biometric techniques under the risk levels of the AIA Proposal 64 LIST OF TABLES Table 1: Admissibility of biometric techniques (based on simplified assumptions) 65 Table 2: Limitations on scope with regard to identification measures 75 IPOL | Policy Department for Citizens’ Rights and Constitutional Affairs 8 PE 696.968 EXECUTIVE SUMMARY Background Biometric identification together with biometric categorisation, behavioural detection, emotion recognition, brain -computer -interfaces (BCIs), and similar techniques are being used to an increasing extent by public and private bodies. (extracted from271.txt)
58 Recommendations 3 and 7 European Parliament resolution of 20 January 2021 on artificial intelligence: questions of interpretation and application of international law in so far as the EU is affected in the areas of civil and military uses a nd of state authority outside the scope of criminal justice (2020/2013(INI)) . (extracted from271.txt)
63 See Legislative Train Schedule , ‘Completion of EU Accession to the European Convention on Human Rights ’, available at <https://www.europarl.europa.eu/legislative-train/theme-area -of-justice -and -fundamental -rights/file-completion -of-euaccession -to-the -echr > (last accessed 09 July 2021). (extracted from271.txt)
102 Regulation (EU) No 603/2013 of the European Parliament and of the Council of 26 June 2013 on the establishment of 'Eurodac' for the comparison of fingerprints for the effective application of Regulation (EU) No 604/2013 establishing the criteria and mechanisms for determining the Member State responsible for examining an application for international protection lodged in one of the Member States by a third -country national or a s tateless person and on requests for the comparison with Eurodac data by Member States' law enforcement authorities and Europol for law enforcement purposes, and amending Regulation (EU) No 1077/2011 establishing a European Agency for the operational management of large -scale IT systems in the area of freedom, security and justice, OJ L 180, 1-30. (extracted from271.txt)
Biometric Recognition and Behavioural D etection PE 696.968 37 Court of Justice of the European Union (CJEU) Currently, there is no EU case law regarding highly sophisticated identification techniques. (extracted from271.txt)
The Austrian Supreme Court of Justice (Oberster Gerichtshof , OGH) ruled in favour of the employees and held that the biometric templates are obtained for the comparatively trivial aim of determining the employee’s times of coming a nd going. (extracted from271.txt)
182 Austrian Supreme Court of Justice 18 October 2006, 9 Ob 109/06d; Austrian Supreme Court of Justice 22 January 2020, 9 Ob 120/19s. (extracted from271.txt)
196 High Court of Justice (Divisional Court of Cardiff) 4 September 2019, EWCH 2341 (Admin), para 159. (extracted from271.txt)
Any deficiencies in this regard may lead to severe unfairness or even to massive discrimination, inc luding on racial or ethnic grounds, and to the undermining of procedural rights, including access to justice and the right to a fair trial. (extracted from271.txt)
Gutheil M and others, ‘ Interoperability of Justice and Home Affairs Informati on Systems’ (European Parliament 2018) . (extracted from271.txt)
2018).Accountability, explainability, privacy, justice, but also other values such as robustness or safety are most easily operationalized mathematically and thus tend to be implemented in terms of technical solutions. (extracted from703.txt)
With reference to the findings of psychologist Carol Gilligan, one could argue at this point that the way AI ethics is performed and structured constitutes a typical instantiation of a male-dominated justice ethics (Gilligan 1982). (extracted from703.txt)
In the 1980s, Gilligan demonstrated in empirical studies that women do not, as men typically do, address moral problems primarily through a “calculating”, “rational”, “logic-oriented” ethics of justice, but rather interpret them within a wider framework of an “empathic”, “emotion-oriented” ethics of care. (extracted from703.txt)
What does it mean to implement justice or transparency in AI-systems? (extracted from703.txt)
broadening the scope of action, uncovering blind spots, promoting autonomy and freedom, and fostering self-responsibility.In view of AI ethics, approaches that focus on virtues aim at cultivating a moral character, expressing technomoral virtues such as honesty, justice, courage, empathy, care, civility, or magnanimity, to name just a few (Vallor 2016). (extracted from703.txt)
This implies that the purposes for which AI systems are developed and applied are not in accordance with societal values or fundamental rights such as beneficence, non-maleficence, justice, and explicability (Taddeo and Floridi 2018; Pekka et al. (extracted from703.txt)
In this report, we will focus on systems that affect justice, equality , participation and public welfare, either directly or indirectly. (extracted from38.txt)
In case automation as a term was not used, we still scanned for key concepts like discrimination, justice, equity/equality to take a closer look, since automation processes tend to be hiding behind these keywords. (extracted from38.txt)
Maze donia EURO pEAN UNION By Kristina Penner The EU is a lot of things: It is a union of 28 Member States that has a commission, a parliament, a council of national ministers, the Court of Justice, and a couple of other institutions. (extracted from38.txt)
” external [eU 2] The Commission announced that it will support (basic and industrial) research and innova-tion in fields built on the guiding principle of “responsible AI” , including investment and encouragement of research and testing in sectors such as health, security, public adminis-tration and justice, with the goal to enable policy makers to gain experience and to devise suitable legal frameworks. (extracted from38.txt)
In addition, there was a mention that fundamental rights “would be at risk if unethical practice is facilitated by virtue of algorithms focused on com-mercial gain, for example because humans ‘allow’ or ‘rely’ on robot sorting techniques that are discriminatory and may be unfair and undermine dignity and justice” . (extracted from38.txt)
external [eU 43] / Police d irective The EU data protection reform package of 2016, which included the GD pR, also involved a directiv e on data protection in the area of police and justice external [eU 44] as a lex specialis, adopted on May 5, 2016, applicable as of May 6, 2018. (extracted from38.txt)
eu-LISA, the “European Agency for the Operational Management of large-scale IT Systems in the Area of Freedom, Security and Justice” , is now managing the “strengthened” databases and applications VIS, SISII and EURODAC together. (extracted from38.txt)
Reflecting on the interoperability of EU information systems to freedom, security and justice the European Data p rotection Supervisor stresses “that interoperability is not primarily a technical choice, it is first and foremost a political choice to be made, with significant legal and societal implications in the years to come” . (extracted from38.txt)
This is a cooperation between the Ministry of Home Affairs, the Digital Agenda and the Ministry of Justice. (extracted from38.txt)
The committee consists of 16 experts from politics, academia and industry and is led by the Federal Ministry for Justice and Consumer Protection and the Federal Ministry of the Interior, Building and Community. (extracted from38.txt)
Lastly, the Digital Government Agenda mentions that the Wetenschappelijk onderzoeks- en documentatiecentrum (Research and Documentation Centre of the Ministry of Justice and Security or WODC) will carry out research on the regulation and legality of algorithms taking autonomous decisions. (extracted from38.txt)
On March 29, 2018, the Ministry of Justice and Security organised a round table discussion on the use of Artificial Intelli-gence in the legal field. (extracted from38.txt)
In January 2018, the Polish Ministry of Justice introduced a system to randomly allocate court cases to judges. (extracted from38.txt)
Adm in Ac T ion / m inistry of Justice – s ystem of random Allocation of c ases on 1 January 2018, the Polish Ministry of Justice introduced the “System of Random Allocation of Cases” ( System Losowego Przydziału Spraw), a digital system that, on a onceper-day basis, assigns cases to judges across the country. (extracted from38.txt)
The Minister of Justice has de-clared that "the selection will be made solely by a machine, a computer system that is blind like Themis, and chooses without emotions, without views or biases, and in a manner fully free from possible accusations of corruption“ . (extracted from38.txt)
SAVRY is in general fair, while the machine learning models tend to discriminate against male defendants, foreigners, or people of specific national groups, sa ys Castillo: “Machine learning could be incorporated into SAVRY, but if aspects of algorithmic justice are not taken into account, it could generate an unfair prediction. (extracted from38.txt)
The Department of Justice of the Catalan government launched the tool in 2010 and applied it to all inmates in all prisons, and not just for cases involving violent crime. (extracted from38.txt)
Anne serves as a chair of the Com-munication and Democracy section of ECREA and is vice-chair of the Activism, Communication and Social Justice Interest Group within ICA. (extracted from38.txt)
The Data Justice Lab at the University of Cardiff examines the relationship between what it calls ‘data-fication’ and social justice. (extracted from38.txt)
external [uK 6] While recognising the potential benefits to society of ADM, it mentions discrimination against job applicants and inequities within the criminal justice system as examples of issues that may arise as a result of ADM. (extracted from38.txt)
external [uK 15] / Data Justice Lab The Data Justice Lab is a research lab at Cardiff University’s School of Journalism, Media and Culture. (extracted from38.txt)
It seeks to examine the relationship between what it calls ‘datafication’—the collection and processing of massive amounts of data for decision-making and governance LINKS: You can find a list of all URLs in the report compiled online at: www.algorithmwatch.org/ automating-societyexternal T aking Stock of Automated Decision-Making in the EU page 137 across more and more areas of social life—and social justice. (extracted from38.txt)
Connectez-vous pour accéder à des contenus exclusifs et à l'ensemble des services en ligne S'identifier avec e-dentitas Voir les Conditions générales d'utilisation (nouvelle fenêtre) Vous êtes dans Accueil Actualités Préconisations d’actions pour les legaltechs du domaine de la jurimétrie 13 octobre 2020 Préconisations d’actions pour les legaltechs du domaine de la jurimétrie Numérique Partager par email.' ' .(ouvre votre boîte de messagerie) Le Conseil national des barreaux a toujours manifesté son intention de prendre un rôle actif sur le sujet de la réutilisation de la donnée judiciaire, en demandant notamment la création d’une instance publique chargée de la régulation et du contrôle des algorithmes utilisés pour l’exploitation de la base de données des décisions de justice ainsi que de leur réutilisation, dont doivent, notamment, être membres la Cour de cassation, le Conseil d’Etat et le Conseil national des barreaux. (extracted from111.txt)
Après avoir constaté la nécessité de déterminer le fonctionnement technique et éthique de chacune des technologies portées par les Legaltechs du domaine de la « Justice prédictive » ainsi que leur utilité pratique pour les professionnels du droit, l’assemblée générale du Conseil national des barreaux (CNB), réunie les 5 et 6 juillet 2019, a donné mandat au groupe de travail Legaltech de piloter une étude comparative. (extracted from111.txt)
Elaborer une stratégie de large intégration du numérique dans les activités des professionnels du droit et de la justice ; 13. (extracted from111.txt)
Les travaux réalisés ont ainsi permis de rappeler l’importance de réguler les nouveaux outils de jurimétrie et d’assurer le respect de principes éthiques dans le cadre de la réutilisation de la donnée de justice. (extracted from111.txt)
En conséquence, une Charte sur la transparence et l’éthique de l’utilisation des données judiciaires annexée au rapport a été adoptée pour garantir l’autorégulation des acteurs tant s’agissant des algorithmes utilisés pour l’exploitation de la base de données des décisions de justice que de la réutilisation des informations qu’elle contient. (extracted from111.txt)
Equality and justice 6. (extracted from461.txt)
Equality and justice Artificial intelligence should not reproduce prejudices that marginalise specific population groups. (extracted from461.txt)
Forexample, social network maps forservices such asTwitter candetermine credit-worthiness [5],opaque recidivism scores canshape criminal justice decisions inaracially disparate manner [6],and categorization based onzipcodes resulted inlessaccess toAmazon Prime same-day delivery service forAfrican-Americans inUnited States cities [7].These high-profile cases show that apparently neutral data canyield discriminatory outcomes, thereby compounding social inequities. (extracted from312.txt)
There areanumber ofgood models forinterdisciplinary ethics research, such asthe trainings offered bytheScience and Justice research center attheUniversity ofCalifornia, Santa Cruz [32] and Values inDesign curricula [33]. (extracted from312.txt)
Science &Justice Research Center (Collaboratio nsGroup. (extracted from312.txt)
Human decisions based on automated and predictive 239 technology are often made in settings such as hiring or criminal justice, and can create harmful 240 impacts and amplify and accelerate existing social inequities or, at minimum, perceptions of 241 inequities. (extracted from474.txt)
Certainly, there is 248 no shortage of examples where bias in some aspect of AI technology and its use has caused harm 249 and negatively impacted people's lives, such as in hiring [5,12,16,17,36,62,118], health care 250 [46,52,55,59,83,88,103,122,123], and criminal justice [7,20,29,41,44,56,66,74,75,78,87, 251 140,142]. (extracted from474.txt)
Such biases may produce unjust outcomes for racial and 315 4 ethnic minorities in areas such as criminal justice [7,41,56,74,75,78,87,140,142], hiring 316 [4,5,12,16,17,36,118,119], and financial decisions [13,65]. (extracted from474.txt)
The broad consensus 325 of the literature is that systems meant for decision making or predictive scenarios should 326 demonstrate validity and reliability under the very specific setting in which it is intended to be 327 deployed (hiring purposes, risk assessments in the criminal justice system, etc.). (extracted from474.txt)
Costanza-Chock, Design Justice, A.I., and Escape from the Matrix of Domination, 825 Journal of Design and Science. (extracted from474.txt)
851 20 [44] EPIC, Algorithms in the Criminal Justice System: Risk Assessment Tools, Electronic 852 Privacy Information Center, 2020. (extracted from474.txt)
These opportunities can make a lasting contribution to freedom, justice and prosperity above all when people’s individual rights are protected and social cohesion is strengthe ned. (extracted from104.txt)
A titre d’exemples non hiérarchisés, des SIA sont d’ores et déjà opérationnels dans les domaines les plus divers : - La gestion des territoires : circulation automobile, entretien de la voirie, gestion des déchets, de l’eau, de l’éclairage public, du nettoyage urbain, transport public par véhicule dit « autonome »… ; - La défense et la sécurité : détection de forces militaires sur des images aériennes et satellite, prévention des attaques informatiques, détection de la désinformation d’origine étrangère avec Viginum, lecture automatisée de plaque d’immatriculation, anticipation des catastrophes naturelles par les services de secours, reconnaissance faciale de suspects ou de victimes par la police judiciaire… ; - Les activités de contrôle et de lutte contre la fraude : ciblage des contrôles fiscaux et douaniers, contrôle aux frontières, détection de constructions non autorisées sur des images satellite… ; - La justice : pseudonymisation des jugements, recherche documentaire, évaluation des préjudices en cas de dommage corporel… ; - La politique de l’emploi : appariement entre offre et demande d’emplois, personnalisation de l’accompagnement… ; Page 7 - L’éducation : prévention du décrochage scolaire, affectation en première année de l’enseignement supérieur… ; - La protection sociale : liquidation des prestations, identification du non-recours aux droits… ; - La santé : aide au diagnostic et à la prescription médicale, alertes sanitaires, robotique médicale… En deuxième lieu, à l’instar de ses voisins européens, la France ne vit pas une révolution de l’IA publique , mais connaît un déploiement très progressif des SIA dans les services publics, très inégal selon les administrations et souvent expérimental. (extracted from110.txt)
Page 18 L’utilisation de ces systèmes pour une politique publique donnée (sécurité, justice, aménagement du territoire, santé, éducation…) mériterait à elle seule la rédaction de plusieurs études. (extracted from110.txt)
A titre d’exemple (fictif), pour construire un système d’IA d’aide à l’examen de la recevabilité d’un recours contentieux, on peut soit transcrire en instructions informatiques les dispositions pertinentes du code de justice administrative et la jurisprudence pour que le programme les applique à un dossier donné (approche symbolique), soit construire un modèle qui va déduire ces règles à partir de l’exploitation d’un très grand nombre de jugements – il constatera, « à l’expérience », que ce délai est de deux mois la plupart du temps, sauf dans certaines matières ; qu’il n’est pas déclenché en l’absence de mention des voies et délais de recours dans la décision attaquée, etc. (extracted from110.txt)
A titre d’exemple, un outil de traitement du langage naturel entraîné sur des articles de l’encyclopédie en ligne Wikipédia devra être ré-entraîné sur des décisions de justice si on souhaite l’utiliser pour rechercher automatiquement une décision dans une base de jurisprudence à partir d’une requête formulée en langage courant). (extracted from110.txt)
a/ On parle d’ apprentissage supervisé lorsque le modèle est alimenté par des données dites labellisées, étiquetées, annotées ou qualifiées, c’est-à-dire auxquelles on a assigné une valeur ou qu’on a rangé dans une catégorie, de manière à permettre à la machine de différencier ce qui est une bonne et une mauvaise réponse : telle photo représente une chaise, tel caractère (présenté à l’endroit, à l’envers, en police Times New Roman ou écrit d’une main ferme ou tremblante…) est le chiffre 1, tel terme est un nom propre qui doit être pseudonymisé dans une décision de justice… Les paramètres « manuels » (non entraînables46) du modèle sont ajustés de telle sorte que la prévision qu’effectue la machine (inférence) à partir de données d’entrée (n’importe quelle photo, n’importe quel caractère ou n’importe quelle décision de justice) soit conforme ou la plus proche possible de la valeur attendue (l’outil devra indiquer qu’une chaise se trouve sur la photo si tel est bien le cas et donner le résultat inverse dans le cas contraire ; il devra reconnaître correctement le chiffre 1 sur un papier, une enveloppe ou dans une image ; il devra proposer ou procéder à la pseudonymisation d’un nom propre dans une décision s’il s’agit bien d’un nom propre). (extracted from110.txt)
les entités nommées à anonymiser dans une décision de justice ; le traitement du courrier pour l’orienter automatiquement vers le service chargé d’y répondre), le sens (repérage des fausses informations par exemple), de les résumer ou de les reformuler, et même d’en identifier la tonalité émotionnelle. (extracted from110.txt)
Le droit conventionnel européen Le Conseil de l’Europe s’est très tôt préoccupé des incidences du développement des systèmes d’IA sur les droits fondamentaux, l’État de droit, la démocratie et la justice (avec l’adoption en 2018 de la charte éthique européenne sur l’utilisation de l’IA dans les systèmes judiciaires, dans le cadre des travaux de la Commission européenne pour l’efficacité de la justice - CEPEJ). (extracted from110.txt)
Dans le champ de la justice, où les SIA publics sont encore rares, les outils de pseudonymisation des décisions juridictionnelles , développés à partir d’algorithmes de traitement du langage naturel, visent à automatiser la suppression des éléments permettant l’identification des personnes mentionnées dans les décisions et, ainsi, à faciliter leur mise à disposition du public, prévue par l’ article 24 de la loi pour une République numérique en 2016, puis précisé par l’ article 33 de la loi du 23 mars 2019 de programmation 2018-2022 et de réforme de la justice89. (extracted from110.txt)
A l’heure actuelle, aucun mécanisme ne garantit de façon systématique et complète l’égalité devant la loi ou devant la justice à l’échelle nationale. (extracted from110.txt)
Ensuite, les ressources humaines libérées des tâches automatisées peuvent être redéployées, non seulement pour assurer l’entraînement (la complémentarité entre le travail humain d’annotation des données et le SIA est manifeste, par exemple, dans le déploiement de l’outil de pseudonymisation des décisions de justice de la Cour de cassation), la supervision et la maintenance des systèmes, mais aussi et surtout pour assurer des prestations qui ne le sont pas actuellement, ou qui le sont mal, faute de moyens humains suffisants. (extracted from110.txt)
Les normes constitutionnelles touchant à la qualité et à la performance de l’action publique sont, pour la plupart, des objectifs de valeur constitutionnelle insusceptibles d’être directement invoqués par le justiciable, quoiqu’ils s’imposent au législateur (objectif de bon usage des deniers publics, objectif de bonne administration de la justice, lutte contre la fraude…). (extracted from110.txt)
Or, ces systèmes sont des outils au service des politiques publiques, des auxiliaires d’administration et de justice, et ne sont que cela. (extracted from110.txt)
Page 89 DataJust Le décret n° 2020-356 du 27 mars 2020 a autorisé le ministère de la justice, à titre expérimental, à développer un outil permettant d’extraire et d’exploiter de façon automatisée les données relatives aux montants des demandes indemnitaires en réparation des préjudices résultant de dommages corporels, par poste de préjudice, des offres indemnitaires des personnes mises en cause, des évaluations effectuées dans le cadre de procédures de règlement amiable et des condamnations prononcées à ce titre, qui figurent dans les décisions rendues par les juridictions d’appel, de l’ordre administratif et de l’ordre judiciaire. (extracted from110.txt)
A terme, le ministère de la justice envisageait de mettre en ligne un « référentiel indicatif d’indemnisation » à destination du grand public. (extracted from110.txt)
Une telle possibilité est en outre exclue pour les décisions entrant dans le champ de la directive « police-justice », les décisions de justice impliquant une appréciation sur le comportement d’une personne, ou encore les services de conciliation, de médiation judiciaire ou d’arbitrage. (extracted from110.txt)
: dans 98% des cas où la machine a proposé de pseudonymiser un mot dans une décision de justice, il s’agissait bien d’un mot à pseudonymiser (nom propre…). (extracted from110.txt)
C’est précisément en raison de ce risque que la Cour de justice de l’Union européenne a, à deux reprises, annulé les instruments qui entendaient encadrer les échanges de données à caractère personnel traitées avec les Etats-Unis174. (extracted from110.txt)
Contrairement à d’autres actes de droit dérivé connexes, comme la directive dite e-privacy178 ou le RGPD et la directive dite « police-justice », la proposition de la Commission n’exclut pas de son champ d’application l’ensemble des matières classiquement exclues du champ du droit de l’Union , à savoir la sécurité nationale (y compris le renseignement), la défense et la politique étrangère. (extracted from110.txt)
En effet, les domaines énumérés par l’annexe III du projet de règlement recouvrent l’essentiel des activités régaliennes (police, justice, immigration), l’accès et le droit aux services publics et aux prestations sociales, l’enseignement et la formation professionnelle, la gestion et l’exploitation de réseaux publics (eau, électricité…) ou encore le recrutement, l’évaluation, la promotion et la sortie de service des agents. (extracted from110.txt)
En tant que simples utilisatrices de systèmes fournis par des tiers, leurs obligations seraient moindres et le texte leur offrirait au contraire des garanties, au prix toutefois d’un renchérissement des prestations dès lors que les fournisseurs répercuteront sur les clients publics les coûts liés à la conformité des systèmes ; - d’autre part, la simple inclusion du système dans le champ d’application du règlement, fût-ce au titre de la soumission à un code de conduite, emporte l’inclusion dans le champ du droit de l’Union, ce qui entraîne l’application du droit primaire, notamment de la Charte des droits fondamentaux de l’Union européenne , dont on sait à quel point, telle que l’interprète la Cour de justice de l’Union européenne, elle peut être exigeante à l’égard des autorités publiques, en particulier pour celles qui sont en charge de missions touchant à la sécurité. (extracted from110.txt)
S’il est vrai que les principes de fonctionnement des SIA ne diffèrent pas selon la finalité poursuivie, de sorte que les obligations prévues par la proposition peuvent, dans leur principe, s’appliquer indifféremment aux SIA répressifs comme à la généralité des systèmes, on ne peut nier la spécificité des finalités de puissance publique et, singulièrement, celles qui touchent aux activités de police et de justice, qui pourraient appeler un encadrement européen plus souple. (extracted from110.txt)
On ne peut se satisfaire de la relative précipitation et du retard dans lesquels le droit national a été adapté au RGPD et la directive « police-justice » transposée en droit interne. (extracted from110.txt)
201 A l’instar de ce qui existe pour la prohibition du profilage des professionnels de justice ( art. (extracted from110.txt)
10 du code de justice administrative et art. (extracted from110.txt)
Ainsi, l’outil d’anonymisation des décisions de justice développé par la Cour de cassation mobilise une vingtaine d’agents annotateurs, encadrés par une directrice des services de greffe. (extracted from110.txt)
Certains SIA publics relèveront de cette exception (ex : identification des variables explicatives 230 Le paragraphe 2 de l’article 4 et l’article 9 de la directive « police-justice » fixent des conditions distinctes pour le traitement ultérieur. (extracted from110.txt)
: système d’aide à l’évaluation du préjudice corporel sur la base des données personnelles contenues dans les décisions de justice), répondant à des motifs d’intérêt général éminents, dont la destination est susceptible de bénéficier aux personnes concernées et, en tous les cas, qui n’est pas susceptible de prendre, de recommander ou de participer à des décisions qui leur seraient défavorables (ex. (extracted from110.txt)
Cette proposition prévoit la possibilité, en-dehors du champ police-justice, d’une mise à disposition contrainte des données du secteur privé au profit des administrations publiques en cas de « besoin exceptionnel » , c’est-à-dire soit pour répondre à ou prévenir une situation d’urgence publique, soit lorsque l’absence de données disponibles fait obstacle à ce qu’une administration s’acquitte d’une tâche d’intérêt général spécifique qui lui est confiée par la loi. (extracted from110.txt)
Les contentieux éventuels sur ces actes de droit souple, portés directement devant le Conseil d’Etat, permettent d’en obtenir rapidement – en un an environ, sous réserve d’éventuelles questions préjudicielles à la Cour de justice de l’Union européenne – la confirmation ou l’infirmation. (extracted from110.txt)
Or, un SIA pouvant décider du sort de questions cruciales peut parfaitement ne traiter aucune donnée personnelle, et ainsi se prêter assez mal, pour le cœur de son contrôle, à une approche individuelle : la justice ou l’équité du système sont des questions globales, reposant autant sur la nature des données traitées que sur les choix faits pour orienter le fonctionnement du système, ou les variables qui vont déterminer son évolution. (extracted from110.txt)
L'étude s'attachera ensuite, au regard de l'évolution du droit européen, à évaluer l'impact potentiel, en termes de qualité de l'action publique, de l'introduction ou du développement de l'intelligence artificielle pour certaines missions telles que, Page 212 notamment, la santé, la justice, l'éducation, l'emploi, la sécurité intérieure ainsi qu'au sein de services disposant de pouvoirs d'enquête (fiscalité, concurrence, douanes). (extracted from110.txt)
Éric Bonnet, directeur de la communication juridique du cabinet Lexing Alain Bensoussan Avocats Benoît Bergeret , directeur exécutif du Metalab, accompagné de Julia Fenart, Head of European Affairs, France Digitale et de Louis Fleuret, directeur adjoint de La French Tech Page 215 Patrick Bezombes, président de la commission de normalisation Intelligence artificielle de l’Association française de normalisation (Afnor), accompagné de Isabelle Blanc , Chief Data Officer, ministère de l'enseignement supérieur et de la recherche Gérard Biau , professeur, Sorbonne Université, directeur du Sorbonne Center for Artificial Intelligence (SCAI) accompagné de Raja Chatila, professeur émérite en robotique et en intelligence artificielle Isabelle Blanc , Chief Data Officer, ministère de l'enseignement supérieur et de la recherche, AMDAC Annabelle Bouchet , représentante du syndicat Snepap-FSU Hélène Brisset , directrice du numérique aux ministères chargés des affaires sociales, AMDAC Michel Cadot , Délégué interministériel aux Jeux olympiques et paralympiques, accompagné de Christophe Delaye, conseiller en charge de la sécurité Anne-Florence Canton, cheffe du service du numérique au ministère de la justice (AMDAC), accompagnée de Fabien Antoine, directeur de projet Stratégie data, de Marine Kettani, chargée de mission près de la cheffe du service de l'expertise et de la modernisation et de Camille le Douaron, chargée de mission data au service de l'expertise et de la modernisation Jean-Yves Capul , administrateur des données, ministère de l'éducation nationale, de la jeunesse et des sports (AMDAC) Emmanuel Chiva , directeur de l’Agence de l’innovation de défense, ministère des armées, accompagné de Michaël Krajecki, directeur de projet IA Julien Chiaroni, directeur du Grand Défi en Intelligence Artificielle au Secrétariat général pour l'investissement Olivier Colliot , directeur de recherche, Centre national de la recherche scientifique (CNRS) Stéphanie Combes, directrice du Health Data Hub (plateforme nationale des données de santé) Stéphane Commans , responsable Portfolio Projets scientifiques et alliances à l’Institut de cardiométabolisme et nutrition (IHU ICAN) Michel Cottura , directeur général adjoint chargé du pilotage des programmes et de la maîtrise d'ouvrage, Pôle Emploi Bertrand Decaix, directeur de cabinet de l’Agence centrale des organismes de sécurité sociale (ACOSS), accompagné de Jean-Baptiste Courouble, directeur des systèmes d'information, de Carole Leclerc, directrice de l'innovation et de Xavier Bonnet, directeur de l'audit du pilotage de la performance et de la stratégie Page 216 Amaury Decludt , chef de la délégation à la stratégie de la direction générale des douanes et droits indirects Maud Decraene , responsable du pôle Juridique & Valorisation à l’Institut de cardiométabolisme et nutrition (IHU ICAN), déléguée à la protection des données (DPO) ICAN Nicolas Deffieux , directeur du pôle d’expertise de la régulation numérique (PEReN), ministère de l’économie, des finances et de la relance, accompagné de Florent Laboy, directeur adjoint, et de Lucas Verney, expert technique Romain Delassus , chef du service du numérique au ministère de la culture, accompagné de Christine Debray, cheffe du département stratégie et pilotage du numérique, ainsi que de Romain Joron et Aurélien Cornaux Marie-Laure Denis , présidente de la Commission nationale de l’informatique et des libertés (CNIL), accompagnée de Louis Dutheillet de Lamothe, secrétaire général, Bertrand Pailhes, directeur des technologies et de l’innovation et Thomas Dautieu, directeur de la conformité Stéphane Donne , directeur du département statistiques, système d’information et big data de la Caisse nationale des allocations familiales (CNAF), accompagné de Agnès-Laurence Nal, attachée de direction à la direction du réseau au ministère du budget, des comptes publics, de la fonction publique et de la réforme de l’État Stéphane Duhieu , délégué de recherche à l’Institut de la vision (IHU Foresight) Nathalie Demont , secrétaire fédérale de la Fédération générale des fonctionnaires force ouvrière (FGFFO) Thomas Dumortier, conseiller juridique à la Commission nationale consultative des droits de l’homme (CNCDH), accompagné de Célia Zolynski, personnalité qualifiée et de Lucien Castex, membre Olivier Esper, gestionnaire principal des politiques publiques chez Google, accompagné de Ludovic Peran, chef de produit pour la recherche IA, Inès Kouraïchi, responsable commercial secteur public France, Italie, Espagne et Portugal, et Léa Manenti, responsable collectivités territoriales Luc Farré , secrétaire général de l’Union nationale des syndicats autonomes (UNSA) Fonction publique Gabriel Ferriol, directeur du service de vigilance et de protection contre les ingérences numériques étrangères (Viginum) au Secrétariat général de la défense et de la sécurité nationale (SGDSN) Fabien Fieschi , directeur du numérique au ministère de l'Europe et des affaires étrangères (AMDAC) Xavier Fischer , Chief Executive Officer, DatakaLab Corinne Fortin, secrétaire générale de l’Institut du cerveau Page 217 Nicolas Goniak , conseiller pour les affaires intérieures à la représentation permanente de la France auprès de l’Union européenne, Benoît Blary, conseiller en charge des télécommunications, du numérique et des postes, Pauline Dubarry, conseillère Justice, Jonathan Cole, conseiller en charge des relations avec le Parlement européen Etienne Grass , Directeur général « secteur public », Capgemini Invent Emmanuel Grégoire, premier adjoint de la maire de Paris, accompagné de Pierre Musseau, conseiller ville intelligente et durable David Gruson , directeur du programme Santé, Jouve Stéphane Hatem , directeur de l’unité mixte de recherche 1166 (maladies cardiovasculaires et métaboliques, faculté de médecine Sorbonne Université Samuel Heuzé, chef de la mission d'organisation des services du Premier ministre (AMDAC) Sylvain Humbert , secrétaire général adjoint du Conseil d’État, chargé des juridictions administratives Mylène Jacquot , secrétaire générale de l’Union des fédérations de fonctionnaires et assimilés (Uffa-CFDT) Dominique Jamme , directeur général des services de la Commission de régulation de l’énergie, accompagné de Didier Lafaille, chef du service de la prospective et de l’innovation Edward Jossa , président de l'Union des groupements d’achats publics (UGAP), accompagné de Frédéric Trinquecoste, directeur des achats informatiques, de Lionel Ferraris, directeur en charge des politiques publiques et d’Emilia Soeiro-Terme, cheffe de département Prestations intellectuelles informatiques Nicolas Kanhonou , directeur de la promotion de l’égalité et de l’accès aux droits, Défenseur des droits accompagné de Sarah Benichou, adjointe au directeur et de Gaëtan Goldberg, chargé de mission numérique, droits et libertés Pascal Kessler , président de la Fédération autonome de la fonction publique (FA-FP) Thierry Kirat , directeur de recherche au Centre national de la recherche scientifique (CNRS), directeur de l’école doctorale « Sciences de la décision, des organisations, de la société et de l'échange » (SDOSE), université Paris Dauphine-PSL Claude Kirchner , directeur de recherche émérite de l’Institut national français de recherche en sciences et technologies du numérique (INRIA) Jérôme Lang , directeur de recherche au Centre national de la recherche scientifique (CNRS), Senior Researcher, Lamsade, université Paris Dauphine-PSL Ivan Laptev , directeur de recherche à l’Institut national français de recherche en sciences et technologies du numérique (INRIA) Page 218 Benoit Le Blanc , directeur de l’École nationale supérieure de cognitique, ENSC Bordeaux, président de l'Association française pour l'intelligence artificielle (AFIA) Marc Le Floch, directeur adjoint du réseau de la Caisse nationale des allocations familiales (CNAF), accompagné de Yasmine Leroueil, adjointe au directeur collaboratif et système d’information des fonctions supports Pascal Le Luong , secrétaire général de la Cour de cassation, accompagné d’Estelle Jond-Necand, conseillère référendaire et directrice du projet open data et de l’équipe-projet « pseudonymisation des décisions de justice » Georges-François Leclerc, préfet de la région Hauts-de-France, accompagné d’Amélie Puccinelli, secrétaire générale adjointe de la préfecture du Nord, d’Olivier Rovère, directeur territorial adjoint Nord de l’Agence régionale de santé des Hautsde-France, Jean-Yves Bessol, inspecteur d’académie, directeur académique des services de l’éducation nationale du Nord, et Jean-François Papineau, directeur zonal Nord de la sécurité publique. (extracted from110.txt)
Philippine Lefèvre-Rottmann , déléguée aux affaires publiques de Numeum, Jawaher Allala, CEO de Systnaps et Katya Lainé, CEO de Kwalys, administratrices, Valentin Hueber, délégué Industrie du futur, innovation et technologies, Lucile Lecomte, déléguée aux usages numériques Fabrice Lenglart , directeur de la recherche, des études, de l’évaluation et des statistiques au ministère des solidarités et de la santé (AMDAC) Thomas Lesueur , commissaire général au développement durable, ministère de la transition écologique (AMDAC), accompagné de Thomas Cottinet, directeur d’Ecolab et Marc Léobet, directeur de projet IA & Transition écologique auprès du directeur d'Ecolab Jérôme Letier , directeur du numérique (DNUM) du ministère de l’intérieur (AMDAC), Jean-Martin Jaspers, délégué ministériel à l'intelligence artificielle et Christophe Marquaille, chef du bureau Laboratoire valorisation des données à la DNUM, datalab Gaëlle Martinez, déléguée générale fonction publique de l’Union syndicale Solidaires Nicolas Mayer-Rossignol , président de la métropole Rouen-Normandie Françoise Mercadal-Delasalles, co-présidente du Conseil national du numérique (CNNum), accompagnée de Jean Cattan, secrétaire général, de Justine Cassell, directrice de recherche INRIA, Gilles Dowek, chercheur INRIA et de Philippine Régniez, rapporteure Rémi Meunier , Directeur « secteur public » de Dataiku etRomain Doutriaux, viceprésident marketing et communication Europe Louise Meyfroit , chargée d’opérations scientifiques à l’Institut de cardiométabolisme et nutrition (IHU ICAN) Page 219 Sarah Michot, Junior Advocacy & Campaign Manager et Anne Mollen, Policy & Advocacy Managerin, AlgorithmWatch Nicolas Monsarrat, directeur général Accenture Health, accompagné de Gabriel Bellenger, Health & Public Service Consulting lead Laurent Nunez, coordonnateur national du renseignement et de la lutte contre le terrorisme Cédric O, secrétaire d’État chargé de la transition numérique et des communications électroniques Akim Oural , adjoint au maire et délégué ville numérique, Ville de Lille Benoît Parizet, directeur de la transformation numérique de la Caisse des dépôts et consignations et de la stratégie digitale de la Banque des territoires, accompagné de Matthieu Blanc, responsable du Pôle Data Lior Perez , responsable du département des développements à la direction des systèmes d’information Météo France et Christophe Morel, directeur de la stratégie de Météo-France Manon Perrière , directrice adjointe de Tracfin, accompagnée de Mélanie Gourié, cheffe du département des systèmes d'information Edouard Philippe , maire de la Ville du Havre Lionel Ploquin , administrateur général des données de la DGFIP, ministère de l'économie, des finances et de la relance, accompagné de Su Yang, responsable du pôle donné de la délégation à la transformation numérique et de François Terrier, directeur de recherches au CEA, directeur du programme Intelligence Artificielle de CEA Tech et de l’inflexion IA de confiance du CEA List Fabrice Popineau , professeur à l’Ecole supérieure d’électricité (Supélec) Guillaume Poupard , directeur général de l’Agence nationale de la sécurité des systèmes d'information (ANSSI) Annie Prévot , directrice de l'Agence du numérique en santé, accompagnée de Marc Loutrel, directeur expertise Laurence Prevost , directrice de la division Consulting Secteur public de Sopra Steria, accompagnée de Nicolas Conso, directeur conseil Secteur public Simon Raout , directeur de la performance au centre hospitalier de Valenciennes Pierre-Louis Rolle , directeur des programmes Société Numérique et Nouveaux Lieux Nouveaux Liens & Mission incubateur de services numériques et AMDAC de l'Agence nationale de la cohésion des territoires (ANCT) Isabelle Ryl, directrice du centre de recherche de Paris de l’Institut national de recherche en sciences et technologies du numérique (INRIA) Page 220 Benoît Sagot , directeur de recherche à l’Institut national français de recherche en sciences et technologies du numérique (INRIA) Philippe Schall , chef du bureau Programmation des contrôles et analyse des données à la direction générale des finances publiques (DGFIP) Mehdi Siaghy , directeur de la recherche et de l’innovation, CHRU de Nancy Sébastien Soriano , directeur général de l’Institut national de l’information géographique et forestière (IGN) Bruno Sportisse , directeur général de l’Institut national de recherche en sciences et technologies du numérique (INRIA) accompagné d’Isabelle Herlin, coordinatrice de l’équipe recherche IA de l’INRIA Périca Sucevic, chef du pôle Droit et société d’Etalab (DINUM) et Paul-Antoine Chevalier, responsable du Lab IA et du pôle exploitation de données d’Etalab Jérôme Teillard , chef de projet Réforme de l'accès à l'enseignement supérieur du ministère de l'enseignement supérieur, de la recherche et de l’innovation (MESRI) Stéphane Trainel , AMDAC des ministères économiques et financiers Francky Trichet , vice-président de Nantes Métropole en charge de l'innovation, du numérique et de l'international et Claire Sacheaud, administratrice générale de la donnée, en charge de la stratégie data de la collectivité Mohammed-Adnène Trojette , conseiller action publique et numérique du Président de la République et conseiller technique numérique du Premier ministre Olivier Vallet , Président directeur général de Docaposte, accompagné de PierreEtienne Bardin, chief data officer du groupe La Poste Renaud Vedel , Coordinateur national pour l’intelligence artificielle Henri Verdier, ambassadeur pour le numérique Julien Vignon , directeur de projets IA, service de l'économie numérique à la direction générale des entreprises, ministère de l'économie, des finances et de la relance Cédric Villani , député, président de l’office parlementaire d’évaluation des choix scientifiques et technologiques Renaud Villard, directeur général de la Caisse nationale d'assurance vieillesse (CNAV), accompagné de Véronique Puche, directrice des systèmes d’Information de la CNAV Vincent Vuiblet , néphrologue au CHU de Reims, maître de conférences des universités, directeur de l’Institut d’intelligence artificielle en santé université de Reims Champagnes-Ardenne * Page 221 Conseil de l’Europe Muriel Décot, secrétaire de la Commission européenne pour l'efficacité de la justice (CEPEJ) Yannick Meneceur , conseiller spécial auprès du secrétariat de la Commission européenne pour l’efficacité de la justice (CEPEJ) Parlement européen Iban Garcia Del Banco , député européen Axel Voss, député européen Dragos Tudorache , député européen Commission européenne Cabinets Lucrezia Busa , membre du cabinet du commissaire européen en charge de la justice (Didier Reynders) Werner Stengg , membre du cabinet de la vice-présidente de la commission européenne Margrethe Vestager Nuria Subirats-Rebull , assistance chargée des politiques au cabinet du commissaire européen pour le marché intérieur (Thierry Breton) DG CONNECT Kilian Gross, chef de l’unité A2 développement et coordination des politiques en matière d’intelligence artificielle DG JUST Eike Gräf, gestionnaire des politiques pour les droits fondamentaux DG HOME Zsuzsana Felkai Janssen , coordinatrice IA Dan Rotenberg , adjoint au chef d’unité D4 Sécurité dans un monde numérique Gilles Robine , END DG JRC Carlos Torrecilla Salinas , chef de l’unité B6 (Economie numérique), Ignacio Sanchez, Luca Tangi et Emilia Gomez Agence Frontex Darek Saunders , directeur de l'Observatoire de la sécurité des frontières et chercheur dans l’unité Recherche et Innovation de Frontex Page 222 Annexe 4 : Glossaire Schéma synthétique du vocabulaire de base de l’intelligence artificielle261 261 J. (extracted from110.txt)
Données - Donnée structurée : donnée formatée selon un référentiel prédéfini, à laquelle sont associées des métadonnées, et qui peut être aisément trouvée et traitée (organisée dans un entrepôt de données, avec des champs normés) : Exemple : numéros de téléphones et adresses dans un fichier de personnel ; références de produits dans une base de données d’un fabricant ; montant des transactions sur un compte bancaire ; référence du dossier contentieux sur une décision de justice... (extracted from110.txt)
Exemple : lac de données, contenu d’un courrier électronique, motifs d’une décision de justice, image satellite, film… - Données d’entrée : données fournies à un système d’IA ou obtenues directement par lui et sur la base desquelles il produit un résultat (projet de règlement IA) - Données de sortie (résultats) : données produites par un système d’IA en appliquant un traitement algorithmique à des données d’entrée. (extracted from110.txt)
Infrastructures critiques et protection de l’environnement Composants de sécurité dans la gestion et l’exploitation du trafic routier et dans la fourniture d’eau, de gaz, de chauffage et d’électricité Composants de sécurité ou de contrôle d’infrastructures digitales Systèmes de contrôle des émissions et de la pollution Education et formation professionnelle Accès, admission et affectation dans les établissements d’enseignement ou de formation à tous niveaux Evaluation des résultats des personnes physiques et pilotage des programmes d’enseignement et de formation dans les établissements à tous niveaux Emploi, gestion de la main d’œuvre et accès à l’emploi indépendant Recrutement et sélection de personnes physiques (diffusion des offres d’emploi, pré-sélection, filtrage) Promotion, licenciement, attribution de tâches basée sur le comportements ou les caractéristiques personnelles de chacun, suivi et évaluation des performances et comportement des travailleurs sous contrat Accès et droit aux services privés, aux services publics et aux prestations sociales Gestion des prestations et services d’aide sociale (éligibilité, octroi, retrait, récupération…) Evaluation de la solvabilité des personnes physiques ou établissement d’une note de crédit (hors « petits fournisseurs » pour leurs besoins propres) Gestion des appels d’urgence (priorisation des interventions des services de secours, pompiers…) Fixation des primes d’assurance, gestion des souscriptions et réclamations Page 245 Systèmes d’aide aux autorités répressives, ou à une autre autorité agissant en leur nom Evaluation de la probabilité de commission d’une infraction ou de récidive Prédiction de la survenance ou de la réitération d’infraction sur la base du profilage individuel ou de l’évaluation des traits de personnalité, des caractéristiques et des antécédents judiciaires Evaluation du risque encouru par les victimes potentielles d’infractions pénales Détection des mensonges, des émotions et des hypertrucages (deepfakes) Evaluation de la fiabilité des preuves Gestion de la migration, de l’asile et des contrôles aux frontières, par les autorités compétentes ou en leur nom Détection de mensonges et d’émotions ; vérification de l’authenticité des documents de voyage et pièces justificatives Evaluation du risque sécuritaire, sanitaire ou migratoire que représente un étranger Vérification d’éligibilité des demandeurs (d’asile, de visa, de titre de séjour) à un statut Administration de la justice et processus démocratiques Utilisation par le juge (ou pour son compte) pour l’interprétation des faits et de la loi, application de la loi à un ensemble concret de faits Au terme d’un réexamen annuel, la Commission serait autorisée à compléter cette liste, dans les domaines limitativement énumérés par cette annexe, pour des usages présentant des risques équivalents ou supérieurs pour la santé, la sécurité, ou l’atteinte aux droits fondamentaux. (extracted from110.txt)
Page 248 En outre, les codes de conduite ont vocation à prendre en compte d’autres exigences : la viabilité environnementale, l’accessibilité aux personnes handicapées, la participation des parties prenantes à la conception et au développement des SIA, la diversité des équipes de développement… VII - Articulation avec les règles relatives au traitement des données à caractère personnel Le principe de base, explicité dans l’exposé des motifs, est que le règlement IA s’appliquerait sans préjudice du RGPD et de la directive police-justice, c’est-à-dire que les deux corps de règles s’appliqueraient cumulativement aux fournisseurs et utilisateurs dès lors que les SIA peuvent être qualifiés de traitements de données à caractère personnel. (extracted from110.txt)
Par exemple, la délégation à l’intelligence artificielle (DMIA) du ministère de l'Intérieur met en commun le développement de certains projets expérimentaux avec le ministère des Armées et celui de la Justice. (extracted from110.txt)
Page 260 Administrations répondantes Ministères - Ministère de l’Agriculture et de l’alimentation - Ministère de la Culture - Ministère de l'Intérieur - Ministère de l'Éducation nationale, de la jeunesse et des sports - Ministère de l'Économie, des Finances et de la Relance - Ministère des solidarités et de la santé - Ministère du Travail, de l’emploi et de l’insertion - Ministère de la justice - Ministère des Armées Autorités indépendantes - Commission nationale de l'informatique et des libertés (CNIL) - Haut Conseil du Commissariat aux Comptes (H3C) - Autorité de régulation des communications électroniques, des postes et de la distribution de la presse (ARCEP) - Autorité de contrôle des nuisances aéroportuaires (ACNUSA) - Commission nationale de contrôle des techniques de renseignement (CNCTR) - Conseil supérieur de l’audiovisuel (CSA) - Haut Conseil de l’évaluation de la recherche et de l’enseignement supérieur (HCERES) - Autorité de régulation des transports (ART) - Agence française de lutte contre le dopage (AFLD) - Haute Autorité pour la transparence de la vie publique (HATVP) - Autorité des marchés financiers (AMF) - Autorité de sûreté nucléaire (ASN) - Médiateur national de l’énergie - Haute Autorité pour la diffusion des œuvres et la protection des droits sur internet (HADOPI) Collectivités territoriales - Ville de Nîmes - Ville de Lyon - Ville de Marseille - Métropole Rouen Normandie - Métropole Nice Côte d’Azur - Métropole de Lyon - Bordeaux Métropole - Aix Marseille Provence Métropole - Montpellier Méditerranée Métropole - Strasbourg Métropole Page 261 - Grenoble-Alpes Métropole - Orléans Métropole - Toulouse Métropole - Région Hauts de France - Région Occitanie - Conseil Régional de Guadeloupe - Région Grand Est - Régions de France - Nantes Métropole et Ville de Nantes Secteur sanitaire et social - Caisse nationale des allocations familiales (Cnaf) - Caisse nationale de solidarité pour l'autonomie (Cnsa) - Hospices civils de Lyon - CHU de Nice - Pôle Emploi - Caisse des dépôts et consignations Page 262 Annexe 8 : Présentation des acteurs de l’écosystème de l’IA publique Les administrations centrales dédiées Bien que son décret d’attribution n’en fasse pas expressément mention, le secrétaire d’État chargé de la transition numérique et des communications électroniques joue un rôle de sponsor et d’impulsion dans la mise en œuvre de la stratégie nationale pour l’IA. (extracted from110.txt)
Page 267 Annexe 9 : Cartographie des cas d’usage des systèmes d’IA dans l’action publique Fiche n° 1 : Gestion des territoires Fiche n° 2 : Défense Fiche n° 3 : Sécurité, activités d’enquête, de contrôle et de sanction Fiche n° 4 : Justice Fiche n° 5 : Travail et emploi Fiche n° 6 : Education Fiche n° 7 : Protection sociale Fiche n° 8 : Santé * Page 268 Fiche n° 1 : Gestion des territoires La gestion et l’aménagement du territoire et de l'espace (voiries et réseaux, urbanisme…) constituent un terrain d’application privilégié pour le déploiement des SIA. (extracted from110.txt)
notamment le rapport – critique – du ministère de la justice américain de novembre 2014, Predictive analytics in law enforcement : a report by the Department of Justice . (extracted from110.txt)
Le systèm LSI-R ( Level of Service Inventory-Revised ), également en service aux EtatsUnis, calcule de la même façon un score de risque de récidive sur la base de 54 données d’entrée, porté à la connaissance de l’administration pénitentiaire et de la justice afin d’examiner les demandes de libération conditionnelle et, plus particulièrement, d’allouer davantage de ressources d’accompagnement et de suivi aux détenus à haut risque. (extracted from110.txt)
Au Royaume-Uni, le ministère de la justice a entraîné un réseau de neurones en matière de traitement de langage naturel, à partir de 500 rapports d’inspection des prisons (représentant 250 000 phrases) et l’a ré-entraîné grâce aux nouveaux rapports rédigés, afin d’identifier les situations à risque et les facteurs explicatifs d’incidents et d’orienter les décisions et les inspections en conséquence. (extracted from110.txt)
Il y a lieu de relever que, quand bien même les images font-elles l’objet, dès leur captation et avant leur transmission à un agent, d’un traitement destiné à empêcher toute identification par un humain (floutage…), elles constituent des données à caractère personnel soumises aux règles applicables en fonction de la finalité du traitement – en particulier le titre III de la loi du 6 janvier 1978, pris pour la transposition de la directive « police-justice » pour ce qui concerne les caméras utilisées à des fins de police administrative ou judiciaire (V. (extracted from110.txt)
Page 297 Fiche n° 4 : Justice L’activité juridictionnelle est habituellement identifiée comme l’un des secteurs prometteurs de l’action publique pour le déploiement de l’intelligence artificielle. (extracted from110.txt)
Elle est aussi l’un de ceux dans lesquels, sur les plans médiatique et marketing, le fantasme a d’emblée pris le pas sur la raison, à travers la notion de « justice prédictive », présentée abusivement comme la capacité d’un système d’IA à deviner par avance le sens des décisions de justice. (extracted from110.txt)
Sur le plan terminologique, elle repose sur une traduction littérale erronée de l’expression anglo-saxonne « predictive justice » qui signifie en réalité « justice prévisible ». (extracted from110.txt)
La prévisibilité du droit est un enjeu majeur, pleinement intégrée à la construction jurisprudentielle à travers, notamment, l’importance du précédent ; la prédictibilité des décisions de justice est quant à elle une illusion présomptueuse et dangereuse, surtout lorsqu’elle vise à anticiper sur la position personnelle de tel ou tel juge, pris individuellement. (extracted from110.txt)
La seconde méprise tient à ce que les outils de « justice prédictive » n’ont évidemment pas la capacité de lire l’avenir, pas plus que l’humain. (extracted from110.txt)
Le scepticisme croissant qu’a produit le discours marketing, en l’absence de concrétisations significatives dans le quotidien des tribunaux et des avocats, a d’ailleurs conduit à un recul progressif de cette notion de « justice prédictive » au profit de celle de « jurimétrie », portée par des acteurs institutionnels comme le Conseil national des barreaux (et inspirée de l’anglais « jurimetrics » qui recouvre les méthodes d’analyse quantitative et statistique du droit), ou encore de « justice algorithmisée »310. (extracted from110.txt)
10 du code de justice administrative et le 3° alinéa de l’art. (extracted from110.txt)
Storchan, Mécanisme d’une justice algorithmisée , Ed. (extracted from110.txt)
Page 298 L’effervescence irrationnelle autour du phénomène de la « justice prédictive » ne saurait toutefois justifier une réaction de rejet, de dénigrement ou de défiance. (extracted from110.txt)
D’une part, elle ne doit pas occulter l’absolue nécessité pour le monde de la justice de développer une réflexion prospective sur l’impact de ces innovations technologiques sur l’avenir du juge et de son office. (extracted from110.txt)
En outre, les algorithmes d’apprentissage-machine – essentiellement dans le domaine du traitement du langage naturel, puisqu’il s’agit en règle générale de traiter des données de textes – peuvent être développés en mettant à profit la structuration naturelle de certaines données, à commencer par les décisions de justice elles-mêmes, qui suivent en général un format relativement standardisé. (extracted from110.txt)
Dans ce champ de l’action publique également, la réflexion sur l’IA doit impérativement s’inscrire dans une réflexion plus vaste sur la numérisation de l’activité juridictionnelle , ce qui suppose à la fois : - de doter les magistrats, les agents de greffe et l’ensemble des collaborateurs y participant des équipements informatiques adaptés à leurs missions, alors que l’informatisation de la justice est parfois incomplète ou en trop fort décalage avec l’état de l’art ; - de privilégier les processus dématérialisés et, plus encore, nativement numériques (c’est-à-dire avec des documents et données produites dès le départ en format numérique et qui n’ont jamais eu de matérialité physique, par 311 V. (extracted from110.txt)
sur cette typologie et des statistiques sur les projets européens en cours ou réalisés : Study on the use of innovative technologies in the justice field – Final Report [Etude sur l’utilisation des technologies innovantes dans le domaine de la justice – Rapport final, commandée par la Commission européenne, septembre 2020. (extracted from110.txt)
Le développement optimal de systèmes d’IA publics et privés dans le domaine du droit suppose en outre d’achever, dans les délais prescrits par l’arrêté du 28 avril 2021, la mise en ligne des décisions de justice dans les conditions prévues par la législation issue de la loi pour une République numérique, en sus de la mise à disposition, dans un format exploitable, des données afférentes aux autres sources du droit, nationales, européennes et internationales. (extracted from110.txt)
La fonction de juger S’agissant de la fonction de juger, au sens strict, la mise en œuvre de systèmes d’IA est théoriquement concevable dans les deux volets classiques que sont la prise de décision automatisée (« juge robot ») et l’aide à la décision de justice (l’IA comme « assistant de justice », intervenant pour aider à identifier, analyser et résoudre des questions posées dans un litige, notamment à travers une fonctionnalité de recommandation de solutions). (extracted from110.txt)
Elle peut en outre emprunter aux deux branches classiques de l’IA : les systèmesexperts, consistant à programmer tout ou partie des raisonnements juridiques sous la forme d’un arbre de causalité (« si délai de recours de deux mois à compter de la notification + décision notifiée avec voies et délais de recours le 2 mars 2021 + requête introduite le 10 mai 2021, alors requête tardive et rejet pour irrecevabilité ») ; et l’apprentissage-machine, consistant à nourrir l’algorithme des décisions de justice rendues par le passé afin de dégager des raisonnements et des appréciations qui pourront ensuite être utilisés pour la résolution de litiges futurs. (extracted from110.txt)
Il pourra s’agir, par exemple, de l’identification et de la pondération des critères ou indices mobilisés par le juge pour fixer le montant d’une prestation ou d’une indemnité, le quantum d’une sanction, le droit d’obtenir telle ou telle décision favorable… L’automatisation de la fonction de juger ne saurait être celle de la décision de justice elle-même . (extracted from110.txt)
En l’état de la loi, la garantie humaine dans la fonction de juger est expressément prévue au premier alinéa de l’ article 47 de la loi du 6 janvier 1978, qui exclut qu’une décision de justice impliquant une appréciation sur le comportement d’une personne puisse avoir pour fondement (exclusif ou non) un traitement automatisé de données à caractère personnel destiné à évaluer certains aspects de la personnalité de la personne. (extracted from110.txt)
Toutefois, il est certain que l’ensemble des dispositions régissant le fonctionnement des juridictions, voire la Constitution elle-même, font obstacle à ce qu’une décision de justice soit rendue Page 300 sur le fondement exclusif d’un traitement algorithmique – autrement dit, que le SIA prenne lui-même la décision de justice. (extracted from110.txt)
Le robot-juge est en général considéré comme présentant les atouts suivants313 : - L’uniformité territoriale de la jurisprudence (pour autant que les textes applicables soient identiques sur le territoire considéré) et l’égalité des citoyens devant la justice, quelle que soit la juridiction à laquelle ils s’adressent. (extracted from110.txt)
Le résultat est censé être identique à situation identique (ou ne différant que sur des aspects non pertinents pour la résolution de la question) ; - Une plus grande sécurité juridique en raison d’une prévisibilité accrue du droit et la suppression (ou la réduction) de « l’aléa judiciaire », par la neutralisation de paramètres susceptibles d’influencer le sens de la décision ou sa motivation314 ; - La prise en compte d’un nombre accru de paramètres pertinents et la capacité, à première vue paradoxale pour des outils reposant sur la catégorisation, à produire des jugements « sur-mesure », épousant plus finement les contours particuliers de chaque litige ; - La célérité de la justice : l’IA calcule plus vite que l’humain, et ne souffre pas de la fatigue ; - La réduction du coût de la justice (baisse d’effectifs, économie des coûts inhérents à des formations longues, possibilité de traiter des litiges de masse à coût quasi constant…) ; - Une baisse du volume du contentieux lui-même, les parties étant davantage susceptibles de régler leurs litiges à l’amiable si elles disposent d’une évaluation de leurs chances de succès. (extracted from110.txt)
Elles tiennent : - à l’acceptabilité sociale d’une justice, civile et plus encore pénale, qui ne serait plus rendue directement par l’homme, tout en l’étant juridiquement et symboliquement au nom du peuple français ; - à la disparition de la fonction « cathartique » de l’oralité dans le procès, qui permet de s’exprimer et d’être écouté par la partie adverse et par des juges : cette fonction, qu’elle se déploie lors de l’instruction ou de l’audience, et a 313 V. (extracted from110.txt)
Van den Branden, La robotisation de la justice , in L’intelligence artificielle et le droit, sous la coordination de H. (extracted from110.txt)
Van den Branden, Les robots à l’assaut de la justice. (extracted from110.txt)
Robin, Justice et intelligence artificielle, préparer demain – épisode I, Dalloz Actualité, 14 avril 2020 : « Nombreuses sont les études qui démontrent que des données fort variées et parfois étonnantes − le peƟt-déjeuner du juge, sa fatigue, l'influence médiatique, son égocentrisme à ses préjugés divers − peuvent inﬂuer sur la décision prise » (et les renvois aux études mentionnés). (extracted from110.txt)
Lassègue, Justice digitale. (extracted from110.txt)
Certes, la rédaction des décisions de justice est relativement normalisée ; mais elles sont loin d’être parfaitement homogènes et des subtilités rédactionnelles peuvent aisément échapper à la machine. (extracted from110.txt)
Il ne semble pas que la voie de la robotisation de la justice ait été empruntée avec succès par d’autres pays occidentaux à ce jour. (extracted from110.txt)
La proposition de règlement de la Commission européenne identifie à cet égard le domaine de « l’administration de la justice » comme susceptible de donner lieu à la mise en service de systèmes d’IA à hauts risques, et prévoit elle-même que les systèmes d’aide à la décision de justice (consistant à rechercher et interpréter les faits et la loi, et à appliquer la loi à un ensemble concret de faits) relèvent de cette catégorie. (extracted from110.txt)
notamment l’expérimentation d’un outil de « justice prédictive » dans les cours d’appel de Douai et de Rennes, qui ne s’est pas avéré plus performant que les moteurs de recherche classiques. (extracted from110.txt)
318 Cette initiative n’est d’ailleurs pas recensée dans l’étude sur l’utilisation des technologies innovantes dans le domaine de la justice réalisée à la demande de la Commission européenne en septembre 2020. (extracted from110.txt)
Mal conçus ou mal utilisés, ils peuvent induire en erreur le juge et aboutir à des résultats exactement inverses à ceux qui étaient recherchés et, partant, au déni de justice. (extracted from110.txt)
La rédaction des décisions de justice constitue également un champ intéressant pour le déploiement des SIA. (extracted from110.txt)
821 du code de justice administrative, énonciation qu’aucun des moyens soulevés n’est de nature à permettre l’admission…). (extracted from110.txt)
le projet « Avvocatura 2020 » en Italie ou encore l’outil, basé sur un réseau de neurones de traitement du langage naturel, développé par le ministère de la justice britannique afin de détecter rapidement des récurrences dans les centaines de rapports d’inspection des établissements pénitentiaires (rédigés par l’administration pénitentiaire, les commissions de surveillance indépendantes et l’Ombudsman des prisons et de la probation, équivalent du Contrôleur général des lieux de privation de liberté), notamment pour l’analyse des incidents, l’identification de facteurs géographiques ayant une incidence sur les établissements, orienter les inspections et mieux allouer les moyens. (extracted from110.txt)
En revanche, il serait opportun de confier à des SIA opérés par les juridictions, et (ré-)entraînés à partir de données issues de textes et d’actes juridiques, le soin d’assurer la traduction des décisions de justice (notamment en anglais), à des fins de rayonnement institutionnel mais aussi de convergence dans l’application du droit européen (en améliorant l’accessibilité des décisions juridictionnelles françaises pour des juridictions étrangères). (extracted from110.txt)
Page 308 Enfin, la tâche qui donne lieu au plus grand nombre de projets de SIA dans le monde judiciaire européen est celle qui consiste à pseudonymiser voire anonymiser les décisions de justice afin de réduire ou supprimer l’atteinte à la protection de la vie privée des protagonistes du dossier au moment de leur publication. (extracted from110.txt)
Pseudonymisation automatisée des décisions judiciaires en France Afin d’assurer la mise en œuvre de l’« open data des décisions de justice » décidée par la loi pour une République numérique, la Cour de cassation a lancé en 2019 la conception d’un SIA reposant sur l’apprentissage machine supervisé, permettant d’automatiser la pseudonymisation des décisions de justice rendues publiques, c’està-dire la suppression des éléments permettant l’identification directe des parties et des tiers qui y sont mentionnés. (extracted from110.txt)
L’exécution des décisions de justice Les SIA pourraient être mobilisés pour dynamiser l’exécution des décisions de justice. (extracted from110.txt)
Le ministère de la justice finlandais a développé un outil (« Robot Process Page 309 Automation ») permettant de relier plus facilement les paiements aux sanctions auxquelles ils correspondent, et de traiter les défauts de paiement comme les surpaiements et les remboursements (en cas d’annulation de la sanction par exemple). (extracted from110.txt)
La relation avec les usagers du service public de la justice et l’accès au juge La numérisation de l’activité juridictionnelle, en particulier le développement des téléprocédures, a considérablement accru les possibilités de suivi, par les parties, de l’avancement du traitement de leur dossier. (extracted from110.txt)
Le ministère de la justice autrichien conçoit également un chatbot qui assiste les parties dans le suivi de leur dossier sur téléphone portable. (extracted from110.txt)
Il pourrait éclairer les justiciables sur la juridiction qu’ils doivent saisir pour contester une décision donnée, les modalités de cette saisine, l’état d’avancement du dossier, les échéances à venir, les modalités de contestation de la décision rendue… Les juridictions, les ordres d’avocats et le Conseil national des barreaux permettraient de fédérer les acteurs de la justice autour de la construction d’une interface. (extracted from110.txt)
Ce n’est pas plus dans l’esprit de ses créateurs que de remplacer les professionnels du droit (ou les syndicats) dans le conseil aux justiciables333, mais de faire en sorte de faire du numérique « le vecteur d’une plus grande justice » dès lors que « la compétence reconnue au professionnel ne saurait justifier une inaction dont pâtirait la partie la plus faible dans la relation de travail »334. (extracted from110.txt)
Robin, « Justice et intelligence artificielle, préparer demain – épisode III2 », Dalloz actualité, 17 avril 2020. (extracted from110.txt)
Il y a lieu d’observer à titre liminaire que l’expression même d’intelligence artificielle ne figure dans aucun de ces textes, et que ni le RGPD, ni la directive police-justice n’évoque expressément les algorithmes. (extracted from110.txt)
Le considérant 15 du RGPD et le considérant 18 de la directive police-justice rappellent le principe de neutralité technologique de la protection des données, c’est-à-dire son application à tout traitement de données à caractère personnel, quelle que soit la technique utilisée. (extracted from110.txt)
Ainsi : - s’agissant des traitements de données à caractère personnel régis par la directive « police-justice », l’article 95 de la loi du 6 janvier 1978, transposant l’article 11 de cette directive dans le sens le plus strict et en cohérence avec la jurisprudence du Conseil constitutionnel ( n° 2003-467 DC du 13 mars 2003), interdit par principe qu’une décision qui produit des effets juridiques ou affecte de manière significative une personne dans ce domaine soit prise sur le seul fondement d’un traitement automatisé destiné à prévoir ou évaluer certains aspects personnels de la personne. (extracted from110.txt)
Il proscrit en outre le profilage entraînant une discrimination sur la base des catégories particulières de données (« données sensibles ») ; - le premier alinéa de l’article 47 de la loi de 1978 interdit l’édiction d’une décision de justice impliquant une appréciation sur le comportement d'une personne sur le fondement d’un traitement automatisé de données à caractère personnel destiné à évaluer certains aspects de la personnalité de cette personne. (extracted from110.txt)
A la lettre, cette prohibition s’étend ainsi à la simple prise de décision assistée ; - l’article 4-3 de la loi n° 2016-1547 du 18 novembre 2016 de modernisation de la justice du XXIème siècle fait obstacle à ce qu’un service en ligne de conciliation, de médiation judiciaire ou d’arbitrage ait « pour seul fondement un traitement algorithmique ou automatisé de données à caractère personnel ». (extracted from110.txt)
Au plan européen, la CEPEJ (commission européenne pour l’efficacité de la justice auprès du Conseil de l’Europe) a adopté, les 3 et 4 décembre 2018, une charte des principes éthiques relatifs à l’utilisation de l’IA dans les systèmes judiciaires, adoptée par les 47 Etats membres du Conseil de l’Europe. (extracted from110.txt)
Venice Commission :: Council of Europe EN FR Login Venice Commission Council of Europe Home The Council in brief Human Rights Democracy Rule of Law Organisation 46 Countries Topics Newsroom Council of Europe Human Rights and Rule of Law Venice Commission EN | FR | DE | IT | RU The Commission About us Types of activities Founding documents News Programme References Members Member states Individual members Map of member States Positions Statements Democratic institutions and fundamental rights Constitutional reforms Fundamental rights Democratic institutions Rule of law Judicial reforms Ombudsman Elections, referendums and political parties Council for Democratic Elections Conferences of Electoral Management Bodies Seminars and assistance Political parties VOTA database Council of Europe and elections Constitutional justice Regional co-operation Constitutional Courts CODICES E-Bulletin World Conference on Constitutional Justice Non-European Partners Southern Mediterranean Overview of activities Highlights by beneficiary Recent events UniDem Med Campus List of UniDem seminars Intercultural Workshops on Democracy Arabic Speaking EMBs Tunisia : support to independent bodies Central Asia Kyrgyzstan – support to electoral reform Central Asia Rule of Law Programme List of events Latin America Overview of activities Recent events Sub-commission on Latin America Main documents Statute Main reference documents Compilations of studies and opinions Annual reports Documents Opinions Ongoing opinions Follow-up Studies Recent documents access by: Countries Topics Series Language Search Web Resources Newsletter Publications Articles on the Commission Links Library Constitutions Contact us For the media Reset password CDL-AD(2019)016-eJoint Report of the Venice Commission and of the Directorate of Information Society and Action against Crime of the Directorate General of Human Rights and Rule of Law (DGI), on Digital Technologies and Elections, adopted by the Council of Democratic Elections at its 65th meeting (Venice, 20 June 2019) and by the Venice Commission at its 119th Plenary Session (Venice, 21-22 June 2019) Show related documents (1) Choose a year all 2019 CDL-AD(2019)016 French 07/02/2020 - Public Rapport conjoint de la Commission de Venise et de la Direction de la société de l’information et de la lutte contre la criminalité, Direction générale Droits de l’homme et État de droit (DGI) sur les technologies numériques et les élections, adopté par le Conseil des élections démocratiques lors de sa 65e réunion (Venise, 20 juin 2019) et par la Commission de Venise lors de sa 119e session plénière (Venise, 21-22 juin 2019) View in full screen Activities Human Rights and Rule of Law DemocracyWho we areHuman Rights Convention Council of Europe Treaties Press Multimedia NewsroomWeb TV Photo galleries Campaigns Useful links Employment Call for tenders Archives Archived web pages SitemapAmicaleAdministrative TribunalE-cards Contact us Secretary General & Deputy Secretary General Media Contacts External Offices Visit us Newsletters Patronage Form close Disclaimer - © Council of Europe 2014 - © photo credit - Webmaster Bookmarks Print RSS © Council of Europe 2007-2025 (extracted from138.txt)
However, “had he been able to examine and contest the logic of the COMPAS system to prove that its score gave a distorted picture of his life, he might have gone home much earlier” (Wexler 2017b ) Rodríguez’s case is an example of the discriminatory use of AI in criminal justice, which also includes prominent AI applications for the purposes of predictive policing. (extracted from689.txt)
In such cases, which include law enforcement and criminal justice applications, the attempt to modify the data to reduce or eliminate underlying biases may inadvertently introduce new challenges. (extracted from689.txt)
The desirability of the use of AI solutions is also something that should be duly considered– with regard to the purpose, advantages and burden imposed by them on social values, justice and the public interest. (extracted from689.txt)
It has links to broader societal structures and the justice of our socio-economic systems and thus relates to the problem of surveillance capitalism. (extracted from689.txt)
( 2018 ) outline how criminal justice risk assessment tools could beneﬁt low-risk individuals through increased pre-trial releases and shorter sentences. (extracted from689.txt)
veriﬁcation of the authenticity of travel documents); and in the administration of justice and democratic processes (e.g. (extracted from689.txt)
Directorate-General for Justice and Consumers, European Commission, Brussels. (extracted from689.txt)
Expert Group on Liability and New Technologies, Directorate-General for Justice and Consumers, European Commission, Brussels. (extracted from689.txt)
For instance, the opening of the Universal Declaration of Human Rights states that “recognition of the inherent dignity and of the equal and inalienable rights of all members of the human family is the foundation of freedom, justice and peace in the world” (UN 1948 ). (extracted from689.txt)
Federal Ministry of Justice and Federal Ofﬁce of Justice, Berlin. (extracted from689.txt)
One could perhaps even argue that AI has been linked directly to international justice and sustainability through the SDGs. (extracted from689.txt)
Hollywood, “Evaluation of the Shreveport Predictive Policing Experiment,” National Institute of Justice, 2014, https://nij.ojp.gov/topics/articles/evaluation-shreveport- predictive-policing-experiment . (extracted from702.txt)
This Declaration notably builds on primary EU law, in particular the Treaty on European Union, the Treaty on the Functioning of the European Union, the Charter of Fundamental Rights of the European Union, as well as on secondary law and the caselaw of the Court of Justice of the European Union. (extracted from299.txt)
"Why Machine Learning May Lead to Unfairness: Evidence from Risk Assessment for Juvenile Justice in Catalonia", Best Paper Award, International Conference on AI and Law , 2019 Certain AI programmes for facial analysis display gender and racial bias, demonstrating low errors for determining the gender of lighter -skinned men but high errors in determining gender for darker -skinned women . (extracted from229.txt)
Individuals and legal entities may face difficulties with effective access to justice in situations where such decisions may negatively affect them . (extracted from229.txt)
This is without prejudice to the question whether, for the purpose of liability to end -users or other parties suffering harm and ensuring effective access to justice, which party should be liable for any damage caused . (extracted from229.txt)
Accept additional cookies Reject additional cookies View cookies Hide cookie message Skip to main content GOV.UK Navigation menu Menu Menu Services and information Benefits Births, death, marriages and care Business and self-employed Childcare and parenting Citizenship and living in the UK Crime, justice and the law Disabled people Driving and transport Education and learning Employing people Environment and countryside Housing and local services Money and tax Passports, travel and living abroad Visas and immigration Working, jobs and pensions Government activity Departments Departments, agencies and public bodies News News stories, speeches, letters and notices Guidance and regulation Detailed guidance, regulations and rules Research and statistics Reports, analysis and official statistics Policy papers and consultations Consultations and strategy Transparency Data, Freedom of Information releases and corporate reports Search GOV.UK × Search GOV.UK Search Search GOV.UK Search Home Business and industry Industrial strategy Policy paper Artificial Intelligence Sector Deal A Sector Deal between government and the Artificial Intelligence (AI) sector. (extracted from188.txt)
Cancel Services and information Benefits Births, death, marriages and care Business and self-employed Childcare and parenting Citizenship and living in the UK Crime, justice and the law Disabled people Driving and transport Education and learning Employing people Environment and countryside Housing and local services Money and tax Passports, travel and living abroad Visas and immigration Working, jobs and pensions Government activity Departments News Guidance and regulation Research and statistics Policy papers and consultations Transparency How government works Get involved Support links Help Privacy Cookies Accessibility statement Contact Terms and conditions Rhestr o Wasanaethau Cymraeg Government Digital Service All content is available under the Open Government Licence v3.0, except where otherwise stated © Crown copyright (extracted from188.txt)
Such systems can therefore be ex pected to act in line with the norms and values of a humane society, including fairness, justice, ethics, responsibility and trustworthinessControl: HCAI preserves human agency and sense of re sponsibility by designing AI systems to give users a high level of understanding of, and control over, their specific and unique processes and outputsThis manifesto emerges from the frustrations and concerns shared among researchers at AiTH: about the state of discourse on AI ethics and trustworthiness, about the unquestioned dominance of Big Tech, and about the deficiencies of techno-solutionist and machine-centred approaches to AI. (extracted from89.txt)
It might also compound further the pro blems with international transfer of data after that the US Safe Harbour and later the EU -US Privacy Shield were invalidated by the Court of Justice (Schrems I and II case- law, C -362/14 and C -311/18), as illustrated by Kiner (2020). (extracted from177.txt)
The Schrems II judgment of the Court of Justice and the future of data transfer regulation. (extracted from177.txt)
European Law Blog: https://europeanlawblog.eu/2020/07/17/the -schrems -iijudgment -of-the-court -of-justice -and -the-future -of-data -transfer- regulation/ . (extracted from177.txt)
• Utilise AI innovation s pro-socially so as to enable bonds of interpersonal solidarity to form and individuals to be socialised and recognised by each other • Use AI technologies to foster this capacity to connect so as to reinforce the edifice of trust, empathy, reciprocal responsibility, and mutual understanding upon which all ethically well founded social orders rest → CARE for the wellbeing of each and all : • Design and deploy AI systems to foster and to cultivate the welfare of all stakeholders whose interests are affected by their use • Do no harm with these technologies and minimise the risks of their misuse or abuse Understanding Artificial Intelligence Ethics and Safety 11 • Prioritise the safety and the mental and physical integri ty of people when scanning horizon s of technological possibility and when conceiving of and deploying AI applications → PROTECT the priorities of social values, justice, and the public interest : • Treat all individuals equally and protect social equity • Use digital technologies as a n essential support for the protection of fair and equal treatment under the law • Prioritise social welfare, public interest, and the consideration of the social and ethical impacts of innovation in determining the legitimacy and desirability of AI technologies • Use AI to empower and to advance the interests and well -being of as many individuals as possible • Think big -picture about the wider impacts of the AI technologies you are conceiving and developing. (extracted from605.txt)
Ethical considerations about looking after patient wellbeing and clinical safety are paramount and wider justice c oncerns about improving healthcare for all and health equity factor in as well. (extracted from605.txt)
I am also incredibly grateful for the impact that our interactions with the Ministry of Justice (MoJ)’s Data Science Hub has had on developing the framing for this guide. (extracted from605.txt)
'It's reducing a human being to a percentage': Perceptions of justice in algorithmic decisions. (extracted from605.txt)
Optimized scoring systems: Toward trust in machine learning for healthcare and criminal justice. (extracted from605.txt)
Criminal Justice and Behavior , 46(2), 185 -209. (extracted from605.txt)
Data Justice Lab. (extracted from605.txt)
Dirty data, bad predictions: How civil rights violations impact police data, predictive policing systems, and justice. (extracted from605.txt)
” 11 ∼ “AI actors should promote social justice, by safeguarding fairness and non -discrimination of any kind in compliance with international law. (extracted from349.txt)
11 Council of Europe Commissioner for Human Rights (2019) – “Unboxing AI: 10 steps to protect Human Rights” 27 the least advanced to ensure that the benefits of AI technologies are shared such that access to and participation in the AI system life c ycle for the latter contributes to a fairer world order with regard to information, communication, culture, education, research, and socio -economic and political stability...AI actors should promote social justice and safeguard fairness and non-discriminat ion of any kind in compliance with international law. (extracted from349.txt)
This should also include the possibility of receiving insight into and challenging AI- informed decisions in the context of law enforcement or justice, including the right to review of such decisions by a human. (extracted from349.txt)
AI can increase the efficiency International Covenant on Civil and Political Rights: -Article 2, International Covenant on Civil and Political Rights – Right to effective remedy -Article 14, International Covenant on Civil and Political Rights – Right to fair tri al Council of Europe Resources: -European Commission for the Efficiency of Justice, ‘European ethical charter on the use of Artificial Intelligence in judicial systems and their environment’ – Council of Europe 22 CAHAI Feasibility Study , Council of Europe CAHAI (2020)23. (extracted from349.txt)
Washington, DC: US De partment of Justice, Office of Justice Programs, Office of Juvenile Justice and Delinquency Prevention. (extracted from349.txt)
59 Use context (Questions 1 -20) • Sector or domain in which the system is being built • Existing law and regulatory environment of the sector or domain • Impact-level of the system • Prohibited systems and uses • Scope of deployment (breadth and temporality) • Technological maturity • Existing system (human or technological) that the application is replacing • Bias and discrimination in sector or domain context • Environmental context • Cybersecurity context Data Lifecycle Context (Questions 21 -40) • Data quality, integrity, and provenance • Means and methods o f data collection • Data types • Dataset linkage • Data labelling and annotating practices Goal Setting and Problem Formulation Context (Questions 41 -42) • Decision to design • Definition of outcome Model Design & Development Context (Questions 43 -46) • AI model characteristics • Pre-processing and feature engineering • Model selection Model Output & Implementation Context (Questions 47 -52) • Model inference • Model verification and validation • Model accuracy and performance metrics System -User Interface and Human Factors Context (Questions 53 -55) • Implementers or users of the system • Level of automation/level of human involvement and choice Rights & Freedoms Context (Questions 56 -71) • Respect for and protection of human dignity • Protection of human freedom and autonomy • Non-discrimination, fairness, and equality • Data protection and privacy context • Accountability and access to justice • Social and economic rights 60 Depending on the form that the section one questions take, certain responses to each question will trigger one of three classes of risk factors, prohibitive, major, or moderate: Prohibitive risk factor Prohibitive risk factors indicate the presence of determinants of potential harms that trigger the precautionary principle and precipitate preemptive measures to prevent adverse impacts on the human rights and fundamental freedoms of affected persons, democracy, and the rule of law. (extracted from349.txt)
NOT APPLICABLE : No message Accountability and access to justice 60) Will sufficient and transparently reported processes be implemented throughout the project’s lifecycle to ensure end -toend accountability across the production and use of the AI system? (extracted from349.txt)
NOT APPLICABLE : No message 168 62) If the AI system is used in the field of justice and law enforcement, will meaningful information be provided to affected rights -holders about the existence and use of the system, its role within law enforcement and the judicial process, and the right to challenge the decisions informed or made thereby? (extracted from349.txt)
YES, WE HAVE PLANS IN PLACE TO DO THIS WE HAD NOT CONSIDERED IT, BUT MAY DO THIS WE HAD NOT CONSIDERED IT, BUT ARE UNLIKELY TO DO THIS NO, WE ARE NOT PLANNING TO DO THIS UNSURE NOT APPLICABLE Example YES, WE HAVE PLANS IN PLACE TO DO THIS : No message WE HAD NOT CONSIDERED IT, BUT MAY DO THIS / WE HAD NOT CONSIDERED IT, BUT ARE UNLIKELY TO DO THIS / NO, WE ARE NOT PLANNING TO DO THIS: Major modifiable risk factor • Where AI system are being used in the field of justice and law enforcement and meaningful information is not provided to affected rights -holders about the existence and use of the system, its role within law enforcement and the judicial process, and the right to challenge the decisions informed or made thereby, this presents a major modifiable risk factor for adverse impacts on the human rights and fundamental freedom of persons. (extracted from349.txt)
NOT APPLICABLE : No message 63) If the AI system is used in the field of justice and law enforcement, will sufficiently and transparently reported processes be implemented throughout the project’s lifecycle to ensure that its deployment is in line with the essential requirements of impac ted individuals' right to a fair trial (equality of arms, right to a natural YES, WE HAVE PLANS IN PLACE TO DO THIS : No message WE HAD NOT CONSIDERED IT, BUT MAY DO THIS / WE HAD NOT CONSIDERED IT, BUT ARE UNLIKELY TO DO THIS / NO, WE ARE NOT PLANNING TO DO THIS: Major modifiable risk factor 170 judge established by law, the right to an independent and impartial tribunal, and respect for the adversarial process)? (extracted from349.txt)
YES, WE HAVE PLANS IN PLACE TO DO THIS WE HAD NOT CONSIDERED IT, BUT MAY DO THIS WE HAD NOT CONSIDERED IT, BUT ARE UNLIKELY TO DO THIS NO, WE ARE NOT PLANNING TO DO THIS UNSURE NOT APPLICABLE Example • Where AI system are being used in the field of justice and law enforcement and sufficiently and transparently reported processes are not implemented throughout the project’s lifecycle to ensure that its deployment is in line with the essential requirements of impacted individuals' right to a fair trial (equality of arms, right to a natural judge established by law, the right to an independent and impartial tribunal, and respect for the adversarial process), this presents a major modifiable risk factor for adverse impacts on the human rights and fundamental freedom of persons. (extracted from349.txt)
FAIRNESS is insepara bly connected with sociolegal conceptions of equity and justice , which may emphasize a variety of features such as non-discrimination , equit able outcomes, or procedural fairness through bias mitigation , but also social and economic equality, diversity, and inclusivenes s. (extracted from349.txt)
But, beyond this, diligent consideration of the practical, social, or policy issue being addressed by the system will also trigger, inter alia , reflection on the complex intersection of potential algorithmic bias, the cascading effects of sociohistorical patterns of racism and discrimination, wider societal and community impacts, and the potential effects of the use of the model on the actors in the criminal justice systems who will become implementers and subjects of the technology. (extracted from349.txt)
In race and social justice discourse, members of society are marginalised by holding an identity or being placed in a demographic category that is not attached to the dominant side of the prevailing structure of power and by which they experience oppression . (extracted from349.txt)
Pre-designated High- Risk or Safety Critical Sector: Annex III of the European Union Proposed Rules on Artificial Intelligence indicates ‘high-risk’ sectors as those concerned with education, emergency services, employment, financial services, public benefits , law enforcement, immigration, border control, and the administration of justice and democratic processes . (extracted from349.txt)
'It's reducing a human being to a percentage': Perceptions of justice in algorithmic decisions. (extracted from349.txt)
Algorithmic accountability and digital justice: A critical assessment of technical and sociotechnical approaches. (extracted from349.txt)
Optimized scoring systems: Toward trust in machine learning for healthcare and criminal justice. (extracted from349.txt)
for technologies in support to Justice as defined in NRRP), initiatives for Transitions 4.0, co-funded by MUR and by private companies with NRRP incentives, for Space data analysis, for Environment and ecological transitions (e.g. (extracted from412.txt)
EUROPEAN COMMISSION FOR THE EFFICIENC Y OF JUSTICE (CEPEJ)E uropean ethical Charter on the use of A rtificial Intelligence in judicial syst ems and their environmentA dopted at the 31st plenary meeting of the CEPE J (Strasbourg, 3-4 December 2018) Adopted at the 31st plenary meeting of the CEPE J (Strasbourg, 3-4 December 2018)C ouncil of EuropeEUROPEAN COMMISSION FOR THE EFFICIENC Y OF JUSTICE (CEPEJ)E uropean Ethical Charter on the U se of Artificial Intelligence in J udicial Systems and their environment French edition:C ommission européenne pour l’efficacité de la justic e (CEPEJ) – Charte éthique eur opéenne d’utilisation de l’intelligence ar tificielle dans les systèmes judiciair es et leur environnementT he opinions expressed in this work are the r esponsibility of the authors and do not nec essarily reflect the official policy of the C ouncil of Europe. (extracted from162.txt)
All other c orrespondence concerning this documen t should be addressed t o European Commission for the Efficienc y of Justice (CEPEJ) c epej@coe.intC over and layout: D ocuments and Publications P roduction Department (SPDP), C ouncil of Europe P hoto: ShutterstockT his publication has not been c opy-edited by the SPDP Editorial Unit t o correct typographical and g rammatical errors.© C ouncil of Europe, February 2019 P rinted at the Council of Europe ► Page 3ContentsI NTRODUCTION 5 T HE FIVE PRINCIPLES OF THE ETHICAL CHARTER ON THE USE O F ARTIFICIAL INTELLIGENCE IN JUDICIAL SYSTEMS AND THEIR E NVIRONMENT 7 1. (extracted from162.txt)
H ow is AI to be applied in civil, commercial and administrative justice? (extracted from162.txt)
I ssues specific to criminal justice: prevention of offences, risk of recidivism a nd assessment of the level of danger 4 8 8 . (extracted from162.txt)
T he potential and limitations of predictive justice tools 5 7 1 0. (extracted from162.txt)
The urgent need f or cyberethics to provide a framework for the development of artificial i ntelligence algorithms while respecting fundamental rights 5 9 Page 4 ► European Commission for the Efficiency of Justice (CEPEJ)APPENDIX II – WHICH USES OF AI IN EUROPEAN JUDICIAL SYSTEMS? (extracted from162.txt)
6 3 U ses to be encouraged 6 4 P ossible uses, requiring considerable methodological precautions 6 4 U ses to be considered following additional scientific studies 6 6 U ses to be considered with the most extreme reservations 6 6 A PPENDIX III – GLOSSARY 6 9 A PPENDIX IV – CHECKLIST FOR INTEGRATING THE CHARTER’S P RINCIPLES INTO YOUR PROCESSING METHOD 7 6 C HECKLIST FOR EVALUATING YOUR PROCESSING METHODS 7 7 ► Page 5IntroductionA cknowledging the increasing importance of artificial intelligence1 (AI) in our modern societies, and the expected benefits when it will be fully used at the service of the efficiency and quality of justice, the CEPEJ for-mally adopts the 5 fundamental principles entitled “European Ethical Charter on the use of AI in the judicial sy stems and their environment” . (extracted from162.txt)
T he use of such tools and services in judicial systems seeks to improve the effi-T he use of such tools and services in judicial systems seeks to improve the effi-c iency and quality of justice, and should be encouraged. (extracted from162.txt)
It must, however, be car-c iency and quality of justice, and should be encouraged. (extracted from162.txt)
It must, however, be car-r ied out with responsibly, with due regard for the fundamental rights of individu-r ied out with responsibly, with due regard for the fundamental rights of individu-a ls as set forth in the European Convention on Human Rights and the Convention a ls as set forth in the European Convention on Human Rights and the Convention o n the Protection of Personal Data, and in compliance with other fundamental o n the Protection of Personal Data, and in compliance with other fundamental p rinciples set out below, which should guide the framing of public justice policies p rinciples set out below, which should guide the framing of public justice policies i n this field.i n this field.J udicial decision processing by artificial intelligence, according to their develop-J udicial decision processing by artificial intelligence, according to their develop-e rs, is likely, in civil, commercial and administrative matters, to help improve the e rs, is likely, in civil, commercial and administrative matters, to help improve the p redictability of the application of the law and consistency of court decisions, sub-p redictability of the application of the law and consistency of court decisions, sub-j ect to compliance with the principles set out below. (extracted from162.txt)
Page 6 ► European Commission for the Efficiency of Justice (CEPEJ)Application of the CharterT he principles of the Charter should be subject to regular application, moni-t oring and evaluation by public and private actors, with a view to continuous impr ovement of practices. (extracted from162.txt)
Page 10 ►3Principle of quality and security : with regard to the pro-c essing of judicial decisions and data, use certified sources and intangible data with models conceived in a multi-disci-plinar y manner, in a secure technological environment■ D esigners of machine learning models should be able to draw widely on the expertise of the relevant justice system professionals (judges, prosecutors, la wyers, etc.) and researchers/lecturers in the fields of law and social sciences (f or example, economists, sociologists and philosophers).■ F orming mixed project teams in short design cycles to produce functional models is one of the organisational methods making it possible to capitalise on this multidisciplinar y approach.■ Existing ethical safeguards should be constantly shared by these project t eams and enhanced using feedback.■ Da ta based on judicial decisions that is entered into a software which implemen ts a machine learning algorithm should come from certified sources and should not be modified until they have actually been used by the learning mechanism. (extracted from162.txt)
► Page 114Principle of transparency, impartiality and fairness: make da ta processing methods accessible and understandable, author ise external audits■ A balance must be struck3 between the intellectual property of certain pr ocessing methods and the need for transparency (access to the design pr ocess), impartiality (absence of bias)4, fairness and intellectual integrity (pr ioritising the interests of justice) when tools are used that may have legal c onsequences or may significantly affect people’s lives. (extracted from162.txt)
Page 12 ►5Principle “under user control”: preclude a prescriptive appr oach and ensure that users are informed actors and in c ontrol of their choices■ U ser autonomy must be increased and not restricted through the use of ar tificial intelligence tools and services.■ P rofessionals in the justice system should, at any moment, be able to r eview judicial decisions and the data used to produce a result and continue not to be necessarily bound by it in the light of the specific features of that par ticular case.■ T he user must be informed in clear and understandable language whether or not the solutions offered by the artificial intelligence tools are binding, of the different options available, and that s/he has the right to legal advice and the right to access a court. (extracted from162.txt)
■ G enerally speaking, when any artificial intelligence-based information sy stem is implemented there should be computer literacy programmes for users and deba tes involving professionals from the justice system. (extracted from162.txt)
► Page 13Appendix II n-depth study on the use of AI in judicial sy stems, notably AI applic ations processing judicial decisions and da tapr epared by Mr Xavier Ronsin, First President of the Court of Appeal of R ennes, scientific expert (France), and M r Vasileios Lampos, principal research fellow at the Computer Science depar tment of University College London (UCL), scientific expert (United K ingdom), and with the contribution of Ms Agnès Maîtrepierre, judge, member of the C onsultative Committee of the Convention for the Protection of Individ-uals with regard to Automatic Processing of Personal Data of the Council of E urope (France)T he following experts also contributed to fintune the Study:M r Francesco Contini, Senior Researcher at the Research Institute on Judicial S ystems – National Research Council (IRSIG-CNR), Bologna (Italy)M r Francesco De Santis, Professor of Human Rights Procedures, University of Naples (I taly)M r Jean Lassègue, philosopher and epistemologist, research fellow at the Cen-tr e National de Recherche Scientifique (CNRS) and associate researcher at the I nstitut des Hautes Etudes sur la Justice (IHEJ) (France)Ms Dory Reiling, Honorary Senior Judge, Independent Expert on Information T echnology and Judicial Reform (Netherlands) M r Aleš Završnik, Chief Researcher at the Institute of Criminology, Associate P rofessor at the Faculty of Law, University of Ljubljana (Slovenia) and EURIAS R esearcher 2017-18 at Collegium Helveticum in Zürich (Switzerland) Page 14 ► European Commission for the Efficiency of Justice (CEPEJ)INTRODUCTION1. (extracted from162.txt)
These private companies even aim to predict judges’ decisions with “predictive justice” tools, although we will see tha t this may not be the best description for them5.3. (extracted from162.txt)
Work on a sample of 584 decisions of the European Court of Human Rights: Nikolaos A letras, Dimitrios Tsarapatsanis, Daniel Preoţiuc-Pietro, Vasileios Lampos, “Predicting judicial decisions of the European Court of Human Rights: a Natural Language Processing perspec tive” , published on 24 October 2016, [Online], https://peerj.com/articles/cs-93/ Appendix I – In-depth study on the use of AI in judicial systems ► Page 154.I n the line of the thought process initiated in its “Guidelines on how to dr ive change towards Cyberjustice” ,8 the CEPEJ proposes to provide public decision-makers and justice professionals with keys for a better understand-ing of the “predictive justice” phenomenon.5. (extracted from162.txt)
Page 16 ► European Commission for the Efficiency of Justice (CEPEJ) document highlights these positive examples and generally advocates the use of AI by legal professionals according to their needs, provided that due r egard is shown for the individual rights guaranteed by the ECHR and Coun-cil of Europe standards, particularly in criminal matters. (extracted from162.txt)
Some private operators did not seem very receptive to this survey and the members of the CEPEJ, who belong for the most part to ministries of justice or higher councils of justice, w ere able to quote only the tools currently used by the public sphere.14. (extracted from162.txt)
La tvia stated that it was exploring the possibilities of machine learning f or the administration of justice. (extracted from162.txt)
See summary bibliography in Appendix IV – substantial contributions from Benoît Charpentier as well as Giuseppe Contissa and Giovanni Sartori (h ttps://media.wix.com/ugd/c21db1_14b -04c49ba7f46bf9a5d88581cbda172.pdf ) and Emmanuel Barthe (h ttp://www.precisement.org/blog/I ntelligence-artificielle-en-droit-derriere-la-hype-la-realite.html#nb14) (F rench only) Page 18 ► European Commission for the Efficiency of Justice (CEPEJ) SoftwareS tateT ypeD octrine.frF ranceF ranceS earch engineS earch engineP rédicticeF ranceA nalysis (except criminal cases)C ase Law AnalyticsFranceA nalysis (except criminal cases)A nalysis (except criminal cases)Jur isData Analytics (L exisNexis)FranceS earch engine, Analysis (except cr iminal cases)L uminanceU nited Kingdom AnalysisU nited Kingdom AnalysisW atson/Ross (IBM) USAA nalysisH ARTU nited Kingdom Analysis (criminal, risk of U nited Kingdom Analysis (criminal, risk of r eoffending)r eoffending)L ex Machina (L exisNexis)USAA nalysis2. (extracted from162.txt)
An open data approach to judicial decisions is t herefore a prerequisite for the work of legal tech companies specialising in search t herefore a prerequisite for the work of legal tech companies specialising in search e ngines or trend analysis (“predictive justice”).e ngines or trend analysis (“predictive justice”).P rocessing of these data raises a number of issues, such as changes in the for-P rocessing of these data raises a number of issues, such as changes in the for-m ation of case-law and protection of personal data (including the names of m ation of case-law and protection of personal data (including the names of p rofessionals).p rofessionals).19. (extracted from162.txt)
Page 20 ► European Commission for the Efficiency of Justice (CEPEJ)purposes regarding individuals or groups. (extracted from162.txt)
Predictive justice using artificial intelli-genc e, advanced search engines applying extremely precise criteria and legal robots are all algorithmic applications which are fed with data but have nothing t o do with the policy of open data itself.26. (extracted from162.txt)
ISRM ARYesNo N ot a membre of CdED ata not supplied Page 22 ► European Commission for the Efficiency of Justice (CEPEJ)29.W ith regard to the protection of personal data, 23 countries declared tha t they are pseudonymising15 at least some types of disputes (e.g. (extracted from162.txt)
Ho wever, there is a real difficulty in measuring the impact of open data on the efficiency and quality of justice. (extracted from162.txt)
They include greater awareness of judicial activity and case la w trends, the increased quality of a justice system that knows it is being obser ved and the creation of a completely new factual reference base. (extracted from162.txt)
Article 12 of the French Code of Civil Procedure Page 24 ► European Commission for the Efficiency of Justice (CEPEJ)from the supposed majority case-law trend on how to resolve the dis-put e (while complying with the relevant rules of law), would this not be tantamount to removing them from office? (extracted from162.txt)
Page 26 ► European Commission for the Efficiency of Justice (CEPEJ)of cross-referencing with other databases, makes it impossible, in practice, to guar antee that the person concerned cannot be re-identified. (extracted from162.txt)
Page 28 ► European Commission for the Efficiency of Justice (CEPEJ)seems that the challenge lies in reconciling often conflicting requirements: mak ing public activities transparent by allowing citizens to know and evalu-a te their judges, on one hand, while protecting the privacy of professionals (whose functions should not limit their fundamental guarantees in this field), on the other hand. (extracted from162.txt)
See the example of the Swiss Federal Court, whose case-law can be downloaded: h ttps://w ww.bger.ch/fr/index/juridiction/jurisdiction-inherit-template/jurisdiction-recht.htm; or, f or the cantons: h ttp://ge.ch/justice/dans-la-jurisprudence (Canton of Geneva for example).28. (extracted from162.txt)
T he theoretical functionalities of “predictive justice” sof tware5 6. (extracted from162.txt)
These probabilities are establ ished through the statistical modelling of previous decisions using methods f rom two broad computer science domains: natural language processing Page 30 ► European Commission for the Efficiency of Justice (CEPEJ)and machine learning. (extracted from162.txt)
I n relation specifically to justice, predictive justice systems are designed f or use by legal departments, insurers (both for their internal needs and for their policyholders) as well as lawyers for them to anticipate the outcome of litiga tion. (extracted from162.txt)
G enerally speaking, it is also important to keep in mind the anthro-pomor phic notion that computing machines are intelligent and that their desig ners have managed to slip a mind inside their mechanisms.30 Unfortu-na tely, this idea still permeates many analyses of predictive justice that lend these devices immediate or future capabilities for the near replication of human intelligence. (extracted from162.txt)
Specifically for predictive justice, the eng ine builds links between the different lexical groups composing judicial decisions. (extracted from162.txt)
Page 32 ► European Commission for the Efficiency of Justice (CEPEJ)at the input stage (facts and reasoning) and those at the output stage (the oper ative part of the decision) then classified.– The reliability of the model (or function) built strongly depends on the qualit y of the data used and the choice of machine learning technique.65. (extracted from162.txt)
Li Gong, “La traduction automatique statistique, comment ça marche ?” , Interstices.info, published on 29 October 2013, [Online], https://interstices.info/jcms/nn_72253/la-traduction-aut omatique-statistique-comment-ca-marche (page accessed on 14 December 2017).Data Rules/Templates Resu lts Machinel earning Page 34 ► European Commission for the Efficiency of Justice (CEPEJ)training process, like a child learning in its environment. (extracted from162.txt)
Other practical applications for these technologies are already aff ecting our daily lives and are beginning to appear in the professional w orld of justice.3770. (extracted from162.txt)
In the social sciences, to which law and justice belong , failure would even appear inevitable in the absence of a convincing model of cognition. (extracted from162.txt)
Page 36 ► European Commission for the Efficiency of Justice (CEPEJ)a set of rules whose meaning remains undetermined, which the legal the-or ist Herbert L. (extracted from162.txt)
Page 38 ► European Commission for the Efficiency of Justice (CEPEJ)Fig. (extracted from162.txt)
Page 40 ► European Commission for the Efficiency of Justice (CEPEJ)administrative courts of appeal in France have made it possible to develop an indica tor of the rejection rate of appeals against obligations to leave French t erritory taken by the administrative authorities. (extracted from162.txt)
F urthermore, how can we account for two distinct philosophical and cultur al approaches to judicial decisions, whereby, in some European coun-tr ies, including France, there is a culture of precedent and a detailed knowl-edge by judges of the factual databases of all 1st and 2nd instance decisions Appendix I – In-depth study on the use of AI in judicial systems ► Page 41(Ariane database) in the field of administrative justice, while other countries or systems favour the intellectual independence of each court, along with a desir e to deal with each situation on a case-by-case basis?92. (extracted from162.txt)
H ow is AI to be applied in civil, commercial and administr ative justice?T he state of development of machine learning techniques does not allow today to r each reliable results regarding the “prediction” of judicial decisions. (extracted from162.txt)
On the other h and, their application in the field of civil, commercial and administrative justice h and, their application in the field of civil, commercial and administrative justice i s to be considered for the creation of scales or the pre-litigation resolution of dis-i s to be considered for the creation of scales or the pre-litigation resolution of dis-p utes online, when a later appeal to the judge remains possible.p utes online, when a later appeal to the judge remains possible.93. (extracted from162.txt)
Page 42 ► European Commission for the Efficiency of Justice (CEPEJ)97.A t the same time, public decision-makers see this as an opportunity to bett er regulate the flow of new proceedings through the courts and provide themselv es with a lever to reduce judicial operating costs. (extracted from162.txt)
Ho wever, these interesting approaches are not unbiased and must not deprive citiz ens of access to a judge or call into question the adversarial principle.E xperiments conducted in FranceA t the initiative of the Ministry of Justice, the two courts of appeal in Rennes and A t the initiative of the Ministry of Justice, the two courts of appeal in Rennes and D ouai agreed to test predictive justice software on various litigation appeals in D ouai agreed to test predictive justice software on various litigation appeals in s pring 2017, which in reality was an analysis of civil, social and commercial deci-s pring 2017, which in reality was an analysis of civil, social and commercial deci-s ions of all French courts of appeal. (extracted from162.txt)
The result of the exper-i ment, contradictorily debated between the two courts of appeal, the Ministry i ment, contradictorily debated between the two courts of appeal, the Ministry o f Justice and the legal tech company who designed the product unfortunately o f Justice and the legal tech company who designed the product unfortunately s tated the absence of added value of the tested version of the software for the s tated the absence of added value of the tested version of the software for the w ork of reflection and decision-making of the magistrates. (extracted from162.txt)
Page 44 ► European Commission for the Efficiency of Justice (CEPEJ)6.2. (extracted from162.txt)
F or those who advocate such solutions, which are of interest to a num-ber of legal professions and the private sector, access to justice could be sig-nifican tly improved by a broad solution combining ODR and AI (or at least e xpert systems, see section 3 above for the distinction). (extracted from162.txt)
Recently the Court of Justice of the E uropean Union in Luxembourg has answered on the requests for a preliminary ruling c oncerning these joined cases (ECLI:EU:C:2018:882)55. (extracted from162.txt)
Darin Thompson, “Creating new pathways to justice using simple artificial intelligence and online disput e resolution” , Osgoode Hall Law School of York University.56. (extracted from162.txt)
The explanatory Page 46 ► European Commission for the Efficiency of Justice (CEPEJ)108.T he potential benefits of an ODR system, its degree of integration into a complete judicial process (from pre-litigation to actual litigation) and the almost decisive role of AI in the execution of the process must therefore be pr operly assessed on a case-by-case basis.109. (extracted from162.txt)
If one speaks of a court, it must be the form of or ganisation defined by the European Convention on Human Rights and not simply a private justice institution with the mere appearance of state justic e60. (extracted from162.txt)
Royaume-Uni, §§ 28-366 3.Resolution 2054 (2015) of the Parliamentary Assembly of the Council of Europe (P ACE), 10 November 2015, h ttp://assembly.coe.int/nw/xml/XRef/Xref-XML2HTML-EN.asp?fileid=22245&lang=en Page 48 ► European Commission for the Efficiency of Justice (CEPEJ) for certain operators (institutions, companies with means, computer liter-a te persons) and, on the contrary, pose difficulties for certain population t ypes that are more uncertain or less familiar with computers. (extracted from162.txt)
In these systems, we can-not rule out the risk that such norms will place indirect pressure on judges when decisions are taken and prompt their approval, or that the executive will monit or those who depart from the norm.R ight to counselA t the beginning of this chapter, we mentioned the advantages derived from the application of predictive justice tools for lawyers and, in particular, the possibilit y of providing their clients with better informed advice by empiri-cally and systematically assessing the chances of a procedure’s success. (extracted from162.txt)
I ssues specific to criminal justice: prevention of offences, risk of r ecidivism and assessment of the level of dangerE ven they are not specifically designed to be discriminatory, the use of statistics a nd AI in criminal proceedings has shown a risk of prompting the resurgence of a nd AI in criminal proceedings has shown a risk of prompting the resurgence of d eterministic doctrines to the detriment of doctrines of individualisation of the d eterministic doctrines to the detriment of doctrines of individualisation of the s anction, which have been widely acquired since 1945 in most European judicial s anction, which have been widely acquired since 1945 in most European judicial s ystems.s ystems.117. (extracted from162.txt)
C riminal justice tools should therefore be designed in accordance with these fundamental principles of rehabilitation, 65including the role of the judge in the individualisation of the sentence, based on objective elements of personalities (training, employment, regular medicals and social care) without any other form of analysis than that carried out by specifically trained pr ofessionals, such as probation officers. (extracted from162.txt)
Page 50 ► European Commission for the Efficiency of Justice (CEPEJ)first category includes “predictive policing” instruments that are used to pre-v ent certain types of offences with elements of regularity in their occurrence such as burglary, street violence, theft from/of vehicles. (extracted from162.txt)
In the literature, these tools are often referred to as “algorithmic justice” or “automated justic e” , or “simulated justice” . (extracted from162.txt)
Page 52 ► European Commission for the Efficiency of Justice (CEPEJ)126.I n tests initially conducted in 2013, during which suspect behaviour w as observed over a two-year period after commission of the crime, HART pr edictions were found to be 98 % effective at predicting low risk and 88 % eff ective at high risk of recidivism. (extracted from162.txt)
Page 54 ► European Commission for the Efficiency of Justice (CEPEJ)judges in criminal trials. (extracted from162.txt)
Suppor ters often argue that they are neutral and that they rely on fac tual and objective methods that help to make justice more accurate and transparent. (extracted from162.txt)
It is incumbent upon the criminal justice system to recognize that in the coming months and years, additional research data will become available. (extracted from162.txt)
The justice system must keep up with the research and continually assess the use of these t ools. (extracted from162.txt)
Appendix I – In-depth study on the use of AI in judicial systems ► Page 55137.I n criminal matters, there are also potential risks of discrimination when one considers that these tools, which are constructed and interpreted by humans , can reproduce unjustified and already existing inequalities in the cr iminal justice system concerned; instead of correcting certain problematic policies , technology may end up legitimising them. (extracted from162.txt)
Page 56 ► European Commission for the Efficiency of Justice (CEPEJ) 140.T he considerations expressed earlier regarding the potentially negative eff ects of these tools on the impartiality of the judge are also valid in criminal ma tters: a judge who decides against the prediction of an algorithm is likely t o take risks as he assumes greater responsibility. (extracted from162.txt)
The potential and limitations of predictive justice toolsT he term predictive justice should be dismissed because it is ambiguous and mis-l eading. (extracted from162.txt)
I n section 3, we already highlighted the ambiguity and fallacy of the c oncept of predictive justice and how it operates a slow shift in the collec-tiv e mind, leading us to believe that machines, devoid of any emotion, will one day be better able to make the act of judging more reliable. (extracted from162.txt)
Similarly, researcher Aurélien Grosdidier considers t hat an algorithm, in itself, is capable of nothing other than allowing us – at b est – to grasp part of the designer’s intention and extends the questioning Page 58 ► European Commission for the Efficiency of Justice (CEPEJ)to the entire information processing chain (designer’s intention, production of computer code, execution of computer code and context of execution then maintenance). (extracted from162.txt)
As mentioned earlier, the risk is that, in the absence of a statistical representation of reality or of being able to predict anything, the results of predictive justice software will be set as standards without any v alidation by the legal system and in conflict with it.150. (extracted from162.txt)
Page 60 ► European Commission for the Efficiency of Justice (CEPEJ)10.1.T he importance of debating, testing and continually r eviewing the application of these tools prior to the implemen tation of public policies154. (extracted from162.txt)
In addition, judicial training and law schools can pla y a key role in raising awareness among justice professionals on these issues , so that they can better understand and practically contribute to cur-r ent developments.156. (extracted from162.txt)
A right to e xamine the components and characteristics of the instruments proposed b y the private sector (or those developed by independent and specialised public institutes, a solution which should be encouraged) seems equally impor tant so that the justice service can effectively carry out its mission. (extracted from162.txt)
It also seems strongly advisable to r egularly assess the impact of these tools on the work of justice professionals.10.2. (extracted from162.txt)
Justice professionals must be closely involved to be able to properly assess the risks and the impact of these applica tions on judicial systems.161. (extracted from162.txt)
Other a pplications (“predictive justice”) should be assigned to the field of research and a pplications (“predictive justice”) should be assigned to the field of research and f urther development (in consultation with legal professionals in order to ensure f urther development (in consultation with legal professionals in order to ensure t hat they fully tie in with actual needs) before contemplating use on a significant t hat they fully tie in with actual needs) before contemplating use on a significant s cale in the public sphere.s cale in the public sphere.I n criminal matters, this is a very sensitive issue but it should not be ignored. (extracted from162.txt)
In the l ight of the many existing questions as to their compatibility with a certain num-l ight of the many existing questions as to their compatibility with a certain num-b er of fundamental rights, the use of algorithms to calculate the potential risks of b er of fundamental rights, the use of algorithms to calculate the potential risks of r ecidivism of an individual brought to justice should be considered with the most r ecidivism of an individual brought to justice should be considered with the most e xtreme reservations. (extracted from162.txt)
Page 64 ► European Commission for the Efficiency of Justice (CEPEJ) Uses to be encouraged► Case-law enhancement : machine Learning techniques have been incr easingly deployed in the field of natural language processing in the past years (this includes initial efforts in natural language under-standing) and are a considerable asset for finding search options to c omplement current keyword or full-text search. (extracted from162.txt)
Document templates (court applications, lease ag reements, etc.) could also be generated online.► Creation of new strategic tools : the use of data science and artificial in telligence techniques on court activity data can help improve the efficienc y of justice by making it possible, for example, to carry out quan titative and qualitative evaluations and to make projections (e.g. (extracted from162.txt)
It is recommended that legal profes-sionals , especially judges, be involved in the implementation of these t ools, in terms of taking ownership of these tools and of analysing the r esults in conjunction with factors relating to the specific features of the court in question or the quality of justice (for example, the need t o preserve access to justice).P ossible uses, requiring considerable metho dological precautions► Help in the drawing up of scales in certain civil disputes : an analysis of all judicial decisions is not statistically meaningful if all the causative fac tors (explicit and implicit in the decisions) are not identified. (extracted from162.txt)
Appendix II – In-depth study on the use of AI in judicial systems ► Page 65►Support for alternative dispute settlement measures in civil matters: in some European countries, “predictive justice” tools are used by insur ance companies to evaluate the chances of success of a dispute and to steer the litigant towards another method of dispute resolu-tion when it is felt that there is little chance of success. (extracted from162.txt)
However, this type of Page 66 ► European Commission for the Efficiency of Justice (CEPEJ) quantitative approach can generate a strong “performative effect” (in a g iven location, there is a greater chance of discovering an offence and this then reinforces the system). (extracted from162.txt)
Page 70 ► European Commission for the Efficiency of Justice (CEPEJ)However, the term artificial intelligence is criticised by experts who distin-guish between “strong” AIs (yet able to contextualise specialised and varied pr oblems in a completely autonomous manner) and “weak” or “moderate” AIs (high performance in their field of training). (extracted from162.txt)
Page 72 ► European Commission for the Efficiency of Justice (CEPEJ)MM ACHINE LEARNING M achine learning makes it possible to construct a ma thematical model from data, incorporating a large number of variables tha t are not known in advance. (extracted from162.txt)
Page 74 ► European Commission for the Efficiency of Justice (CEPEJ)Open data should not be confused with unitary public information available on websites, where the entire database cannot be downloaded (for exam-ple , a database of court decisions). (extracted from162.txt)
The software can therefore be freely used, modified and r edistributed.P PERSONAL DATA Any information concerning an identified or identifiable na tural person (the “person concerned”), directly or indirectly.T hese include sensitive data relating to genetic data, biometric data uniquely iden tifying an individual, data relating to offences, criminal proceedings and c onvictions and related security measures, and any data for information they r eveal on racial or ethnic origin, political opinions, trade union membership, r eligious or other beliefs, health or sex life.PREDIC TIVE JUSTICE P redictive justice is the analysis of large amounts of judicial decisions by artificial intelligence technologies in order to make pre-dic tions for the outcome of certain types of specialised disputes (for exam-ple , redundancy payments or alimentary pensions).T he term “predictive” used by legal tech companies comes from the branches of science (principally statistics) that make it possible to predict future results thr ough inductive analysis. (extracted from162.txt)
The European Court of Human R ights oversees the implementation of the C onvention in the member states.www.coe.intThe Charter provides a framework of principles that can guide policy makers , legislators and justice professionals when they grapple with the rapid development of Artificial Intelligence in national judicial pr ocesses.T he CEPEJ’s view is that the application of Artificial Intelligence in the field of justice can contribute to improve the efficiency and qualit y. (extracted from162.txt)
It is essential t o ensure that Artificial Intelligence remains a tool in the service of the gener al interest and that its use respects individual rights.T he Charter defines five core principles to be respected in the field of Artificial Intelligence and justice: respect of fundamental rights; nondiscrimination; quality and security; transparency, impartiality and fairness; “under user control” . (extracted from162.txt)
” 11 ∼ “AI actors should promote social justice, by safeguarding fairness and non -discrimination of any kind in compliance with international law. (extracted from604.txt)
11 Council of Europe Commissioner for Human Rights (2019) – “Unboxing AI: 10 steps to protect Human Rights” 27 the least advanced to ensure that the benefits of AI technologies are shared such that access to and participation in the AI system life c ycle for the latter contributes to a fairer world order with regard to information, communication, culture, education, research, and socio -economic and political stability...AI actors should promote social justice and safeguard fairness and non-discriminat ion of any kind in compliance with international law. (extracted from604.txt)
This should also include the possibility of receiving insight into and challenging AI- informed decisions in the context of law enforcement or justice, including the right to review of such decisions by a human. (extracted from604.txt)
AI can increase the efficiency International Covenant on Civil and Political Rights: -Article 2, International Covenant on Civil and Political Rights – Right to effective remedy -Article 14, International Covenant on Civil and Political Rights – Right to fair tri al Council of Europe Resources: -European Commission for the Efficiency of Justice, ‘European ethical charter on the use of Artificial Intelligence in judicial systems and their environment’ – Council of Europe 22 CAHAI Feasibility Study , Council of Europe CAHAI (2020)23. (extracted from604.txt)
Washington, DC: US De partment of Justice, Office of Justice Programs, Office of Juvenile Justice and Delinquency Prevention. (extracted from604.txt)
59 Use context (Questions 1 -20) • Sector or domain in which the system is being built • Existing law and regulatory environment of the sector or domain • Impact-level of the system • Prohibited systems and uses • Scope of deployment (breadth and temporality) • Technological maturity • Existing system (human or technological) that the application is replacing • Bias and discrimination in sector or domain context • Environmental context • Cybersecurity context Data Lifecycle Context (Questions 21 -40) • Data quality, integrity, and provenance • Means and methods o f data collection • Data types • Dataset linkage • Data labelling and annotating practices Goal Setting and Problem Formulation Context (Questions 41 -42) • Decision to design • Definition of outcome Model Design & Development Context (Questions 43 -46) • AI model characteristics • Pre-processing and feature engineering • Model selection Model Output & Implementation Context (Questions 47 -52) • Model inference • Model verification and validation • Model accuracy and performance metrics System -User Interface and Human Factors Context (Questions 53 -55) • Implementers or users of the system • Level of automation/level of human involvement and choice Rights & Freedoms Context (Questions 56 -71) • Respect for and protection of human dignity • Protection of human freedom and autonomy • Non-discrimination, fairness, and equality • Data protection and privacy context • Accountability and access to justice • Social and economic rights 60 Depending on the form that the section one questions take, certain responses to each question will trigger one of three classes of risk factors, prohibitive, major, or moderate: Prohibitive risk factor Prohibitive risk factors indicate the presence of determinants of potential harms that trigger the precautionary principle and precipitate preemptive measures to prevent adverse impacts on the human rights and fundamental freedoms of affected persons, democracy, and the rule of law. (extracted from604.txt)
NOT APPLICABLE : No message Accountability and access to justice 60) Will sufficient and transparently reported processes be implemented throughout the project’s lifecycle to ensure end -toend accountability across the production and use of the AI system? (extracted from604.txt)
NOT APPLICABLE : No message 168 62) If the AI system is used in the field of justice and law enforcement, will meaningful information be provided to affected rights -holders about the existence and use of the system, its role within law enforcement and the judicial process, and the right to challenge the decisions informed or made thereby? (extracted from604.txt)
YES, WE HAVE PLANS IN PLACE TO DO THIS WE HAD NOT CONSIDERED IT, BUT MAY DO THIS WE HAD NOT CONSIDERED IT, BUT ARE UNLIKELY TO DO THIS NO, WE ARE NOT PLANNING TO DO THIS UNSURE NOT APPLICABLE Example YES, WE HAVE PLANS IN PLACE TO DO THIS : No message WE HAD NOT CONSIDERED IT, BUT MAY DO THIS / WE HAD NOT CONSIDERED IT, BUT ARE UNLIKELY TO DO THIS / NO, WE ARE NOT PLANNING TO DO THIS: Major modifiable risk factor • Where AI system are being used in the field of justice and law enforcement and meaningful information is not provided to affected rights -holders about the existence and use of the system, its role within law enforcement and the judicial process, and the right to challenge the decisions informed or made thereby, this presents a major modifiable risk factor for adverse impacts on the human rights and fundamental freedom of persons. (extracted from604.txt)
NOT APPLICABLE : No message 63) If the AI system is used in the field of justice and law enforcement, will sufficiently and transparently reported processes be implemented throughout the project’s lifecycle to ensure that its deployment is in line with the essential requirements of impac ted individuals' right to a fair trial (equality of arms, right to a natural YES, WE HAVE PLANS IN PLACE TO DO THIS : No message WE HAD NOT CONSIDERED IT, BUT MAY DO THIS / WE HAD NOT CONSIDERED IT, BUT ARE UNLIKELY TO DO THIS / NO, WE ARE NOT PLANNING TO DO THIS: Major modifiable risk factor 170 judge established by law, the right to an independent and impartial tribunal, and respect for the adversarial process)? (extracted from604.txt)
YES, WE HAVE PLANS IN PLACE TO DO THIS WE HAD NOT CONSIDERED IT, BUT MAY DO THIS WE HAD NOT CONSIDERED IT, BUT ARE UNLIKELY TO DO THIS NO, WE ARE NOT PLANNING TO DO THIS UNSURE NOT APPLICABLE Example • Where AI system are being used in the field of justice and law enforcement and sufficiently and transparently reported processes are not implemented throughout the project’s lifecycle to ensure that its deployment is in line with the essential requirements of impacted individuals' right to a fair trial (equality of arms, right to a natural judge established by law, the right to an independent and impartial tribunal, and respect for the adversarial process), this presents a major modifiable risk factor for adverse impacts on the human rights and fundamental freedom of persons. (extracted from604.txt)
FAIRNESS is insepara bly connected with sociolegal conceptions of equity and justice , which may emphasize a variety of features such as non-discrimination , equit able outcomes, or procedural fairness through bias mitigation , but also social and economic equality, diversity, and inclusivenes s. (extracted from604.txt)
But, beyond this, diligent consideration of the practical, social, or policy issue being addressed by the system will also trigger, inter alia , reflection on the complex intersection of potential algorithmic bias, the cascading effects of sociohistorical patterns of racism and discrimination, wider societal and community impacts, and the potential effects of the use of the model on the actors in the criminal justice systems who will become implementers and subjects of the technology. (extracted from604.txt)
In race and social justice discourse, members of society are marginalised by holding an identity or being placed in a demographic category that is not attached to the dominant side of the prevailing structure of power and by which they experience oppression . (extracted from604.txt)
Pre-designated High- Risk or Safety Critical Sector: Annex III of the European Union Proposed Rules on Artificial Intelligence indicates ‘high-risk’ sectors as those concerned with education, emergency services, employment, financial services, public benefits , law enforcement, immigration, border control, and the administration of justice and democratic processes . (extracted from604.txt)
'It's reducing a human being to a percentage': Perceptions of justice in algorithmic decisions. (extracted from604.txt)
Algorithmic accountability and digital justice: A critical assessment of technical and sociotechnical approaches. (extracted from604.txt)
Optimized scoring systems: Toward trust in machine learning for healthcare and criminal justice. (extracted from604.txt)
admission and grading ·Employment, worker management, and access to self-employment opportunities, including systems that make or inform decisions about hiring, firing, and task allocation ·Access to and enjoyment of essential private services and public services and benefits ·Specific uses of law enforcement ·Specific uses in migration, asylum, and border control management ·Administration of justice and democratic processes, in particular when used to research and establish facts or applying the law to some factsProviders of high-risk systems must perform a conformity assessment to make sure that they are compliant with requirements including: ·Risk management system ·Data requirements ·T echnical documentation ·Record-keeping ·T ransparency on the system’s functioning ·Human oversight ·Accuracy, robustness, and cybersecurity ·Post-market monitoringFines up to 4% of global revenue or 20mn euros, whichever is higher, for everything except the data requirements, where the same fines apply as for the prohibited systems LimitedRisk: Transparency Obligations (TitleIV)·AI systems interacting with natural persons ·Emotion recognition systems or biometric categorisation systems ·AI system that generates or manipulates image, audio, or video content that appears realNotify the user that they are engaging with an AI systemFines up to 4% of global revenue or 20mn euros, whichever is higher MinimalRisk: Voluntary Codesof Conduct (TitleIX)All AI systems that are not either prohibited or high- riskProviders can choose to comply with voluntary codes of conduct. (extracted from88.txt)
276European Commission and Directorate-General for Justice and Consumers, Liability for Artificial Intelligence and Other Emerging Digital Technologies (Publications Office of the European Union, 2019); European Commission, “InceptionImpactAssessment:ProposalforaDirectiveAdaptingLiability RulestotheDigitalAgeandArtificialIntelligence.” 277“The EU Product Liability Directive (PLD), that governs the responsibility for such defects, should be applied ‘without prejudice’ to the product safety regime” European Parliament, “Directive2001/95/ECoftheEuropeanParliamentandoftheCouncilof3December2001onGeneralProductSafety (TextwithEEARelevance),” CELEX number: 32001L0095, Official Journal of the European Union L 11, January 15, 2002, 4–17, art. (extracted from88.txt)
Schwartz, “Global Data Privacy: The EU Way.” 406Court of Justice of the European Union, “Judgment in Case C-362/14 Maximillian Schrems v Data Protection Commissioner: The Court of Justice DeclaresThattheCommission’sUSSafeHarbourDecisionIsInvalid,” Press Release 117/15 (Court of Justice of the European Union , October 6, 2015); European Commission, “EU-US Data Transfers: How Personal Data Transferred between the EU and US Is Protected,” European Commission, accessed July 14, 2022. (extracted from88.txt)
Both agreements were adopted even though the US data privacy standards were not equivalent to the EU, which is a requirement for data transmission agreements in both the DPD and GDPR.405Consequently, both data transmission agreements were declared invalid by the European Court of Justice (ECJ) in 2015 and 2020, respectively.406Since 2020, the US and the Commission have stated their intention to negotiate a new agreement. (extracted from88.txt)
Court of Justice of the European Union. (extracted from88.txt)
“ Judgment in Case C-362/14 Maximillian Schrems v Data Protection Commissioner: The Court of Justice Declares That the Commission’s US Safe Harbour Decision Is Invalid. (extracted from88.txt)
Court of Justice of the European Union , October 6, 2015. (extracted from88.txt)
https://ec.europa.eu/info/policies/justice-and-funda- THEBRUSSELSEFFECTANDARTIFICIALINTELLIGENCE•87 mental-rights/combatting-discrimination/racism-and-xenophobia/eu-code-conduct-countering-illegalhate-speech-online_en . (extracted from88.txt)
European Commission, and Directorate-General for Justice and Consumers. (extracted from88.txt)
Ideal for small agencies.TASER X26PA single-shot TASER Smart Weapon.CIVILIAN SERIESShop TASER products designed for personal and home use.MoreCamerasAxon Body 4The next-generation body worn cameraAxon Body 3See truth in the momentAxon Body WorkforceSafety in every shift for frontline workers.Axon Fleet 3Drive the future of in-car videoAxon Flex 2Capture point-of-view video evidence in HD with a 120-degree field of view.Axon AirImprove officer safety, provide tactical support, and reconstruct scenes.MoreSoftwareAxon EvidenceStreamline management, storage and sharing of all your digital evidence.Axon RecordsSwitch to one dynamic, integrated report that allows officers to focus on what matters.Axon RespondReal-time situational awareness through your Axon devices.Axon DispatchA faster, more informed CAD experience.MoreView All ProductsTrainingAxon AcademyVirtual learning platform for end-user product trainingVR TrainingTraining for the reality of todayAxon TrainingInnovative training technologies, content and peer networksIndustriesLaw EnforcementMunicipal, county, and state law enforcement agenciesFederalFederal civilian and defense departments and agenciesCorrectionsPrisons, probation and parole, and juvenile justice FireFire response and investigationEMSFirst Responders and emergency medical professionalsCampusCampus law enforcement and school safety teams JusticeLegal teams including DAs and prosecutorsHealthcareHospitals and other healthcare facilities RetailRetail trade, food and beverage, and other shopping establishments Private SecurityGuard services, venues and other on-site businessesPersonal SafetyTASER devices for personal protection and self-defenseView All IndustriesProductsSolutionsTrainingCareersNewsResourcesSupportProductsProduct BundlesSmart WeaponsCamerasSoftwareView All ProductsResourcesResourcesEventsPartnersProducts / Resources /CompanyOverviewLeadershipNewsInvestorCareersContactIndustriesSolutionsLaw EnforcementFederalCorrectionsFireEMSCampusJusticeHealthcareRetailPrivate SecurityPersonal SafetyTrainingAxon AcademyVR TrainingAxon TrainingNews & Resources TechnologyAxon Committed to Listening and Learning So That We can Fulfill our Mission to Protect Life, TogetherJun 05, 2022Rick SmithAxon CEO + FounderJun 05, 2022Axon was founded on our mission to protect life. (extracted from63.txt)
The specific human rights implications for AI systems can be viewed through provisions of the European Convention of Human Rights (ECHR) and the European Social Charter (ESC), including its specific guarantees regarding liberty and justice, privacy, freedom of expression, equality and non-discrimination, and social and economic rights. (extracted from606.txt)
Liberty and Justice: AI can adversely affect the liberty and justice of individuals, particularly when implemented in high impact contexts such as criminal justice. (extracted from606.txt)
-Member States are obliged to ensure that the AI systems they deploy do not result in unlawful discrimination, harmful stereotypes (including but not limited to gender stereotypes), and wider social inequality, and should therefore apply the highest level of scrutiny when using or promoting the use of AI systems in sensitive public policy areas, including but not limited to law enforcement, justice, asylum and migration, health, social security, and employment. (extracted from606.txt)
This should also include the possibility of receiving insight into and challenging AIinformed decisions in the context of law enforcement or justice, including the right to review of such decision by a human. (extracted from606.txt)
13 ECHR), also in case of unlawful harm or breach an individual’s human rights in the context of AI systems.-Member States must ensure that AI systems used in the field of justice and law enforcement are in line with the essential requirements of the right to a fair trial. (extracted from606.txt)
Such information must especially be provided when AI systems are used in the field of justice and law enforcement, both as concerns the role of AI systems within the process, and the right to challenge the decisions informed or made thereby. (extracted from606.txt)
We hope that, taken together, this material can function as a kind of launching pad for meaningful reflection on the prospects for a principles-based legal framework for governing AI research and innovation in accordance with the Council of Europe's stewardship of fundamental rights and freedoms, justice, and democratic values. (extracted from606.txt)
Policy makers, scholars, and activists are tasked with proposing and critiquing strategies and actions aimed at promoting general well-being and social justice. (extracted from606.txt)
Work in the field of justice 4 3European Committee on Democracy and Governance (CDDG) Currently preparing a study on the impact of digital transformation on democracy and governance Venice Commission: Principles for a fundamental rights-compliant use of digital technologies in electoral processes (2020) Emphasised the need for a human rights-compliant approach to eight principles involving the use of digital technologies in elections The eight principles are described in greater detail in the document, but they are outlined below and have been taken directly from the original document 1. (extracted from606.txt)
46 3.6 Justice and solidarity .......................................................................................... (extracted from174.txt)
212 7.3 Algorithmic systems in the dispensation of justice ....................................................... (extracted from174.txt)
Justice and Solidarity In view of the vast amounts of power being accumulated using data and technologies, and the new threats of exclusion and discrimination, the safeguarding of equitable access and distributive justice is an urgent task. (extracted from174.txt)
Regardless of the position that data protection authorities and the European Court of Justice will ultimately take with regard to the prohibition under the GDPR of “tying” or “bundling” consent with the provision of a service, the Data Ethics Commission believes that consumers must be offered reasonable alternatives to releasing their data for commercial use (e. (extracted from174.txt)
8 The Data Ethics Commission advises the Federal Government not to consider the issues falling under the heading of “digital inheritance” as having been settled by the Federal Court of Justice’s 2018 ruling. (extracted from174.txt)
69 In the areas of law-making and the dispensation of justice , algorithmic systems may at most be used for peripheral tasks. (extracted from174.txt)
The first took place on 7 February 2019 at the Federal Ministry of Justice and Consumer Protection ( Bundesministerium der Justiz und für Verbraucherschutz ), and centred around the issue of “Selfdetermination and external determination in the age of artificial intelligence”. (extracted from174.txt)
Another vital aspect of self-determination is that people must not only be allowed to assume responsibility , but must do so and do justice to the task. (extracted from174.txt)
3.6 Justice and solidarity Observance of the principles of justice by society and its institutions is another fundamental factor that allows us to live together in peace, prosperity, freedom and democracy. (extracted from174.txt)
in the workplace and the healthcare sector raises other questions relating to equitable access and distributive justice , however, for example in relation to income and the provision of healthcare; these developments may mean that scarce resources can be distributed more fairly, but they may also mean that individual groups of people suffer disadvantage or discrimination. (extracted from174.txt)
There is also a close link between justice and opportunities for participation. (extracted from174.txt)
Finally, questions of justice arise in connection with situations where the use of algorithmic systems – in particular self-learning algorithmic systems – means that individuals or groups of people suffer discrimination for no justifying reason. (extracted from174.txt)
This raises questions with regard to sustainable economic and ecological development, and also questions of international justice concerning the use of natural resources and global responsibility for future generations. (extracted from174.txt)
To further this aim, in October 2018, the Federal Ministry of Justice and Consumer Protection launched an initiative to clarify the principles and concepts of corporate digital responsibility (www.bmjv.de/cdr ). (extracted from174.txt)
The issue raises fundamental questions about distributive and participatory justice, and about what a just economic system looks like. (extracted from174.txt)
eu/transparency/regdoc/rep/1/2017/EN/COM-2017-9-F1-EN-MAIN-PART-1.PDF ); Arbeitsgruppe “Digitaler Neustart” der Konferenz der Justizministerinnen und Justizminister der Länder [Working Group “Digital New Start” of the Conference of Ministers of Justice of the Länder]: Report of 15 May 2017, pp. (extracted from174.txt)
However, given the huge number of individuals that contribute to the generation and processing of data, the level of complexity of a fair remuneration system and the 24/7 monitoring that would be required to measure data flows would be out of all proportion to any potential gains in terms of justice. (extracted from174.txt)
Personal data are therefore often referred to in shorthand terms as “counter-performance” for digital content or services, for example in the original draft of Article 3(1) of the Digital Content Directive (although the term was removed at a later point in the legislative procedure).11 The extent to which the economic model described above is, in fact, compatible with the prohibition under Article 7(4) GDPR of “tying” or “bundling” consent with the provision of a service12 must ultimately be clarified by the European Court of Justice. (extracted from174.txt)
16 Judgment by the German Federal Court of Justice of 12 July 2018, ref. (extracted from174.txt)
III ZR 183/17.The principle set forth by the Federal Court of Justice – that an estate should be transferred to the deceased’s heirs – is linked to the existence of a contractual relationship. (extracted from174.txt)
Regardless of the position that data protection authorities and the European Court of Justice will ultimately take with regard to the prohibition under the GDPR of “tying” or “bundling” consent with the provision of a service, the Data Ethics Commission believes that consumers must be offered reasonable alternatives to releasing their data for commercial use (e. (extracted from174.txt)
8 The Data Ethics Commission advises the Federal Government not to consider the issues falling under the heading of “digital inheritance” as having been settled by the Federal Court of Justice’s 2018 ruling. (extracted from174.txt)
Systems already exist which can relieve state bodies of repetitive tasks (thereby expediting processes and freeing up human resources for complex cases) and which, in certain set-ups, improve the consistency and quality of state activity or, in the form of chatbots or voice assistants, for example, can facilitate citizens’ access to justice. (extracted from174.txt)
7.3 Algorithmic systems in the dispensation of justice The Data Ethics Commission is of the view that the use of algorithmic systems in the dispensation of justice is permissible only for peripheral tasks . (extracted from174.txt)
Justice is administered “in the name of the people”, and that means, at least in contentious proceedings as well as in administrative court proceedings and criminal proceedings, always administered by human judges. (extracted from174.txt)
Due to the often high level of trust placed in the supposed “infallibility” of technical systems (automation bias) as well as the low level of willingness to make divergent decisions, in particular if this is associated with an additional burden of reasoning and proof and the risk of a “miscarriage of justice” (default effects), even legally non-binding proposals for decisions for judgments by algorithmic systems are generally highly problematic from the perspective of the parties concerned. (extracted from174.txt)
Such systems could, for example, work out whether decisions were influenced by external factors and, if so, which ones in order to provide judges in future with ways to prevent such distortions themselves and thus contribute to better and more consistent dispensation of justice. (extracted from174.txt)
69 In the areas of law-making and the dispensation of justice , algorithmic systems may at most be used for peripheral tasks. (extracted from174.txt)
Dr Mario Martini ●Professor of Public Administration, Public Law, Administrative Law and European Law at the German University of Administrative Sciences Speyer (DUV Speyer) ●Head of the Programme Area “Transfor mation of the State in the Digital Age” and Deputy Director of the German Research Institute for Public Administration (FÖV) Klaus Müller ●Executive Director of the Federation of German Consumer Organisations (vzbv) ●Lecturer at Heinrich Heine University Düsseldorf (HHU) Paul Nemitz ●Principle Advisor at the European Commis sion, Directorate-General for Justice and Consumers Prof. (extracted from174.txt)
Dr Thomas Wischmeyer ●Assistant Professor (Tenure Track) for Public Law and Information Law at the University of Bielefeld Current as of: 10 October 2019 Imprint Berlin, December 2019 Opinion of the Data Ethics CommissionPublisher Data Ethics Commission of the Federal GovernmentFederal Ministry of the Interior, Building and CommunityAlt-Moabit 140, 10557 BerlinFederal Ministry of Justice and Consumer ProtectionMohrenstraße 37, 10117 Berlin E-mail datenethikkommission_gs@bmi.bund.dedatenethikkommission_gs@bmjv.bund.de Website www.datenethikkommission.de Design Atelier Hauer + Dörfler GmbH, Berlin Photo credits p. (extracted from174.txt)
The relevant concepts include freedom and self-determination, privacy and intimacy, sovereignty and power, beneficence and non-maleficence, as well as justice, solidarity and responsibility. (extracted from612.txt)
73) The collection and transmission of large volumes of health-relevant data touches on fundamental questions of justice. (extracted from612.txt)
As a normalising principle of social relations, justice demands that the arbitrary privi-leging of certain persons or groups be avoided. (extracted from612.txt)
25 25 ExEcutIvE Summary 74) As regards big data applications in the healthcare sector, four sets of problems stand out as especially relevant to questions of justice: first, access to datasets for the research sector; second, the insidious con-solidation of monopolistic structures; third, the inclusion of health apps, as well as various devices that facilitate private self-tracking, in determining health insurance premiums; and fourth, aspects of social justice, understood in terms of the capabilities approach, as they concern the responsible handling of health-relevant data. (extracted from612.txt)
Solidarity is frequently understood as complimentary to – and often subsidiary to – the concept of justice. (extracted from612.txt)
The shaping of such freedom is responsible when it also orients itself towards the legal and societal demands of solidarity and justice. (extracted from612.txt)
These aim to, firstly, realise the potentials of big data; secondly, to ensure individual freedom and privacy; thirdly, to ensure justice and solidarity; and fourthly, to promote responsibility and trust. (extracted from612.txt)
Ensure justice and solidarity C1. (extracted from612.txt)
To do justice to the complexity and significance of this issue, for example, companies and institutions could expand their efforts to establish internal data science departments. (extracted from612.txt)
justice, health, transport, etc). (extracted from362.txt)
Quick links Work programme Drafts and new work items Working area Working documents (user account required) ISO Electronic applications IT Tools that help support the standards development process Public material Browse documents made available by this group This committee contributes with 54 standards to the following Sustainable Development Goals: 1 No Poverty 3 Good Health and Well-being 4 Quality Education 5 Gender Equality 6 Clean Water and Sanitation 7 Affordable and Clean Energy 8 Decent Work and Economic Growth 9 Industry, Innovation and Infrastructure 10 Reduced Inequalities 11 Sustainable Cities and Communities 12 Responsible Consumption and Production 13 Climate Action 14 Life Below Water 15 Life on Land 16 Peace, Justice and Strong Institutions 39 Published ISO standards * 46 ISO standards under development * 50 Participating members 25 Observing members * number includes updates Structure Liaisons Meetings Reference Title Type ISO/IEC JTC 1/SC 42/AHG 4 Liaison with SC 27 Working group ISO/IEC JTC 1/SC 42/AHG 8 Best practices for new proposals Working group ISO/IEC JTC 1/SC 42/JAG Joint Advisory Group on AI and sustainability with ISO/IEC JTC1/SC 39 and JTC1/SC 42 Working group ISO/IEC JTC 1/SC 42/JWG 2 Joint Working Group ISO/IEC JTC1/SC 42 - ISO/IEC JTC1/SC 7 Working group ISO/IEC JTC 1/SC 42/JWG 3 Joint Working Group ISO/IEC JTC1/SC42 - ISO/TC 215 WG : AI enabled health informatics Working group ISO/IEC JTC 1/SC 42/JWG 4 Joint Working Group ISO/IEC JTC1/SC42 - IEC TC65/SC65A: Functional safety and AI systems Working group ISO/IEC JTC 1/SC 42/JWG 5 Joint Working Group ISO/IEC JTC1/SC42 - ISO/TC 37 WG: Natural language processing Working group ISO/IEC JTC 1/SC 42/JWG 6 Joint Working Group ISO/IEC JTC1/SC42 - ISO/CASCO: Conformity assessment schemes for AI systems Working group ISO/IEC JTC 1/SC 42/JWG 7 Joint Working Group ISO/IEC JTC1/SC 42 - ISO/TC 68: Artificial intelligence Working group ISO/IEC JTC 1/SC 42/WG 1 Foundational standards Working group ISO/IEC JTC 1/SC 42/WG 2 Data Working group ISO/IEC JTC 1/SC 42/WG 3 Trustworthiness Working group ISO/IEC JTC 1/SC 42/WG 4 Use cases and applications Working group ISO/IEC JTC 1/SC 42/WG 5 Computational approaches and computational characteristics of AI systems Working group Liaison Committees to ISO/IEC JTC 1/SC 42 The committees below can access the documents of ISO/IEC JTC 1/SC 42: Reference Title ISO/IEC IEC/SC 45A Instrumentation, control and electrical power systems of nuclear facilities IEC IEC/SC 62C Equipment for radiotherapy, nuclear medicine and radiation dosimetry IEC IEC/SC 65A System aspects IEC IEC/SyC AAL Active Assisted Living IEC IEC/SyC SM Smart Manufacturing IEC IEC/SyC Smart Cities Electrotechnical aspects of Smart Cities IEC IEC/TC 9 Electrical equipment and systems for railways IEC IEC/TC 56 Dependability IEC IEC/TC 62 Medical equipment, software, and systems IEC IEC/TC 65 Industrial-process measurement, control and automation IEC IEC/TC 100 Audio, video and multimedia systems and equipment IEC ISO/CASCO Committee on conformity assessment ISO ISO/IEC JTC 1 Information technology ISO/IEC ISO/IEC JTC 1/SC 6 Telecommunications and information exchange between systems ISO/IEC ISO/IEC JTC 1/SC 7 Software and systems engineering ISO/IEC ISO/IEC JTC 1/SC 24 Computer graphics, image processing and environmental data representation ISO/IEC ISO/IEC JTC 1/SC 27 Information security, cybersecurity and privacy protection ISO/IEC ISO/IEC JTC 1/SC 29 Coding of audio, picture, multimedia and hypermedia information ISO/IEC ISO/IEC JTC 1/SC 31 Automatic identification and data capture techniques ISO/IEC ISO/IEC JTC 1/SC 32 Data management and interchange ISO/IEC ISO/IEC JTC 1/SC 34 Document description and processing languages ISO/IEC ISO/IEC JTC 1/SC 35 User interfaces ISO/IEC ISO/IEC JTC 1/SC 36 Information technology for learning, education and training ISO/IEC ISO/IEC JTC 1/SC 37 Biometrics ISO/IEC ISO/IEC JTC 1/SC 38 Cloud computing and distributed platforms ISO/IEC ISO/IEC JTC 1/SC 39 Sustainability, IT and data centres ISO/IEC ISO/IEC JTC 1/SC 40 IT service management and IT governance ISO/IEC ISO/IEC JTC 1/SC 41 Internet of things and digital twin ISO/IEC ISO/IEC JTC 1/SC 43 Brain-computer interfaces ISO/IEC ISO/IEC JTC 1/SC 44 Consumer protection in the field of privacy by design ISO/IEC ISO/TC 20 Aircraft and space vehicles ISO ISO/TC 20/SC 16 Uncrewed aircraft system ISO ISO/TC 22/SC 32 Electrical and electronic components and general system aspects ISO ISO/TC 24/SC 4 Particle characterization ISO ISO/TC 36 Cinematography ISO ISO/TC 37 Language and terminology ISO ISO/TC 37/SC 3 Management of terminology resources ISO ISO/TC 37/SC 5 Translation, interpreting and related technology ISO ISO/TC 42 Photography ISO ISO/TC 46/SC 11 Archives/records management ISO ISO/TC 68 Financial services ISO ISO/TC 69 Applications of statistical methods ISO ISO/TC 108/SC 5 Condition monitoring and diagnostics of machine systems ISO ISO/TC 172/SC 5 Microscopes and endoscopes ISO ISO/TC 199 Safety of machinery ISO ISO/TC 204 Intelligent transport systems ISO ISO/TC 210 Quality management and corresponding general aspects for products with a health purpose including medical devices ISO ISO/TC 211 Geographic information/Geomatics ISO ISO/TC 215 Health informatics ISO ISO/TC 225 Market, opinion and social research ISO ISO/TC 232 Education and learning services ISO ISO/TC 258 Project, programme and portfolio management ISO ISO/TC 260 Human resource management ISO ISO/TC 261 Additive manufacturing ISO ISO/TC 262 Risk management ISO ISO/TC 267 Facility management ISO ISO/TC 268 Sustainable cities and communities ISO ISO/TC 269 Railway applications ISO ISO/TC 279 Innovation management ISO ISO/TC 299 Robotics ISO ISO/TC 307 Blockchain and distributed ledger technologies ISO ISO/TC 309 Governance of organizations ISO ISO/TC 312 Excellence in service ISO ISO/TC 321 Transaction assurance in E-commerce ISO ISO/TC 322 Sustainable finance ISO ISO/TC 324 Sharing economy ISO ISO/TC 347 Data-driven agrifood systems ISO Liaison Committees from ISO/IEC JTC 1/SC 42 ISO/IEC JTC 1/SC 42 can access the documents of the committees below: Reference Title ISO/IEC IEC/ISO JTC 3 Quantum technologies ISO/IEC IEC/SyC AAL Active Assisted Living IEC IEC/SyC SM Smart Manufacturing IEC IEC/SyC Smart Cities Electrotechnical aspects of Smart Cities IEC IEC/TC 44 Safety of machinery - Electrotechnical aspects IEC IEC/TC 62 Medical equipment, software, and systems IEC IEC/TC 65 Industrial-process measurement, control and automation IEC IEC/TC 100 Audio, video and multimedia systems and equipment IEC ISO/CASCO Committee on conformity assessment ISO ISO/IEC JTC 1 Information technology ISO/IEC ISO/IEC JTC 1/SC 7 Software and systems engineering ISO/IEC ISO/IEC JTC 1/SC 24 Computer graphics, image processing and environmental data representation ISO/IEC ISO/IEC JTC 1/SC 27 Information security, cybersecurity and privacy protection ISO/IEC ISO/IEC JTC 1/SC 29 Coding of audio, picture, multimedia and hypermedia information ISO/IEC ISO/IEC JTC 1/SC 32 Data management and interchange ISO/IEC ISO/IEC JTC 1/SC 34 Document description and processing languages ISO/IEC ISO/IEC JTC 1/SC 35 User interfaces ISO/IEC ISO/IEC JTC 1/SC 36 Information technology for learning, education and training ISO/IEC ISO/IEC JTC 1/SC 37 Biometrics ISO/IEC ISO/IEC JTC 1/SC 38 Cloud computing and distributed platforms ISO/IEC ISO/IEC JTC 1/SC 39 Sustainability, IT and data centres ISO/IEC ISO/IEC JTC 1/SC 40 IT service management and IT governance ISO/IEC ISO/IEC JTC 1/SC 41 Internet of things and digital twin ISO/IEC ISO/IEC JTC 1/SC 43 Brain-computer interfaces ISO/IEC ISO/IEC JTC 1/SC 44 Consumer protection in the field of privacy by design ISO/IEC ISO/TC 20/SC 16 Uncrewed aircraft system ISO ISO/TC 22/SC 32 Electrical and electronic components and general system aspects ISO ISO/TC 24/SC 4 Particle characterization ISO ISO/TC 36 Cinematography ISO ISO/TC 37/SC 3 Management of terminology resources ISO ISO/TC 46 Information and documentation ISO ISO/TC 46/SC 11 Archives/records management ISO ISO/TC 68 Financial services ISO ISO/TC 69 Applications of statistical methods ISO ISO/TC 106 Dentistry ISO ISO/TC 145 Graphical symbols ISO ISO/TC 159 Ergonomics ISO ISO/TC 172/SC 5 Microscopes and endoscopes ISO ISO/TC 199 Safety of machinery ISO ISO/TC 204 Intelligent transport systems ISO ISO/TC 210 Quality management and corresponding general aspects for products with a health purpose including medical devices ISO ISO/TC 215 Health informatics ISO ISO/TC 225 Market, opinion and social research ISO ISO/TC 260 Human resource management ISO ISO/TC 262 Risk management ISO ISO/TC 268 Sustainable cities and communities ISO ISO/TC 299 Robotics ISO ISO/TC 307 Blockchain and distributed ledger technologies ISO ISO/TC 309 Governance of organizations ISO ISO/TC 312 Excellence in service ISO ISO/TC 321 Transaction assurance in E-commerce ISO ISO/TC 324 Sharing economy ISO Organizations in liaison (Category A and B) Acronym Title Category BCI Blockchain & Climate Institute A BDVA Big Data Value AISBL A CI Consumers International A Cloud security alliance Cloud security alliance A EC - European Commission European Commission A ETSI European Telecommunications Standards Institute A ETUC European Trade Union Confederation A euRobotics AISBL euRobotics AISBL A EUROCAE The European Organization for Civil Aviation Equipment A HL7 Health Level Seven International A IEEE Institute of Electrical and Electronics Engineers, Inc A IIOA Independent International Organisation for Assurance A ITU International Telecommunication Union A MedTech Europe Alliance of European medical technology industry associations A ML Commons MLCommons Association A OECD Organisation for Economic Co-operation and Development, OECD A OGC Open Geospatial Consortium, Inc. (extracted from411.txt)
These opportunities can make a lasting contribution to freedom, justice and prosperity above all when people’s individual rights are protected and social cohesion is strengthe ned. (extracted from175.txt)
Other common principles include human oversight, explainability or interpretability, legal status of AI systems, and the equitable economic effect of AI.31 A separate analysis of 84 AI ethics documents done in 2019 found that there has been a global convergence around “transparency, justice and fairness, non-maleficence, responsibility and privacy.”32 While much progress has been made aligning on responsible AI, there remain differences—even among FCAI participants. (extracted from74.txt)
Rules, Standards, and R&D Projects: Key areas for collaboration | ⮌ contents 62the importance of cross-border data flows as well as challenges to privacy and other values.211 Work to advance data free with trust has continued through the World Economic Forum.212 The “Schrems II” judgment by the Court of Justice of the European Union (CJEU) in 2020, however, has been a seismic event for international transfers of personal information, the aftershocks of which are still reverberating and magnify the impact of the EU regime. (extracted from74.txt)
Kerry, ”The oracle at Luxembourg: The EU Court of Justice judges the world on surveillance and privacy,” The Brookings Institution, January 11, 2021, https://www.brookings.edu/ research/the-oracle-at-luxembourg-the-eu-court-of-justice-judges-the-world-on-surveillance-and-privacy/. (extracted from74.txt)
Kerry, ”The oracle at Luxembourg: The EU Court of Justice judges the world on surveillance and privacy,” The Brookings Institution, January 11, 2021, https://www.brookings.edu/research/the-oracle-at-luxembourg-the-eu-court-of-justice-judges-the-world-onsurveillance-and-privacy/. (extracted from74.txt)
The secretariat is provided by Directorate C (Fundamental Rights and Union Citizenship) of the European Commission, Directorate General Justice, B -1049 Brussels, Belgium, Office No MO -59 03/075. (extracted from48.txt)
Website: http://ec.europa.eu/justice/data -protection/index_en.htm 17/EN WP 251 Guidelines on Automated individual decision -making and Profiling for the purposes of Regulation 2016/679 Adopted on 3 October 2017 2 THE WORKING PARTY ON THE PROTECTION OF INDIVIDUALS WITH REGARD TO THE PROCESSING OF PERSONAL DATA set up by Directive 95/46/EC of the European Parliament and of the Council of 24 October 1995, having regard to Articles 29 and 30 thereof, having regard to its Rules of Procedure, HAS ADOPTED THE PRESENT GUIDELINES: 3 TABLE OF CONTENT S I. (extracted from48.txt)
http://ec.europa.eu/justice/data -protection/article -29/documentation/opinion recommendation/files/2013/wp203_en.pdf . (extracted from48.txt)
http://ec.europa.eu/justice/data -protection/article -29/documentation/opinion recommendation/files/2014/wp217_en.pdf . (extracted from48.txt)
Page 26 http://ec.europa.eu/justice/data -protection/article -29/documentation/opinion recommendation/files/2014/wp217_en.pdf . (extracted from48.txt)
Department of Justice. (extracted from559.txt)
(2019), ‘Dirty data, bad predictions: How civil rights violations impact police data, predictive policing systems, and justice’, NYU Law Review, Vol. (extracted from203.txt)
Beyond that, gender identity is mentioned only in Recital 9 of the Victims’ Rights Directive75 in the context of criminal law.76 According to the Court of Justice of the European Union, gender identity is only partly covered by the principle of equal treatment between men and women.77 Legal protection against discrimination based on religion is currently also limited under EU law.78 Nevertheless, one may argue that many comments referring to people who identify as lesbian, gay, bisexual, transgender and intersex (LGBTI), Jewish or Muslim fall under either the Racial Equality Directive or the Gender Goods and Services Directive, because discrimination based on sexual orientation, gender identity or religion predominantly affects a specific race or gender. (extracted from203.txt)
88 The use of algorithms may further increase the opacity of content moderation and further increase challenges linked to fairness and justice.89 Without proper safeguards, such tools can lead to censorship and biased enforcement of laws and platforms’ terms and conditions.90 A potential increase in discrimination is just one of the challenges when using algorithms to support speech detection for content moderation purposes. (extracted from203.txt)
Article 29 Working Party (2017a), Opinion on some key issues of the Law Enforcement Directive (EU 2016/680), WP 258, Brussels, European Commission Directorate-General Justice and Consumers. (extracted from203.txt)
Article 29 Working Party (2017b), Guidelines on automated individual decisionmaking and profiling for the purposes of Regulation 2016/679, WP251rev.01, Brussels, European Commission Directorate-General Justice, p. (extracted from203.txt)
(2019), ‘ CS224n: Natural language processing with deep learning 1 – Lecture notes: Part I ’, course instructor: Manning, C., Winter 2021.CJEU (Court of Justice of the European Union) (1991), C-184/89, Helga Nimz v. (extracted from203.txt)
(2021), Automating Injustice – The use of artificial intelligence and automated decision-making systems in criminal justice in Europe, London, Fair Trials.Finck, M. (extracted from203.txt)
307–335.FRA (2016), Ensuring justice for hate crime victims: Professional perspectives, Luxembourg, Publications Office.FRA (2017), Second European Union Minorities and Discrimination Survey – Main results, Luxembourg, Publications Office.FRA (2018a), #BigData: Discrimination in data-supported decision making, Luxembourg, Publications Office. (extracted from203.txt)
(2019), ‘ Dirty data, bad predictions: How civil rights violations impact police data, predictive policing systems, and justice’, NYU Law Review, Vol. (extracted from203.txt)
354–365.The Law Society (2019), Algorithms in the criminal justice system , London, The Law Society, p. (extracted from203.txt)
The Member States share a "society in which pluralism, non -discrimination, tolerance, justice, solidarity and equality between women and men prevail." 3 Launching a European initi ative on AI In May 2017, the Commission published its mid-term review of the Digital Single Market strategy6. (extracted from213.txt)
This includes in vestments in projects in key application areas such as health, connected and automated driving, agriculture, manufacturing, energy, next generation internet technologies, security and public administrations (including justice). (extracted from213.txt)
Seven values underpin these ethical guidelines: well -being, autonomy, justice, privacy, knowledge, democracy and responsibility. (extracted from58.txt)
Considering the impact of AI systems on everyday life , in addition to the accelerated trends and risks due to COVID19 , the present Declaration highlights : a) The exclusionary consequences of the fast-paced development of AI technologies driven by the private sector that leave many stake holders, especially youth actors and human rights activists , but also policy makers, behind, and result in normatively questionable and ineffective self -regulation in the private sector b) The need to develop and ensure l egal safeguards both by international organisations through existing or new legal instruments and by national governments responsible for securing and implementing them at national level c) The a bsence of youth in the emerging AI governance processes as a denial of the right to participation in democratic pro cesses which impedes a whole sector from co -shaping the discourse about development, assessment, implementation and regulation of AI technologies d) The imperative to respect ethical principles which must be at the core of all AI developments and deployment (transparency, justice and fairness, responsibility, safety and security, privacy) e) The need to assess the value of AI technologies on the impact of their consequences and benefits on individuals and soc iety. (extracted from159.txt)
17 http://ec.europa.eu/justice/gender -equality/tools/statistics -indicators/platform action/index_en.htm 15308/18 PL/mk 15 ANNEX II to ANNEX LIFE 1.C EN - Council Conclusions on Enhancing the Skills of Women and Men in the EU Labour Market (6889/17). (extracted from165.txt)
- Opinion of the Advisory Committee on Equal Opportunities for Women and Men on how to overcome occupational segregation http://ec.europa.eu/justice/gender equality/files/opinions_advisory_committee/151125_opinion_occ_segregation_en. (extracted from165.txt)
The mission of the Data Science Initiative is to harness the value of data science and artificial intelligence for peace, justice and security. (extracted from617.txt)
79 Hackathon for Peace, Justice and Security, The Hague, (2018, 2019) www.hackathonforgood.org. (extracted from617.txt)
Criminal justice, public housing, welfare and health provision are examples of areas where “black box” systems should not be developed or used. (extracted from429.txt)
29 In “Fairness in Criminal Justice Risk Assessments: The State of the Art” Berk et al, 2017 provide a through review of the technical pathways towards promoting fairness in machine learning. (extracted from429.txt)
Berk et al, 2017, Fairness in Criminal Justice Risk Assessments: The State of the Art https://arxiv.org/ abs/1703.0920730 Ai Now Institute 2017 Report 13 How to Prevent Discriminatory Outcomes in Machine LearningBringing principles of non-discrimination to life: Human rights due diligence for machine learning Companies developing and using ML systems must integrate these principles of non-discrimination into their human rights due diligence – a process by which businesses take ongoing, proactive, and reactive steps to ensure that they uphold people’s dignity and do not cause or contribute to human rights abuses. (extracted from429.txt)
For instance, even if a machine's impact is only ever good, the distribution of that good (concerns for justice) might be in question”. (extracted from367.txt)
Regarding these rights, we focus on issues relating to the right to respect private life, human dignity, ownership, safety and liability, freedom of expression and the prohibition of discrimination as well as acc ess to justice and the right to a fair trial. (extracted from372.txt)
39 3.8 Access to justice and the right to a fair trial ................................ (extracted from372.txt)
The next three sections are dedicated to how automated decisions may affect freedom of expression, the prohibition of discrimination, access to justice and the right to a fair trial (sections 3.6, 3.7 and 3.8). (extracted from372.txt)
Intelligent artefacts may challenge different dimensions of human rights, for example in terms of freedoms (privacy and data protection, ownership, autonomy, personality), equality ( more specifically , non-discrimination) and justice (fair trial, access to justice). (extracted from372.txt)
Thus, the impact of intelligent artefacts is first discussed in the context of seven existing human rights: the right to respect for private and family life (3.2), the right to human dignity (3.3), the right to the peaceful enjoyment of possessions (3.4), tort rights and safety (3.5), the right to freedom of expression as well as the freedom of thought, conscience and religion (3.6), the prohibition of discrimination (3.7) and access to justice as well as the right to a fair trial (3.8). (extracted from372.txt)
18 Court of Justice of the European Union 19 October 2016, C -582/14 (Breyer ) and Court of Justice of the European Union 24 November 2011, C -70/10 (Scarlet/SABAM ), paragraph 51. (extracted from372.txt)
For instance, o n an international level, both the European Court of Human Rights and the Court of Justice of the European Union have voiced their concerns on numerous occasions regarding state surveillance activi ties and the impact on privacy rights.25 For 21 The Dutch Data Protection Authority imposed penalty payments on a company that co uld not demonstrate that Wi -Fi tracking in public spaces was necessary for a legitimate purpose. (extracted from372.txt)
21 instance when the EU Court of Justice declared the Data Retention Directive to be invalid.26 On a national level, the British Investigatory Powers Tribunal , for instance , ruled that during a time span of more than a decade, the British intelligence agencies illegally collected data about innocent citizens and illegally tracked their phone and web use.27 With regard to surveillance enacted by companies , the European Commission proposed the reform of EU data protect ion regulations , which ultimately led to the G eneral Data Protection Regulation . (extracted from372.txt)
Court of Justice of the European Union 8 April 2014, Joined Cases C -293/12 and C -594/12 ( Digital Rights Ireland and Seitlinger and Others ). (extracted from372.txt)
See also: Court of Justice of the European Union 21 December 2016, Joined Cases C -203/15 ( Tele2 Sverige AB v Post -och telestyrelsen ) and C -698/15 ( Secretary of State for the Home Department v Tom Watson and Others ). (extracted from372.txt)
28 For an overview of (a selec tion of) Court of Justice of the European Union and European Court of Human Rights cases, see Annex I of Article 29 Data Protection Working Party, Working Document 01/2016 on the justification of interferences with the fundamental rights to priva cy and dat a protection through surveillance measures when transferring personal data (European Essential Guarantees), 13 April 2016. (extracted from372.txt)
In addition, wi th regard to decisions of the Court of Justice of the European Union concerning data protection, see: L. (extracted from372.txt)
These letters are available at: http://ec.europa.e u/justice/data -protection/article -29/documentation/other -document/index_en.htm . (extracted from372.txt)
42 3.8 Access to justice and the right to a fair trial Access to justice and the right to a fair trial Article 6 (2) ECHR plays an important role with regard to predictive AI . (extracted from372.txt)
The increased use of risk -assessing algorithms in the American justice system raises accountability a nd transparency issues.100 It has been reported that software used to set bail , conditions for parole and sentencing decisions is biased against Afr ican Americans (Angwin et al. (extracted from372.txt)
To give another example, in response to worries by consumers about Wi -Fi tracking by shop owners, the former Dutch minister of economic affairs and the state secretary of security and justice stated that people should just turn off their smartphone if they do not want to be tracked.102 On the basis of this response , it seems that tracking and tracing people is a right which is deemed more important than peoples’ (privacy) rights. (extracted from372.txt)
Besides affecting the right to respect for private lif e in numerous ways, digiti sation, virtuali sation and roboti sation influence human dignity, the right to the peaceful enjoyment of possessions, safety and tort rights, the right to freedom of expression, the prohibition of discrimination, access to justice and the right to a fair trial. (extracted from372.txt)
Access to justice and the right to a fair trial (see subsection 3.8) The Council of Europe could establish a framework of minimum norms to be taken into account when a court uses AI. (extracted from372.txt)
(2007) E-Justice, beginselen van behoorlijke elektronische rechtspraak, The Hague: SDu Uitgevers. (extracted from372.txt)
Ordered to be printed 21 March 2022 and published 30 March 2022 Published by the Authority of the House of LordsHOUSE OF LORDS Justice and Home Affairs Committee 1st Report of Session 2021–22 HL Paper 180Technology rules? (extracted from366.txt)
The advent of new technologies in the justice system Justice and Home Affairs Committee The Justice and Home Affairs Committee was appointed by the House of Lords on 14 April 2021 to consider justice and home affairs, including the domestic criminal justice system, and international cooperation in respect of criminal justice, civil justice, migration and asylum. (extracted from366.txt)
Membership The Members of the Justice and Home Affairs Committee are: Lord Blunkett Baroness Kennedy of The Shaws Baroness Chakrabarti Baroness Pidding Lord Dholakia Baroness Primarolo Baroness Hallett Lord Ricketts Baroness Hamwee (Chair) Baroness Sanderson of Welton Lord Hunt of Wirral Baroness Shackleton of Belgravia Declaration of interests See Appendix 1. (extracted from366.txt)
A full list of Members’ interests can be found in the Register of Lords’ Interests: http://www.parliament.uk/hlregister Publications All publications of the Committee are available at: http://www.parliament.uk/ 519/justice-and-home-affairs-committee / Parliament Live Live coverage of debates and public sessions of the Committee’s meetings are available at: http://www.parliamentlive .tv Further information Further information about the House of Lords and its Committees, including guidance to witnesses, details of current inquiries and forthcoming meetings is available at: http://www.parliament .uk/business/lords Committee staff The staff who worked on this inquiry were Sam Kenny (Clerk), Achille Versaevel (Policy Analyst) and Amanda McGrath (Committee Operations Officer). (extracted from366.txt)
Contact details General correspondence should be addressed to the Clerk of the Justice and Home Affairs Committee, Committee Office, House of Lords, London SW1A 0PW. (extracted from366.txt)
3 THE ADVE nT OF n EW TECH nOLOGIES I n THE JUSTICE SYSTEM SUMMARY In recent years, and without many of us realising it, Artificial Intelligence has begun to permeate every aspect of our personal and professional lives. (extracted from366.txt)
Our Committee has limited its investigation to only one area–how these advanced technologies are used in our justice system. (extracted from366.txt)
Algorithms are being used to improve crime detection, aid the security categorisation of prisoners, streamline entry clearance processes at our borders and generate new insights that feed into the entire criminal justice pipeline. (extracted from366.txt)
When deployed within the justice system, AI technologies have serious implications for a person’s human rights and civil liberties. (extracted from366.txt)
Without transparency, there can not only be no scrutiny, but no 4 THE ADVE nT OF n EW TECH nOLOGIES I n THE JUSTICE SYSTEM accountability for when things go wrong. (extracted from366.txt)
Proper trials methodology is fully embedded into medical science but there are no minimum scientific or ethical standards that an AI tool must meet before it can be used in the criminal justice sphere. (extracted from366.txt)
5 THE ADVE nT OF n EW TECH nOLOGIES I n THE JUSTICE SYSTEM Yet without sufficient safeguards, supervision, and caution, advanced technologies may have a chilling effect on a range of human rights, undermine the fairness of trials, weaken the rule of law, further exacerbate existing inequalities, and fail to produce the promised effectiveness and efficiency gains. (extracted from366.txt)
The advent of new technologies in the justice system CHAPTER 1: INTRODUCTION 1. (extracted from366.txt)
Within the application of the law, we included a broad view of the justice system, examining instances where advanced tools were used to discover, deter, rehabilitate, or punish people who breach the law in England and Wales, as well as border management. (extracted from366.txt)
8 THE ADVE nT OF n EW TECH nOLOGIES I n THE JUSTICE SYSTEM Automated decision making (ADM): ADM is the process of making a decision by automated means without any human involvement. (extracted from366.txt)
Written evidence from Dr Miri Zilka, Dr Adrian Weller and Detective Sergeant Laurence Cartwright laid out some categories of tools used in the justice system. (extracted from366.txt)
9 THE ADVE nT OF n EW TECH nOLOGIES I n THE JUSTICE SYSTEM (b) Data analysis: software and tools primarily used to analyse data to create insights. (extracted from366.txt)
We heard the most about tools used by the Home Office, the Ministry of Justice, HM Prisons and Probation Service, and individual police forces. (extracted from366.txt)
We were told, for example, about the use of polygraphs to monitor sex offenders on parole and manage their level of compliance with parole conditions.14 8 Written evidence from Dr Miri Zilka, Detective Sergeant Laurence Cartwright and Dr Adrian Weller (nTL0040 ) 9 Written evidence from Association of Police and Crime Commissioners, national Police Chiefs’ Council and Police Digital Service ( nTL0049 ) 10 Written evidence from Avon and Somerset Police ( nTL0052 ) 11 Written evidence from the Serious Fraud Office ( nTL0034 ) 12 Written evidence from Public Law Project ( nTL0046 ) 13 Written evidence from Liberty ( nTL0020 ) 14 Written evidence from Dr Kyriakos n Kotsoglou ( nTL0007 ) 10 THE ADVE nT OF n EW TECH nOLOGIES I n THE JUSTICE SYSTEM 6. (extracted from366.txt)
11 THE ADVE nT OF n EW TECH nOLOGIES I n THE JUSTICE SYSTEM the end of which it would become costly for the customer to opt out. (extracted from366.txt)
Avon and Somerset Constabulary thought their use of data analytics placed “better insights into the hands of those delivering the business to help empower and support more effective decision making.”23 The Rt Hon Kit Malthouse MP, the Minister for Crime and Policing at the Home Office and Ministry of Justice, told us that he was “very excited about the use of artificial intelligence and machine learning in policing.”24 We also acknowledge that, as many submissions pointed out, advanced tools can provide substantial assistance towards enacting the crucial duties of the police to protect and prevent harm. (extracted from366.txt)
Matthew Gill, Senior Fellow at the Institute for Government, facilitated a seminar for us to consider the institutional and regulatory frameworks which 22 Q 45 (Professor Elizabeth Joh) 23 Written evidence from Avon and Somerset Police ( nTL0052 ) 24 Q 99 (Kit Malthouse MP) 25 Q 39 (Professor Elizabeth Joh) 26 Written evidence from SAS UK&I ( nTL0041 ) 27 Written evidence from the Information Commissioner’s Office ( nTL0016 ) 12 THE ADVE nT OF n EW TECH nOLOGIES I n THE JUSTICE SYSTEM may be put in place. (extracted from366.txt)
• In 2020, the Scottish Parliament Justice Sub-Committee on Policing published Facial recognition: how policing in Scotland makes use of this technology.30 • In 2021, the European Parliament Committee on Civil Liberties, Justice and Home Affairs published a report on “artificial intelligence in criminal law and its use by the police and judicial authorities in criminal matters”.31 • In 2021, nATO adopted its first Artificial Intelligence Strategy, including principles of the responsible use of AI in Defence and announcing further work to set international AI standards.32 • In 2021, U nESCO adopted a Recommendation on the Ethics of Artificial Intelligence and is working towards establishing the first-ever global normative instrument on the ethics of AI.33 15. (extracted from366.txt)
We decided to examine the use of these tools throughout the “criminal justice pipeline”34 and in border management, identifying where change was needed, and identifying some principles for the safe and ethical use of such tools. (extracted from366.txt)
(Report of Session 2017–19, HL Paper 100) 29 Council of Europe, CAHAI Ad Hoc Committee on Artificial Intelligence, ‘Terms of Reference’: https://www.coe.int/en/web/artificial-intelligence/cahai [accessed 6 February 2022] 30 The Scottish Parliament, Justice Sub-Committee on Policing, Facial Recognition: How Policing in Scotland Makes Use of This Technology (1st Report, Session 5, SP Paper 678) 31 Committee on Civil Liberties, Justice and Home Affairs, Report on artificial intelligence in criminal law and its use by the police and judicial authorities in criminal matters (13 July 2021): https://www.europarl. (extracted from366.txt)
org/ark:/48223/pf0000380455 [accessed 6 February 2022] 34 Written evidence from Dr Miri Zilka, Detective Sergeant Laurence Cartwright and Dr Adrian Weller (nTL0040 ) 13 THE ADVE nT OF n EW TECH nOLOGIES I n THE JUSTICE SYSTEM Chapter 3, we look at transparency: its necessity and proposals to increase it. (extracted from366.txt)
The key issues we have identified, however, hold true for a much wider context: their application to all functions of the justice system and to border management. (extracted from366.txt)
14 THE ADVE nT OF n EW TECH nOLOGIES I n THE JUSTICE SYSTEM CHAPTER 2: LEGAL AND INSTITUTIONAL FRAMEWORKS 18. (extracted from366.txt)
It was also indicated that “public failures” could “lead to not just operational defects or inefficiencies, but miscarriages of justice”,42 and that where weaknesses were exposed, they “exacerbate the low level and negative trend in public trust for relevant technology”.43 Professor n igel Harvey and Tobias Harvey referred to accountability for errors and misuse, saying that the use of algorithms “may leave people open to dangers for which no person can be identified as responsible”.44 “A chilling effect”45 22. (extracted from366.txt)
Various contributors told us that the use of some technologies, notably the use of live facial recognition, created fear or disquiet, and that this risked 35 Written evidence from the Home Office ( nTL0055 ) 36 QQ 103–104 (Kit Malthouse MP) 37 Royal Court of Justice , R v The Chief Constable of South Wales Police , [2020] EWCA Civ 1058 . (extracted from366.txt)
(nTL0022 ) 43 Written evidence from Archie Drake and Perry Keller ( nTL0011 ) 44 Written evidence from Professor n igel Harvey and Tobias Harvey ( nTL0025 ) 45 Written evidence from Dr Joe Purshouse, Dr n essa Lynch, Dr Marcin Betkier and Professor Liz Campbell ( nTL0021 ) 15 THE ADVE nT OF n EW TECH nOLOGIES I n THE JUSTICE SYSTEM damaging the democratic process. (extracted from366.txt)
16 THE ADVE nT OF n EW TECH nOLOGIES I n THE JUSTICE SYSTEM The right to a fair trial 23. (extracted from366.txt)
We were concerned that, in some instances, the use of advanced tools at certain points of the criminal justice pipeline may impede an individual’s right to a fair trial: whether by a lack of awareness that they were being used, unreliable evidence, or an inability to understand and therefore challenge proceedings. (extracted from366.txt)
Kotsoglou ( nTL0006 ) 55 Written evidence from Big Brother Watch ( nTL0037 ) 17 THE ADVE nT OF n EW TECH nOLOGIES I n THE JUSTICE SYSTEM discrimination”,56 and Professor n igel Harvey and Tobias Harvey wrote that “learning algorithms based on historical data would preserve bias”.57 28. (extracted from366.txt)
56 Written evidence from Liberty ( nTL0020 ) 57 Written evidence from Professor n igel Harvey and Tobias Harvey ( nTL0025 ) 58 Q 60 (Professor Karen Yeung) 59 Written evidence from Liberty ( nTL0020 ) 18 THE ADVE nT OF n EW TECH nOLOGIES I n THE JUSTICE SYSTEM Figure 1: Predictive policing—a vicious circle? (extracted from366.txt)
We have noted over 30 public bodies, initiatives, and programmes playing a role in the governance of new technologies for the application of the law 60 Q 99 (Kit Malthouse MP) 19 THE ADVE nT OF n EW TECH nOLOGIES I n THE JUSTICE SYSTEM (see Box 4). (extracted from366.txt)
gov.uk/government/uploads/system/uploads/attachment_data/file/1020402 [accessed 24 February 2022] 63 Centre for Data Ethics and Innovation, ‘About us’: https://www.gov.uk/government/organisations/ centre-for -data-ethics-and-innovation/about [accessed 6 February 2022] 64 HM Government, AI Council: https://www.gov.uk/government/ groups/ai-council [accessed 6 February 2022] 20 THE ADVE nT OF n EW TECH nOLOGIES I n THE JUSTICE SYSTEM Box 4: List of entities and programmes • Her Majesty’s Inspectorate of Constabulary and Fire and Rescue Services • The AI Council • The Association of Police and Crime Commissioners (APCC), and its various working groups and initiatives, including the APCC Biometrics and Data Ethics Working Group • The Biometrics and Forensics Ethics Group • The Biometrics and Surveillance Camera Commissioner • The Centre for Data Ethics and Innovation • The College of Policing • The Data Analytics Community of Practice • The Equalities and Human Rights Commission • The Forensic Science Regulator • The Home Office Digital, Data and Technology function • The Independent Office for Police Conduct • The Information Commissioner’s Office • The n ational Crime Agency, and its TRACER programme • The n ational Data Analytics Solution • The n ational Digital and Data Ethics Guidance Group • The n ational Digital Exploitation Centre • The national Police Chiefs’ Council, and its eleven co-ordination committees, each responsible for a specific aspect related to new technologies • The n ational Police Ethics Group • The n ational Policing Chief Scientific Adviser • The Office for AI • The Police Digital Service, its Data Office and Chief Data Officer • The Police Rewired initiative • The Police Science, Technology, Analysis and Research (STAR) fund • The Police, Science, and Technology Investment Board • The Royal Statistical Society • The Science Advisory Council to the national Policing Chief Scientific Adviser • The Senior Data Governance Panel within the Ministry of Justice • The specialist and generalist ethics committees of some police forces • The Tackling Organised Exploitation programme 21 THE ADVE nT OF n EW TECH nOLOGIES I n THE JUSTICE SYSTEM 33. (extracted from366.txt)
65 Q 98 (Professor Paul Taylor) 66 Written evidence from Association of Police and Crime Commissioners, national Police Chiefs’ Council and Police Digital Service ( nTL0049 ) 67 Department for Digital, Culture, Media & Sport, Data: A new direction (10 September 2021), para 409: https://assets.publishing.service.gov.uk/government/uploads/system/uploads/attachment_data/ file/1022315/Data_Reform_Consultation_Document__Accessible_.pdf [accessed 28 January 2022] 68 Q 99 (Kit Malthouse MP) 69 Q 108 (Kit Malthouse MP) 22 THE ADVE nT OF n EW TECH nOLOGIES I n THE JUSTICE SYSTEM Figure 2: “Family tree” of relevant governance arrangements Chief Constable Operational decisionSLT, ethics committee, technical experts, etc.Police and Crime Commissioner Oversight and strategic decision makingPublic Public consultation APCC Support, sharing best practice NPCC Coordination Committees 11 Committees and their members, some of which are national leads for new technologies, bring forces together to coordinate, reform, improve and provide value for money. (extracted from366.txt)
Dr Christopher Lawless referred to a series of bodies that all play “key roles in oversight” but which “vary in their remit and the extent of their powers”.70 Robin Allen QC and Dee Masters believed that “there has been too much thinking in ‘silos’”.71 There may also be confusion over responsibilities—Dr Lawless gave the example of facial recognition technology 70 Written evidence from Dr Christopher Lawless ( nTL0029 ) 71 Written evidence from Robin Allen QC and Dee Masters ( nTL0019 ) 23 THE ADVE nT OF n EW TECH nOLOGIES I n THE JUSTICE SYSTEM to argue that there is a “potentially significant lacuna [in governance] where it is unclear who is statutorily responsible for regulation and oversight”.72 36. (extracted from366.txt)
79 Q 99 (Kit Malthouse MP), see also Home Office, New Biometrics and Surveillance Camera Commissioner appointed (15 March 2021): https://www.gov.uk/government/news/new-biometrics-and-surveillancecamera -commissioner-appointed [accessed 28 January 2022] 80 Q 84 (Professor Paul Taylor) 24 THE ADVE nT OF n EW TECH nOLOGIES I n THE JUSTICE SYSTEM • In September 2021, the Government published its National Artificial Intelligence Strategy . (extracted from366.txt)
While the Minister said that, as a Minister in both the Home Office and the Ministry of Justice, he was the “living embodiment” of crossdepartmental working, this does not appear to have impacted strategic 81 Department for Business, Energy & Industrial Strategy, ‘Guidance national AI Strategy’ (22 September 2021): https://www.gov.uk/government/publications /national-ai-strategy/national-aistrategy-html-version [accessed 1 February 2022] 82 Department for Digital, Culture, Media & Sport, Data : A new direction , para 409. (extracted from366.txt)
83 DCMS Consultation: ‘Data: A new direction’ Response by the Biometrics and Surveillance Camera Commissioner: https://assets.publishing.service.gov.uk/government/uploads/system/uploads/ attachment _data/file/1030248/BSCC_DCMS_Consultation_Response.pdf [accessed 1 February 2022] 84 Written evidence from Association of Police and Crime Commissioners, national Police Chiefs’ Council and Police Digital Service ( nTL0049 ) 85 HL Deb, 3 november 2021, cols 1301–1305 25 THE ADVE nT OF n EW TECH nOLOGIES I n THE JUSTICE SYSTEM thinking on the use of new technologies in applying the law, and there is no indication of any collective governmental effort towards a single strategy.86 On the contrary, there are many indications of siloed thinking. (extracted from366.txt)
Similarly, BEIS and DCMS are collectively responsible for the implementation of the National AI Strategy , which focuses on businesses and the benefits of innovation and does not appear to have considered the needs of the Ministry of Justice or the Home Office at any length, or AI’s potential in their sectors. (extracted from366.txt)
David Tucker, Faculty Lead on Crime and Criminal Justice at the College of Policing, told us: “We have seen that where decisions are challenged or doubted cases go to court and affect the way policing operates. (extracted from366.txt)
The Appeal Court said that there was an absence of policy, so we are filling that gap and moving to apply these principles to this piece of technology”.87 86 Q 100 (Kit Malthouse MP) 87 Q 89 (David Tucker) 26 THE ADVE nT OF n EW TECH nOLOGIES I n THE JUSTICE SYSTEM Box 5: The Bridges case and the Public Sector Equality Duty 1. (extracted from366.txt)
88 Ministry of Justice, Public sector equality duty (6 July 2021): https://www.gov.uk/government/ publications/public-sector-equality-duty [accessed 4 February 2022] 89 Royal Court of Justice, R v The Chief Constable of South Wales Police , [2020] EWCA Civ 1058 90 Q 110 (Kit Malthouse MP) 91 Q 92 (Alun Michael) 27 THE ADVE nT OF n EW TECH nOLOGIES I n THE JUSTICE SYSTEM We have also been told that both domestic courts and the European Court of Human Rights had been relied upon in the past.92 49. (extracted from366.txt)
(nTL0022 ) 93 Written evidence from n CC Group ( nTL0005 ) 94 Written evidence from the Bar Council ( nTL0048 ) 95 Written evidence from n CC Group ( nTL0005 ) 96 Written evidence from Archie Drake and Perry Keller ( nTL0011 ) 97 Written evidence from the Home Office ( nTL0055 ) 98 Q 73 (David Lewis) 99 Written evidence from n CC Group ( nTL0005 ) 100 Q 36 (Dr David Leslie) 28 THE ADVE nT OF n EW TECH nOLOGIES I n THE JUSTICE SYSTEM of the EU Commission’s proposed AI regulation” (see Box 6). (extracted from366.txt)
Robin Allen QC and Dee Masters argued that “public actors and private companies need clear, pragmatic and effective regulatory frameworks because it provides a safety net within which ‘good’ AI can be developed whilst also protecting the fundamental rights of the public.”104 Archie Drake and Perry Keller had a similar view, stating that “legal uncertainty tends to harm business and innovation as well as public trust in the criminal justice system (and technology).”105 In a joint submission, three police bodies wrote that “Government should seek to clarify public appetite for new technologies and legislate so that policing has a clearer basis on which to make policies and decisions about deployment.”106 55. (extracted from366.txt)
Professor Sandra Wachter, Associate Professor at the University of Oxford, thought that “soft regulation would be irresponsible” because the criminal justice system is “one of the most highrisk areas [she] can think of”.107 Dr Joe Purshouse and his co-contributors reflected that while guidance documents may be cited in court, they “do not provide actionable grounds for an individual to make a complaint”, adding that “non-compliance would not impact on the admissibility of any material gleaned.”108 101 Written evidence from Public Law Project ( nTL0046 ) 102 Written evidence from Professor Pete Fussey, Dr Daragh Murray and Dr Amy Stevens ( nTL0017 ) 103 Ibid. (extracted from366.txt)
104 Written evidence from Robin Allen QC and Dee Masters ( nTL0019 ) 105 Written evidence from Archie Drake and Perry Keller ( nTL0011 ) 106 Written evidence from the Association of Police and Crime Commissioners (APCC), national Police Chiefs’ Council ( nPCC), and Police Digital Service (PDS) ( nTL0049 ) 107 Q 73 (Dr Liam Owens and Professor Sandra Wachter) 108 Written evidence from Dr Joe Purshouse, Dr n essa Lynch, Dr Marcin Betkier and Professor Liz Campbell ( nTL0021 ) 29 THE ADVE nT OF n EW TECH nOLOGIES I n THE JUSTICE SYSTEM Box 6: The EU Artificial Intelligence Regulation Proposal 1. (extracted from366.txt)
30 THE ADVE nT OF n EW TECH nOLOGIES I n THE JUSTICE SYSTEM of the national Police Chiefs’ Council, who, among others, thought that any new legal framework should be adopted at national level, but that “it would be helpful if it was not too divergent from international regulation”.115 The practicalities 58. (extracted from366.txt)
Others favoured a value- and principles-based approach, with Alun Michael saying that “the values and principles need to be established in law”.118 As our witnesses pointed out, “certain things with AI will always be the same … we will always have a data issue, a bias issue and an explainability issue.”119 Professor Raab similarly told us that among the “plethora” of guidance, research and reviews, a consensus had emerged on some principles: “privacy protection, accountability, fairness, non-discrimination, justice, transparency, safety and cybersecurity, serving the common good, explainability, and human oversight”.120 As we highlighted in paragraph 56, the Minister himself said that the Government’s preferred approach was to produce a set of principles—our view is that these should be translated into statute. (extracted from366.txt)
We 115 Q 73 (David Lewis) 116 Written evidence from n CC Group ( nTL0005 ) 117 Written evidence from Professor Pete Fussey, Dr Daragh Murray and Dr Amy Stevens ( nTL0017 ) 118 Q 98 (Alun Michael) 119 Q 69 (Professor Sandra Wachter) 120 Written evidence from Professor Charles Raab ( nTL0014 ) 31 THE ADVE nT OF n EW TECH nOLOGIES I n THE JUSTICE SYSTEM acknowledge the demands that the development of new legislation would place on parliamentary time and Government capacity, and that legislation is not a ‘quick fix’. (extracted from366.txt)
32 THE ADVE nT OF n EW TECH nOLOGIES I n THE JUSTICE SYSTEM 64. (extracted from366.txt)
The Home Office told us about guidance on the CAID programme (the Child Abuse Image Database, a facial recognition tool which helps identify victims and offenders)126, while guidance on facial recognition is currently being developed by the College of Policing, and the Ministry of Justice is working with the Alan Turing Institute to extend existing guidance on the use of data-driven technologies within the justice system.127 There is also various guidance available from the Surveillance Camera and Information Commissioners128, and a host of guidance from non-governmental sources such as the ALGO-care framework (a practical decision-making framework for the policing context).129 The Metropolitan Police Service noted that the application of the “variety of guidance, opinion, codes, directions and proposals for ethical frameworks … risks confusion and inconsistency”.130 126 Written evidence from the Home Office ( nTL0055 ) 127 College of Policing, ‘Police use of live facial recognition technology—have your say’ (17 May 2021): https://www.college.police.uk/article/police-use -live-facial-recognition-technology-have-your-say [accessed 26 January 2022] and written evidence from the Ministry of Justice ( nTL0053 ) 128 Information Commissioner’s Office, ‘Guidance index’: https://ico.org .uk/for-organisations/guidanceindex/ and Biometrics and Surveillance Camera Commissioner, ‘Surveillance camera guidance, tools and templates’ (22 October 2018): https://www.gov. (extracted from366.txt)
uk/government/collections/surveillance-cameraguidance-tools-and-templates [accessed 7 February 2022] 129 Marion Oswald, ‘Algorithmic risk assessment policing models: lessons from the Durham HART model and ‘Experimental’ proportionality’, Information & Communications Technology Law , vol.27, (3 April 2018): https://www.tandfonline.com/doi/full/ 10.1080/13600834.2018.1458455 [accessed 7 February 2022] 130 Written evidence from the Metropolitan Police Service ( nTL0031 ) 33 THE ADVE nT OF n EW TECH nOLOGIES I n THE JUSTICE SYSTEM Box 9: Application of the Equality Act 2010 • To illustrate the confusion in the application of law, several submissions referred to the Equality Act 2010. (extracted from366.txt)
As Professor Raab pointed out, comprehensive and practical guidance on 131 Written evidence from Archie Drake and Perry Keller ( nTL0011 ) 132 Cloisters, In the matter of automated data processing in Government decision making (7 September 2019): https://www.cloisters.com/ wp-content/uploads/2019/10/Open-opinion-pdf-version-1.pdf [accessed 25 January 2022] 133 Centre for Data Ethics and Innovation, Review into bias in algorithmic decision-making (november 2020), p 12: https://assets.publishing.service.gov.uk/government/uploads/ system/uploads/attachment_data/ file/957259/Review_into_bias_in_ algorithmic_decision-making.pdf [accessed 2 February 2022] 134 Written evidence from Professor Pete Fussey, Dr Daragh Murray and Dr Amy Stevens ( nTL0017 ) 135 Written evidence from Professor Charles Raab ( nTL0014 ) 136 Written evidence from Robin Allen QC and Dee Masters ( nTL0019 ) 137 College of Policing, ‘APP content’ (4 n ovember 2015): https://www.app.college.police.uk/appcontent/ [accessed 4 February 2022] 138 Written evidence from the Serious Fraud Office ( nTL0034 ) 34 THE ADVE nT OF n EW TECH nOLOGIES I n THE JUSTICE SYSTEM the use of types of technologies will require consistent review and ongoing updates as tools are used in operational settings, and practical operational issues identified.139 It could not therefore be expected that such guidance will ever tackle all of the specificities of particular tools. (extracted from366.txt)
The Government should require that national guidance for the use of advanced technological tools in policing and criminal justice is drawn up and, as part of their response to this report, should outline concrete plans for this. (extracted from366.txt)
142 Written evidence from the Association of Police and Crime Commissioners, n ational Police Chiefs’ Council, and Police Digital Service ( nTL0049 ) 143 Written evidence from the Law Society of England and Wales ( nTL0023 ) 144 Written evidence from Professor Pete Fussey, Dr Daragh Murray and Dr Amy Stevens ( nTL0017 ) 145 Written evidence from BAE Systems ( nTL0056 ) 146 Written evidence from the Information Commissioner’s Office ( nTL0016 ) 35 THE ADVE nT OF n EW TECH nOLOGIES I n THE JUSTICE SYSTEM potential influence on individuals, groups and society”147, this trust is critical. (extracted from366.txt)
Kit Malthouse MP said that forces must be allowed to fail “before we jump on everything.” It is important to note that the Minister was speaking in this context about technology which proves “not to be terribly useful”152, rather than about a failure to comply with minimum standards or where a miscarriage of justice had occurred. (extracted from366.txt)
In particular, they thought that the Home Secretary, the Lord Chancellor and Secretary of State for Justice, and the Minister for Crime and Policing should be answerable for “how the Government’s vision of technological change in the system safeguards its effectiveness and legitimacy.”156 The Minister for Crime and Policing agreed that he, and Government as a whole, are “broadly—whether [they] like it or not—responsible for most things.”157 147 Written evidence from Dr Matthias Wienroth et al. (extracted from366.txt)
(nTL0022 ) 148 Committee on Standards in Public Life, ‘The Seven Principles of Public Life’ (31 May 1995): https:// www.gov.uk/government/publications/the-7-principles-of- public-life/the-7-principles-of-publiclife--2 [accessed 27 January 2022] 149 Written evidence from Public Law Project ( nTL0046 ) 150 Q 72 (Dr Liam Owens) 151 Q 76 (Dr Liam Owens) 152 Q 106 (Kit Malthouse MP) 153 Written evidence from Privacy International ( nTL0051 ) 154 Written evidence from Dr Miri Zilka, Detective Sergeant Laurence Cartwright and Dr Adrian Weller (nTL0040 ) and Q 31 (Professor Michael Wooldridge) 155 Written evidence from the Bar Council ( nTL0048 ) 156 Written evidence from Archie Drake and Perry Keller ( nTL0011 ) 157 Q 107 (Kit Malthouse MP) 36 THE ADVE nT OF n EW TECH nOLOGIES I n THE JUSTICE SYSTEM • Chief Constables. (extracted from366.txt)
(nTL0012 ) 167 Written evidence from n CC Group ( nTL0005 ) 168 Written evidence from BAE Systems ( nTL0056 ) 169 Written evidence from Dr Christopher Lawless ( nTL0029 ) 37 THE ADVE nT OF n EW TECH nOLOGIES I n THE JUSTICE SYSTEM Lack of recourse 82. (extracted from366.txt)
174 Written evidence from Robin Allen QC and Dee Masters ( nTL0019 ) 175 Written evidence from Liberty ( nTL0020 ) and Big Brother Watch ( nTL0037 ) 38 THE ADVE nT OF n EW TECH nOLOGIES I n THE JUSTICE SYSTEM to ban “clearly harmful or high-risk applications of technology” which lack robust accountability arrangements.176 87. (extracted from366.txt)
176 Written evidence from Archie Drake and Perry Keller ( nTL0011 ) 177 Written evidence from the Metropolitan Police Service ( nTL0031 ) 178 Committee on Civil Liberties, Justice and Home Affairs, Report on artificial intelligence in criminal law and its use by the police and judicial authorities in criminal matters (13 July 2021): https://www.europarl. (extracted from366.txt)
europa.eu/doceo/document/A-9-2021–0232_ En.html [accessed 3 February 2022] 179 United n ations Human Rights Office of the High Commissioner, Artificial intelligence risks to privacy demand urgent action — Bachelet (15 September 2021): https://www.ohchr.org/en/2021/09/artificialintelligence-risks-privacy-demand-urgent-action-bachelet [accessed 3 February 2022] 39 THE ADVE nT OF n EW TECH nOLOGIES I n THE JUSTICE SYSTEM CHAPTER 3: TRANSPARENCY 90. (extracted from366.txt)
(nTL0022 ) 185 Written evidence from Public Law Project ( nTL0046 ), see also written evidence from Dr Joe Purshouse, Dr n essa Lynch, Dr Marcin Betkier and Professor Liz Campbell ( nTL0021 ) 186 Written evidence from Robin Allen QC and Dee Masters ( nTL0019 ), see also Q 65 (Peter Dawson) 187 Q 78 (Professor Sandra Wachter) 188 Written evidence from The Bar Council ( nTL0048 ) 189 Written evidence from Public Law Project ( nTL0046 ) 190 Q 57 (Professor Karen Yeung) 40 THE ADVE nT OF n EW TECH nOLOGIES I n THE JUSTICE SYSTEM of particular technologies but also for ensuring that the decision-making process for the use of technology is open to public scrutiny.”191 94. (extracted from366.txt)
The Home Office told us that they were “supporting law enforcement organisations to address … the need for transparency”,192 and that “policing is committed to being transparent.”193 The Ministry of Justice also informed us about an annual review of “analytical algorithms—only a small subset of [which] involve data and decisions about individuals”. (extracted from366.txt)
196 Written evidence from Robin Allen QC and Dee Masters ( nTL0019 ) 197 Written evidence from the Metropolitan Police Service ( nTL0031 ) 41 THE ADVE nT OF n EW TECH nOLOGIES I n THE JUSTICE SYSTEM the use of technological solutions as they cannot know who is using what, for how long, for what purpose, or with what safeguards. (extracted from366.txt)
This risks undermining trust in the police, the justice system, and the rule of law. (extracted from366.txt)
42 THE ADVE nT OF n EW TECH nOLOGIES I n THE JUSTICE SYSTEM to constantly refine and refresh the model to comply with appropriate ethical and legal oversight and governance.”203 While the full study is expected to be published (at a date yet to be confirmed), there are no commitments as to what information it will contain. (extracted from366.txt)
Box 10: Previous support for a register • In 2018, the House of Commons Science and Technology Committee recommended that “the Government should produce, publish, and maintain a list of where algorithms with significant impacts are being used within Central Government, along with projects underway or planned for public service algorithms”.207 • A 2019 report by the Law Society of England and Wales concluded that “a national register of algorithmic systems in the criminal justice system should be created”.208 203 Police Professional, ‘Artificial intelligence ‘marginally better’ at predicting re-offending’ (25 January 2022): https://www.policeprofessional.com/ news/artificial-intelligence-marginally-better-at-predict ing-reoffending/ [accessed 24 February 2022] 204 Q 65 (Silkie Carlo, Peter Dawson, Professor Karen Yeung) and Q 78 (David Lewis, Dr Liam Owens, Professor Sandra Wachter) 205 Written evidence from Robin Allen QC and Dee Masters ( nTL0019 ) 206 Written evidence from Professor Colin Gavaghan ( nTL0047 ), see also written evidence from Archie Drake and Perry Keller ( nTL0011 ) and Professor Lilian Edwards, Professor Derek McAuley, Dr Lachlan Urquhart and Dr Jiahong Chen ( nTL0035 ). (extracted from366.txt)
207 Science and Technology Committee, Algorithms in decision-making (Fourth Report, Session 2017– 2019, HC 351) 208 Q 11 (Professor Sylvie Delacroix) see also the Law Society of England and Wales, Algorithm use in the criminal justice system report , p 66: https://www.lawsociety.org .uk/en/topics/research/algorithm-use-inthe-criminal-justice -system-report [accessed 10 January 2022]. (extracted from366.txt)
43 THE ADVE nT OF n EW TECH nOLOGIES I n THE JUSTICE SYSTEM • A 2020 report by the Royal United Services Institute (RUSI) commissioned by the Centre for Data Ethics and Innovation (CDEI) found that “the nPCC and APCC should … maintain a high-level catalogue for all algorithms used by police forces nationwide”.209 David Lewis told us that the n PCC had recently accepted this recommendation, although caveating that “there is a matter of degree to be debated”.210 There is no indication that such a catalogue would be published. (extracted from366.txt)
One contributor thought that “it is not always feasible or even desirable to make algorithms in criminal justice fully transparent.”214 In the following paragraphs we examine those arguments. (extracted from366.txt)
For instance, the information published on a register “could be used to infer how a [Machine Learning] model would make a specific legal decision, and thus what inputs could be crafted to manipulate a desired legal 209 RUSI, Data Analytics and Algorithms in Policing in England and Wales (February 2020), p xi: https:// static.rusi.org/rusi_pub_165_2020_01_ algorithmic_policing_babuta_final_web_copy.pdf [accessed 21 January 2022] 210 Q 78 (David Lewis) 211 Centre for Data Ethics and Innovation, Review into bias in algorithmic decision making ( november 2020): https://assets.publishing.service.gov.uk/government/uploads/ system/uploads/attachment_data/ file/957259/Review_into_bias_in_ algorithmic_decision-making.pdf [accessed 21 January 2022] 212 Commission on Race and Ethnic Disparities, Independent report, Forward, introduction and full recommendations , (28 April 2021): https://www.gov.uk/ government/publications/the-report-of-thecommission-on-race-and- ethnic-disparities/foreword-introduction-and-full-recommendations#fullrecommendations [accessed 10 January 2022] 213 Written evidence from the Independent Office for Police Conduct ( nTL0054 ) 214 Written evidence from Dr Miri Zilka, Detective Sergeant Laurence Cartwright and Dr Adrian Weller (nTL0040 ) 215 Q 78 (Dr Liam Owens) 216 Written evidence from BAE Systems ( nTL0056 ) 217 Q 78 (Professor Sandra Wachter) and Q 73 (Professor Sandra Wachter) 44 THE ADVE nT OF n EW TECH nOLOGIES I n THE JUSTICE SYSTEM outcome.”218 Indeed, several witnesses were worried that a technological solution “could be ‘gamed’ by criminals” if algorithms were published.219 This is what BAE Systems calls “data poisoning”220 and the nCC Group calls “adversarial Machine Learning”.221 107. (extracted from366.txt)
When some circumscribed their recommendation to “the application of the law”228 or “the criminal justice system”229 only, others advised that it should cover “the public sector”230 or “Government”231 in general. (extracted from366.txt)
226 Written evidence from the Information Commissioner’s Office ( nTL0016 ) 227 Written evidence from Professor Colin Gavaghan ( nTL0047 ) 228 Written evidence from Big Brother Watch ( nTL0037 ) 229 Q 11 (Professor Sylvie Delacroix) 230 Q 65 (Professor Karen Yeung) 231 Written evidence from the Public Law Project ( nTL0046 ) 232 Written evidence from Professor Colin Gavaghan ( nTL0047 ) 233 Written evidence from Big Brother Watch ( nTL0037 ) 45 THE ADVE nT OF n EW TECH nOLOGIES I n THE JUSTICE SYSTEM • The Public Law Project considered that each entry in the register should be accompanied with “executable versions of listed algorithms” and an explanation of how the technology works.234 • Citing the EU’s proposed AI Regulation currently being discussed within the European Union as a reference (see Box 6), Professor Sandra Wachter suggested that the register could include algorithms themselves, the data on which they are trained, as well as information on tests carried out and on oversight mechanisms.235 • Several witnesses asked for “detailed impact assessments”236 to be included, such as Equality Impact Assessments237 or Human Rights Impact Assessments.238 The Algorithmic Transparency Standard 109. (extracted from366.txt)
240 Cabinet Office, UK government publishes pioneering standard for algorithmic transparency 46 THE ADVE nT OF n EW TECH nOLOGIES I n THE JUSTICE SYSTEM allow public bodies the time to submit entries without diverting effort away from operational activities. (extracted from366.txt)
47 THE ADVE nT OF n EW TECH nOLOGIES I n THE JUSTICE SYSTEM CHAPTER 4: HUMAN-TECHNOLOGY INTERACTIONS 114. (extracted from366.txt)
( nTL0012 ) 248 Article 22 of Regulation (EU) 2016/679 of 23 April 2016 on the protection of natural persons with regard to the processing of personal data and on the free movement of such data, and repealing Directive 95/46/EC (General Data Protection Regulation) (Article 22) 4 May 2016 ( OJ L 119/1 ), see written evidence from Professor n igel Harvey and Tobias Harvey ( nTL0025 ) see also written evidence from Professor Lilian Edwards, Professor Derek McAuley, Dr Lachlan Urquhart and Dr Jiahong Chen ( nTL0035 ) 48 THE ADVE nT OF n EW TECH nOLOGIES I n THE JUSTICE SYSTEM enforcement body may not take a qualifying significant decision based solely on automated processing unless that decision is required or authorised by law.249 These provisions aim to guarantee that there is a “human in the loop” but only for narrowly specified decisions. (extracted from366.txt)
251 Oral evidence taken on 27 October 2021 (Session 2021–22) Q 13 (The Rt Hon Priti Patel MP, Home Secretary) 252 Q 86 (Professor Paul Taylor) 253 See, for example, written evidence from the Law Society of England and Wales ( nTL0023 ), Big Brother Watch ( nTL0037 ) and the Public Law Project ( nTL0046 ) 254 Q 33 (Professor Michael Wooldridge) 255 European Commission, Guidelines on Automated individual decision making and Profiling for the purposes of regulation 2016/679 (wp251rev.01) , 22 August 2018: https://ec.europa.eu/newsroom/article29/ redirection/document/49826 [accessed 24 January 2022] 49 THE ADVE nT OF n EW TECH nOLOGIES I n THE JUSTICE SYSTEM • reviewers must ‘weigh-up’ and ‘interpret’ the recommendation, consider all available input data, and also take into account other additional factors.”256 122. (extracted from366.txt)
Evidence from the Ministry of Justice stated that “operational decisions are informed by analytical tools rather than being automatic consequences of tool outputs.”257 Similarly, the Home Office stated that they “strongly disagree … that we are allowing sensitive decisions to be delegated to machines in a way that is either contrary to the law or the core principles of the [criminal justice system]”.258 Interactions to date 123. (extracted from366.txt)
We were told, for instance, that attendees to an event about facial recognition (many of whom had access to and used facial recognition technology) “had a limited understanding of both face recognition technology and human face recognition.”264 A supplier of some tools had found that “criminal justice professionals are typically also lacking an expert, or even good, understanding.”265 Dee Masters and Robin Allen QC had, similarly, “become increasingly aware of the lack of understanding by regulators and the general public of the way in which AI systems are being used in their field of activity.”266 256 Information Commissioner’s Office, ‘Guidance on AI and data protection’: https://ico.org.uk/for organisations/guide-to-data-protection/key-dp-themes/guidance-on -artificial-intelligence-and-dataprotection/ [accessed 17 January 2022] 257 Written evidence from the Ministry of Justice ( nTL0053 ) 258 Written evidence from the Home Office ( nTL0055 ) 259 Written evidence from BAE systems ( nTL0056 ) 260 Q 8 (Professor Charles Raab) 261 Written evidence from Professor n igel Harvey and Tobias Harvey ( nTL0025 ) 262 Written evidence from n CC Group ( nTL0005 ) 263 Written evidence from Archie Drake and Perry Keller ( nTL0011 ) 264 Written evidence from Dr Eilidh n oyes and Dr Reuben Moreton ( nTL0026 ) 265 Written evidence from SAS UK&I ( nTL0041 ) 266 Written evidence from Robin Allen QC and Dee Masters ( nTL0019 ) 50 THE ADVE nT OF n EW TECH nOLOGIES I n THE JUSTICE SYSTEM 125. (extracted from366.txt)
Professor Carole McCartney, Professor of Law and Criminal Justice at the University of northumbria, explained this further: “If the humans do not understand the technology and how it is working, how will they spot if it has failed or if they have made a mistake? (extracted from366.txt)
The Prison Reform Trust were similarly concerned about confidence to challenge outcomes which might appear discriminatory: “managers and policy makers are likely to be less inclined to ‘look under the bonnet’ when the technology they find there is unfamiliar.”268 The lack of understanding does not only apply to the people who are using the tool, but to those who commission it—and those who interact with its outputs later ‘down the justice pipeline’, including judiciary reviewing the conduct and findings of an investigation. (extracted from366.txt)
Amy Stevens ( nTL0017 ) 270 Police Professional, Artificial intelligence ‘marginally better’ at predicting re-offending (25 January 2022): https://www.policeprofessional.com/ news/artificial-intelligence-marginally-better-at-predictingreoffending/ [accessed 24 February 2022] 51 THE ADVE nT OF n EW TECH nOLOGIES I n THE JUSTICE SYSTEM • An automated triage system used by the Home Office, known as the sham marriage algorithm, is used to help “determine whether a proposed marriage should be investigated as a ‘sham’”. (extracted from366.txt)
uk /24946/1/ London-Met-Police-Trial-of-Facial-Recognition-Tech- Report-2.pdf [accessed 7 February 2022] 52 THE ADVE nT OF n EW TECH nOLOGIES I n THE JUSTICE SYSTEM 128. (extracted from366.txt)
The Home Office should, in conjunction with the Ministry of Justice and the College of Policing, undertake or commission appropriate research to determine how the use of predictive algorithms affects decision making, and under what circumstances meaningful human interaction is most likely. (extracted from366.txt)
283 Q 87 (David Tucker) 284 Written evidence from the Public Law Project ( nTL0046 ) 285 Written evidence from Dr Eilidh n oyes and Dr Reuben Moreton ( nTL0026 ) 286 Q 70 (David Lewis) 287 Q 58 (Peter Dawson) 53 THE ADVE nT OF n EW TECH nOLOGIES I n THE JUSTICE SYSTEM ongoing288, and regularly reviewed.289 It could also address a skills shortage in the workforce: references were made in the evidence to the difficulty in attracting employees highly skilled in technological tools who can command extremely high salaries in the private sector.290 134. (extracted from366.txt)
David Tucker, Head of Criminal Justice at the College of Policing, told the Committee that “we have to wait for a moment of maturity, because if we do not we run the risk of trying to give guidance on something that has not settled down and is developing.”291 We are unconvinced by this argument. (extracted from366.txt)
There is also ensuring that we have systems and processes in place so that does not occur.”293 The Ministry of Justice also outlined some of its safeguarding systems, which included “clear guidance” for “when the data should and should not be used, and support (and sometimes training) … made available to staff”.294 136. (extracted from366.txt)
It would not be reasonable to expect every police officer, or every Ministry of Justice official (to take but two examples), to be trained in data analytics and in the specificities of sophisticated technological solutions. (extracted from366.txt)
(nTL0012 ) 290 Written evidence from the Serious Fraud Office ( nTL0034 ) 291 Q 91 (David Tucker) 292 RUSI, ‘Data analytics and algorithms in policing in England and Wales: Towards a new policy framework’ (2020): https://rusi.org/explore-our-research/publications/occasional-papers/data-analyti cs-and-algorithms-policing-england-and-wales-towards-new-policy-framework [accessed 9 March 2022] 293 Q 89 (Professor Paul Taylor) 294 Written evidence from the Ministry of Justice ( nTL0053 ) 54 THE ADVE nT OF n EW TECH nOLOGIES I n THE JUSTICE SYSTEM a part,295 and thus also need a thorough understanding of how algorithmic tools work. (extracted from366.txt)
As part of continuing professional development, training should also be made available to lawyers, members of the Judiciary, and other professionals involved in the justice system. (extracted from366.txt)
The Ministry of Justice referred to their sexual offender predictive tool, which is supported by an “overarching” policy framework to “support consistency of use”.298 Evidence from the Prison Reform Trust acknowledged the need for structures and policies enabling a clear route for challenge, but emphasised that these must work well in practice. (extracted from366.txt)
295 See paras 23–26 296 Independent Chief Inspector of Borders and Immigration, An inspection of entry clearance processing operations in Croydon and Istanbul 297 Written evidence from Professor Colin Gavaghan ( nTL0047 ) 298 Written evidence from the Ministry of Justice ( nTL0053 ) 55 THE ADVE nT OF n EW TECH nOLOGIES I n THE JUSTICE SYSTEM 144. (extracted from366.txt)
Professor Karen Yeung, for instance, warned against tools that produce “a nice colour” such as “a little risk assessment, red, green or yellow” because they are so “easy to interpret” that they do not encourage challenge or critical thinking.304 In a similar vein, the Ministry of Justice told us that when designing “tools which 299 Q 57 (Peter Dawson) 300 Written evidence from the Prison Reform Trust ( nTL0004 ) 301 Independent Chief Inspector of Borders and Immigration, An inspection of entry clearance processing operations in Croydon and Istanbul 302 Q 8 (Professor Delacroix) 303 Written evidence from Gary Pugh ( nTL0036 ) 304 Q 57 (Professor Karen Yeung) 56 THE ADVE nT OF n EW TECH nOLOGIES I n THE JUSTICE SYSTEM are used to support decisions about individuals”, they “strive to ensure that the tool is designed to be intuitive.”305 150. (extracted from366.txt)
If achieved, algorithmic explainability would therefore provide more compelling explanations of decisions than humans currently provide.314 Certainly, procurement managers need to understand how the tools they are commissioning or purchasing function.315 Importantly, Professor Wachter added that introducing an explainability requirement on technologies used for the application of the law could help with this: “When people do not want to tell you how [a technological solution] is working, it is either because they do not want to or because they do 305 Written evidence from the Ministry of Justice ( nTL0053 ) 306 Written evidence from the Bar Council ( nTL0048 ) 307 Written evidence from BAE Systems ( nTL0056 ) 308 See Marion Oswald, ‘Algorithm-assisted decision-making in the public sector: framing the issues using administrative law rules governing discretionary power’, University of Winchester, (6 August 2018), p 8–9: https://royalsocietypublishing.org/doi/10.1098/rsta. (extracted from366.txt)
57 THE ADVE nT OF n EW TECH nOLOGIES I n THE JUSTICE SYSTEM not know. (extracted from366.txt)
I do not think either is acceptable, especially in the criminal justice sector. (extracted from366.txt)
58 THE ADVE nT OF n EW TECH nOLOGIES I n THE JUSTICE SYSTEM CHAPTER 5: EVALUATION AND OVERSIGHT 156. (extracted from366.txt)
The Law Society of England and Wales told us that: “Bias, both conscious and unconscious, can be baked into algorithms and undermine consistently reliable results, and that using algorithms without questioning them or explaining them to the public could lead to decisions which threaten human rights and undermine trust in the justice system”.323 158. (extracted from366.txt)
ethnicity-facts-figures.service.gov.uk/crime-justice-and-the-law/policing/stop-and-search/ latest#byethnicity [accessed 7 February 2022] 327 Q 71 (Professor Sandra Wachter) 59 THE ADVE nT OF n EW TECH nOLOGIES I n THE JUSTICE SYSTEM the application of the categorisation algorithm and on which it depends. (extracted from366.txt)
60 THE ADVE nT OF n EW TECH nOLOGIES I n THE JUSTICE SYSTEM Assessments are a common approach to complying with the Public Sector Equality Duty,335 while Data Protection Impact Assessments are required for data processing operations which are likely to result in a high risk to individuals.336 163. (extracted from366.txt)
61 THE ADVE nT OF n EW TECH nOLOGIES I n THE JUSTICE SYSTEM and in demonstrating trustworthiness in ways that are more convincing than slogans and pledges, or compliance with legal requirements.344 Community consultation 166. (extracted from366.txt)
(nTL0022 ) 350 Written evidence from the Metropolitan Police Service ( nTL0031 ) 351 Written evidence from Big Brother Watch ( nTL0037 ) 62 THE ADVE nT OF n EW TECH nOLOGIES I n THE JUSTICE SYSTEM Technical considerations “The system will fail” 170. (extracted from366.txt)
63 THE ADVE nT OF n EW TECH nOLOGIES I n THE JUSTICE SYSTEM Box 12: Scientific standards In this report, a reference to ‘scientific standards’ is intended to mean a regime of quality standards and processes consistently applied to a person or body developing, maintaining or manufacturing a particular scientific product or technology, providing a scientific service, or incorporating a scientific method into their public service, combined with independent regulatory enforcement. (extracted from366.txt)
64 THE ADVE nT OF n EW TECH nOLOGIES I n THE JUSTICE SYSTEM issues”.362 For instance, we heard that “the accuracy of face recognition technology depends on … the environment in which the technology is deployed” because “in real settings, images may be of suboptimal quality or environmental conditions may be inhibitory to realising the full accuracy of face recognition technology.”363 176. (extracted from366.txt)
371 Q 40 (Professor Colin Gavaghan) 372 Q 66 (Silkie Carlo), Q 46 (Dr Rosamunde van Brakel), and Q 4 (Professor Charles Raab) 373 Written evidence from Archie Drake and Perry Keller, Kings College London ( nTL0011 ) 65 THE ADVE nT OF n EW TECH nOLOGIES I n THE JUSTICE SYSTEM confirmed. (extracted from366.txt)
66 THE ADVE nT OF n EW TECH nOLOGIES I n THE JUSTICE SYSTEM was also critical of the Government’s justification for the increased use of polygraph testing—a technological solution with controversial scientific grounds. (extracted from366.txt)
David Spreadborough, a forensic analyst, told us that “a technology introduced in the judiciary system should be validated and approved by people technically competent on the matter.”389 BAE Systems agreed that a designated body could “develop a certificate of conformance (or ‘Kitemark’/CE label) for approved AI applications”.390 nCC Group concurred that it is “essential that clear processes are established to vet technologies before they are deployed”391, whereas Dr Liam Owens of technology provider Semantics21 told us about a “review” by “an intermediary”.392 388 Written evidence from the Information Commissioner’s Office ( nTL0016 ) 389 Written evidence from David Spreadborough ( nTL0015 ) 390 Written evidence from BAE Systems ( nTL0056 ) 391 Written evidence from n CC Group ( nTL0005 ) 392 Q 78 (Dr Liam Owens) 67 THE ADVE nT OF n EW TECH nOLOGIES I n THE JUSTICE SYSTEM Privacy International agreed with them and detailed mechanisms by which a technological solution should be “approved for use”.393 187. (extracted from366.txt)
The Ministry of Justice referred us to a peer-reviewed research study that evaluated the OASys Sexual reoffending Predictor (OSP) before this technological solution was approved by the Sexual Offending Management Board of Her Majesty’s Prison and Probation Service.394 Similarly, the Public Law Project drew our attention to the proposed AI Regulation in the European Union, which foresees central “certification indicating conformity to regulatory standards.”395 188. (extracted from366.txt)
Former Deputy Chief Constable David Lewis told us that “there probably should be more centralised procurement”, alluding to the success of “regional procurement hubs” bringing police forces together.398 BAE Systems agreed 393 Written evidence from Privacy International ( nTL0051 ) 394 Written evidence from the Ministry of Justice ( nTL0053 ) 395 Written evidence from Public Law Project ( nTL0046 ) 396 College of Policing, Fundamental review of the College of Policing : https://assets.college.police.uk/s3fspublic/2022–02/Fundamental- review-of-the-College-of-Policing.pdf [accessed 24 February 2022] 397 HMICFRS, Inspectorate with College standards letter (10 February 2022): https://www.justicein spectorates.gov.uk/hmicfrs/publication-html/ inspectorate-relationship-with-college-standards-letter/ [accessed 24 February 2022] 398 Q 77 (David Lewis) 68 THE ADVE nT OF n EW TECH nOLOGIES I n THE JUSTICE SYSTEM that they “would support some form of centralised AI procurement within policing and justice.”399 192. (extracted from366.txt)
The Ministry of Justice and HM Prison & Probation Service have developed several of them, such as: • the Offender Group Reconviction Score (OGRS), a “predictor of proven reoffending within one and two years of noncustodial sentence or discharge from custody”404 • the Offender Assessment System (OASys), which “aims to assess the risk of harm offenders pose to others and how likely an offender is to reoffend”405 • the Digital Categorisation Service (DCS), an algorithm used to support decisions on security categorisations in prisons. (extracted from366.txt)
405 Written evidence from Big Brother Watch ( nTL0037 ) 406 Written evidence from the Prison Reform Trust ( nTL0004 ) 69 THE ADVE nT OF n EW TECH nOLOGIES I n THE JUSTICE SYSTEM Police and the University College London.407 The n ational Data Analytics Solution, a nationwide project sponsored by the Home Office and led by West Midlands Police in partnership with the national Crime Agency and Accenture, also falls in this category.408 We were told that, in n ew Zealand, such partnerships were “by far the most common practice” when the government procures technological solutions.409 196. (extracted from366.txt)
415 Written evidence from the n CC Group ( nTL0005 ) 416 Written evidence from Dr Eilidh n oyes and Dr Reuben Moreton ( nTL0026 ) 417 Q 24 (Professor Charles Raab) 70 THE ADVE nT OF n EW TECH nOLOGIES I n THE JUSTICE SYSTEM has been overtaken by marketing” because of salespeople who “will take something they do not understand and shout a number that they do not understand” to make accuracy claims.418 200. (extracted from366.txt)
pdf [accessed 26 January 2022] 424 World Economic Forum, AI Procurement Guidelines (11 June 2020): https://www.weforum.org/reports/ ai-procurement-in-a-box/ai-government-procurement-guidelines [accessed 26 January 2022] 425 Q 73 (David Lewis) 71 THE ADVE nT OF n EW TECH nOLOGIES I n THE JUSTICE SYSTEM Chiefs’ Council, and the Police Digital Service confirmed that they “inform contract implementation and management”.426 203. (extracted from366.txt)
BAE Systems argued that the guidelines should be “more technical”, more “supplier-focused”, and more specific to “policing and the justice context”.427 Professor Wachter told us that these “vague” guidelines were “not good enough” because they were too soft to induce change in procurement practices.428 Dr Liam Owens agreed that the guidelines are “very broad” and “non-specific” and would need to be better tailored to address the needs of technology providers in the context of the application of the law.429 Furthermore, this document refers to ‘guidance’ as well as ‘guidelines’. (extracted from366.txt)
432 World Economic Forum, AI Procurement Guidelines 72 THE ADVE nT OF n EW TECH nOLOGIES I n THE JUSTICE SYSTEM used in the application of the law. (extracted from366.txt)
73 THE ADVE nT OF n EW TECH nOLOGIES I n THE JUSTICE SYSTEM The West Midlands Ethics Committee model 211. (extracted from366.txt)
These included: • A recruitment based on merit, 443; with independent membership444 with a range of expertise; 445 • A commitment to publish meetings papers, minutes and conclusions;446 • The Committee’s independence from the police force whose use of technology it is scrutinising; and447 • The Committee’s remit to consider technological solutions throughout their lifecycle.448 441 See, for instance, Q 2 (Professor Carole McCartney), Q 110 (Kit Malthouse MP), also written evidence from Archie Drake and Perry Keller ( nTL0011 ), and defenddigitalme ( nTL0044 ) 442 Written evidence from Association of Police and Crime Commissioners, national Police Chiefs’ Council and Police Digital Service ( nTL0049 ), see also Q 11 (Professor McCartney), Q 48 (Dr Rosamunde van Brakel), and written evidence from BAE Systems ( nTL0056 ) 443 Written evidence from Association of Police and Crime Commissioners, national Police Chiefs’ Council and Police Digital Service ( nTL0049 ) 444 Written evidence from Association of Police and Crime Commissioners, national Police Chiefs’ Council and Police Digital Service ( nTL0049 ) and BAE Systems ( nTL0056 ) 445 Q 11 (Professor McCartney), Q 48 (Dr Rosamunde van Brakel) and written evidence from BAE Systems ( nTL0056 ) 446 Q 11 (Professor McCartney), written evidence from Association of Police and Crime Commissioners, national Police Chiefs’ Council and Police Digital Service ( nTL0049 ), and BAE Systems ( nTL0056 ) 447 Written evidence from Association of Police and Crime Commissioners, n ational Police Chiefs’ Council and Police Digital Service ( nTL0049 ), Archie Drake and Perry Keller ( nTL0011 ) and BAE Systems ( nTL0056 ) 448 Written evidence from Association of Police and Crime Commissioners, national Police Chiefs’ Council and Police Digital Service ( nTL0049 ) and BAE Systems ( nTL0056 ) 74 THE ADVE nT OF n EW TECH nOLOGIES I n THE JUSTICE SYSTEM Box 14: The West Midlands Police Ethics Committee • The West Midlands Police and Crime Commissioner (PCC) and West Midlands Police (WMP) jointly established a specialist Ethics Committee in early 2019. (extracted from366.txt)
pdf ?x41638 [accessed 1 February 2022] 450 HL Deb, 3 n ovember 2021, cols 1301–1305 451 Written evidence from the Home Office ( nTL0055 ) 75 THE ADVE nT OF n EW TECH nOLOGIES I n THE JUSTICE SYSTEM which everything in society operates”. (extracted from366.txt)
76 THE ADVE nT OF n EW TECH nOLOGIES I n THE JUSTICE SYSTEM SUMMARY OF CONCLUSIONS AND RECOMMENDATIONS Legal and institutional frameworks 1. (extracted from366.txt)
(Paragraph 66) 77 THE ADVE nT OF n EW TECH nOLOGIES I n THE JUSTICE SYSTEM 11. (extracted from366.txt)
The Government should require that national guidance for the use of advanced technological tools in policing and criminal justice is drawn up and, as part of their response to this report, should outline concrete plans for this. (extracted from366.txt)
This risks undermining trust in the police, the justice system, and the rule of law. (extracted from366.txt)
(Paragraph 113) 78 THE ADVE nT OF n EW TECH nOLOGIES I n THE JUSTICE SYSTEM Human-technology interactions 21. (extracted from366.txt)
The Home Office should, in conjunction with the Ministry of Justice and the College of Policing, undertake or commission appropriate research to determine how the use of predictive algorithms affects decision making, and under what circumstances meaningful human interaction is most likely. (extracted from366.txt)
As part of continuing professional development, training should also be made available to lawyers, members of the Judiciary, and other professionals involved in the justice system. (extracted from366.txt)
(Paragraph 183) 79 THE ADVE nT OF n EW TECH nOLOGIES I n THE JUSTICE SYSTEM 30. (extracted from366.txt)
With the assurance brought by the certification process and the register of algorithms, police forces and other 80 THE ADVE nT OF n EW TECH nOLOGIES I n THE JUSTICE SYSTEM public bodies would remain free to procure the technological solutions of their choice, as long as the products have been certified. (extracted from366.txt)
(Paragraph 219) 81 THE ADVE nT OF n EW TECH nOLOGIES I n THE JUSTICE SYSTEM APPENDI x 1: LIST OF MEMBERS AND DECLARATIONS OF INTEREST Members Lord Blunkett Baroness Chakrabarti Lord Dholakia Baroness Hallett Baroness Hamwee (Chair) Lord Hunt of Wirral Baroness Kennedy of The Shaws Baroness Pidding Baroness Primarolo Lord Ricketts Baroness Sanderson of Welton Baroness of Shackleton of Belgravia Declarations of Interest Lord Blunkett Non- Financial: Non-executive Chairman, Cyber Essentials Direct Limited Directorship: Director and Chairman of the Board, University of Law Limited (subsidiary and affiliated institution of Global University Systems and Interactive Pro Limited) Baroness Chakrabarti No relevant interests to declare Lord Dholakia Trustee of the Police Foundation which produced a report on the Strategic Review of Policing in England and Wales on 8 March 2022 Baroness Hallett Retired judge Baroness Hamwee No relevant interests to declare Lord Hunt of Wirral Partner, DAC Beachcroft LLP (International commercial law firm) Honorary Bencher, Inner Temple Baroness Kennedy of The Shaws Member of Microsoft Technology and Human Rights Advisory Council Baroness Pidding No relevant interests to declare Baroness Primarolo Non-Executive Director on the Board of Thompson’s Solicitors LLP. (extracted from366.txt)
Lord Ricketts No relevant interests to declare Baroness Sanderson of Welton No relevant interests to declare Baroness Shackleton of Belgravia No relevant interests to declare other than those on the Register 82 THE ADVE nT OF n EW TECH nOLOGIES I n THE JUSTICE SYSTEM Specialist Adviser Dr Marion Oswald Associate Professor, Northumbria Law School Advisory Board member, Centre for Data Ethics and Innovation Senior Research Associate, The Alan Turing Institute Associate Fellow, Royal United Services Institute for Defence and Security Studies; Independent Chair, West Midlands Police & Crime Commissioner and West Midlands Police Data Ethics Committee; Member, New Zealand Police Expert Panel on Emergent Technologies; Advisory board member of the UKRI Trustworthy Autonomous Systems Hub; Member of the Royal Society Working Group on Privacy Enhancing Technologies (2018–19 and reconstituted 2021); Member of National Statistician’s Data Ethics Advisory Committee since its foundation (2016-date); Executive member, British & Irish Law, Education & Technology Association; Member, Arts and Humanities Research Council Peer Review College. (extracted from366.txt)
83 THE ADVE nT OF n EW TECH nOLOGIES I n THE JUSTICE SYSTEM APPENDI x 2: LIST OF WITNESSES Evidence is published online at https://committees .parliament.uk/committee/519/ justice-and-home-affairs-committee/publications / and available for inspection at the Parliamentary Archives (020 7219 3074). (extracted from366.txt)
Oral evidence in chronological order * Professor Sylvie Delacroix, Professor in Law and Ethics at University of BirminghamQQ 1–24 * Professor Carole McCartney, Professor of Law and Criminal Justice at n orthumbria UniversityQQ 1–24 ** Professor Charles Raab, Professorial Fellow, Politics and International Relations, School of Social and Political Science at The University of Edinburgh)QQ 1–24 * Dr David Leslie, Ethics Theme Lead at Alan Turing InstituteQQ 25–38 * Professor Michael Wooldridge, Head of Department of Computer Science, Professor of Computer Science at University of OxfordQQ 25–38 ** Professor Colin Gavaghan, Director n ew Zealand Law Foundation Centre for Law and Policy in Emerging Technologies at University of OtagoQQ 39–51 * Professor Elizabeth E Joh, Martin Luther King Jr. (extracted from366.txt)
Professor of Law at University of California, DavisQQ 39–51 * Dr Rosamunde Elise van Brakel, Co-Director, Surveillance Studies n etwork, Associate Professor Tilburg University/Vrije Universiteit Brussel (VUB)QQ 39–51 ** Silkie Carlo, Director, Big Brother Watch QQ 52–67 ** Peter Dawson, Director, Prison Reform Trust QQ 52–67 * Professor Karen Yeung, Interdisciplinary Professorial Fellow in Law, Ethics and Informatics, Birmingham Law School at The University of BirminghamQQ 52–67 * David Lewis, Former Deputy Chief Constable and former ethics lead n PCC at Dorset PoliceQQ 68–82 * Dr Liam Owens, Founder and Chief Executive Officer, Semantics 21QQ 68–82 ** Professor Sandra Wachter, Associate Professor and Senior Research Fellow at University of OxfordQQ 68–82 ** Professor Paul Taylor, Chief Scientific Adviser, national Police Chiefs’ CouncilQQ 83–98 84 THE ADVE nT OF n EW TECH nOLOGIES I n THE JUSTICE SYSTEM ** Alun Michael, Police and Crime Commissioner for South Wales and Joint Lead for Data and Bioethics, Association of Police and Crime CommissionersQQ 83–98 ** Darryl Preston, Police and Crime Commissioner for Cambridgeshire and Peterborough and Joint Lead for Data and Bioethics, Association of Police and Crime CommissionersQQ 83–98 * David Tucker, Faculty Lead on Crime and Criminal Justice, College of Policing.QQ 83–98 * The Rt Hon Kit Malthouse MP, Minister of State for Crime and Policing at the Home Office and Ministry of JusticeQQ 99–111 * Dr Christophe Prince, Director for Data and Identity, Home OfficeQQ 99–111 Alphabetical list of all witnesses Robin Allen QC, Barristers at A1 Law Consultancy/ Cloisters ChambersnTL0019 Dr Arianna Andreangeli, Senior Lecturer in Competition Law, Edinburgh Law School, University of EdinburghnTL0038 nTL0039 Dr Philip Avenell, Managing Director and Forensic Biologist at Forensic AccessnTL0024 Avon and Somerset Police nTL0052 BAE Systems nTL0056 Professor Melanie Bailey, Professor at University of SurreynTL0024 The Bar Council nTL0048 Dr Marcin Betkier, Lecturer in Law at Victoria University of WellingtonnTL0021 Dr Stephen Bleay, Senior Lecturer in Forensic Science at London South Bank UniversitynTL0024 Katy Bourne OBE, Sussex Police and Crime CommissionernTL0045 Dr Rebecca Brown, Research Fellow at University of OxfordnTL0030 Professor Dame Vicki Bruce DBE, Professor Emerita at newcastle UniversitynTL0012 Professor A Mike Burton, Professor of Psychology at University of YorknTL0012 Professor Liz Campbell, Professor and Francine V Mcniff Chair in Criminal Jurisprudence at Monash UniversitynTL0021 85 THE ADVE nT OF n EW TECH nOLOGIES I n THE JUSTICE SYSTEM ** Silkie Carlo, Director, Big Brother Watch (QQ 52–67 )nTL0037 Detective Sergeant Laurence Cartwright, Data Analytics lead at Sussex PolicenTL0040 Dr Jiahong Chen, Lecturer in Law at Sheffield Law School, University of SheffieldnTL0035 The Crown Prosecution Service nTL0018 Dr Benjamin Davies, Wellcome Trust Society & Ethics Research Fellow at University of OxfordnTL0030 ** Peter Dawson, Director, The Prison Reform Trust (QQ 52–67 )nTL0004 defenddigitalme nTL0044 Dr Delphine Defossez (Lecturer in Law), northumbria UniversitynTL0022 * Professor Sylvie Delacroix, Professor in Law and Ethics at University of Birmingham ( QQ 1–24 ) Professor Thomas Douglas, Professor of Applied Philosophy University of OxfordnTL0013 nTL0030 Archie Drake, Research Associate at Kings College, LondonnTL0011 Professor Gary Edmond, Professor of Law at U nSW SydneynTL0012 Professor Lilian Edwards, Professor of Law, Innovation and Society at n ewcastle Law School, newcastle UniversitynTL0035 Professor Seena Fazel, Professor of Forensic Psychiatry & Wellcome Trust Senior Research Fellow in Clinical Science at University of OxfordnTL0030 Dr Lisa Forsberg, British Academy Postdoctoral Fellow at University of OxfordnTL0013 nTL0030 Professor Simona Francese, Professor of Forensic and Bioanalytical Mass Spectrometry at Sheffield Hallam UniversitynTL0024 Professor Pete Fussey, Professor of Sociology, University of EssexnTL0017 Professor Angela Gallop, Professor of Practice/ Director of Forensic Science at University of Strathclyde/Forensic AccessnTL0024 ** Professor Colin Gavaghan, Director n ew Zealand Law Foundation Centre for Law and Policy in Emerging Technologies at University of Otago (QQ 39–51 )nTL0047 86 THE ADVE nT OF n EW TECH nOLOGIES I n THE JUSTICE SYSTEM Dr Jamie Grace, Senior Lecturer in Law at Sheffield Hallam UniversitynTL0001 Professor n igel Harvey, Professor of Judgment and Decision Research at UCL LondonnTL0025 Tobias Harvey, Student of Law at Kings College LondonnTL0025 Dr Binesh Hass, Research Fellow at University of OxfordnTL0030 The Home Office nTL0055 Independent Office for Police Conduct nTL0054 The Information Commissioner’s Office (ICO) nTL0016 Istanbul Bar Association nTL0028 * Professor Elizabeth E Joh, Martin Luther King Jr. (extracted from366.txt)
Professor of Law at University of California, Davis (QQ 39–51 ) Perry Keller, Reader in Media and Information Law, Director of Doctoral Studies at King’s College LondonnTL0011 Professor Paul Kelly, Professor of Inorganic Chemistry at University of LoughboroughnTL0024 Professor Richard I.Kemp, Professor of Psychology at UnSW SydneynTL0012 Dr Kyriakos n Kotsoglou, Senior Lecturer in Law, northumbria University/Research Fellow, University of LausannenTL0006 nTL0007 The Law Society of England and Wales nTL0023 Dr Christopher Lawless, Associate Professor at Durham UniversitynTL0029 * Dr David Leslie, Ethics Theme Lead at Alan Turing Institute ( QQ 25–38 ) * David Lewis, Former Deputy Chief Constable and former ethics lead n PCC at Dorset Police ( QQ 68– 82) Liberty nTL0020 Sjors Ligthart LLM PhD candidate at Tilburg UniversitynTL0013 Dr n essa Lynch, Associate Professor of Law at Victoria University of WellingtonnTL0021 * The Rt Hon Kit Malthouse MP, Minister of State for Crime and Policing at the Home Office and Ministry of Justice ( QQ 99–111 ) Stephen Mason, Associate Research Fellow, Institute of Advanced Legal StudiesnTL0002 87 THE ADVE nT OF n EW TECH nOLOGIES I n THE JUSTICE SYSTEM Dee Masters, Barristers at A1 Law Consultancy/ Cloisters ChambersnTL0019 Professor Derek McAuley, Director of Horizon Digital Economy Research Institute at University of nottinghamnTL0035 ** Professor Carole McCartney, Professor of Law and Criminal Justice at n orthumbria University (QQ 1–24 )nTL0022 medConfidential nTL0050 The Metropolitan Police Service nTL0031 Professor Gerben Meynen, Professor of Forensic Psychiatry and Bioethics at Utrecht University and VU University AmsterdamnTL0013 ** Alun Michael, Police and Crime Commissioner for South Wales and Joint Lead for Data and Bioethics, Association of Police and Crime Commissioners (QQ 83–98 )nTL0049 nTL0057 Migrants’ Rights n etwork nTL0042 The Ministry of Justice nTL0053 Abhishek Mishra, Doctoral Student at University of OxfordnTL0030 Dr Brent Mittelstadt of the Oxford Internet Institute (OII)nTL0058 Dr Reuben Moreton, Reli Ltd nTL0026 Professor Ruth Morgan, Professor of Crime and Forensic Sciences at University College LondonnTL0024 Dr Daragh Murray, Senior Lecturer in Human Rights at University of EssexnTL0017 nCC Group nTL0005 Dr Eilidh n oyes, University of Huddersfield nTL0026 * Dr Liam Owens, Founder and Chief Executive Officer, Semantics 21 ( QQ 68–82 ) Ms Angela Paul (PhD candidate in Law), northumbria UniversitynTL0022 Police Scotland nTL0043 Dr Susan Pope, D nA expert—chair of the Forensic Science Regulator D nA Specialist Group & is an assessor for the n etherlands Register of Court Experts at Principal Forensic ServicesnTL0024 ** Darryl Preston, Police and Crime Commissioner for Cambridgeshire and Peterborough and Joint Lead for Data and Bioethics, Association of Police and Crime Commissioners ( QQ 83–98 )nTL0049 nTL0057 88 THE ADVE nT OF n EW TECH nOLOGIES I n THE JUSTICE SYSTEM Dr Christophe Prince, Director for Data and Identity, Home Office ( QQ 99–111 ) Privacy International nTL0051 Public Law Project nTL0046 nTL0059 Gary Pugh, Forensic Science Regulator nTL0036 Dr Jonathan Pugh, Parfit Radcliffe Senior Research Fellow at University of OxfordnTL0030 Dr Joe Purshouse, Senior Lecturer in Criminal Law and Justice at University of SheffieldnTL0021 ** Professor Charles Raab, Professorial Fellow, School of Social and Political Science, University of Edinburgh Fellow, The Alan Turing Institute (QQ 1–24 )nTL0014 Dr Liam Ralph (Lecturer in Criminology and Policing) at the Centre for Crime and Policing & the Science and Justice Research, n orthumbria UniversitynTL0022 Dr Kay L. (extracted from366.txt)
Mehera San Roque, U nSW SydneynTL0012 SAS UK&I nTL0041 Professor Julian Savulescu, Professor of Practical Ethics at University of OxfordnTL0030 Serious Fraud Office nTL0034 Professor Ilina Singh, Professor of n euroscience and Society at University of OxfordnTL0030 David Spreadborough, CFVA, Forensic Analyst at Amped SoftwarenTL0015 Dr Amy Stevens, Senior Research Officer, Human Rights, Big Data and Technology Project at University of EssexnTL0017 Dr Clare Sutherland, Senior Lecturer at University of AberdeennTL0012 ** Professor Paul Taylor, Chief Scientific Adviser, national Police Chiefs’ Council ( QQ 83–98 )nTL0049 nTL0057 Dr Alice Towler, Research Fellow at U nSW Sydney nTL0012 ** David Tucker, Faculty Lead on Crime and Criminal Justice, College of Policing ( QQ 83–98 )nTL0057 89 THE ADVE nT OF n EW TECH nOLOGIES I n THE JUSTICE SYSTEM Professor Gillian Tully, Professor of Practice for Forensic Science Policy and Regulation at King’s College LondonnTL0024 UCL Centre for the Forensic Sciences nTL0010 Dr Lachlan Urquhart, Lecturer in Technology Law, and Co-Investigator of the UKRI Trustworthy Autonomous Systems n ode in Governance and Regulation at School of Law University of EdinburghnTL0035 * Dr Rosamunde Elise van Brakel, Co-Director, Surveillance Studies n etwork, Associate Professor Tilburg University/Vrije Universiteit Brussel (VUB) (QQ 39–51 ) ** Professor Sandra Wachter, Associate Professor and Senior Research Fellow at Oxford Internet Institute (OII), University of Oxford ( QQ 68–82 )nTL0058 Dr Adrian Weller, Principal Research Fellow in Machine Learning at the University of CambridgenTL0040 Dr David White, Senior Lecturer at U nSW Sydney nTL0012 Dr Matthias Wienroth (Vice-Chancellor’s Senior Fellow in Criminology/Sociology), n orthumbria UniversitynTL0022 Professor Kim Wolff, Director King’s Forensics at King’s College LondonnTL0024 * Professor Michael Woodridge, Head of Department of Computer Science, Professor of Computer Science at University of Oxford ( QQ 25–38 ) * Professor Karen Yeung, Interdisciplinary Professorial Fellow in Law, Ethics and Informatics, Birmingham Law School at The University of Birmingham (QQ 52–67 ) Dr Miri Zilka, Research Associate in Machine Learning at the University of CambridgenTL0040 90 THE ADVE nT OF n EW TECH nOLOGIES I n THE JUSTICE SYSTEM APPENDI x 3: CALL FOR EVIDENCE Scope of the inquiry The Committee seeks to explore the use of new technologies in the application of the law and the experience of people currently or previously engaged with them. (extracted from366.txt)
Are safeguards needed to ensure that 91 THE ADVE nT OF n EW TECH nOLOGIES I n THE JUSTICE SYSTEM technologies cannot be used to serve purposes incompatible with a democratic society? (extracted from366.txt)
92 THE ADVE nT OF n EW TECH nOLOGIES I n THE JUSTICE SYSTEM APPENDI x 4: ABBREVIATIONS, ACRONYMS AND TECHNICAL TERMS ADM Automated Decision Making AFR Automated Facial Recognition AI Artificial Intelligence Algorithm A series of instructions for performing a calculation or solving a problem, especially with a computer. (extracted from366.txt)
93 THE ADVE nT OF n EW TECH nOLOGIES I n THE JUSTICE SYSTEM ML Machine Learning nDAS national Data Analytics Solution (a national analytics capability being developed by West Midlands Police in conjunction with the Home Office). (extracted from366.txt)
Projet AJC | ACT Project | Autonomisation des acteurs Judiciaires par la Cyberjustice et l’intelligence artificielle | Autonomy Through Cyberjustice Technologies and AI Laboratoire de CYBERJUSTICE Laboratory Espace exclusif – membres AJC en AJC Présentation Entrevues des chefs de groupe Entrevues des chercheurs Infrastructure de recherche Gouvernance Comité scientifique Actualités Nous joindre Organisation Groupes de travail Chantier 1 – Subproject 1 Chantier 2 – Subproject 2 Chantier 3 – Subproject 3 Chantier 4 – Subproject 4 Chantier 5 – Subproject 5 Chantier 6 – Subproject 6 Chantier 7 – Subproject 7 Chantier 8 – Subproject 8 Chantier 9 – Subproject 9 Chantier 10 – Subproject 10 Chantier 11 – Subproject 11 Chantier 12 – Subproject 12 Chantier 13 – Subproject 13 Chantier 14 – Subproject 14 Chantier 15 – Subproject 15 Chantier 16 – Subproject 16 Chercheurs Partenaires Partenaires institutionnel Partenaires issus du milieu de l’action sociale et communautaire Partenaires issus du milieu universitaire Partenaires issus du monde professionnel Partenaires issus de l’industrie Publications Bibliothèque numérique sur la cyberjustice – Cyberjustice Digital Library Rapport mi-parcours | Annexe bibliographique Articles scientifiques Guides de meilleures pratiques Livres ou chapitres de livres Documents de travail Inventaires Blogues Articles de presse Présentations Conférences et activités Études de cas & Évaluations Gouvernance juridique Conférences Conférence 2025 du partenariat AJC AI, Large Language Models and Justice L’accessibilité : la nouvelle frontière de la justice en ligne Fintech: Réguler aujourd’hui les technologies financières de demain Vidéos Conférences enregistrées Rencontre annuelle | Juin 2020 Webinaires des partenaires Webconférences cyberjustice AJC Présentation Entrevues des chefs de groupe Entrevues des chercheurs Infrastructure de recherche Gouvernance Comité scientifique Actualités Nous joindre Organisation Groupes de travail Chantier 1 – Subproject 1 Chantier 2 – Subproject 2 Chantier 3 – Subproject 3 Chantier 4 – Subproject 4 Chantier 5 – Subproject 5 Chantier 6 – Subproject 6 Chantier 7 – Subproject 7 Chantier 8 – Subproject 8 Chantier 9 – Subproject 9 Chantier 10 – Subproject 10 Chantier 11 – Subproject 11 Chantier 12 – Subproject 12 Chantier 13 – Subproject 13 Chantier 14 – Subproject 14 Chantier 15 – Subproject 15 Chantier 16 – Subproject 16 Chercheurs Partenaires Partenaires institutionnel Partenaires issus du milieu de l’action sociale et communautaire Partenaires issus du milieu universitaire Partenaires issus du monde professionnel Partenaires issus de l’industrie Publications Bibliothèque numérique sur la cyberjustice – Cyberjustice Digital Library Rapport mi-parcours | Annexe bibliographique Articles scientifiques Guides de meilleures pratiques Livres ou chapitres de livres Documents de travail Inventaires Blogues Articles de presse Présentations Conférences et activités Études de cas & Évaluations Gouvernance juridique Conférences Conférence 2025 du partenariat AJC AI, Large Language Models and Justice L’accessibilité : la nouvelle frontière de la justice en ligne Fintech: Réguler aujourd’hui les technologies financières de demain Vidéos Conférences enregistrées Rencontre annuelle | Juin 2020 Webinaires des partenaires Webconférences cyberjustice Conférence 2025 du partenariat AJC À la une À la une Actualités Quoi savoir avant d’aller à la Conférence AJC 202516 septembre 2025 Merci de vous être inscrit à la conférence annuelle du partenariat AJC (Autonomisation des acteurs judiciaires par la cyberjustice) 2025 ! (extracted from170.txt)
À tout moment, si vous avez besoin d’aide, le personnel de la conférence sera […] Lire la suite À la une Actualités Événements Conférence le mercredi 5 mars : « Considerations on AI Governance and concrete applications in the field of Law and Justice »3 mars 2025 Le mercredi 5 mars 2025 à 13:00, heure de Montréal (19:00, heure de Madrid) se tiendra en hybride la conférence « Consideraciones sobre la gobernanza de la IA y aplicaciones concretas en el ámbito del Derecho y la Justicia » (Considerations on AI Governance and concrete applications in the field of Law and Justice), organisée par la […] Lire la suite À la une Actualités IA générative & Résolution des litiges | Publication de Karim Benyekhlef, Valentin Callipel et Aurore Clément10 décembre 2024 Le Pr Karim Benyekhlef, Valentin Callipel et Aurore Clément viennent de publier « IA générative & Résolution des litiges ». (extracted from170.txt)
publication intégrale ﻿IERDJ_EDC-volume-2-301024Télécharger Lire la suite À la une Actualités Offre d’emploi | 2 auxiliaires de recherche au Laboratoire de cyberjustice14 novembre 2024 Le Laboratoire de cyberjustice est un espace unique de réflexion et de création où les processus de justice sont modélisés et réimaginés afin d’améliorer l’accès à la justice. (extracted from170.txt)
Nos travaux se distinguent par l’expérimentation d’innovations technologiques, comme l’intelligence artificielle, pour optimiser le fonctionnement actuel de la justice, de manière à en accroître l’efficacité, à en […] Lire la suite Voir les précédents Nouvelles À la une Actualités Quoi savoir avant d’aller à la Conférence AJC 202516 septembre 2025 Merci de vous être inscrit à la conférence annuelle du partenariat AJC (Autonomisation des acteurs judiciaires par la cyberjustice) 2025 ! (extracted from170.txt)
À tout moment, si vous avez besoin d’aide, le personnel de la conférence sera […] Lire la suite À la une Actualités Événements Conférence le mercredi 5 mars : « Considerations on AI Governance and concrete applications in the field of Law and Justice »3 mars 2025 Le mercredi 5 mars 2025 à 13:00, heure de Montréal (19:00, heure de Madrid) se tiendra en hybride la conférence « Consideraciones sobre la gobernanza de la IA y aplicaciones concretas en el ámbito del Derecho y la Justicia » (Considerations on AI Governance and concrete applications in the field of Law and Justice), organisée par la […] Lire la suite À la une Actualités IA générative & Résolution des litiges | Publication de Karim Benyekhlef, Valentin Callipel et Aurore Clément10 décembre 2024 Le Pr Karim Benyekhlef, Valentin Callipel et Aurore Clément viennent de publier « IA générative & Résolution des litiges ». (extracted from170.txt)
Cet article explore la controverse entourant la demande de traduction des plus […] Lire la suite À la une Actualités Événements Découvrez la conférence ‘Montreal 2024, Generative AI and Justice’ à travers nos vidéos7 août 2024 Le Laboratoire de cyberjustice a le plaisir de vous inviter à la conférence Montreal 2024: Generative AI and Justice, qui se tiendra le 1er octobre 2024 au laboratoire. (extracted from170.txt)
Informations pratiques Lieu de la conférence : Laboratoire de cyberjustice (B-2215), Pavillon Jean-Brillant, 3200 rue Jean-Brillant, Montréal, Québec H3T 1N8 Date : 2024/10/01 de 9:00 à 17:30 Inscription […] Lire la suite À la une Événements Conférence à venir : La justice à l’épreuve de l’IA. (extracted from170.txt)
(11 septembre 2024)24 juillet 2024 Le Laboratoire de cyberjustice a le plaisir de vous inviter à la conférence La justice à l’épreuve de l’IA. (extracted from170.txt)
Informations pratiques Quand : 11 septembre 2024 de 16:30 à 19:30 Où: Salon François-Chevrette, A-3464, Pavilion Maximilien-Caron, 3101 chemin de la Tour, Montréal, Québec, H3T 1N8 […] Lire la suite À la une Actualités Conférence à venir : « Montreal 2024, Generative AI and Justice » (1 octobre 2024)18 juin 2024 Le Laboratoire de cyberjustice a le plaisir de vous inviter à la conférence Montreal 2024, Generative AI and Justice, qui se tiendra le 1er octobre 2024 au Laboratoire. (extracted from170.txt)
Cette conférence est une occasion unique pour les acteurs du droit et de la justice de mieux comprendre et de se positionner sur les usages émergents […] Lire la suite À la une Actualités Dans les médias Prof. (extracted from170.txt)
Hannes Westermann, premier intervenant des webinaires « AI & Access to Justice » du Stanford Legal Design Lab18 juin 2024 Le Laboratoire de cyberjustice a le plaisir de vous annoncer que Prof. (extracted from170.txt)
Hannes Westermann a été le premier intervenant des webinaires “AI & Access to Justice” du Stanford Legal Design Lab. (extracted from170.txt)
Dans ce premier webinaire intitulé “Generative AI for Access to Justice: Challenges and Opportunities”, Prof. (extracted from170.txt)
Jinzhe Tan (Laboratoire de cyberjustice) dans le cadre du cycle de conférences de 2024 « AI, Large Language Models, and Justice Considerations for Legal Practitioners Judges, Law Schools and Public Legal Educators » Biographie […] Lire la suite À la une Actualités Web-conférences cyberjustice Vidéo – Conférence « L’IA générative dans le domaine juridique : une approche nuancée » (DL4T) de Prof. (extracted from170.txt)
Amy Salyzyn (UOttawa) & Dre Katie Szilagyi (University of Manitoba) dans le cadre du cycle de conférences de 2024 « AI, Large Language Models, and Justice Considerations for Legal Practitioners Judges, Law Schools and Public Legal Educators » […] Lire la suite À la une Actualités Événements Nouvelles Conférence à venir : « AI and Judging » de Pre Tania Sourdin (7 mai 2024)6 mai 2024 Le Laboratoire de cyberjustice a le plaisir de vous inviter à la conférence en ligne « AI and Judging« de Pre Tania Sourdin. (extracted from170.txt)
Cliquer ici pour vous inscrire Informations pratiques Quand : 7 mai 2024, 16h30 F Format : Sur Zoom Biographie Pre Tania Sourdin is President of the Academic Senate at the University […] Lire la suite À la une Actualités Rapport de recherche – « Le numérique et l’intelligence artificielle comme outil de justice sociale : les initiatives par et pour les groupes et communautés marginalisés »1 mai 2024 Le Laboratoire de cyberjustice a le plaisir de vous présenter le rapport de recherche intitulée Le numérique et l’intelligence artificielle comme outil de justice sociale : les initiatives par et pour les groupes et communautés marginalisés par Pre Karine Gentelet (UQO), membre du Laboratoire de cyberjustice et du projet AJC. (extracted from170.txt)
Ce rapport met en lumière les initiatives […] Lire la suite À la une Actualités Offre d’emploi | Étudiant.e en informatique aux cycles supérieurs1 mai 2024 Le Laboratoire de cyberjustice est un espace unique de réflexion et de création où les processus de justice sont modélisés et réimaginés afin d’améliorer l’accès à la justice. (extracted from170.txt)
Nos travaux se distinguent par l’expérimentation d’innovations technologiques, comme l’intelligence artificielle, pour optimiser le fonctionnement actuel de la justice, de manière à en accroître l’efficacité, à en […] Lire la suite À la une Actualités Nouvelles Offre d’emploi | 2 auxiliaires de recherche au Laboratoire de cyberjustice22 avril 2024 Le Laboratoire de cyberjustice est un espace unique de réflexion et de création où les processus de justice sont modélisés et réimaginés afin d’améliorer l’accès à la justice. (extracted from170.txt)
Nos travaux se distinguent par l’expérimentation d’innovations technologiques, comme l’intelligence artificielle, pour optimiser le fonctionnement actuel de la justice, de manière à en accroître l’efficacité, à en […] Lire la suite À la une Actualités Événements Nouvelles Conférence à venir: Colloque sur l’intelligence artificielle et le droit pénal « Des ordres et (désordres) de l’intelligence artificielle : Propagande – Pornographie – Profilage » (25 avril 2024)22 avril 2024 La Chaire Lexum et le Laboratoire de cyberjustice ont le plaisir de vous inviter au colloque sur l’intelligence artificielle et le droit pénal « Des ordres et (désordres) de l’intelligence artificielle : Propagande – Pornographie – Profilage. (extracted from170.txt)
Marina Pavlović dans le cadre du cycle de conférences de 2024 « AI, Large Language Models, and Justice Considerations for Legal Practitioners Judges, Law Schools and Public Legal Educators » Biographie des conférenciers Prof. (extracted from170.txt)
La Bourse doctorale de la Chaire LexUM en information juridique et du Laboratoire de cyberjustice est conçue pour encourager deux […] Lire la suite À la une Actualités Appel à candidatures – Projet « Intelligence artificielle pour la découvrabilité »2 avril 2024 POSTE : Étudiant en droit des nouvelles technologies Le Laboratoire de cyberjustice est un espace unique de réflexion et de création où les processus de justice sont modélisés et réimaginés afin d’améliorer l’accès à la justice. (extracted from170.txt)
Nos travaux se distinguent par l’expérimentation d’innovations technologiques, comme l’intelligence artificielle, pour optimiser le fonctionnement actuel de la justice, de […] Lire la suite À la une Actualités Web-conférences cyberjustice Conférence à venir: (Free)lance content and GenerativeAI: The persisting plight of freelance authors across the creative industries de Dre Pina D’Agostino (9 avril 2024)2 avril 2024 date 9 avril 2024, 13h00 Réunion zoom https://mcgill.zoom.us/meeting/register/tZEqcOyppzguHd3ddDeKt5Ylo5x5BS9dDBLq description sommaire Recycling existing copyright-protected works in new media is an age-old recurrence, which continues to challenge copyright law and its future on a global scale. (extracted from170.txt)
Mark Daley (Western University) dans le cadre du cycle de conférences de 2024 « AI, Large Language Models, and Justice Considerations for Legal Practitioners Judges, Law Schools and Public Legal Educators » Biographie du conférencier Mark is the Chief AI Officer at Western University and a […] Lire la suite À la une Événements Conférence à venir: Understanding Large Language Models de Dr. (extracted from170.txt)
Plusieurs fonctionnalités ont été évoquées, telles que la détection de messages incendiaires, la capacité de proposer une reformulation de ceux-ci, ainsi que la possibilité de demander à l’agent […] Lire la suite À la une Appel à communications: Conférence « Can Artificial Intelligence Contribute to Justice? (extracted from170.txt)
Ce numéro de la RIDP, intitulé « Artificial Intelligence and Administration of Criminal Justice », présente les résultats d’une recherche collective entreprise en 2020. (extracted from170.txt)
Il s’inspire des travaux antérieurs influents de l’OCDE sur l’accès à la justice, par exemple les […] Lire la suite À la une Actualités Offre d’emploi | Auxiliaire.s de recherche au Laboratoire de cyberjustice4 décembre 2023 Le Laboratoire de cyberjustice est un espace unique de réflexion et de création où les processus de justice sont modélisés et réimaginés afin d’améliorer l’accès à la justice. (extracted from170.txt)
Nos travaux se distinguent par l’expérimentation d’innovations technologiques, comme l’intelligence artificielle, pour optimiser le fonctionnement actuel de la justice, de manière à en accroître l’efficacité, à en […] Lire la suite À la une Conférences de la Chaire Lexum Conférence à venir : IA, enjeux de pouvoir, de puissance et de souveraineté dans un monde en transition de Dre Asma Mhalla (4 décembre 2023)1 décembre 2023 La Chaire Lexum et le Centre de recherche en droit public ont le plaisir de vous inviter à la conférence « IA, enjeux de pouvoir, de puissance et de souveraineté dans un monde en transition » de Dre Asma Mhalla. (extracted from170.txt)
Cliquer ici pour vous inscrire Informations pratiques Quand : 4 décembre 2023, 16h30 Format : Présentiel et distanciel Où […] Lire la suite À la une Conférences et colloques La Conférence Cyberjustice Europe 2023 en images24 novembre 2023 La conférence Cyberjustice Europe 2023 a été un succès, et nous souhaitons exprimer nos remerciements à l’équipe de l’Institut des Études et de la Recherche sur le Droit et la Justice, ainsi qu’au Conseil de l’Europe, avec qui nous avons collaboré pour organiser cet événement. (extracted from170.txt)
Cette conférence a été une opportunité unique de discuter de l’impact […] Lire la suite À la une Actualités Publication à venir (15 novembre 2023) – Les intelligences artificielles au prisme de la justice sociale – Considering Artificial Intelligence Through the Lens of Social Justice10 novembre 2023 Le livre « Les intelligences artificielles au prisme de la justice sociale – Considering Artificial Intelligence Through the Lens of Social Justice » sous la direction de Pre Karine Gentelet sera publiée le 15 novembre 2023 par PUL (Presses de l’Université Laval). (extracted from170.txt)
La Commission avait également sollicité certains acteurs à partager […] Lire la suite À la une Rapport du sous-projet 11: L’accès à la justice par l’IA – Par et pour les communautés marginalisées et/ou sous-représentées30 octobre 2023 Karine Gentelet Ce rapport écrit par Pre Karine Gentelet (UQO), Mme Marie Zumstein & Mme Lily-Cannelle Mathieu a pour objectif de détailler les raisons pour lesquelles des tensions peuvent survenir lors du déploiement d’un outil d’IA dédiée à l’accès à la justice auprès de populations marginalisées. (extracted from170.txt)
Ce rapport a pour objectif d’établir une cartographie des différentes Legaltechs œuvrant dans le domaine de la « Justice prédictive » en France et à l’étranger, dans le but d’informer […] Lire la suite À la une Événements Visite de la délégation du Conseil national des barreaux et du cabinet d’avocats FÉRAL au Laboratoire de cyberjustice26 octobre 2023 Nous sommes ravis d’avoir accueilli la délégation du Conseil national des barreaux et l’équipe du cabinet d’avocats FÉRAL dans notre laboratoire. (extracted from170.txt)
A lawyer […] Lire la suite À la une Actualités From Wetware to Robots to Airware: Imagining a Trajectory for AI-facilitated JusticeAlexis Leblanc-Roy 20 septembre 2023 This installation is intended to interrogate the involvement of AI in the justice system. (extracted from170.txt)
predictive justice systems. (extracted from170.txt)
Télécharger ici Lire la suite À la une Actualités Lunch and Learn: Large Language Models – Applications in the legal field10 février 2023 Le mercredi, 8 février dernier a eu lieu le Lunch and Learn: Large Language Models – Applications in the legal field où l’équipe du Laboratoire a eu l’occasion de participer à une séance de brainstorming sur l’utilisation de ChatGPT dans le domaine juridique avec des contributions de Karim Benyekhlef, Valentin Callipel, Mark Likhten, Philippe Langlais (Département d’informatique […] Lire la suite Actualités À l’invitation du ministère de la Justice du Canada, le Pr. (extracted from170.txt)
Karim Benyekhlef et Me Valentin Callipel représenteront le Canada dans le cadre du dialogue Canada-Europe sur la numérisation de la justice13 janvier 2023 Karim Benyekhlef, directeur du Laboratoire de cyberjustice, et Valentin Callipel, chargé de mission, rencontreront des experts européens et canadiens dans le cadre d’une discussion portant sur l’échange des meilleures pratiques en matière de numérisation de la justice. (extracted from170.txt)
Il s’agit du Martin Felsky Award pour leur article intitulé « Judging by the Numbers : Judicial Analytics, the Justice System and its Stakeholders ». (extracted from170.txt)
Le Martin Felsky Award a été […] Lire la suite À la une Événements La justice en ligne comme solution aux barrières à l’accès à la justice21 septembre 2022 • 15:00Laboratoire de cyberjustice & En ligne (Hybride)3 août 2022 Présentateur(trice)s Julia Atack, Yannick Labelle, Responsable des affaires juridiques et auteure de la recherche Résumé de la présentation De nombreuses barrières se dressent devant les consommateurs qui désirent avoir accès à la justice. (extracted from170.txt)
Malgré les multiples mesures envisagées ou tentées par les législateurs canadiens, les solutions proposées jusqu’à présent ne semblent pas être à la […] Lire la suite À la une Table ronde sur la « Justice décentralisée et Web 3.0 » – Que pouvons-nous apprendre des projets de résolution de conflits en ligne par blockchain ?29 juillet 2022 Dans le cadre de la rencontre annuelle du Projet AJC Issue du Web 3.0 – une idée de nouvelle itération du World Wide Web basée sur la technologie blockchain qui intègre des concepts tels que la décentralisation et l’économie basée sur les jetons – la « justice décentralisée » est une nouvelle approche de la résolution des […] Lire la suite À la une Actualités Événements Conférence internationale du partenariat AJC4 octobre 2022 6 octobre 2022Laboratoire de cyberjustice7 juillet 2022 La conférence internationale de mi-parcours du partenariat AJC, qui se déroulera du 4 au 6 octobre 2022, arrive à grands pas! (extracted from170.txt)
Elle se déroulera maintenant du 4 […] Lire la suite Actualités Existe-t-il un juste usage et une réelle utilité de l’IA en justice ?| Karim Benyekhlef30 juin 2022 Karim Benyekhlef Dans le cadre du congrès Time World 2022, ayant eu lieu du 5 au 7 mai 2022, Pr Benyekhlef s’est penché sur la question de l’impact de l’utilisation de l’intelligence artificielle sur le droit. (extracted from170.txt)
Il a présenté une conférence intitulée « Existe-t-il un juste usage et une réelle utilité de l’IA en justice ? (extracted from170.txt)
La discussion portera sur comment un cadre d’alphabétisation fonctionnelle peut être utilisé pour réduire la complexité des formulaires des cours et des tribunaux et ultimement améliorer l’accès à la justice. (extracted from170.txt)
Depuis sa création, il a […] Lire la suite Événements The role of courts and access to justice in the digital era 9 juin 2022 10 juin 2022En ligne - Online & Radboud University Nijmegen29 avril 2022 Les inscriptions sont ouvertes pour la conférence hybride « The role of courts and access to justice in the digital era » (Université Radboud de Nimègue) qui se déroulera les 9 et 10 juin 2022. (extracted from170.txt)
Dans le cadre du Cycle d’ateliers « L’accessibilité : la nouvelle frontière de la justice en ligne », nous vous invitons à l’atelier « Cultural Accessibility ». (extracted from170.txt)
Elle est associée au Centre de recherche en droit, technologie et société et professeure agrégée au Département des sciences sociales […] Lire la suite À la une Actualités Justice et IA | Karim Benyekhlef participe à la série de balados UtopIA19 avril 2022 Karim Benyekhlef Pr Karim Benyekhlef, directeur du Laboratoire de cyberjustice, a participé, avec Pr Jocelyn Maclure, au balado « Justice et IA », animé par Christian Auger. (extracted from170.txt)
Description de l’épisode Une société gouvernée par les algorithmes peut-elle rendre justice? (extracted from170.txt)
Comment les acteur.rice.s de la justice perçoivent-ils l’avènement de cette technologie? (extracted from170.txt)
Une justice automatisée est-elle en marche? (extracted from170.txt)
Summary Online Dispute Resolution (ODR) can increase access to justice, but the expense and scarcity of facilitators […] Lire la suite À la une Événements Artificial Intelligence and Access to Justice: Perspectives from the legal and technical domain29 mars 2022 • 16:30Laboratoire de cyberjustice & En ligne - Online11 mars 2022 Cet atelier se déroulera en anglais. (extracted from170.txt)
Nye a plus de 20 ans d’expérience dans la direction de projets sophistiqués et multidisciplinaires dans le secteur de la justice en Ontario. (extracted from170.txt)
En anglais Conférencier Pr Kieran Tranter is the Chair of […] Lire la suite À la une Nouvelles Le Laboratoire de cyberjustice obtient le statut d’observateur auprès du Réseau européen sur la cyberjustice8 février 2022 Le Laboratoire de cyberjustice est heureux d’annoncer qu’il a obtenu le statut d’observateur auprès du Réseau européen sur la cyberjustice (European Cyberjustice Network) de la CEPEJ (Commission européenne pour l’efficacité de la justice) du Conseil de l’Europe. (extracted from170.txt)
Les bases de données sur lesquelles […] Lire la suite À la une Événements AJC|ACT Workshop: «What have we learned from the accelerated experience of remote hearings during the pandemic?»8 février 2022 • 9:00 11:30En ligne - Online17 janvier 2022 En anglais Introduction The pandemic had an unprecedented accelerating effect on the transition towards remote hearings within the justice system. (extracted from170.txt)
[…] Lire la suite À la une Nouvelles Nomination du professeur Karim Benyekhlef au Conseil scientifique du nouvel Institut des Études et de la Recherche sur le Droit et la Justice (IERDJ)10 janvier 2022 Karim Benyekhlef Le Laboratoire de cyberjustice est heureux d’annoncer la nomination du professeur Karim Benyekhlef au Conseil scientifique du nouvel Institut des Études et de la Recherche sur le Droit et la Justice (IERDJ), issu de la fusion de l’Institut des hautes études sur la justice et de la Mission de recherche Droit et justice, deux organismes de recherche […] Lire la suite À la une Événements Afrofuturism, Critical Race Theory, and the Future of Policing3 février 2022 • 16:30En ligne - Online7 janvier 2022 Dans le cadre du Cycle de conférences « Droit et Littérature: Représentations littéraires des identités et transcriptions juridiques », le Laboratoire de cyberjustice vous invite à la conférence « Afrofuturism, Critical Race Theory, and the Future of Policing » présentée par Pr Bennett Capers (Fordham Law School). (extracted from170.txt)
Cette subvention est accordée pour 3 ans (2021-2024) pour le projet « Les incidences de l’architecture logicielle des tribunaux en ligne sur l’accès à la justice », avec les professeurs Karim Benyekhlef et Pierre-Luc […] Lire la suite Nouvelles Le professeur Pierre-Luc Déziel publie une étude sur l’incidence des technologies sur la formation des juristes au Québec24 mars 2021 Une équipe de la Faculté de droit de l’Université Laval dirigée par le professeur Pierre-Luc Déziel et composée de Hélène Zimmermann et Satchel Dell’olio Delpech publie une étude relative à l’incidence des technologies de l’information et des communications sur la formation des juristes au Québec. (extracted from170.txt)
Financée par le ministère de la Justice du Québec, l’étude […] Lire la suite À la une Le Laboratoire de cyberjustice sélectionné pour le Comité en gouvernement ouvert du Québec23 mars 2021 Le Laboratoire de cyberjustice est heureux d’annoncer qu’il a été sélectionné en tant qu’organisme de la société civile pour participer au Comité en gouvernement ouvert du Québec du Secrétariat du Conseil du trésor. (extracted from170.txt)
Le mandat du comité sera : *de conseiller le Secrétariat du Conseil du trésor quant à l’élaboration de nouveaux plans d’action pour un […] Lire la suite Nouvelles Podcast | Jena McGill and Amy Salyzyn on Judicial Analytics8 mars 2021 Amy Salyzyn Listen to Jena McGill and Amy Salyzyn talk about their upcoming paper “Judging by Numbers: How will Judicial Analytics Impact the Justice System and its Stakeholders?” Jena and Amy explain some of the benefits of judicial analytics software and a few concerns that arise with its use. (extracted from170.txt)
Also available on Spotify Read this paper on SSRN McGill, Jena and […] Lire la suite Nouvelles Kevin Ashley, chercheur du projet AJC, reçoit une subvention de 357 000$ pour le projet Using AI to Increase Fairness by Improving Access to Justice29 janvier 2021 Kevin Ashley Le Laboratoire de cyberjustice est heureux d’annoncer que la National Science Foundation (USA) attribue, dans le cadre du programme Fairness in Artificial Intelligence, une subvention de 357 000$ (3 ans) au projet Using AI to Increase Fairness by Improving Access to Justice mené par le professeur Kevin Ashley (University of Pittsburgh), chef du Chantier 2 […] Lire la suite À la une Publication de l’ouvrage collectif AI and Law: a Critical Overview dirigé par Karim Benyekhlef28 janvier 2021 Karim Benyekhlef Le Laboratoire de cyberjustice et la Chaire LexUM en information juridique sont heureux d’annoncer la publication de l’ouvrage collectif AI and Law: a Critical Overview aux éditions Thémis. (extracted from170.txt)
Le livre sera disponible début 2021 en version imprimée, […] Lire la suite Nouvelles Chaire Abeona-ENS-OBVIA | Appel à contribution pour un ouvrage collectif sur « Justice sociale et IA »17 novembre 2020 Karine Gentelet Cet ouvrage collectif s’inscrit dans le cadre des travaux de la Chaire Abeona-ENS-OBVIA et aura pour objectif de proposer une réflexion renouvelée et multidisciplinaire sur les enjeux des usages de l’intelligence artificielle à partir d’une perspective de justice sociale. (extracted from170.txt)
Devant les enjeux de mobilité, […] Lire la suite Nouvelles Rapport | To Surveil and Predict : A Human Rights Analysis of Algorithmic Policing in Canada 5 octobre 2020 Ce rapport rédigé par Kate Robertson — avocate et chercheuse au Citizien Lab, Cynthia Khoo — chercheuse au Citizen Lab et avocate spécialisée dans la technologie et les droits de l’homme — et Yolanda Song — avocate chez Stevenson Whelton LLP et associée de recherche pro bono à l’IHRP — examine les technologies algorithmiques qui […] Lire la suite Nouvelles Tribune de Karim Benyekhlef et Valentin Callipel |Algorithmes et Justice : une prudente avancée1 octobre 2020 Karim Benyekhlef / Valentin Callipel Karim Benyekhlef — directeur du Laboratoire de cyberjustice — et Valentin Callipel — chargé de mission au Laboratoire — s’intéressent dans une tribune publiée par Business & Legal Forum For Ethics & Performance aux enjeux des algorithmes dans le domaine judiciaire. (extracted from170.txt)
Le Privacy Shield en est l’illustration parfaite, reflétant à lui seul les difficultés d’un couple américano-européen tentant tant bien que mal de recoller les morceaux d’un compromis juridiquement […] Lire la suite Nouvelles Winkler Institute for Dispute Resolution ANNUAL REPORT 2019-2020 17 septembre 2020 Trevor Farrow / Valentin Callipel Basé à la Osgoode Hall Law School à Toronto et nommé en l’honneur de l’ancien juge en chef de l’Ontario Warren Winkler, le Winkler Institute for Dispute Resolution est un centre de recherche qui travaille depuis 2014 dans le domaine du règlement des litiges, l’accès à la justice, et l’avenir de la profession juridique et […] Lire la suite Nouvelles Prof. (extracted from170.txt)
Karine Gentelet, nouvelle titulaire de la Chaire Abeona-ENS-OBVIA Intelligence artificielle et justice sociale10 septembre 2020 Karine Gentelet Prof. (extracted from170.txt)
Karine Gentelet – chercheuse AJC – est récipiendaire de la Chaire 2020-2021 Abeona-École normale supérieure (ENS)-Observatoire international sur les impacts sociétaux de l’IA et du numérique (OBVIA) au concours IA et justice social. (extracted from170.txt)
Cette chaire permet à un·e professeur·e invité·e de développer, pendant une année, des travaux sur l’intelligence artificielle (IA) et la justice […] Lire la suite Nouvelles CFCJ | The Justice Crisis : Un nouveau livre sur l’accès à la Justice au Canada3 septembre 2020 Les Jacobs / Trevor Farrow The Justice Crisis: The Cost and Value of Accessing Law publié sous la direction de Trevor Farrow et Lesley A. (extracted from170.txt)
Jacobs – chercheurs AJC – fournit un aperçu approfondi, basé sur de nouvelles recherches empiriques, de ce qui fonctionne et ne fonctionne pas pour améliorer l’accès à la justice civile et familiale au Canada. (extracted from170.txt)
Alors que les algorithmes sont censés améliorer l’efficience et la qualité des services publics, certains se sont […] Lire la suite Blogue Retour sur l’expérience judiciaire en temps de pandémie : quelle technologisation de notre justice ?6 août 2020 Écrit par Jie Zhu, auxiliaire de recherche au Laboratoire de cyberjustice – Été 2020. (extracted from170.txt)
La Bourse doctorale de la Chaire LexUM en information juridique et du Laboratoire de cyberjustice est conçue pour encourager deux […] Lire la suite À la une Actualités Offre d’emploi | 2 auxiliaires de recherche au Laboratoire de cyberjustice14 novembre 2024 Le Laboratoire de cyberjustice est un espace unique de réflexion et de création où les processus de justice sont modélisés et réimaginés afin d’améliorer l’accès à la justice. (extracted from170.txt)
Nos travaux se distinguent par l’expérimentation d’innovations technologiques, comme l’intelligence artificielle, pour optimiser le fonctionnement actuel de la justice, de manière à en accroître l’efficacité, à en […] Lire la suite Voir les précédents Calendrier Actualités Conférence annuelle 2025 : Autonomisation des acteurs judiciaires par la cyberjustice (AJC)15 octobre 2025 16 octobre 2025Laboratoire de cyberjustice (B-2215)30 juin 2025 Actualités Nous joindre Espace exclusif – membres AJC Inscription - Infolettre Adresse courriel © 2018 AJC • Crédits et mentions légales propulsé par forcerouge sur OpenUM.ca,un projet de la Chaire L.R. (extracted from170.txt)
On the other hand, there is some consensus in crediting AI with the potential of being a formidable transformative force that is impacting, or will impact, our societies and solve many problems in all areas of human intervention, such as education, medicine, justice, agriculture or climate change . (extracted from158.txt)
The trouble with the AI revolution is that it is held up as a solution to all problems, not just in media but also in criminal justice and governance in education in health care, in this push towards a quantum-filled society . (extracted from158.txt)
D’un autre côté, il y a un certain consensus sur le fait que l’IA est potentiellement une extraordinaire force de transformation qui a ou aura un impact sur nos sociétés et résoudra de nombreux problèmes dans tous les domaines d’intervention humaine, tels que l’éducation, la médecine, la justice, l’agriculture ou le changement climatique . (extracted from158.txt)
Dans cette marche forcée vers une société quantique, le problème, c’est que la révolution de l’IA est présentée comme une solution à tous les problèmes, non seulement dans les médias, mais aussi au niveau de la justice pénale, de la gouvernance de l’éducation et des soins de santé . (extracted from158.txt)
The structure has published a series of studies, among them analysing issues related to Human-AI collaboration, algorithmic risk in US criminal justice system, the impact of AI on the economy and labour market . (extracted from158.txt)
De CarvalhoCascais Jovem President of Youth Party Ana Teresa Santos Fundação Calouste GulbenkianProject Manager Republic of MoldovaIrina Buzu NGO Action for Justice Vice President Russian FederationAidar Khusainov Tatarstan Youth Public Fund “Select” Institute of Applied SemioticsTeacher, Researcher Slovenia Simon Vrbanič National Youth Council of SloveniaProject Manager Turkey Kadir Soran Emoner7840 Robotics & AI Co-founder Goksel Ucak European Network Member of Independent Living Youth Cihat Ucak Personal Assistant to G . (extracted from158.txt)
Yet, they pose many threats and concerns, including to our privacy and social justice. (extracted from560.txt)
4.4.2 POTs: Protective Optimization Technologies Carmela Troncoso, Bogdan Kulynych, Rebeka Overdorf, and Seda G urses Although the rise of AI has undoubtedly improved our everyday lives in the past decades in many ways, we have also become increasingly aware of the severe risks it poses to our privacy and social justice. (extracted from560.txt)
123 “The Federal government should therefore emphasize AI investments in areas of strong societal impor tance that are not aimed at consumer markets —areas such as AI for public health, urban systems and smart communities, social welfare, criminal justice, environmental sust ainability, and national security, as well as long -term research that accelerates the production of AI knowledge and technologies.” 124 The following subareas are described in the plan: Data analysis; Perception; Theoretical limitations for AI; General AI; Scalable AI; Human -like AI; Robotics; AI Hardware 125 Sub -areas: Human -AI communication; Strengthening of human ability; Natural language processing; Interface and visualisation. (extracted from589.txt)
Apply EUR-Lex Access to European Union law This document is an excerpt from the EUR-Lex website You are here EUROPA EUR-Lex home EUR-Lex - 52021DC0118 - EN Help Print Menu EU law Treaties Treaties currently in force Founding Treaties Accession Treaties Other treaties and protocols Chronological overview Legal acts Consolidated texts International agreements Preparatory documents EFTA documents Lawmaking procedures Summaries of EU legislation Browse by EU institutions European Parliament European Council Council of the European Union European Commission Court of Justice of the European Union European Central Bank European Court of Auditors European Economic and Social Committee European Committee of the Regions Browse by EuroVoc EU case-law Case-law Reports of cases Directory of case-law Official Journal Access the Official Journal Official Journal L series daily view Official Journal C series daily view Browse the Official Journal Legally binding printed editions Special editions National law and case-law National transposition National case-law JURE case-law Information Themes in focus EUR-Lex developments Statistics ELI register About ELI Technical information ELI implementation overview Resources for implementing ELI ELI highlights ELI testimonials Legislation in schema.org EU budget online Quick search Use quotation marks to search for an "exact phrase". (extracted from210.txt)
Using Green Public Procurement criteria 34 can boost demand for a green digital transformation The digital transformation should also enable modern and efficient justice systems 35 , enforcement of consumer rights and an increased effectiveness of public action including law enforcement and investigation capacities 36 – what is illegal offline is also illegal online, and law enforcement must be best equipped to deal with more and more sophisticated digital crimes. (extracted from210.txt)
The digital principles are rooted in primary EU law, notably the Treaty on European Union (TEU), the Treaty on the Functioning of the European Union (TFEU), the Charter of Fundamental Rights and the case-law of the Court of Justice of the European Union, as well as in secondary legislation 37 . (extracted from210.txt)
(34) https://ec.europa.eu/environment/gpp/eu_gpp_criteria_en.htm (35) Communication from the Commission on the Digitalisation of justice in the European Union A toolbox of opportunities, COM(2020) 710 final. (extracted from210.txt)
Help pages Contact Sitemap Follow us X Legal Legal notice Cookies policy Accessibility Privacy statement Information About EUR-Lex Newsletter Useful links Other services European Data EU tenders EU research results EU Whoiswho EU publications N-Lex EU Law Tracker Discover more on europa.eu Contact the EU Call us 00 800 6 7 8 9 10 11 Use other telephone options Write to us via our contact form Meet us at one of the EU centres Social media Search for EU social media channels Legal Languages on our websites Privacy policy Legal notice Cookies EU institutions European Parliament European Council Council of the European Union European Commission Court of Justice of the European Union (CJEU) European Central Bank (ECB) European Court of Auditors European External Action Service (EEAS) European Economic and Social Committee European Committee of Regions (CoR) European Investment Bank European Ombudsman European Data Protection Supervisor (EDPS) European Data Protection Board European Personnel Selection Office Publications Office of the European Union Agencies Switch to mobile Switch to desktop (extracted from210.txt)
Page 10 - Unboxing Artificial IntelligenceIf an AI system is used for interaction with individuals in the context of public services, especially justice, welfare, and healthcare, the user needs to be notified and the possibility of recourse to a professional upon request and without delay must be communicated. (extracted from73.txt)
The Commissioner for Human Rights11, the Consultative Commit tee of the Convention for the Protection of Individuals with regard to Automatic Processing of Personal Data (T -PD12) and the European Commission for the Efficiency of Justice (CEPEJ13) use a relatively similar generic definition referring to a set of scienc es, theories and techniques. (extracted from67.txt)
the reports of the Parliamentary Assembly of the Council of Europe, in particu lar on the need for democratic governance of AI ; the role of AI in policing and criminal justice systems ; preventing discrimination caused by AI ; ethical and legal frameworks for the research and development of neurotechnology ; AI and health care ; consequences of AI on labour markets ; and legal aspects of ‘autonomous vehicles’ . (extracted from67.txt)
5, 6, 7 ECHR) when these systems are used in situations where physical freedom or personal security is at stake (such as justice and law enforcement). (extracted from67.txt)
If applied responsibly and with prudence, however, certain AI applications can also make the work of justice and law enforcement professionals more efficient and hence have a positive impact on these rights. (extracted from67.txt)
4.4 Work in the field of justice 50. (extracted from67.txt)
The European Commission for the Efficiency of Justice (CEPEJ) adopted in December 2018 the European Ethical Charter for the use of artificial intelligence in judicial systems75 which sets five key principles (respect of fundamental rights, non -discriminatio n, quality and security, transparency, impartiality and fairness, "under the control" of the user) for the use of AI systems in this field. (extracted from67.txt)
On 22 October 2020, the PACE adopted 7 reports, focusing on: the need for democratic governance of AI; the role of AI in policing and criminal justice systems; discrimination caused by AI; threats to fundamental freedoms; medical, legal and ethical challenges in the fie ld of health care; consequences on labour markets; and legal aspects of ‘autonomous vehicles’. (extracted from67.txt)
19 non-binding instruments in four core areas (data protection, health, de mocracy and justice) and was complemented by an overview of the Council of Europe’s instruments in other fields. (extracted from67.txt)
The principles of privacy, justice and fairness showed the least variation across Council of Europe’s member States, observers and the rest of the world, and hence the highest degree of cross -geographical and cross -cultural stability. (extracted from67.txt)
The development and use of AI systems has also been considered in sectorial strategies on agriculture, e -justice, public services, health, environment, education, security and defence, mobility and data. (extracted from67.txt)
❖ Key obligations: o Member States are obliged to ensure that the AI systems they deploy do not result in unlawful discrimination, harmful stereotypes (including but not limited to gender stereotypes) and wider social inequality, and should therefore apply the highest level of scrutiny when using or promoting the use of AI systems in sensitive public policy areas, including but not limited to law enforcement, justice, asylum and migration, health, social security and employment. (extracted from67.txt)
This should also include the possibility to get insight into and challenge a n AI-informed decision in the context of law enforcement or justice, including the right to review of such decision by a human. (extracted from67.txt)
❖ Key obligations o Member States must ensure that AI systems used in the field of justice and law enforcement are in line with the essential requirements of the right to a fair trial. (extracted from67.txt)
Moreover, this would secure access to justice should they fail to meet these obligations.173 125. (extracted from67.txt)
If the data is not representative for minority populations then it could be potentially harmful.’ Berk Ustun, Postdoctoral Fellow, Center for Research in Computation and Society, Harvard University ‘There should be a notion amongst patients, society and the general population that there is a societal good in sharing their data to make sure that health related algorithms are as fair and beneficial as possible.’ Finale Doshi-Velez, Assistant Professor in Computer Science at the Harvard Paulson School of Engineering and Applied ScienceRacial bias in criminal justice algorithms The Correctional Offender Management Profiling for Alternative Sanctions (COMPAS) is an algorithmic risk score used to help judges in certain US states decide sentencing, by predicting a defendant’s risk of reoffending. (extracted from9.txt)
Alarming reports have detailed how discriminatory algorithms are already deployed in the justice system, wherein judges use these tools for sentencing that purport to predict the likelihood a criminal defendant will reoffend.21 In Automating Inequality , Virginia Eubanks details how government actors implement automated and surveillance technologies that harm marginalized groups.22 Eubanks studied automated systems in the US that discriminated against the poor’s receipt of DATA & SOCIETY 11 GOVERNING ARTIFICIAL INTELLIGENCEgovernment assistance. (extracted from172.txt)
Baluarte and Christian De Vos, From Judgment to Justice: Implementing International and Regional Human Rights Decisions , Open Society Justice Initiative (November 2010), https:/ / www.opensocietyfoundations.org/sites/default/files/from-judgment-to-justice-20101122.pdf . (extracted from172.txt)
The third principle, justice, considers who gets chosen to be a research subject and what population segments stand to benefit — or be harmed — by the research results. (extracted from370.txt)
JoaAnne Stonier, Chief Data Officer at MasterCard, acknowledged another challenge to data management, which is the urge to “do good” with data when a company is approached by an academic researcher or non-profit with a social justice mission. (extracted from370.txt)
Similarly, traditional values such as dignity, freedom, democracy, equality, autonomy, and justice are part of discussions around digital ethics.10 Privacy and data protection today represent intrinsic and foundational concepts for our modern society, which enable individual freedom of choice and user control. (extracted from403.txt)
8 December 2020 CEPEJ(2020) 15Rev EUROPEAN COMMISSION FOR THE EFFIC IENCY OF JUSTICE (CEPEJ) Possible introduction of a mechanism for certif ying artificial intelligence tools and services in the sphere of justice and the judiciary : Feasibility S tudy In December 2018, the European Commission for the Efficiency of Justice (CEPEJ) adopt ed the Ethical Charter on the use of artificial intelligence in judicial systems and their environment . (extracted from72.txt)
31 3 Feasibility st udy on the possible introduction of a mechanism for certif ying artificial intelligence tools and services Introduction Justice is not currently the preferred focus of companies that are innova ting in the field of artificial intelligence . (extracted from72.txt)
The in -depth s tudy on the use of AI in judicial systems, notably AI applications processing judicial decisions and data , reprodu ced i n Appendix I to the CEPEJ Charte r6, defines the main categories of artificial intelligence in the sphere of justice for illustrative purposes as follows : - Advanced case -law search engines - Online dispute resolution , - Assistance in drafting deeds - Analysis (predictive, scales ), - Categorisation of contracts according to different criteria and detection of divergent or incompatible contractual clauses, - "Chatbots " to inform litigants or support them in their legal proceedings . (extracted from72.txt)
Two additional categories with strong ethics implications could also be discussed, namely algorithmic justice, which could be seen as comparable to the aforementioned category of "online dispute resolution", and tools for enhanced judges' decision -making , which could be linked to the "Analys is" category, still at an experimental stage chiefly geared to aid for determining damages and sanctions. (extracted from72.txt)
Besides these considerations l inked to the categories of artificial intelligence and its state of technological advancement in terms of contextual learning, we should distinguish between AI applications in the justice sphere according to their functions and purposes . (extracted from72.txt)
Specifically in the judicial field where artificial intelligence tools provide aid for decision -making for judges, if, for example, the machine propose s a detention measure rather than a security measure, the defendant and society are entitled to demand an explanation for that choice if it guide s an official decision of justice. (extracted from72.txt)
The reasoning of decisions of justice demands a certain level of explainability. (extracted from72.txt)
The justice sphere has not been included in the areas listed for experimentation33. (extracted from72.txt)
The sole reference to artificial intelligence in the justice sphere in this document is to be seen in a survey finding that only half of the respondents are comfortable with 26 Article 6, Regulation (EC) No 1980/2000 of the European Parliament and of the Council of 17 July 2000 on a revised Community eco -label award scheme. (extracted from72.txt)
9 AI in the area of justice, compared to 80% in the area of transport ation34, which appears to have been the reason why it was exclu ded from the pilot experiments . (extracted from72.txt)
- Proportion ate processing of person al data (priv acy) and clear purpose s - Anonymisation of the parties and particip ants (physi cal individual s) and their counsels o Checking by consultation of data sets - Absence of evalu ation and class ification of physi cal individuals or legal entities on the basis of judicial decisions o Checking by consultation of the interface - Anonymisation of the name of the judge and the location of the court in decisions u sed for predictive justice ( with th e aim of avoiding forum shopping ) o Checking by consultation of data sets - Hermetic separation of artificial intelligence services having different purpos es (such as dissoci ating the search engine service from the aid for decision -making service) o Checking of databases and data sources used by each system - Right of access to the judge and the right to a fair trial - Presence of clear information indicating, where applicable, th at a report generated by an artificial intelligence system is not explainable . (extracted from72.txt)
If successfully applied to artificial intelligence in the sphere of justice, the human rights by design approach could be applied more widely to artificial intelligence systems used in other fields and could also potentially be tes ted in legislative processes, after enactment of legislation, to limit the need for the review of compatibility of domestic legislation with international conventions and treaties before courts of law. (extracted from72.txt)
The use of artificial intelligence in the legal and judicial sphere represents an important societal challenge, particularly in the field of algorithmic justice. (extracted from72.txt)
While technological advances may bring improvements to the judicial system, facilitate the work of legal professionals and improve access to justice and information for defendant s/litigants , increased vigilance is necessary in this sector which the European Commission descri bes as a high -risk sector. (extracted from72.txt)
28 Appendixes Summary table of indicators and certification criteria Objective Criteria Assessment method Target AI category Proportionate processing of personal data Anonymisation of the parties and participants (physical individuals) and their counsels Consultation of data sets Unprocessed data All Absence of evaluation and classification of physical individuals or legal entities on the basis of judicial decisions Checking of interface Interface Connectionist Checking of database Limit forum shopping Anonymisation of the judge and the court ’s location in decisions used for predictive justice Consultation of data sets Unprocessed data Connectionist Clear purposes for processing Hermetic separation of artificial intelligence services Checking of databases and data sources used by each system Databases All Fair trial Information indicating to the judge and the defendant, if relevant that a report generated by an artificial intelligence system is not explainable (See also below : Defendant ’s/litigant's right to opt out of the use of artificial intelligence ) Checking of AI category and checking of existence and clarity of information (See also below : Checking of a notification system for the defendant ’s/litigant's decision and for effective redirection to conventional proceedings before a court within the meaning of A rticle 6 of the ECHR ) Learning model and interface Connectionist Judges’ independence in their decision making process Safeguard against the profiling of judges A/B testing checking of search results Search engine and processed data All Match between the criterion displayed and the actual pattern of classification of search results Auditing of search results Search engine All Transparency of weighting of criteria for multicriteria searches Checking of the existence of explanatory information and auditing of search results Interface and search engine All Verification by auditing of search results Checking of the existence of Interface and search engine All 29 Summary table of indicators and certification criteria Transparency of criteria used for searches by “relevance” explanatory information and auditing of search results Verification by auditing of search results Ethics and Human rights by design No human rights violation Report presenting decision trees and explaining how fundamental rights and freedoms are taken into account Report Symbolic No human rights violation Report presenting training data and methods and explaining how fundamental rights and freedoms are taken into account Report Connectionist Avoiding discrimination based on sensitive data Elimination of the tags that could be linked to parties ’ sensitive data (home address, income, family situation, registered capital) Checking by consultation of data sets Unprocessed data Not under public authority control A/B testing using information and tags that could be linked to sensitive data by changing, where applicable, one of the following parameters during each test: name, home address, income, family situation, registered capital, relevant specific contextual information, etc. (extracted from72.txt)
Four of them are core principles commonly used in bioethics: beneficence, non-maleficence, autonomy, and justice. (extracted from563.txt)
In the ensuing discussion, we note the limitations and assess the implications of this ethical framework for future efforts to create laws, rules, technical standards, and best practices for ethical AI in a wide range of contexts.Keywords: accountability, autonomy, artificial intelligence, beneficence, ethics, explicability, fairness, intelligibility, justice, non-maleficence.1. (extracted from563.txt)
This convergence can most clearly be shown by comparing the sets of principles with the four core principles commonly used in bioethics: beneficence, non-maleficence, autonomy, and justice (Beauchamp & Childress, 2012). (extracted from563.txt)
Justice: Promoting Prosperity, Preserving Solidarity, Avoiding UnfairnessThe decision to make or delegate decisions does not take place in a vacuum. (extracted from563.txt)
The consequences of this disparity in autonomy are addressed in the principle of justice. (extracted from563.txt)
The importance of ‘justice’ is explicitly cited in the Montreal Declaration, which argues that “the development of AI should promote justice and seek to eliminate all types of discrimination,” while the Asilomar Principles include the need for both “shared benefit” and “shared prosperity” from AI. (extracted from563.txt)
Under its principle named “Justice, equity and solidarity,” the EGE argues that AI should “contribute to global justice and equal access to the benefits” of AI technologies. (extracted from563.txt)
It also warns against the risk of bias in datasets used to train AI systems, and – unique among the documents – argues for the need to defend against threats to “solidarity,” including “systems of mutual assistance such as in social insurance and healthcare.” Elsewhere ‘justice’ has still other meanings (especially in the sense of fairness), variously relating to the use of AI to correct past wrongs such as eliminating unfair discrimination, promoting diversity, and preventing the rise of new threats to justice. (extracted from563.txt)
The diverse ways in which justice is characterised hints at a broader lack of clarity over AI as a human-made reservoir of ‘smart agency.’ Put simply, are we (humans) the patient, receiving the ‘treatment’ of AI, the doctor prescribing it? (extracted from563.txt)
This list was designed by consensus of a large diverse interdisciplinary committee to give the public something better than Asimov’s Laws (which covered beneficence & justice), but extended to five in order to bring in transparency and accountability. (extracted from563.txt)
Our results reveal a global convergence emerging around five ethical principles (transparency, justice and fairness, non-maleficence, responsibility and privacy), with substantive divergence in relation to how these principles are interpreted; why they are deemed important; what issue, domain or actors they pertain to; and how they should be implemented. (extracted from205.txt)
Protecting individuals’ privacy and data in the artificial intelligence world Intel Corporation USA Introducing Unity’s Guiding Principles for Ethical AI – Unity Blog Unity Technologies USA Digital Decisions Center for Democracy & Technology USA Science, Law and Society (SLS) Initiative The Future Society USA AI Now 2018 Report AI Now Institute USA Responsible bots: 10 guidelines for developers of conversational AI Microsoft USA Preparing for the future of Artificial Intelligence Executive Office of the President; National Science and Technology Council; Committee on Technology USA The National Artificial Intelligence Research and Development Strategic Plan National Science and Technology Council; Networking and Information Technology Research and Development Subcommittee USA 5 AI Now 2017 Report AI Now Institute USA Position on Robotics and Artificial Intelligence The Greens (Green Working Group Robots) EU Report with recommendations to the Commission on Civil Law Rules on Robotics European Parliament EU Ethics Guidelines for Trustworthy AI High-Level Expert Group on Artificial Intelligence EU AI4People—An Ethical Framework for a Good AI Society: Opportunities, Risks, Principles, and Recommendations AI4People EU European ethical Charter on the use of Artificial Intelligence in judicial systems and their environment Concil of Europe: European Commission for the efficiency of Justice (CEPEJ) EU Statement on Artificial Intelligence, Robotics and 'Autonomous' Systems European Commission, European Group on Ethics in Science and New Technologies EU Artificial Intelligence and Machine Learning: Policy Paper Internet Society international Report of COMEST on Robotics Ethics COMEST/UNESCO international Ethical Principles for Artificial Intelligence and Data Analytics Software & Information Industry Association (SIIA), Public Policy Division international ITI AI Policy Principles Information Technology Industry Council (ITI) international Ethically Aligned Design. (extracted from205.txt)
These are, by frequency of the number of sources in which they were featured: transparency, justice and fairness, non-maleficence, responsibility, privacy, beneficence, freedom and autonomy, trust, dignity, sustainability, and solidarity (cf. (extracted from205.txt)
7 Table 2 – Ethical principles identified in existing AI guidelines Ethical principle Number of documents Included codes Transparency 73/84 Transparency, explainability, explicability, understandability, interpretability, communication, disclosure, showing Justice & fairness 68/84 Justice, fairness, consistency, inclusion, equality, equity, (non-)bias, (non-)discrimination, diversity, plurality, accessibility, reversibility, remedy, redress, challenge, access and distribution Non-maleficence 60/84 Non-maleficence, security, safety, harm, protection, precaution, prevention, integrity (bodily or mental), non-subversion Responsibility 60/84 Responsibility, accountability, liability, acting with integrity Privacy 47/84 Privacy, personal or private information Beneficence 41/84 Benefits, beneficence, well-being, peace, social good, common good Freedom & autonomy 34/84 Freedom, autonomy, consent, choice, self-determination, liberty, empowerment Trust 28/84 Trust Sustainability 14/84 Sustainability, environment (nature), energy, resources (energy) Dignity 13/84 Dignity Solidarity 6/84 Solidarity, social security, cohesion No single ethical principle appeared to be common to the entire corpus of documents, although there is an emerging convergence around the following principles: transparency, justice and fairness, non-maleficence, responsibility, and privacy. (extracted from205.txt)
Justice, fairness, and equity Justice is mainly expressed in terms of fairness23,25,27–29,48,50,58,60,66,72–77, and of prevention, monitoring or mitigation of unwanted bias23,28,33,40,47,52,54,58,64,69,73,74,78–80 and discrimination28,33,36,38,44,45,50,55,56,60,68,81–84, the latter being significantly less referenced than the first two by the private sector. (extracted from205.txt)
Whereas some sources focus on justice as respect for diversity31,38,56,59,65,66,70,72,78,80,85,86, inclusion31,45,47,51,72,80 and equality41,45,51,59,60,72,78, others call for a possibility to appeal or challenge decisions28,35–37,74,79, or the right to redress33,42,45,46,50,68,85 and remedy45,48. (extracted from205.txt)
9 If specified, the preservation and promotion of justice are proposed to be pursued through: (a) technical solutions such as standards50,68,89 or explicit normative encoding28,37,43,67; (b) transparency54,62, notably by providing information36,38,79 and raising public awareness of existing rights and regulation28,59; (c) testing52,58,67,69, monitoring54,56 and auditing39,46,50,67, the preferred solution of notably data protection offices; (d) developing or strengthening the rule of law and the right to appeal, recourse, redress, or remedy37,38,42,45,46,48,68,74,79; (e) via systemic changes and processes such as governmental action42,45,87,92 and oversight94, a more interdisciplinary47,65,85,93 or otherwise diverse58,59,70,85,87,95 workforce, as well as better inclusion of civil society or other relevant stakeholders in an interactive manner28,33,41,46,55,57,58,65,68,69,79,80,86 and increased attention to the distribution of benefits25,33,38,48,63,76. (extracted from205.txt)
Our analysis shows the emergence of an apparent cross-stakeholder convergence on promoting the ethical principles of transparency, justice, non-maleficence, responsibility, and privacy. (extracted from205.txt)
Although no single ethical principle is explicitly endorsed by all existing guidelines, transparency, justice and fairness, non-maleficence, responsibility and privacy are each referenced in more than half of all guidelines. (extracted from205.txt)
In particular, the prevalence of calls for transparency, justice and fairness points to an emerging moral priority to require transparent processes throughout the entire AI continuum (from transparency in the development and design of algorithms to transparent practices for AI use), and to caution the global community against the risk that AI might increase inequality if justice and fairness considerations are not adequately addressed. (extracted from205.txt)
Both these themes appear to be intertwined with the theme of responsibility, as the promotion of both transparency and justice seems to postulate increased responsibility and accountability on the side of AI makers and deployers. (extracted from205.txt)
Justice and Justification: Reflective Equilibrium in Theory and Practice. (extracted from205.txt)
Associa-tion/Society 26-Feb-2019 self Manual in-clusion European ethical Charter on the use of Artificial Intelligence in judicial systems and their environment "The five principles of the Ethical Char-ter on the Use of Artificial Intelli-gence in Judicial Systems and their environment" Concil of Europe: Eu-ropean Commission for the efficiency of Justice (CEPEJ) EU IGO/supra-na-tional xx-Feb-2019 multiple (public and private stakeholders) Manual in-clusion Ethically Aligned De-sign: A Vision for Pri-oritizing Human Well-being with Autono-mous and Intelligent Systems, First Edition (EAD1e) General Principles Institute of Electrical and Electronics Engi-neers (IEEE), The IEEE Global Initiative on Ethics of Autono-mous and Intelligent Systems interna-tional Prof. (extracted from205.txt)
Justice, Fairness & Equity VIII. (extracted from205.txt)
Discrimination (duplicate in Justice&Fairness) VI. (extracted from205.txt)
39% 56%5% Yes No Don't know For example, in The Netherlands , the Ministry of Justice and Safety now has a number of people working in the Justice and Law Enforcement department tasked on focusing on AI. (extracted from239.txt)
2020 37 As presented at the Data Justice Lab in 2019 on AI Realism and structural alternatives 50 With that purpose in mind, one should consider how existing data governan ce regimes and national regulatory practices can be transforming and not just intensifying existing power asymmetries . (extracted from239.txt)
As an example, the strategy highlights the use of hackathons in the justice domain to develop AI solutions for concrete policy issues. (extracted from239.txt)
A number of policy do mains where the Dutch government is exploring the use of AI or will stimulate other ac tors to use AI in their fields are mention ed and are listed below :  The use of AI in the field of security and justice. (extracted from239.txt)
A good starting point can be Article 2 of the Treaty on the European Union, which defines the values on which the Union is founded and are common to the Member States as ‘a society in which pluralism, non discrimination, tolerance, justice, solidarity an d equality between women and men prevail ’51 An additional set of shared European values (rights and freedoms) is defined by the Charter of Fundamental Rights of the European Union52 and only apply in cases where Member States implement EU regulation directly or transpose it into national legislation. (extracted from239.txt)
In addition, there are also preventive administrative measures, such as the decision of the Belgian police regulator to forbid piloting the use of face recognition technology at the Zaventem ai rport57 ; or the negative advice by the French data protection regulator regarding two pilots using facial recognition technology in French schools58; or the cease and desist letter issued by the French data protection regulator to the French Ministry of the Interior regarding the use of Automatic number plate recognition (ANPR) systems 59 54 http://www.sigmaweb.org/publications/principles -public -administration.htm 55 https://www.europarl.europa.eu/charter/pdf/text_en.pdf 56 The French Justice Reform Act, Article 33, https://www.legifrance.gouv.fr/affichTexteArticle.do;jsessionid=98B09D0394DAE57F1618DC21F30405F6.tplgfr34s_1?idArticle=JORFAR TI 000038261761&categorieLien=id&cidTexte=JORFTEXT000038261631&dateTexte = 57 https://www.vrt.be/vrtnws/nl/2019/09/20/politie -mag-geen-automatische -gezichtsherkenning -gebruiken -op-de 58 https://www.cnil.fr/fr/experimentation -de-la-reconnaissance -faciale -dans-deux-lycees -la-cnil-precise -sa-position 59 https://www.cnil.fr/fr/radars -troncons -mise-en-demeure -du-ministere -de-linterieur 74 In our initial framework it is thus proposed to consider the multiple elements of AI in public services that can be grouped into macro -areas labelled as: Digital Infrastruct ure, Organisation al Resources, Digital Government Development and Digital Society Development, as described in Figure 16 below. (extracted from239.txt)
PAGE 5STEP 1: DATA GENERATION AND ACQUISITION JUSTICE The recommendations based on justice are intended to ensure reciprocity, so that when data are collected, subjects are offered benefits that are proportionate to the data you request. (extracted from591.txt)
With reference to the basic orientation of justice, the physician can convince Patient U. (extracted from591.txt)
PAGE 11STEP 2: DATA STORAGE AND MANAGEMENT JUSTICE Justice requires enabling customers to use their data for the benefit of third parties and the common good. (extracted from591.txt)
Source: https:// deepmind.com/blog/article/specifying-ai-safety-problems PAGE 17STEP 3: DATA ANALYSIS AND KNOWLEDGE GENERATION JUSTICE The recommendations on justice address the problem of indirect discrimination as it relates to data-based models and predictions. (extracted from591.txt)
PAGE 23STEP 4: USE OF DATA-BASED PRODUCTS AND SERVICES JUSTICE Justice requires ensuring that the use of data-based products and services does not lead to undesirable social effects such as indirect discrimination or the stigmatisation or exclusion of groups of people. (extracted from591.txt)
If the app were made compulsory, or even linked to significant social advantages (such as the right to use public transportation), not knowing what groups are notified and inequalities in the rate of false positives would imply ignoring important elements about justice in deployment. (extracted from591.txt)
Thus, the app as designed protects privacy, but prevents the collection on information relevant to the justice implications of the app’s use and functionality. (extracted from591.txt)
There is thus a tension between justice and privacy in the deployment of the app. (extracted from591.txt)
Increasing the availability of open data - Continuing the development open data portal - Project to support both the open data demand and publishing MKM Ongoing activity --- 2 Abbreviations: EAS – Enterprise Estonia, HITSA – Information Technology Foundation for Education, HTM – Ministry of Education and Research, JM – Ministry of Justice, MKM – Ministry of Economic Affairs and Communications, STAT – Statistics Estonia 3 ’---’ marks action item which will not need additional or targeted budget, or it is not possible to distinguish such costs from rest of activity’s budget 4 See https://www.etag.ee/en/funding/programmes/rita/ Estonia’s National AI Strategy – Government of the Republic of Estonia – July 2019 3 Expert group proposals and existing measures Action item Responsible agency2 Deadline Budget3 Additional activities and measures: 1.8. (extracted from220.txt)
How Responsible AI can improve business and preserve value | PwC Skip to content Skip to footer Industries Services Issues About us Careers More Search Menu Industries Industries See all industries Consumer markets Energy, utilities and resources Financial services Government and public sector Health industries Industrials and services Private equity and principal investors Technology, media and telecommunications Menu Industries See all industries Menu Industries Consumer markets Consumer goods Hospitality and leisure Retail Transportation and logistics Menu Industries Energy, utilities and resources Chemicals Mining and metals Oil and gas Power and utilities Menu Industries Financial services Asset and wealth management Banking and capital markets Insurance Real estate Menu Industries Government and public sector Defence Education and skills Public safety, justice and security Menu Industries Health industries Health services Pharmaceuticals and life sciences Menu Industries Industrials and services Aerospace, defence and security Automotive Business services Engineering and construction Industrial manufacturing Space Menu Industries Private equity and principal investors Capital projects and infrastructure Menu Industries Technology, media and telecommunications Entertainment and media Technology Telecommunications Featured Value in motion Industry edge Engine by Starling: From launching a bank to launching a software business Menu Services Services See all services Alliances Artificial Intelligence Audit and assurance services Business model reinvention Business transformation Consulting Crisis management Deals Entrepreneurial and private business Family business Forensics Legal business solutions Managed Services Risk services Strategy Sustainability and climate change Tax Trust Workforce Menu Services See all services Menu Services Alliances Menu Services Artificial Intelligence Menu Services Audit and assurance services Actuarial services Capital markets Corporate reporting Financial audit IFRS Internal audit Next Generation Audit Risk assurance Sustainability assurance Menu Services Business model reinvention Menu Services Business transformation Menu Services Consulting Cloud transformation Digital operations Deals Finance transformation Forensics Front office transformation HR transformation Risk and regulation Strategy Technology Workforce Menu Services Crisis management Menu Services Deals Acquisitions Capital markets Corporate finance Deals strategy Joint ventures and alliances M&A legal M&A tax Performance and Restructuring​ Sovereign wealth funds Menu Services Entrepreneurial and private business Menu Services Family business Menu Services Forensics Menu Services Legal business solutions Employment Entity governance and compliance International business reorganisations Mergers and acquisitions NewLaw Menu Services Managed Services Menu Services Risk services Menu Services Strategy Menu Services Sustainability and climate change Climate risk, resilience and adaptation Energy solutions Impact management for sustainable business strategy Legal and sustainability Net zero transformation Social sustainability Sustainability assurance Sustainable capital Sustainability reporting Tax and sustainability Menu Services Tax Indirect taxes International tax services Mergers and acquisitions Sightline Tax code of conduct Tax controversy and dispute resolution Tax policy and administration Tax reporting and strategy Transfer pricing Menu Services Trust Menu Services Workforce Culture, leadership and change Inclusion Employment law Employment tax and payroll HR transformation and technology Organisational design People analytics and insights People in deals Retirement and pensions Reward and benefits Workforce risk Workforce strategy Featured Climate risk, resilience and adaptation Business transformation Sustainability assurance Menu Issues Issues See all issues Business model reinvention Business transformation C-suite insights Cybersecurity Climate and sustainability Megatrends Risk and regulation Technology Trust Upskilling Workforce Menu Issues See all issues Menu Issues Business model reinvention Menu Issues Business transformation Menu Issues C-suite insights Accelerating performance Global CEO Survey PwC at Davos strategy+business digital issue Take on Tomorrow: a strategy+business podcast The Leadership Agenda Menu Issues Cybersecurity Menu Issues Climate and sustainability Menu Issues Megatrends Menu Issues Risk and regulation Menu Issues Technology Menu Issues Trust Menu Issues Upskilling Menu Issues Workforce Featured Value in motion PwC’s 28th Annual Global CEO Survey The Fearless Future: 2025 Global AI Jobs Barometer Menu About us About us See more About Us Alumni Analyst relations Client case studies Ethics and compliance Committing to Net Zero Corporate sustainability Inclusion PwC's Global Annual Review Global regulatory affairs Human rights policy Network Leadership, Governance, and Structure New Ventures and Innovation News room Purpose, values and behaviours PwC office locations PwC's Code of Conduct Strategy& Tax Code of Conduct Third party code of conduct Transparency Report Menu About us See more About Us Menu About us Alumni Menu About us Analyst relations Menu About us Client case studies Menu About us Ethics and compliance Menu About us Committing to Net Zero Menu About us Corporate sustainability Menu About us Inclusion Menu About us PwC's Global Annual Review Menu About us Global regulatory affairs Menu About us Human rights policy Menu About us Network Leadership, Governance, and Structure Menu About us New Ventures and Innovation Menu About us News room Menu About us Purpose, values and behaviours Menu About us PwC office locations Menu About us PwC's Code of Conduct Menu About us Strategy& Menu About us Tax Code of Conduct Menu About us Third party code of conduct Menu About us Transparency Report Featured Value in motion Committing to Net Zero Global Transparency Report 2024 Menu Careers Careers Find out more about careers Search for a job Menu Careers Find out more about careers Menu Careers Search for a job Featured PwC’s Global Annual Review Committing to net zero The CEO’s ESG dilemma Loading Results No Match Found View All Results Designing, building and operating AI that delivers real-world impact Responsible AI Artificial intelligence is transforming business by streamlining activities, enhancing customer offerings, making workers more effective and speeding up innovation—prompting executives to deploy intelligent applications and agentic systems. (extracted from552.txt)
At a global level, the UNESCO Global Recommendations on the Ethics of AI in November 2021 forefronts the principles of human dignity, inclusive growth and social justice. (extracted from234.txt)
But while like goes Artificial Intelligence: Australia’s Ethics Framework (A Discussion Paper) Page 7 with like, justice sometimes demands that different situations be treated differently. (extracted from57.txt)
- Justice means that like situations should deliver like outcomes, but different situations can deliver different outcomes. (extracted from57.txt)
It also highlighted the f act that ethics frameworks on their own are not enough, because concrete actions need to be taken to ensure accountability and justice. (extracted from57.txt)
This technology could be especially useful in industries that require decision makers to generate frequent, accurate and replicable predictions and judgements such as the areas of justice, policing and med icine. (extracted from57.txt)
Miscarriages of justice are frequently attributable to human error or misconduct. (extracted from57.txt)
- Justice means that like situations should deliver like outcomes, but different situations can deliver different outcomes. (extracted from57.txt)
Available from: https://ec .europa.eu/commission/priorities/justice -and-fundamental -rights/data -protection/2018 reform -eu-data -protection -rules_en . (extracted from57.txt)
Hungry judges dispense rough justice. (extracted from57.txt)
President: Pascal Pichonnaz First Vice-President: Lord John Thomas Second Vice-President: Anne Birgitte Gammeljord Treasurer: Pietro Sirena Speaker of the Senate: Reinhard Zimmermann Secretary-General: Vanessa Wilcox Scientific Director: Christiane Wendehorst ISBN: 978-3-9505192-7-3 © European Law Institute 2022 Cover image: ShutterstockThe European Law Institute This publication was co-funded by the European Union’s Justice Programme. (extracted from156.txt)
21 Guiding Principle 8: No limitations to the exercise of rights and access to justice Automation shall not prevent, limit, or render unfeasible the exercise of rights and access to justice by affected persons. (extracted from156.txt)
The provision of public services or the exercise of public functions likely to materially impact citizens’ rights and liberties should be subject to special regulatory scrutiny.10 Likewise, legislators may consider it unacceptable to admit fully automated dispute resolution as access to justice would thus be deprived of human intervention. (extracted from156.txt)
Should damage or personal injuries be caused by an accident, a collision with buildings or windows, or by a drone crashing in a garden, the university, as the operator, bears the risk, without prejudice to the liability of the producer, if damage is caused by a defect of the product.Guiding Principle 8: No limitations to the exercise of rights and access to justice Automation shall not prevent, limit, or render unfeasible the exercise of rights and access to justice by affected persons. (extracted from156.txt)
As a specific application of Guiding Principle 1, this Principle focuses on the risk that the exercise of rights by the affected person and effective access to justice may be prevented, hampered or limited by the inadequate use of automation. (extracted from156.txt)
Second, where the affected person is deprived of the possibility of exercising a right or access to justice solely on the grounds that the contested decision was made by ADM. (extracted from156.txt)
Merlin gets quite a few of the films correct, including other superhero movies like X Men: Apocalypse , Doctor Strange , and Batman v Supe rman: Dawn of Justice . (extracted from142.txt)
AI ethics can be viewed from several points: data protection, freedom of expression, cultural diversity, productivity and economics, social justice, individual’s happiness, human autonomy and uniqueness, stability of society and so on. (extracted from142.txt)
Considering the impact of AI systems on everyday life , in addition to the accelerated trends and risks due to COVID19 , the present Declaration highlights : a) The exclusionary consequences of the fast-paced development of AI technologies driven by the private sector that leave many stake holders, especially youth actors and human rights activists , but also policy makers, behind, and result in normatively questionable and ineffective self -regulation in the private sector b) The need to develop and ensure l egal safeguards both by international organisations through existing or new legal instruments and by national governments responsible for securing and implementing them at national level c) The a bsence of youth in the emerging AI governance processes as a denial of the right to participation in democratic pro cesses which impedes a whole sector from co -shaping the discourse about development, assessment, implementation and regulation of AI technologies d) The imperative to respect ethical principles which must be at the core of all AI developments and deployment (transparency, justice and fairness, responsibility, safety and security, privacy) e) The need to assess the value of AI technologies on the impact of their consequences and benefits on individuals and soc iety. (extracted from94.txt)
Veber 18 Big Data in Criminal Justice – Few Chances and Serious Risks 18 Uwe Ewald 18 Big Data, Data Protection and Citizen Empowerment: The Revival of Individual Participation Principle as a Response to New Technological Challenges 19 Wenlong Li 19 Big Data, Psychodiagnostics and Threats to Personal Autonomy 19 Friderik Klampfer, Bojan Musil, Nenad Čuš Babič and Domen Bajde 19 Big health data on social networking platforms: The legal and ethical questions 20 Maria Tzanou 20 Cross-border exchange of big data - innovative technology meets outdated legal framework 21 Stanislaw Tosza 21 Databases and Due Process with regard to European Court of Human Rights’ Case-Law 21 Begüm Bulak Uygun 21 Dispensable humans and indispensable machines in the context of class and social control 22 Zoran Kanduč 22 Economic Cyber Espionage and Regulation of Big Data Theft at the International Level 23 Maša Kovič Dine 23 Finding the right balance between security and privacy: NATO and the big data analyses 23 Mitko Bogdanoski, Metodi Hadji-Janev 23 Five Reasons Not to Personify AI 23 Joanna J. (extracted from397.txt)
Current issues and future perspectives 31 Federico Costantini 31 State’s Due Diligence in Cyberspace in the Era of Big Data 31 Vasilka Sancin 31 The alluring promise of objectivity: Big data in criminal justice 32 Mojca Plesničar 32 We don’t know what the Questions are, but we know we're gonna find the Answers 32 Alexander Czadilek, Christof Tschohl and Walter Hötzendorfer 32 About the Venue 34 Map of the Building of the Faculty of Law 35 VENUE: Faculty of Law in Ljubljana 36 HOST CITY: Ljubljana 38 OTHER IMPORTANT INFORMATION 39 Notes 40 4 About the Conference “Big Data” is a phrase that has been used pervasively by the media and the lay public in the last several years. (extracted from397.txt)
Criminal justice systems are using technological solution too, for instance, to predict future crimes of those applying for bail or those to be sent on a parole. (extracted from397.txt)
• Which programmes and systems of algorithmic predictions are already in place in the criminal justice systems around the globe? (extracted from397.txt)
Themes of interest include (tentative list): • big data and crime control • predictive policing • automated justice • big data and discrimination • big data and social sorting • ethical dilemmas and predictive analytics • big data and international law • big data and personal data protection law • big data and cyber espionage • big data and citizen empowerment 5 Keynote speakers • Dean Wilson, University of Sussex, Brighton, UK • Nadya Purtova, TILT, Tilburg University, The Netherlands • Joanna J. (extracted from397.txt)
Dean’s key research interests are in surveillance and policing, and he has published widely in the areas of histories of urban policing, contemporary policing, surveillance and most recently pre-emption and criminal justice. (extracted from397.txt)
Together with a preliminary request from Ireland this lead to the abolition of the Data Retention Directive by the European Court of Justice (CJEU) in April 2014. (extracted from397.txt)
Maša Galič: Living labs and big data in practice: Stratumseind 2.0 - A discussion of a living lab in the Netherlands 15:30 – 16:00 Coffee break 11 HUMAN RIGHTS, CRIMINAL JUSTICE BIG DATA POLICING AND BIG DATA Session 4 Seminar room 5 Chair: Miha Hafner 1. (extracted from397.txt)
Plesničar: The alluring promise of objectivity: Big data in criminal justice 3. (extracted from397.txt)
Uwe Ewald: Big Data in Criminal Justice – Few Chances and Serious Risks Hour 17:30 – WELCOME RECEPTION – FACULTY OF LAW Main Hall 12 TUESDAY 23rd MAY Hour Lecture hall 8:30 – 9:00 REGISTRATION Red Hour Keynote session 3 Chair: Mojca M. (extracted from397.txt)
Algorithmic prediction in crime control Aleš Završnik Institute of Criminology at the Faculty of Law The paper will present several existent uses of big data in the criminal justice system, for example, for the prevention of payment card fraud by means of skimming; for the prediction of crime with predictive software; the use of algorithms to predict the recidivism of parolees. (extracted from397.txt)
It will present the pitfalls of reliance on big data 17 predictions used by law enforcement and criminal justice agencies and the risks big data carries as regards encroachment on fundamental liberties. (extracted from397.txt)
These are main reasons why Slovenian ministry of justice wants to publish all court decisions on the Internet. (extracted from397.txt)
Big Data in Criminal Justice – Few Chances and Serious Risks Uwe Ewald Ruhr-Universität Bochum in Germany Starting from the Foucauldian concept of the “regime of truth” in criminal justice this paper will present findings of a case study analyzing a complex organized crime case in Germany were huge amounts of digital data have been introduced into evidence. (extracted from397.txt)
As findings show Big Data Evidence (BDE) are about to alter the traditional way professionals in law enforcement and criminal justice act in the evidentiary process. (extracted from397.txt)
Finally, some suggestions should be offered on how to conceptualize truth-finding and BDE and how to limit the risks for a fundamental human rights and rule of law centered approach in criminal justice. (extracted from397.txt)
Big Data, Psychodiagnostics and Threats to Personal Autonomy Friderik Klampfer, Bojan Musil, Nenad Čuš Babič and Domen Bajde University of Maribor Experts and institutions have warned of the threat that big data analysis poses to our right to privacy, the challenge it raises to our traditional notions of criminal responsibility and justice, as well as concerns about the rising levels of invisible and unaccountable social control (EDPS 2015). (extracted from397.txt)
Databases are in widespread use in the criminal justice field across the world. (extracted from397.txt)
In particular, the privacy challenges associated with surveillance, primarily within the realm of the criminal justice databases will be highlighted. (extracted from397.txt)
Examining the necessity and proportionality of the laws in accordance with the United Nations’ (UN) Universal Declaration of Human Rights (UDHR) (UN 1948), this paper sets out to establish how the average social media savvy citizen is likely to be affected by this restrictive and conservative turn in the Australian criminal justice and anti-terrorism landscape. (extracted from397.txt)
Judicial oversight of (mass) collecting and processing of personal data Primož Gorkič Faculty of Law, University of Ljubljana The paper explores different approaches to securing judicial oversight of collecting and processing of personal data in a criminal justice system. (extracted from397.txt)
The alluring promise of objectivity: Big data in criminal justice Mojca Plesničar Institute of Criminology at the Faculty of Law Criminal justice systems have long aimed at preventing judges’ subjectivity from having any impact on in the courtroom. (extracted from397.txt)
Big data has so far entered criminal justice at three levels: bail, sentencing, and parole. (extracted from397.txt)
Seen as more objective, such algorithms could instil the long-lost trust of the public in the fairness of the criminal justice system. (extracted from397.txt)
However, there are some important considerations to be made before embarking on the big-data-saviour-of-justice wagon which we will discuss in detail. (extracted from397.txt)
We will then delve into the new legislative acts of EU Data Protection Law, the General Data Protection Regulation (Regulation (EU) 2016/679, GDPR) and the Police and Criminal Justice Authorities Directive (Directive (EU) 2016/680) to analyse their implications on the use of Big Data. (extracted from397.txt)
33 Another important issue is profiling and automated decision making for law enforcement, criminal justice and other purposes based on Big Data. (extracted from397.txt)
, UC Berkeley model) (4) Leverage existing resources currently serving a cross -functional role, e.g., MIT Libraries ’ successful efforts to incorporate diversity, inclusion, and social justice into the Libraries' educational and research mission. (extracted from432.txt)
PAGE 5STEP 1: DATA GENERATION AND ACQUISITION JUSTICE The recommendations based on justice are intended to ensure reciprocity, so that when data are collected, subjects are offered benefits that are proportionate to the data you request. (extracted from341.txt)
With reference to the basic orientation of justice, the physician can convince Patient U. (extracted from341.txt)
PAGE 11STEP 2: DATA STORAGE AND MANAGEMENT JUSTICE Justice requires enabling customers to use their data for the benefit of third parties and the common good. (extracted from341.txt)
Source: https:// deepmind.com/blog/article/specifying-ai-safety-problems PAGE 17STEP 3: DATA ANALYSIS AND KNOWLEDGE GENERATION JUSTICE The recommendations on justice address the problem of indirect discrimination as it relates to data-based models and predictions. (extracted from341.txt)
PAGE 23STEP 4: USE OF DATA-BASED PRODUCTS AND SERVICES JUSTICE Justice requires ensuring that the use of data-based products and services does not lead to undesirable social effects such as indirect discrimination or the stigmatisation or exclusion of groups of people. (extracted from341.txt)
If the app were made compulsory, or even linked to significant social advantages (such as the right to use public transportation), not knowing what groups are notified and inequalities in the rate of false positives would imply ignoring important elements about justice in deployment. (extracted from341.txt)
Thus, the app as designed protects privacy, but prevents the collection on information relevant to the justice implications of the app’s use and functionality. (extracted from341.txt)
There is thus a tension between justice and privacy in the deployment of the app. (extracted from341.txt)
The relevant concepts include freedom and self-determination, privacy and intimacy, sovereignty and power, beneficence and non-maleficence, as well as justice, solidarity and responsibility. (extracted from355.txt)
73) The collection and transmission of large volumes of health-relevant data touches on fundamental questions of justice. (extracted from355.txt)
As a normalising principle of social relations, justice demands that the arbitrary privi-leging of certain persons or groups be avoided. (extracted from355.txt)
25 25 ExEcutIvE Summary 74) As regards big data applications in the healthcare sector, four sets of problems stand out as especially relevant to questions of justice: first, access to datasets for the research sector; second, the insidious con-solidation of monopolistic structures; third, the inclusion of health apps, as well as various devices that facilitate private self-tracking, in determining health insurance premiums; and fourth, aspects of social justice, understood in terms of the capabilities approach, as they concern the responsible handling of health-relevant data. (extracted from355.txt)
Solidarity is frequently understood as complimentary to – and often subsidiary to – the concept of justice. (extracted from355.txt)
The shaping of such freedom is responsible when it also orients itself towards the legal and societal demands of solidarity and justice. (extracted from355.txt)
These aim to, firstly, realise the potentials of big data; secondly, to ensure individual freedom and privacy; thirdly, to ensure justice and solidarity; and fourthly, to promote responsibility and trust. (extracted from355.txt)
Ensure justice and solidarity C1. (extracted from355.txt)
To do justice to the complexity and significance of this issue, for example, companies and institutions could expand their efforts to establish internal data science departments. (extracted from355.txt)
Accept additional cookies Reject additional cookies View cookies Hide cookie message Skip to main content GOV.UK Navigation menu Menu Menu Services and information Benefits Births, death, marriages and care Business and self-employed Childcare and parenting Citizenship and living in the UK Crime, justice and the law Disabled people Driving and transport Education and learning Employing people Environment and countryside Housing and local services Money and tax Passports, travel and living abroad Visas and immigration Working, jobs and pensions Government activity Departments Departments, agencies and public bodies News News stories, speeches, letters and notices Guidance and regulation Detailed guidance, regulations and rules Research and statistics Reports, analysis and official statistics Policy papers and consultations Consultations and strategy Transparency Data, Freedom of Information releases and corporate reports Search GOV.UK × Search GOV.UK Search Search GOV.UK Search Home Defence and armed forces Ambitious, safe, responsible: our approach to the delivery of AI-enabled capability in Defence Ministryof Defence Policy paper Ambitious, safe, responsible: our approach to the delivery of AI-enabled capability in Defence Published 15 June 2022 This was published under the 2019 to 2022 Johnson Conservative government Contents Executive Summary Ambitious delivery of capability Our approach and AI-enabled weapons Key challenges to Defence AI Adoption Using AI Safely Using AI Legally Using AI Ethically Partnerships and Consultation Governance Implementation – building justified trust Annex A: Ethical Principles for AI in Defence Annex B: The Ministry of Defence AI Ethics Advisory Panel ANNEX C: Lethal Autonomous Weapon Systems (LAWS) Print this page © Crown copyright 2022 This publication is licensed under the terms of the Open Government Licence v3.0 except where otherwise stated. (extracted from382.txt)
Cancel Services and information Benefits Births, death, marriages and care Business and self-employed Childcare and parenting Citizenship and living in the UK Crime, justice and the law Disabled people Driving and transport Education and learning Employing people Environment and countryside Housing and local services Money and tax Passports, travel and living abroad Visas and immigration Working, jobs and pensions Government activity Departments News Guidance and regulation Research and statistics Policy papers and consultations Transparency How government works Get involved Support links Help Privacy Cookies Accessibility statement Contact Terms and conditions Rhestr o Wasanaethau Cymraeg Government Digital Service All content is available under the Open Government Licence v3.0, except where otherwise stated © Crown copyright (extracted from382.txt)
Venice Commission :: Council of Europe EN FR Login Venice Commission Council of Europe Home The Council in brief Human Rights Democracy Rule of Law Organisation 46 Countries Topics Newsroom Council of Europe Human Rights and Rule of Law Venice Commission EN | FR | DE | IT | RU The Commission About us Types of activities Founding documents News Programme References Members Member states Individual members Map of member States Positions Statements Democratic institutions and fundamental rights Constitutional reforms Fundamental rights Democratic institutions Rule of law Judicial reforms Ombudsman Elections, referendums and political parties Council for Democratic Elections Conferences of Electoral Management Bodies Seminars and assistance Political parties VOTA database Council of Europe and elections Constitutional justice Regional co-operation Constitutional Courts CODICES E-Bulletin World Conference on Constitutional Justice Non-European Partners Southern Mediterranean Overview of activities Highlights by beneficiary Recent events UniDem Med Campus List of UniDem seminars Intercultural Workshops on Democracy Arabic Speaking EMBs Tunisia : support to independent bodies Central Asia Kyrgyzstan – support to electoral reform Central Asia Rule of Law Programme List of events Latin America Overview of activities Recent events Sub-commission on Latin America Main documents Statute Main reference documents Compilations of studies and opinions Annual reports Documents Opinions Ongoing opinions Follow-up Studies Recent documents access by: Countries Topics Series Language Search Web Resources Newsletter Publications Articles on the Commission Links Library Constitutions Contact us For the media Reset password CDL-AD(2020)037-eStudy - Principles for a fundamental rights-compliant use of digital technologies in electoral processes, approved by the Council for Democratic Elections at its 70th meeting (online, 10 December 2020) and adopted by the Venice Commission at its 125th Plenary Session (online, 11-12 December 2020) Show related documents (3) Choose a year all 2020 CDL(2020)043 English 24/11/2020 - Public Draft principles for a fundamental rights-compliant use of digital technologies in electoral processes (H. (extracted from157.txt)
If the data is not representative for minority populations then it could be potentially harmful.’ Berk Ustun, Postdoctoral Fellow, Center for Research in Computation and Society, Harvard University ‘There should be a notion amongst patients, society and the general population that there is a societal good in sharing their data to make sure that health related algorithms are as fair and beneficial as possible.’ Finale Doshi-Velez, Assistant Professor in Computer Science at the Harvard Paulson School of Engineering and Applied ScienceRacial bias in criminal justice algorithms The Correctional Offender Management Profiling for Alternative Sanctions (COMPAS) is an algorithmic risk score used to help judges in certain US states decide sentencing, by predicting a defendant’s risk of reoffending. (extracted from4.txt)
4 Soraya Amrani Mekki, "Justice prédictive et accès au juge", La Justice Prédictive, Actes du Colloque of 12 February 2018 organised by Conseil d’Etat and Cour de cassation Lawyers Council for its bicentenary in partnership with the Paris-Dauphine PSL University, Paris, Dalloz, 2018. (extracted from180.txt)
15-25 of 1 December 2015 on security in stations; Report titled "Lutte contre la fraude aux prestations sociales : à quel prix pour les droits des usagers ?" , September 2017, Parcoursup decisions (2018-323 of 21 December 2018 and 2019-21 of 18 January 2019), Opinion 18-26 of 31 October 2018 on the Draft Programming and Reform Act for Justice, opinion 19-11 of 5 September 2019 on the Draft Act on Bioethics.Introductory remarks 4 Algorithms: preventing automated discrimination | 2020At first glance, algorithms sort, categorise and organise information by eliminating any prejudice and bias specific to human beings. (extracted from180.txt)
“Algorithms and Economic Justice: A Taxonomy of Harms and a Path Forward for the Federal Trade Commission. (extracted from235.txt)
4.2.6 Ethics and society It is necessary to guarantee that the uses of artificial intelligence are focused on humans’ well -being: artificial intelligenc e must be developed, applied and used with an ethical purpose based on fundamental rights, our social and cultural values, and the ethical principles of beneficence, autonomy of human beings, justice and the necessary explainability of their results . (extracted from221.txt)
Artificial intelligence should be used in a responsible, sensible and secure way and must include ethical reasoning, in terms of following and maintaining tradition and the European differentiating fact for everything that affects people and their development, as well as guarantee ing justice, transparency and lawfulness. (extracted from221.txt)
Court of Appeals for the Second Circuit and Justice David H. (extracted from584.txt)
Mariano-Florentino Cuéllar is a Justice on the Supreme Court of California, the Herman Phleger Visiting Professor of Law at Stanford University, and a faculty affiliate at the Stanford Center for AI Safety. (extracted from584.txt)
We also appreciate superb editorial assistance and dedicated research support from three members of Justice Cuéllar’s staff: Ryan Azad, Alexandra Havrylyshyn, and Mikayla Hardisty. (extracted from584.txt)
A large number of use cases fell under health- and law-enforcement-focused subagencies such as the Food and Drug Administration, the Office of Justice Programs, and the Transportation Safety Administration and Customs and Border Protection. (extracted from584.txt)
As a result, the Department of Health and Human Services, the Department of Justice, and the Department of Homeland Security account for a collective 51 use cases. (extracted from584.txt)
TOP TEN AGENCIES AND SUBAGENCIES BY NUMBER OF USE CASES Agency NameNumber of Use Cases Office of Justice Programs 12 Securities and Exchange Commission 10 National Aeronautics and Space Administration 9 Food and Drug Administration 8 United States Geological Survey 8 United States Postal Service 8 Social Security Administration 7 United States Patent and Trademark Office 6 Bureau of Labor Statistics 5 Customs and Border Protection 4 Table 2: The above list excludes overarching department-level agencies. (extracted from584.txt)
For example, the Department of Health and Human Services (19 use cases), the Department of Justice (16 use cases), and the Department of Homeland Security (16 use cases) have been refactored into respective sub-agencies (e.g., the Food and Drug Administration, the Office of Justice Programs, and Customs and Border Protection). (extracted from584.txt)
Others, such as the Federal Bureau of Investigation Terrorist Screening Database and the Department of Justice National Crime Information Center, come from peer agencies. (extracted from584.txt)
While the foregoing cannot possibly do justice to ongoing debate about the proper role of technology-enabled surveillance, it is a crucial debate to have. (extracted from584.txt)
Loomis, the Wisconsin Supreme Court did not find a due process violation when gender was used in a criminal risk assessment score, finding that the “use of gender promotes accuracy that ultimately inures to the benefit of the justice system.”101 Due to the doctrinal uncertainty, states and localities using criminal risk assessment scores remain split in whether they rely on gender.102 To the extent that the machine learning literature calls for awareness of protected attributes to promote fairness, it is on a collision course with equal protection doctrine. (extracted from584.txt)
2013), https://www.governing.com/topics/public-justice-safety/gov-social-media-transforms-chicago-policing.html. (extracted from584.txt)
Dep’t of Justice, Exec. (extracted from584.txt)
68 Linesight: Advanc ed Targeting Analytics Solution for Border Security, Unisys, https://www.unisys.com/offerings/industry-solutions/public-sector-industry-solutions/justice-law-enforcement-and-border-security-solutions/linesight (last visited Apr 7, 2019). (extracted from584.txt)
Mashaw, Bureaucratic Justice: Managing Social Security Disabilit y Claims (1985); David Ames, Cassandra Handan-Nader, Daniel E. (extracted from584.txt)
11 Some 390 Immigra tion Judges work for the Executive Office of Immigration Review in the Department of Justice and decide immigration cases. (extracted from584.txt)
Immigration Judge, Dep’t of Justice (June 9, 2017), https://www.justice.gov/legal-careers/job/immigration-judge; 4 Executive Off. (extracted from584.txt)
Dep’t of Justice, Executive Off. (extracted from584.txt)
One of the more interesting arguments for “internal” constraints on algorithmic decision-making is that, while marquee uses of algorithmic decisions systems—e.g., the criminal justice context—will draw certain judicial scrutiny. (extracted from584.txt)
Mashaw, Bureaucratic Justice: Managing Social Security (1985). (extracted from584.txt)
101, 109 (2019); Rebecca Wexler, Life, Liberty, and Trade Secrets: Intellectual Property in the Criminal Justice System, 70 Stan. (extracted from584.txt)
T yler, What Is Procedural Justice?: Criteria Used by Citizens to Assess the Fairness of Legal Procedures 22 L. (extracted from584.txt)
Mashaw, Bureaucratic Justice: Managing Social Security Disabilit y Claims (1983). (extracted from584.txt)
124 Eugene V olokh, Chief Justice Robots, 68 Duke L.J. (extracted from584.txt)
PAGE 5STEP 1: DATA GENERATION AND ACQUISITION JUSTICE The recommendations based on justice are intended to ensure reciprocity, so that when data are collected, subjects are offered benefits that are proportionate to the data you request. (extracted from592.txt)
With reference to the basic orientation of justice, the physician can convince Patient U. (extracted from592.txt)
PAGE 11STEP 2: DATA STORAGE AND MANAGEMENT JUSTICE Justice requires enabling customers to use their data for the benefit of third parties and the common good. (extracted from592.txt)
Source: https:// deepmind.com/blog/article/specifying-ai-safety-problems PAGE 17STEP 3: DATA ANALYSIS AND KNOWLEDGE GENERATION JUSTICE The recommendations on justice address the problem of indirect discrimination as it relates to data-based models and predictions. (extracted from592.txt)
PAGE 23STEP 4: USE OF DATA-BASED PRODUCTS AND SERVICES JUSTICE Justice requires ensuring that the use of data-based products and services does not lead to undesirable social effects such as indirect discrimination or the stigmatisation or exclusion of groups of people. (extracted from592.txt)
If the app were made compulsory, or even linked to significant social advantages (such as the right to use public transportation), not knowing what groups are notified and inequalities in the rate of false positives would imply ignoring important elements about justice in deployment. (extracted from592.txt)
Thus, the app as designed protects privacy, but prevents the collection on information relevant to the justice implications of the app’s use and functionality. (extracted from592.txt)
There is thus a tension between justice and privacy in the deployment of the app. (extracted from592.txt)
Make it happen with PwC​ Skip to content Skip to footer Industries Services Issues About us Careers More Search Menu Industries Industries See all industries Consumer markets Energy, utilities and resources Financial services Government and public sector Health industries Industrials and services Private equity and principal investors Technology, media and telecommunications Menu Industries See all industries Menu Industries Consumer markets Consumer goods Hospitality and leisure Retail Transportation and logistics Menu Industries Energy, utilities and resources Chemicals Mining and metals Oil and gas Power and utilities Menu Industries Financial services Asset and wealth management Banking and capital markets Insurance Real estate Menu Industries Government and public sector Defence Education and skills Public safety, justice and security Menu Industries Health industries Health services Pharmaceuticals and life sciences Menu Industries Industrials and services Aerospace, defence and security Automotive Business services Engineering and construction Industrial manufacturing Space Menu Industries Private equity and principal investors Capital projects and infrastructure Menu Industries Technology, media and telecommunications Entertainment and media Technology Telecommunications Featured Value in motion Industry edge Engine by Starling: From launching a bank to launching a software business Menu Services Services See all services Alliances Artificial Intelligence Audit and assurance services Business model reinvention Business transformation Consulting Crisis management Deals Entrepreneurial and private business Family business Forensics Legal business solutions Managed Services Risk services Strategy Sustainability and climate change Tax Trust Workforce Menu Services See all services Menu Services Alliances Menu Services Artificial Intelligence Menu Services Audit and assurance services Actuarial services Capital markets Corporate reporting Financial audit IFRS Internal audit Next Generation Audit Risk assurance Sustainability assurance Menu Services Business model reinvention Menu Services Business transformation Menu Services Consulting Cloud transformation Digital operations Deals Finance transformation Forensics Front office transformation HR transformation Risk and regulation Strategy Technology Workforce Menu Services Crisis management Menu Services Deals Acquisitions Capital markets Corporate finance Deals strategy Joint ventures and alliances M&A legal M&A tax Performance and Restructuring​ Sovereign wealth funds Menu Services Entrepreneurial and private business Menu Services Family business Menu Services Forensics Menu Services Legal business solutions Employment Entity governance and compliance International business reorganisations Mergers and acquisitions NewLaw Menu Services Managed Services Menu Services Risk services Menu Services Strategy Menu Services Sustainability and climate change Climate risk, resilience and adaptation Energy solutions Impact management for sustainable business strategy Legal and sustainability Net zero transformation Social sustainability Sustainability assurance Sustainable capital Sustainability reporting Tax and sustainability Menu Services Tax Indirect taxes International tax services Mergers and acquisitions Sightline Tax code of conduct Tax controversy and dispute resolution Tax policy and administration Tax reporting and strategy Transfer pricing Menu Services Trust Menu Services Workforce Culture, leadership and change Inclusion Employment law Employment tax and payroll HR transformation and technology Organisational design People analytics and insights People in deals Retirement and pensions Reward and benefits Workforce risk Workforce strategy Featured Climate risk, resilience and adaptation Business transformation Sustainability assurance Menu Issues Issues See all issues Business model reinvention Business transformation C-suite insights Cybersecurity Climate and sustainability Megatrends Risk and regulation Technology Trust Upskilling Workforce Menu Issues See all issues Menu Issues Business model reinvention Menu Issues Business transformation Menu Issues C-suite insights Accelerating performance Global CEO Survey PwC at Davos strategy+business digital issue Take on Tomorrow: a strategy+business podcast The Leadership Agenda Menu Issues Cybersecurity Menu Issues Climate and sustainability Menu Issues Megatrends Menu Issues Risk and regulation Menu Issues Technology Menu Issues Trust Menu Issues Upskilling Menu Issues Workforce Featured Value in motion PwC’s 28th Annual Global CEO Survey The Fearless Future: 2025 Global AI Jobs Barometer Menu About us About us See more About Us Alumni Analyst relations Client case studies Ethics and compliance Committing to Net Zero Corporate sustainability Inclusion PwC's Global Annual Review Global regulatory affairs Human rights policy Network Leadership, Governance, and Structure New Ventures and Innovation News room Purpose, values and behaviours PwC office locations PwC's Code of Conduct Strategy& Tax Code of Conduct Third party code of conduct Transparency Report Menu About us See more About Us Menu About us Alumni Menu About us Analyst relations Menu About us Client case studies Menu About us Ethics and compliance Menu About us Committing to Net Zero Menu About us Corporate sustainability Menu About us Inclusion Menu About us PwC's Global Annual Review Menu About us Global regulatory affairs Menu About us Human rights policy Menu About us Network Leadership, Governance, and Structure Menu About us New Ventures and Innovation Menu About us News room Menu About us Purpose, values and behaviours Menu About us PwC office locations Menu About us PwC's Code of Conduct Menu About us Strategy& Menu About us Tax Code of Conduct Menu About us Third party code of conduct Menu About us Transparency Report Featured Value in motion Committing to Net Zero Global Transparency Report 2024 Menu Careers Careers Find out more about careers Search for a job Menu Careers Find out more about careers Menu Careers Search for a job Featured PwC’s Global Annual Review Committing to net zero The CEO’s ESG dilemma Loading Results No Match Found View All Results We map the industries of tomorrow so you can start navigating yours today Explore now Your browser does not support the video tag. (extracted from551.txt)
Forexample, social network maps forservices such asTwitter candetermine credit-worthiness [5],opaque recidivism scores canshape criminal justice decisions inaracially disparate manner [6],and categorization based onzipcodes resulted inlessaccess toAmazon Prime same-day delivery service forAfrican-Americans inUnited States cities [7].These high-profile cases show that apparently neutral data canyield discriminatory outcomes, thereby compounding social inequities. (extracted from545.txt)
There areanumber ofgood models forinterdisciplinary ethics research, such asthe trainings offered bytheScience and Justice research center attheUniversity ofCalifornia, Santa Cruz [32] and Values inDesign curricula [33]. (extracted from545.txt)
Science &Justice Research Center (Collaboratio nsGroup. (extracted from545.txt)
Ordered to be printed 21 March 2022 and published 30 March 2022 Published by the Authority of the House of LordsHOUSE OF LORDS Justice and Home Affairs Committee 1st Report of Session 2021–22 HL Paper 180Technology rules? (extracted from223.txt)
The advent of new technologies in the justice system Justice and Home Affairs Committee The Justice and Home Affairs Committee was appointed by the House of Lords on 14 April 2021 to consider justice and home affairs, including the domestic criminal justice system, and international cooperation in respect of criminal justice, civil justice, migration and asylum. (extracted from223.txt)
Membership The Members of the Justice and Home Affairs Committee are: Lord Blunkett Baroness Kennedy of The Shaws Baroness Chakrabarti Baroness Pidding Lord Dholakia Baroness Primarolo Baroness Hallett Lord Ricketts Baroness Hamwee (Chair) Baroness Sanderson of Welton Lord Hunt of Wirral Baroness Shackleton of Belgravia Declaration of interests See Appendix 1. (extracted from223.txt)
A full list of Members’ interests can be found in the Register of Lords’ Interests: http://www.parliament.uk/hlregister Publications All publications of the Committee are available at: http://www.parliament.uk/ 519/justice-and-home-affairs-committee / Parliament Live Live coverage of debates and public sessions of the Committee’s meetings are available at: http://www.parliamentlive .tv Further information Further information about the House of Lords and its Committees, including guidance to witnesses, details of current inquiries and forthcoming meetings is available at: http://www.parliament .uk/business/lords Committee staff The staff who worked on this inquiry were Sam Kenny (Clerk), Achille Versaevel (Policy Analyst) and Amanda McGrath (Committee Operations Officer). (extracted from223.txt)
Contact details General correspondence should be addressed to the Clerk of the Justice and Home Affairs Committee, Committee Office, House of Lords, London SW1A 0PW. (extracted from223.txt)
3 THE ADVE nT OF n EW TECH nOLOGIES I n THE JUSTICE SYSTEM SUMMARY In recent years, and without many of us realising it, Artificial Intelligence has begun to permeate every aspect of our personal and professional lives. (extracted from223.txt)
Our Committee has limited its investigation to only one area–how these advanced technologies are used in our justice system. (extracted from223.txt)
Algorithms are being used to improve crime detection, aid the security categorisation of prisoners, streamline entry clearance processes at our borders and generate new insights that feed into the entire criminal justice pipeline. (extracted from223.txt)
When deployed within the justice system, AI technologies have serious implications for a person’s human rights and civil liberties. (extracted from223.txt)
Without transparency, there can not only be no scrutiny, but no 4 THE ADVE nT OF n EW TECH nOLOGIES I n THE JUSTICE SYSTEM accountability for when things go wrong. (extracted from223.txt)
Proper trials methodology is fully embedded into medical science but there are no minimum scientific or ethical standards that an AI tool must meet before it can be used in the criminal justice sphere. (extracted from223.txt)
5 THE ADVE nT OF n EW TECH nOLOGIES I n THE JUSTICE SYSTEM Yet without sufficient safeguards, supervision, and caution, advanced technologies may have a chilling effect on a range of human rights, undermine the fairness of trials, weaken the rule of law, further exacerbate existing inequalities, and fail to produce the promised effectiveness and efficiency gains. (extracted from223.txt)
The advent of new technologies in the justice system CHAPTER 1: INTRODUCTION 1. (extracted from223.txt)
Within the application of the law, we included a broad view of the justice system, examining instances where advanced tools were used to discover, deter, rehabilitate, or punish people who breach the law in England and Wales, as well as border management. (extracted from223.txt)
8 THE ADVE nT OF n EW TECH nOLOGIES I n THE JUSTICE SYSTEM Automated decision making (ADM): ADM is the process of making a decision by automated means without any human involvement. (extracted from223.txt)
Written evidence from Dr Miri Zilka, Dr Adrian Weller and Detective Sergeant Laurence Cartwright laid out some categories of tools used in the justice system. (extracted from223.txt)
9 THE ADVE nT OF n EW TECH nOLOGIES I n THE JUSTICE SYSTEM (b) Data analysis: software and tools primarily used to analyse data to create insights. (extracted from223.txt)
We heard the most about tools used by the Home Office, the Ministry of Justice, HM Prisons and Probation Service, and individual police forces. (extracted from223.txt)
We were told, for example, about the use of polygraphs to monitor sex offenders on parole and manage their level of compliance with parole conditions.14 8 Written evidence from Dr Miri Zilka, Detective Sergeant Laurence Cartwright and Dr Adrian Weller (nTL0040 ) 9 Written evidence from Association of Police and Crime Commissioners, national Police Chiefs’ Council and Police Digital Service ( nTL0049 ) 10 Written evidence from Avon and Somerset Police ( nTL0052 ) 11 Written evidence from the Serious Fraud Office ( nTL0034 ) 12 Written evidence from Public Law Project ( nTL0046 ) 13 Written evidence from Liberty ( nTL0020 ) 14 Written evidence from Dr Kyriakos n Kotsoglou ( nTL0007 ) 10 THE ADVE nT OF n EW TECH nOLOGIES I n THE JUSTICE SYSTEM 6. (extracted from223.txt)
11 THE ADVE nT OF n EW TECH nOLOGIES I n THE JUSTICE SYSTEM the end of which it would become costly for the customer to opt out. (extracted from223.txt)
Avon and Somerset Constabulary thought their use of data analytics placed “better insights into the hands of those delivering the business to help empower and support more effective decision making.”23 The Rt Hon Kit Malthouse MP, the Minister for Crime and Policing at the Home Office and Ministry of Justice, told us that he was “very excited about the use of artificial intelligence and machine learning in policing.”24 We also acknowledge that, as many submissions pointed out, advanced tools can provide substantial assistance towards enacting the crucial duties of the police to protect and prevent harm. (extracted from223.txt)
Matthew Gill, Senior Fellow at the Institute for Government, facilitated a seminar for us to consider the institutional and regulatory frameworks which 22 Q 45 (Professor Elizabeth Joh) 23 Written evidence from Avon and Somerset Police ( nTL0052 ) 24 Q 99 (Kit Malthouse MP) 25 Q 39 (Professor Elizabeth Joh) 26 Written evidence from SAS UK&I ( nTL0041 ) 27 Written evidence from the Information Commissioner’s Office ( nTL0016 ) 12 THE ADVE nT OF n EW TECH nOLOGIES I n THE JUSTICE SYSTEM may be put in place. (extracted from223.txt)
• In 2020, the Scottish Parliament Justice Sub-Committee on Policing published Facial recognition: how policing in Scotland makes use of this technology.30 • In 2021, the European Parliament Committee on Civil Liberties, Justice and Home Affairs published a report on “artificial intelligence in criminal law and its use by the police and judicial authorities in criminal matters”.31 • In 2021, nATO adopted its first Artificial Intelligence Strategy, including principles of the responsible use of AI in Defence and announcing further work to set international AI standards.32 • In 2021, U nESCO adopted a Recommendation on the Ethics of Artificial Intelligence and is working towards establishing the first-ever global normative instrument on the ethics of AI.33 15. (extracted from223.txt)
We decided to examine the use of these tools throughout the “criminal justice pipeline”34 and in border management, identifying where change was needed, and identifying some principles for the safe and ethical use of such tools. (extracted from223.txt)
(Report of Session 2017–19, HL Paper 100) 29 Council of Europe, CAHAI Ad Hoc Committee on Artificial Intelligence, ‘Terms of Reference’: https://www.coe.int/en/web/artificial-intelligence/cahai [accessed 6 February 2022] 30 The Scottish Parliament, Justice Sub-Committee on Policing, Facial Recognition: How Policing in Scotland Makes Use of This Technology (1st Report, Session 5, SP Paper 678) 31 Committee on Civil Liberties, Justice and Home Affairs, Report on artificial intelligence in criminal law and its use by the police and judicial authorities in criminal matters (13 July 2021): https://www.europarl. (extracted from223.txt)
org/ark:/48223/pf0000380455 [accessed 6 February 2022] 34 Written evidence from Dr Miri Zilka, Detective Sergeant Laurence Cartwright and Dr Adrian Weller (nTL0040 ) 13 THE ADVE nT OF n EW TECH nOLOGIES I n THE JUSTICE SYSTEM Chapter 3, we look at transparency: its necessity and proposals to increase it. (extracted from223.txt)
The key issues we have identified, however, hold true for a much wider context: their application to all functions of the justice system and to border management. (extracted from223.txt)
14 THE ADVE nT OF n EW TECH nOLOGIES I n THE JUSTICE SYSTEM CHAPTER 2: LEGAL AND INSTITUTIONAL FRAMEWORKS 18. (extracted from223.txt)
It was also indicated that “public failures” could “lead to not just operational defects or inefficiencies, but miscarriages of justice”,42 and that where weaknesses were exposed, they “exacerbate the low level and negative trend in public trust for relevant technology”.43 Professor n igel Harvey and Tobias Harvey referred to accountability for errors and misuse, saying that the use of algorithms “may leave people open to dangers for which no person can be identified as responsible”.44 “A chilling effect”45 22. (extracted from223.txt)
Various contributors told us that the use of some technologies, notably the use of live facial recognition, created fear or disquiet, and that this risked 35 Written evidence from the Home Office ( nTL0055 ) 36 QQ 103–104 (Kit Malthouse MP) 37 Royal Court of Justice , R v The Chief Constable of South Wales Police , [2020] EWCA Civ 1058 . (extracted from223.txt)
(nTL0022 ) 43 Written evidence from Archie Drake and Perry Keller ( nTL0011 ) 44 Written evidence from Professor n igel Harvey and Tobias Harvey ( nTL0025 ) 45 Written evidence from Dr Joe Purshouse, Dr n essa Lynch, Dr Marcin Betkier and Professor Liz Campbell ( nTL0021 ) 15 THE ADVE nT OF n EW TECH nOLOGIES I n THE JUSTICE SYSTEM damaging the democratic process. (extracted from223.txt)
16 THE ADVE nT OF n EW TECH nOLOGIES I n THE JUSTICE SYSTEM The right to a fair trial 23. (extracted from223.txt)
We were concerned that, in some instances, the use of advanced tools at certain points of the criminal justice pipeline may impede an individual’s right to a fair trial: whether by a lack of awareness that they were being used, unreliable evidence, or an inability to understand and therefore challenge proceedings. (extracted from223.txt)
Kotsoglou ( nTL0006 ) 55 Written evidence from Big Brother Watch ( nTL0037 ) 17 THE ADVE nT OF n EW TECH nOLOGIES I n THE JUSTICE SYSTEM discrimination”,56 and Professor n igel Harvey and Tobias Harvey wrote that “learning algorithms based on historical data would preserve bias”.57 28. (extracted from223.txt)
56 Written evidence from Liberty ( nTL0020 ) 57 Written evidence from Professor n igel Harvey and Tobias Harvey ( nTL0025 ) 58 Q 60 (Professor Karen Yeung) 59 Written evidence from Liberty ( nTL0020 ) 18 THE ADVE nT OF n EW TECH nOLOGIES I n THE JUSTICE SYSTEM Figure 1: Predictive policing—a vicious circle? (extracted from223.txt)
We have noted over 30 public bodies, initiatives, and programmes playing a role in the governance of new technologies for the application of the law 60 Q 99 (Kit Malthouse MP) 19 THE ADVE nT OF n EW TECH nOLOGIES I n THE JUSTICE SYSTEM (see Box 4). (extracted from223.txt)
gov.uk/government/uploads/system/uploads/attachment_data/file/1020402 [accessed 24 February 2022] 63 Centre for Data Ethics and Innovation, ‘About us’: https://www.gov.uk/government/organisations/ centre-for -data-ethics-and-innovation/about [accessed 6 February 2022] 64 HM Government, AI Council: https://www.gov.uk/government/ groups/ai-council [accessed 6 February 2022] 20 THE ADVE nT OF n EW TECH nOLOGIES I n THE JUSTICE SYSTEM Box 4: List of entities and programmes • Her Majesty’s Inspectorate of Constabulary and Fire and Rescue Services • The AI Council • The Association of Police and Crime Commissioners (APCC), and its various working groups and initiatives, including the APCC Biometrics and Data Ethics Working Group • The Biometrics and Forensics Ethics Group • The Biometrics and Surveillance Camera Commissioner • The Centre for Data Ethics and Innovation • The College of Policing • The Data Analytics Community of Practice • The Equalities and Human Rights Commission • The Forensic Science Regulator • The Home Office Digital, Data and Technology function • The Independent Office for Police Conduct • The Information Commissioner’s Office • The n ational Crime Agency, and its TRACER programme • The n ational Data Analytics Solution • The n ational Digital and Data Ethics Guidance Group • The n ational Digital Exploitation Centre • The national Police Chiefs’ Council, and its eleven co-ordination committees, each responsible for a specific aspect related to new technologies • The n ational Police Ethics Group • The n ational Policing Chief Scientific Adviser • The Office for AI • The Police Digital Service, its Data Office and Chief Data Officer • The Police Rewired initiative • The Police Science, Technology, Analysis and Research (STAR) fund • The Police, Science, and Technology Investment Board • The Royal Statistical Society • The Science Advisory Council to the national Policing Chief Scientific Adviser • The Senior Data Governance Panel within the Ministry of Justice • The specialist and generalist ethics committees of some police forces • The Tackling Organised Exploitation programme 21 THE ADVE nT OF n EW TECH nOLOGIES I n THE JUSTICE SYSTEM 33. (extracted from223.txt)
65 Q 98 (Professor Paul Taylor) 66 Written evidence from Association of Police and Crime Commissioners, national Police Chiefs’ Council and Police Digital Service ( nTL0049 ) 67 Department for Digital, Culture, Media & Sport, Data: A new direction (10 September 2021), para 409: https://assets.publishing.service.gov.uk/government/uploads/system/uploads/attachment_data/ file/1022315/Data_Reform_Consultation_Document__Accessible_.pdf [accessed 28 January 2022] 68 Q 99 (Kit Malthouse MP) 69 Q 108 (Kit Malthouse MP) 22 THE ADVE nT OF n EW TECH nOLOGIES I n THE JUSTICE SYSTEM Figure 2: “Family tree” of relevant governance arrangements Chief Constable Operational decisionSLT, ethics committee, technical experts, etc.Police and Crime Commissioner Oversight and strategic decision makingPublic Public consultation APCC Support, sharing best practice NPCC Coordination Committees 11 Committees and their members, some of which are national leads for new technologies, bring forces together to coordinate, reform, improve and provide value for money. (extracted from223.txt)
Dr Christopher Lawless referred to a series of bodies that all play “key roles in oversight” but which “vary in their remit and the extent of their powers”.70 Robin Allen QC and Dee Masters believed that “there has been too much thinking in ‘silos’”.71 There may also be confusion over responsibilities—Dr Lawless gave the example of facial recognition technology 70 Written evidence from Dr Christopher Lawless ( nTL0029 ) 71 Written evidence from Robin Allen QC and Dee Masters ( nTL0019 ) 23 THE ADVE nT OF n EW TECH nOLOGIES I n THE JUSTICE SYSTEM to argue that there is a “potentially significant lacuna [in governance] where it is unclear who is statutorily responsible for regulation and oversight”.72 36. (extracted from223.txt)
79 Q 99 (Kit Malthouse MP), see also Home Office, New Biometrics and Surveillance Camera Commissioner appointed (15 March 2021): https://www.gov.uk/government/news/new-biometrics-and-surveillancecamera -commissioner-appointed [accessed 28 January 2022] 80 Q 84 (Professor Paul Taylor) 24 THE ADVE nT OF n EW TECH nOLOGIES I n THE JUSTICE SYSTEM • In September 2021, the Government published its National Artificial Intelligence Strategy . (extracted from223.txt)
While the Minister said that, as a Minister in both the Home Office and the Ministry of Justice, he was the “living embodiment” of crossdepartmental working, this does not appear to have impacted strategic 81 Department for Business, Energy & Industrial Strategy, ‘Guidance national AI Strategy’ (22 September 2021): https://www.gov.uk/government/publications /national-ai-strategy/national-aistrategy-html-version [accessed 1 February 2022] 82 Department for Digital, Culture, Media & Sport, Data : A new direction , para 409. (extracted from223.txt)
83 DCMS Consultation: ‘Data: A new direction’ Response by the Biometrics and Surveillance Camera Commissioner: https://assets.publishing.service.gov.uk/government/uploads/system/uploads/ attachment _data/file/1030248/BSCC_DCMS_Consultation_Response.pdf [accessed 1 February 2022] 84 Written evidence from Association of Police and Crime Commissioners, national Police Chiefs’ Council and Police Digital Service ( nTL0049 ) 85 HL Deb, 3 november 2021, cols 1301–1305 25 THE ADVE nT OF n EW TECH nOLOGIES I n THE JUSTICE SYSTEM thinking on the use of new technologies in applying the law, and there is no indication of any collective governmental effort towards a single strategy.86 On the contrary, there are many indications of siloed thinking. (extracted from223.txt)
Similarly, BEIS and DCMS are collectively responsible for the implementation of the National AI Strategy , which focuses on businesses and the benefits of innovation and does not appear to have considered the needs of the Ministry of Justice or the Home Office at any length, or AI’s potential in their sectors. (extracted from223.txt)
David Tucker, Faculty Lead on Crime and Criminal Justice at the College of Policing, told us: “We have seen that where decisions are challenged or doubted cases go to court and affect the way policing operates. (extracted from223.txt)
The Appeal Court said that there was an absence of policy, so we are filling that gap and moving to apply these principles to this piece of technology”.87 86 Q 100 (Kit Malthouse MP) 87 Q 89 (David Tucker) 26 THE ADVE nT OF n EW TECH nOLOGIES I n THE JUSTICE SYSTEM Box 5: The Bridges case and the Public Sector Equality Duty 1. (extracted from223.txt)
88 Ministry of Justice, Public sector equality duty (6 July 2021): https://www.gov.uk/government/ publications/public-sector-equality-duty [accessed 4 February 2022] 89 Royal Court of Justice, R v The Chief Constable of South Wales Police , [2020] EWCA Civ 1058 90 Q 110 (Kit Malthouse MP) 91 Q 92 (Alun Michael) 27 THE ADVE nT OF n EW TECH nOLOGIES I n THE JUSTICE SYSTEM We have also been told that both domestic courts and the European Court of Human Rights had been relied upon in the past.92 49. (extracted from223.txt)
(nTL0022 ) 93 Written evidence from n CC Group ( nTL0005 ) 94 Written evidence from the Bar Council ( nTL0048 ) 95 Written evidence from n CC Group ( nTL0005 ) 96 Written evidence from Archie Drake and Perry Keller ( nTL0011 ) 97 Written evidence from the Home Office ( nTL0055 ) 98 Q 73 (David Lewis) 99 Written evidence from n CC Group ( nTL0005 ) 100 Q 36 (Dr David Leslie) 28 THE ADVE nT OF n EW TECH nOLOGIES I n THE JUSTICE SYSTEM of the EU Commission’s proposed AI regulation” (see Box 6). (extracted from223.txt)
Robin Allen QC and Dee Masters argued that “public actors and private companies need clear, pragmatic and effective regulatory frameworks because it provides a safety net within which ‘good’ AI can be developed whilst also protecting the fundamental rights of the public.”104 Archie Drake and Perry Keller had a similar view, stating that “legal uncertainty tends to harm business and innovation as well as public trust in the criminal justice system (and technology).”105 In a joint submission, three police bodies wrote that “Government should seek to clarify public appetite for new technologies and legislate so that policing has a clearer basis on which to make policies and decisions about deployment.”106 55. (extracted from223.txt)
Professor Sandra Wachter, Associate Professor at the University of Oxford, thought that “soft regulation would be irresponsible” because the criminal justice system is “one of the most highrisk areas [she] can think of”.107 Dr Joe Purshouse and his co-contributors reflected that while guidance documents may be cited in court, they “do not provide actionable grounds for an individual to make a complaint”, adding that “non-compliance would not impact on the admissibility of any material gleaned.”108 101 Written evidence from Public Law Project ( nTL0046 ) 102 Written evidence from Professor Pete Fussey, Dr Daragh Murray and Dr Amy Stevens ( nTL0017 ) 103 Ibid. (extracted from223.txt)
104 Written evidence from Robin Allen QC and Dee Masters ( nTL0019 ) 105 Written evidence from Archie Drake and Perry Keller ( nTL0011 ) 106 Written evidence from the Association of Police and Crime Commissioners (APCC), national Police Chiefs’ Council ( nPCC), and Police Digital Service (PDS) ( nTL0049 ) 107 Q 73 (Dr Liam Owens and Professor Sandra Wachter) 108 Written evidence from Dr Joe Purshouse, Dr n essa Lynch, Dr Marcin Betkier and Professor Liz Campbell ( nTL0021 ) 29 THE ADVE nT OF n EW TECH nOLOGIES I n THE JUSTICE SYSTEM Box 6: The EU Artificial Intelligence Regulation Proposal 1. (extracted from223.txt)
30 THE ADVE nT OF n EW TECH nOLOGIES I n THE JUSTICE SYSTEM of the national Police Chiefs’ Council, who, among others, thought that any new legal framework should be adopted at national level, but that “it would be helpful if it was not too divergent from international regulation”.115 The practicalities 58. (extracted from223.txt)
Others favoured a value- and principles-based approach, with Alun Michael saying that “the values and principles need to be established in law”.118 As our witnesses pointed out, “certain things with AI will always be the same … we will always have a data issue, a bias issue and an explainability issue.”119 Professor Raab similarly told us that among the “plethora” of guidance, research and reviews, a consensus had emerged on some principles: “privacy protection, accountability, fairness, non-discrimination, justice, transparency, safety and cybersecurity, serving the common good, explainability, and human oversight”.120 As we highlighted in paragraph 56, the Minister himself said that the Government’s preferred approach was to produce a set of principles—our view is that these should be translated into statute. (extracted from223.txt)
We 115 Q 73 (David Lewis) 116 Written evidence from n CC Group ( nTL0005 ) 117 Written evidence from Professor Pete Fussey, Dr Daragh Murray and Dr Amy Stevens ( nTL0017 ) 118 Q 98 (Alun Michael) 119 Q 69 (Professor Sandra Wachter) 120 Written evidence from Professor Charles Raab ( nTL0014 ) 31 THE ADVE nT OF n EW TECH nOLOGIES I n THE JUSTICE SYSTEM acknowledge the demands that the development of new legislation would place on parliamentary time and Government capacity, and that legislation is not a ‘quick fix’. (extracted from223.txt)
32 THE ADVE nT OF n EW TECH nOLOGIES I n THE JUSTICE SYSTEM 64. (extracted from223.txt)
The Home Office told us about guidance on the CAID programme (the Child Abuse Image Database, a facial recognition tool which helps identify victims and offenders)126, while guidance on facial recognition is currently being developed by the College of Policing, and the Ministry of Justice is working with the Alan Turing Institute to extend existing guidance on the use of data-driven technologies within the justice system.127 There is also various guidance available from the Surveillance Camera and Information Commissioners128, and a host of guidance from non-governmental sources such as the ALGO-care framework (a practical decision-making framework for the policing context).129 The Metropolitan Police Service noted that the application of the “variety of guidance, opinion, codes, directions and proposals for ethical frameworks … risks confusion and inconsistency”.130 126 Written evidence from the Home Office ( nTL0055 ) 127 College of Policing, ‘Police use of live facial recognition technology—have your say’ (17 May 2021): https://www.college.police.uk/article/police-use -live-facial-recognition-technology-have-your-say [accessed 26 January 2022] and written evidence from the Ministry of Justice ( nTL0053 ) 128 Information Commissioner’s Office, ‘Guidance index’: https://ico.org .uk/for-organisations/guidanceindex/ and Biometrics and Surveillance Camera Commissioner, ‘Surveillance camera guidance, tools and templates’ (22 October 2018): https://www.gov. (extracted from223.txt)
uk/government/collections/surveillance-cameraguidance-tools-and-templates [accessed 7 February 2022] 129 Marion Oswald, ‘Algorithmic risk assessment policing models: lessons from the Durham HART model and ‘Experimental’ proportionality’, Information & Communications Technology Law , vol.27, (3 April 2018): https://www.tandfonline.com/doi/full/ 10.1080/13600834.2018.1458455 [accessed 7 February 2022] 130 Written evidence from the Metropolitan Police Service ( nTL0031 ) 33 THE ADVE nT OF n EW TECH nOLOGIES I n THE JUSTICE SYSTEM Box 9: Application of the Equality Act 2010 • To illustrate the confusion in the application of law, several submissions referred to the Equality Act 2010. (extracted from223.txt)
As Professor Raab pointed out, comprehensive and practical guidance on 131 Written evidence from Archie Drake and Perry Keller ( nTL0011 ) 132 Cloisters, In the matter of automated data processing in Government decision making (7 September 2019): https://www.cloisters.com/ wp-content/uploads/2019/10/Open-opinion-pdf-version-1.pdf [accessed 25 January 2022] 133 Centre for Data Ethics and Innovation, Review into bias in algorithmic decision-making (november 2020), p 12: https://assets.publishing.service.gov.uk/government/uploads/ system/uploads/attachment_data/ file/957259/Review_into_bias_in_ algorithmic_decision-making.pdf [accessed 2 February 2022] 134 Written evidence from Professor Pete Fussey, Dr Daragh Murray and Dr Amy Stevens ( nTL0017 ) 135 Written evidence from Professor Charles Raab ( nTL0014 ) 136 Written evidence from Robin Allen QC and Dee Masters ( nTL0019 ) 137 College of Policing, ‘APP content’ (4 n ovember 2015): https://www.app.college.police.uk/appcontent/ [accessed 4 February 2022] 138 Written evidence from the Serious Fraud Office ( nTL0034 ) 34 THE ADVE nT OF n EW TECH nOLOGIES I n THE JUSTICE SYSTEM the use of types of technologies will require consistent review and ongoing updates as tools are used in operational settings, and practical operational issues identified.139 It could not therefore be expected that such guidance will ever tackle all of the specificities of particular tools. (extracted from223.txt)
The Government should require that national guidance for the use of advanced technological tools in policing and criminal justice is drawn up and, as part of their response to this report, should outline concrete plans for this. (extracted from223.txt)
142 Written evidence from the Association of Police and Crime Commissioners, n ational Police Chiefs’ Council, and Police Digital Service ( nTL0049 ) 143 Written evidence from the Law Society of England and Wales ( nTL0023 ) 144 Written evidence from Professor Pete Fussey, Dr Daragh Murray and Dr Amy Stevens ( nTL0017 ) 145 Written evidence from BAE Systems ( nTL0056 ) 146 Written evidence from the Information Commissioner’s Office ( nTL0016 ) 35 THE ADVE nT OF n EW TECH nOLOGIES I n THE JUSTICE SYSTEM potential influence on individuals, groups and society”147, this trust is critical. (extracted from223.txt)
Kit Malthouse MP said that forces must be allowed to fail “before we jump on everything.” It is important to note that the Minister was speaking in this context about technology which proves “not to be terribly useful”152, rather than about a failure to comply with minimum standards or where a miscarriage of justice had occurred. (extracted from223.txt)
In particular, they thought that the Home Secretary, the Lord Chancellor and Secretary of State for Justice, and the Minister for Crime and Policing should be answerable for “how the Government’s vision of technological change in the system safeguards its effectiveness and legitimacy.”156 The Minister for Crime and Policing agreed that he, and Government as a whole, are “broadly—whether [they] like it or not—responsible for most things.”157 147 Written evidence from Dr Matthias Wienroth et al. (extracted from223.txt)
(nTL0022 ) 148 Committee on Standards in Public Life, ‘The Seven Principles of Public Life’ (31 May 1995): https:// www.gov.uk/government/publications/the-7-principles-of- public-life/the-7-principles-of-publiclife--2 [accessed 27 January 2022] 149 Written evidence from Public Law Project ( nTL0046 ) 150 Q 72 (Dr Liam Owens) 151 Q 76 (Dr Liam Owens) 152 Q 106 (Kit Malthouse MP) 153 Written evidence from Privacy International ( nTL0051 ) 154 Written evidence from Dr Miri Zilka, Detective Sergeant Laurence Cartwright and Dr Adrian Weller (nTL0040 ) and Q 31 (Professor Michael Wooldridge) 155 Written evidence from the Bar Council ( nTL0048 ) 156 Written evidence from Archie Drake and Perry Keller ( nTL0011 ) 157 Q 107 (Kit Malthouse MP) 36 THE ADVE nT OF n EW TECH nOLOGIES I n THE JUSTICE SYSTEM • Chief Constables. (extracted from223.txt)
(nTL0012 ) 167 Written evidence from n CC Group ( nTL0005 ) 168 Written evidence from BAE Systems ( nTL0056 ) 169 Written evidence from Dr Christopher Lawless ( nTL0029 ) 37 THE ADVE nT OF n EW TECH nOLOGIES I n THE JUSTICE SYSTEM Lack of recourse 82. (extracted from223.txt)
174 Written evidence from Robin Allen QC and Dee Masters ( nTL0019 ) 175 Written evidence from Liberty ( nTL0020 ) and Big Brother Watch ( nTL0037 ) 38 THE ADVE nT OF n EW TECH nOLOGIES I n THE JUSTICE SYSTEM to ban “clearly harmful or high-risk applications of technology” which lack robust accountability arrangements.176 87. (extracted from223.txt)
176 Written evidence from Archie Drake and Perry Keller ( nTL0011 ) 177 Written evidence from the Metropolitan Police Service ( nTL0031 ) 178 Committee on Civil Liberties, Justice and Home Affairs, Report on artificial intelligence in criminal law and its use by the police and judicial authorities in criminal matters (13 July 2021): https://www.europarl. (extracted from223.txt)
europa.eu/doceo/document/A-9-2021–0232_ En.html [accessed 3 February 2022] 179 United n ations Human Rights Office of the High Commissioner, Artificial intelligence risks to privacy demand urgent action — Bachelet (15 September 2021): https://www.ohchr.org/en/2021/09/artificialintelligence-risks-privacy-demand-urgent-action-bachelet [accessed 3 February 2022] 39 THE ADVE nT OF n EW TECH nOLOGIES I n THE JUSTICE SYSTEM CHAPTER 3: TRANSPARENCY 90. (extracted from223.txt)
(nTL0022 ) 185 Written evidence from Public Law Project ( nTL0046 ), see also written evidence from Dr Joe Purshouse, Dr n essa Lynch, Dr Marcin Betkier and Professor Liz Campbell ( nTL0021 ) 186 Written evidence from Robin Allen QC and Dee Masters ( nTL0019 ), see also Q 65 (Peter Dawson) 187 Q 78 (Professor Sandra Wachter) 188 Written evidence from The Bar Council ( nTL0048 ) 189 Written evidence from Public Law Project ( nTL0046 ) 190 Q 57 (Professor Karen Yeung) 40 THE ADVE nT OF n EW TECH nOLOGIES I n THE JUSTICE SYSTEM of particular technologies but also for ensuring that the decision-making process for the use of technology is open to public scrutiny.”191 94. (extracted from223.txt)
The Home Office told us that they were “supporting law enforcement organisations to address … the need for transparency”,192 and that “policing is committed to being transparent.”193 The Ministry of Justice also informed us about an annual review of “analytical algorithms—only a small subset of [which] involve data and decisions about individuals”. (extracted from223.txt)
196 Written evidence from Robin Allen QC and Dee Masters ( nTL0019 ) 197 Written evidence from the Metropolitan Police Service ( nTL0031 ) 41 THE ADVE nT OF n EW TECH nOLOGIES I n THE JUSTICE SYSTEM the use of technological solutions as they cannot know who is using what, for how long, for what purpose, or with what safeguards. (extracted from223.txt)
This risks undermining trust in the police, the justice system, and the rule of law. (extracted from223.txt)
42 THE ADVE nT OF n EW TECH nOLOGIES I n THE JUSTICE SYSTEM to constantly refine and refresh the model to comply with appropriate ethical and legal oversight and governance.”203 While the full study is expected to be published (at a date yet to be confirmed), there are no commitments as to what information it will contain. (extracted from223.txt)
Box 10: Previous support for a register • In 2018, the House of Commons Science and Technology Committee recommended that “the Government should produce, publish, and maintain a list of where algorithms with significant impacts are being used within Central Government, along with projects underway or planned for public service algorithms”.207 • A 2019 report by the Law Society of England and Wales concluded that “a national register of algorithmic systems in the criminal justice system should be created”.208 203 Police Professional, ‘Artificial intelligence ‘marginally better’ at predicting re-offending’ (25 January 2022): https://www.policeprofessional.com/ news/artificial-intelligence-marginally-better-at-predict ing-reoffending/ [accessed 24 February 2022] 204 Q 65 (Silkie Carlo, Peter Dawson, Professor Karen Yeung) and Q 78 (David Lewis, Dr Liam Owens, Professor Sandra Wachter) 205 Written evidence from Robin Allen QC and Dee Masters ( nTL0019 ) 206 Written evidence from Professor Colin Gavaghan ( nTL0047 ), see also written evidence from Archie Drake and Perry Keller ( nTL0011 ) and Professor Lilian Edwards, Professor Derek McAuley, Dr Lachlan Urquhart and Dr Jiahong Chen ( nTL0035 ). (extracted from223.txt)
207 Science and Technology Committee, Algorithms in decision-making (Fourth Report, Session 2017– 2019, HC 351) 208 Q 11 (Professor Sylvie Delacroix) see also the Law Society of England and Wales, Algorithm use in the criminal justice system report , p 66: https://www.lawsociety.org .uk/en/topics/research/algorithm-use-inthe-criminal-justice -system-report [accessed 10 January 2022]. (extracted from223.txt)
43 THE ADVE nT OF n EW TECH nOLOGIES I n THE JUSTICE SYSTEM • A 2020 report by the Royal United Services Institute (RUSI) commissioned by the Centre for Data Ethics and Innovation (CDEI) found that “the nPCC and APCC should … maintain a high-level catalogue for all algorithms used by police forces nationwide”.209 David Lewis told us that the n PCC had recently accepted this recommendation, although caveating that “there is a matter of degree to be debated”.210 There is no indication that such a catalogue would be published. (extracted from223.txt)
One contributor thought that “it is not always feasible or even desirable to make algorithms in criminal justice fully transparent.”214 In the following paragraphs we examine those arguments. (extracted from223.txt)
For instance, the information published on a register “could be used to infer how a [Machine Learning] model would make a specific legal decision, and thus what inputs could be crafted to manipulate a desired legal 209 RUSI, Data Analytics and Algorithms in Policing in England and Wales (February 2020), p xi: https:// static.rusi.org/rusi_pub_165_2020_01_ algorithmic_policing_babuta_final_web_copy.pdf [accessed 21 January 2022] 210 Q 78 (David Lewis) 211 Centre for Data Ethics and Innovation, Review into bias in algorithmic decision making ( november 2020): https://assets.publishing.service.gov.uk/government/uploads/ system/uploads/attachment_data/ file/957259/Review_into_bias_in_ algorithmic_decision-making.pdf [accessed 21 January 2022] 212 Commission on Race and Ethnic Disparities, Independent report, Forward, introduction and full recommendations , (28 April 2021): https://www.gov.uk/ government/publications/the-report-of-thecommission-on-race-and- ethnic-disparities/foreword-introduction-and-full-recommendations#fullrecommendations [accessed 10 January 2022] 213 Written evidence from the Independent Office for Police Conduct ( nTL0054 ) 214 Written evidence from Dr Miri Zilka, Detective Sergeant Laurence Cartwright and Dr Adrian Weller (nTL0040 ) 215 Q 78 (Dr Liam Owens) 216 Written evidence from BAE Systems ( nTL0056 ) 217 Q 78 (Professor Sandra Wachter) and Q 73 (Professor Sandra Wachter) 44 THE ADVE nT OF n EW TECH nOLOGIES I n THE JUSTICE SYSTEM outcome.”218 Indeed, several witnesses were worried that a technological solution “could be ‘gamed’ by criminals” if algorithms were published.219 This is what BAE Systems calls “data poisoning”220 and the nCC Group calls “adversarial Machine Learning”.221 107. (extracted from223.txt)
When some circumscribed their recommendation to “the application of the law”228 or “the criminal justice system”229 only, others advised that it should cover “the public sector”230 or “Government”231 in general. (extracted from223.txt)
226 Written evidence from the Information Commissioner’s Office ( nTL0016 ) 227 Written evidence from Professor Colin Gavaghan ( nTL0047 ) 228 Written evidence from Big Brother Watch ( nTL0037 ) 229 Q 11 (Professor Sylvie Delacroix) 230 Q 65 (Professor Karen Yeung) 231 Written evidence from the Public Law Project ( nTL0046 ) 232 Written evidence from Professor Colin Gavaghan ( nTL0047 ) 233 Written evidence from Big Brother Watch ( nTL0037 ) 45 THE ADVE nT OF n EW TECH nOLOGIES I n THE JUSTICE SYSTEM • The Public Law Project considered that each entry in the register should be accompanied with “executable versions of listed algorithms” and an explanation of how the technology works.234 • Citing the EU’s proposed AI Regulation currently being discussed within the European Union as a reference (see Box 6), Professor Sandra Wachter suggested that the register could include algorithms themselves, the data on which they are trained, as well as information on tests carried out and on oversight mechanisms.235 • Several witnesses asked for “detailed impact assessments”236 to be included, such as Equality Impact Assessments237 or Human Rights Impact Assessments.238 The Algorithmic Transparency Standard 109. (extracted from223.txt)
240 Cabinet Office, UK government publishes pioneering standard for algorithmic transparency 46 THE ADVE nT OF n EW TECH nOLOGIES I n THE JUSTICE SYSTEM allow public bodies the time to submit entries without diverting effort away from operational activities. (extracted from223.txt)
47 THE ADVE nT OF n EW TECH nOLOGIES I n THE JUSTICE SYSTEM CHAPTER 4: HUMAN-TECHNOLOGY INTERACTIONS 114. (extracted from223.txt)
( nTL0012 ) 248 Article 22 of Regulation (EU) 2016/679 of 23 April 2016 on the protection of natural persons with regard to the processing of personal data and on the free movement of such data, and repealing Directive 95/46/EC (General Data Protection Regulation) (Article 22) 4 May 2016 ( OJ L 119/1 ), see written evidence from Professor n igel Harvey and Tobias Harvey ( nTL0025 ) see also written evidence from Professor Lilian Edwards, Professor Derek McAuley, Dr Lachlan Urquhart and Dr Jiahong Chen ( nTL0035 ) 48 THE ADVE nT OF n EW TECH nOLOGIES I n THE JUSTICE SYSTEM enforcement body may not take a qualifying significant decision based solely on automated processing unless that decision is required or authorised by law.249 These provisions aim to guarantee that there is a “human in the loop” but only for narrowly specified decisions. (extracted from223.txt)
251 Oral evidence taken on 27 October 2021 (Session 2021–22) Q 13 (The Rt Hon Priti Patel MP, Home Secretary) 252 Q 86 (Professor Paul Taylor) 253 See, for example, written evidence from the Law Society of England and Wales ( nTL0023 ), Big Brother Watch ( nTL0037 ) and the Public Law Project ( nTL0046 ) 254 Q 33 (Professor Michael Wooldridge) 255 European Commission, Guidelines on Automated individual decision making and Profiling for the purposes of regulation 2016/679 (wp251rev.01) , 22 August 2018: https://ec.europa.eu/newsroom/article29/ redirection/document/49826 [accessed 24 January 2022] 49 THE ADVE nT OF n EW TECH nOLOGIES I n THE JUSTICE SYSTEM • reviewers must ‘weigh-up’ and ‘interpret’ the recommendation, consider all available input data, and also take into account other additional factors.”256 122. (extracted from223.txt)
Evidence from the Ministry of Justice stated that “operational decisions are informed by analytical tools rather than being automatic consequences of tool outputs.”257 Similarly, the Home Office stated that they “strongly disagree … that we are allowing sensitive decisions to be delegated to machines in a way that is either contrary to the law or the core principles of the [criminal justice system]”.258 Interactions to date 123. (extracted from223.txt)
We were told, for instance, that attendees to an event about facial recognition (many of whom had access to and used facial recognition technology) “had a limited understanding of both face recognition technology and human face recognition.”264 A supplier of some tools had found that “criminal justice professionals are typically also lacking an expert, or even good, understanding.”265 Dee Masters and Robin Allen QC had, similarly, “become increasingly aware of the lack of understanding by regulators and the general public of the way in which AI systems are being used in their field of activity.”266 256 Information Commissioner’s Office, ‘Guidance on AI and data protection’: https://ico.org.uk/for organisations/guide-to-data-protection/key-dp-themes/guidance-on -artificial-intelligence-and-dataprotection/ [accessed 17 January 2022] 257 Written evidence from the Ministry of Justice ( nTL0053 ) 258 Written evidence from the Home Office ( nTL0055 ) 259 Written evidence from BAE systems ( nTL0056 ) 260 Q 8 (Professor Charles Raab) 261 Written evidence from Professor n igel Harvey and Tobias Harvey ( nTL0025 ) 262 Written evidence from n CC Group ( nTL0005 ) 263 Written evidence from Archie Drake and Perry Keller ( nTL0011 ) 264 Written evidence from Dr Eilidh n oyes and Dr Reuben Moreton ( nTL0026 ) 265 Written evidence from SAS UK&I ( nTL0041 ) 266 Written evidence from Robin Allen QC and Dee Masters ( nTL0019 ) 50 THE ADVE nT OF n EW TECH nOLOGIES I n THE JUSTICE SYSTEM 125. (extracted from223.txt)
Professor Carole McCartney, Professor of Law and Criminal Justice at the University of northumbria, explained this further: “If the humans do not understand the technology and how it is working, how will they spot if it has failed or if they have made a mistake? (extracted from223.txt)
The Prison Reform Trust were similarly concerned about confidence to challenge outcomes which might appear discriminatory: “managers and policy makers are likely to be less inclined to ‘look under the bonnet’ when the technology they find there is unfamiliar.”268 The lack of understanding does not only apply to the people who are using the tool, but to those who commission it—and those who interact with its outputs later ‘down the justice pipeline’, including judiciary reviewing the conduct and findings of an investigation. (extracted from223.txt)
Amy Stevens ( nTL0017 ) 270 Police Professional, Artificial intelligence ‘marginally better’ at predicting re-offending (25 January 2022): https://www.policeprofessional.com/ news/artificial-intelligence-marginally-better-at-predictingreoffending/ [accessed 24 February 2022] 51 THE ADVE nT OF n EW TECH nOLOGIES I n THE JUSTICE SYSTEM • An automated triage system used by the Home Office, known as the sham marriage algorithm, is used to help “determine whether a proposed marriage should be investigated as a ‘sham’”. (extracted from223.txt)
uk /24946/1/ London-Met-Police-Trial-of-Facial-Recognition-Tech- Report-2.pdf [accessed 7 February 2022] 52 THE ADVE nT OF n EW TECH nOLOGIES I n THE JUSTICE SYSTEM 128. (extracted from223.txt)
The Home Office should, in conjunction with the Ministry of Justice and the College of Policing, undertake or commission appropriate research to determine how the use of predictive algorithms affects decision making, and under what circumstances meaningful human interaction is most likely. (extracted from223.txt)
283 Q 87 (David Tucker) 284 Written evidence from the Public Law Project ( nTL0046 ) 285 Written evidence from Dr Eilidh n oyes and Dr Reuben Moreton ( nTL0026 ) 286 Q 70 (David Lewis) 287 Q 58 (Peter Dawson) 53 THE ADVE nT OF n EW TECH nOLOGIES I n THE JUSTICE SYSTEM ongoing288, and regularly reviewed.289 It could also address a skills shortage in the workforce: references were made in the evidence to the difficulty in attracting employees highly skilled in technological tools who can command extremely high salaries in the private sector.290 134. (extracted from223.txt)
David Tucker, Head of Criminal Justice at the College of Policing, told the Committee that “we have to wait for a moment of maturity, because if we do not we run the risk of trying to give guidance on something that has not settled down and is developing.”291 We are unconvinced by this argument. (extracted from223.txt)
There is also ensuring that we have systems and processes in place so that does not occur.”293 The Ministry of Justice also outlined some of its safeguarding systems, which included “clear guidance” for “when the data should and should not be used, and support (and sometimes training) … made available to staff”.294 136. (extracted from223.txt)
It would not be reasonable to expect every police officer, or every Ministry of Justice official (to take but two examples), to be trained in data analytics and in the specificities of sophisticated technological solutions. (extracted from223.txt)
(nTL0012 ) 290 Written evidence from the Serious Fraud Office ( nTL0034 ) 291 Q 91 (David Tucker) 292 RUSI, ‘Data analytics and algorithms in policing in England and Wales: Towards a new policy framework’ (2020): https://rusi.org/explore-our-research/publications/occasional-papers/data-analyti cs-and-algorithms-policing-england-and-wales-towards-new-policy-framework [accessed 9 March 2022] 293 Q 89 (Professor Paul Taylor) 294 Written evidence from the Ministry of Justice ( nTL0053 ) 54 THE ADVE nT OF n EW TECH nOLOGIES I n THE JUSTICE SYSTEM a part,295 and thus also need a thorough understanding of how algorithmic tools work. (extracted from223.txt)
As part of continuing professional development, training should also be made available to lawyers, members of the Judiciary, and other professionals involved in the justice system. (extracted from223.txt)
The Ministry of Justice referred to their sexual offender predictive tool, which is supported by an “overarching” policy framework to “support consistency of use”.298 Evidence from the Prison Reform Trust acknowledged the need for structures and policies enabling a clear route for challenge, but emphasised that these must work well in practice. (extracted from223.txt)
295 See paras 23–26 296 Independent Chief Inspector of Borders and Immigration, An inspection of entry clearance processing operations in Croydon and Istanbul 297 Written evidence from Professor Colin Gavaghan ( nTL0047 ) 298 Written evidence from the Ministry of Justice ( nTL0053 ) 55 THE ADVE nT OF n EW TECH nOLOGIES I n THE JUSTICE SYSTEM 144. (extracted from223.txt)
Professor Karen Yeung, for instance, warned against tools that produce “a nice colour” such as “a little risk assessment, red, green or yellow” because they are so “easy to interpret” that they do not encourage challenge or critical thinking.304 In a similar vein, the Ministry of Justice told us that when designing “tools which 299 Q 57 (Peter Dawson) 300 Written evidence from the Prison Reform Trust ( nTL0004 ) 301 Independent Chief Inspector of Borders and Immigration, An inspection of entry clearance processing operations in Croydon and Istanbul 302 Q 8 (Professor Delacroix) 303 Written evidence from Gary Pugh ( nTL0036 ) 304 Q 57 (Professor Karen Yeung) 56 THE ADVE nT OF n EW TECH nOLOGIES I n THE JUSTICE SYSTEM are used to support decisions about individuals”, they “strive to ensure that the tool is designed to be intuitive.”305 150. (extracted from223.txt)
If achieved, algorithmic explainability would therefore provide more compelling explanations of decisions than humans currently provide.314 Certainly, procurement managers need to understand how the tools they are commissioning or purchasing function.315 Importantly, Professor Wachter added that introducing an explainability requirement on technologies used for the application of the law could help with this: “When people do not want to tell you how [a technological solution] is working, it is either because they do not want to or because they do 305 Written evidence from the Ministry of Justice ( nTL0053 ) 306 Written evidence from the Bar Council ( nTL0048 ) 307 Written evidence from BAE Systems ( nTL0056 ) 308 See Marion Oswald, ‘Algorithm-assisted decision-making in the public sector: framing the issues using administrative law rules governing discretionary power’, University of Winchester, (6 August 2018), p 8–9: https://royalsocietypublishing.org/doi/10.1098/rsta. (extracted from223.txt)
57 THE ADVE nT OF n EW TECH nOLOGIES I n THE JUSTICE SYSTEM not know. (extracted from223.txt)
I do not think either is acceptable, especially in the criminal justice sector. (extracted from223.txt)
58 THE ADVE nT OF n EW TECH nOLOGIES I n THE JUSTICE SYSTEM CHAPTER 5: EVALUATION AND OVERSIGHT 156. (extracted from223.txt)
The Law Society of England and Wales told us that: “Bias, both conscious and unconscious, can be baked into algorithms and undermine consistently reliable results, and that using algorithms without questioning them or explaining them to the public could lead to decisions which threaten human rights and undermine trust in the justice system”.323 158. (extracted from223.txt)
ethnicity-facts-figures.service.gov.uk/crime-justice-and-the-law/policing/stop-and-search/ latest#byethnicity [accessed 7 February 2022] 327 Q 71 (Professor Sandra Wachter) 59 THE ADVE nT OF n EW TECH nOLOGIES I n THE JUSTICE SYSTEM the application of the categorisation algorithm and on which it depends. (extracted from223.txt)
60 THE ADVE nT OF n EW TECH nOLOGIES I n THE JUSTICE SYSTEM Assessments are a common approach to complying with the Public Sector Equality Duty,335 while Data Protection Impact Assessments are required for data processing operations which are likely to result in a high risk to individuals.336 163. (extracted from223.txt)
61 THE ADVE nT OF n EW TECH nOLOGIES I n THE JUSTICE SYSTEM and in demonstrating trustworthiness in ways that are more convincing than slogans and pledges, or compliance with legal requirements.344 Community consultation 166. (extracted from223.txt)
(nTL0022 ) 350 Written evidence from the Metropolitan Police Service ( nTL0031 ) 351 Written evidence from Big Brother Watch ( nTL0037 ) 62 THE ADVE nT OF n EW TECH nOLOGIES I n THE JUSTICE SYSTEM Technical considerations “The system will fail” 170. (extracted from223.txt)
63 THE ADVE nT OF n EW TECH nOLOGIES I n THE JUSTICE SYSTEM Box 12: Scientific standards In this report, a reference to ‘scientific standards’ is intended to mean a regime of quality standards and processes consistently applied to a person or body developing, maintaining or manufacturing a particular scientific product or technology, providing a scientific service, or incorporating a scientific method into their public service, combined with independent regulatory enforcement. (extracted from223.txt)
64 THE ADVE nT OF n EW TECH nOLOGIES I n THE JUSTICE SYSTEM issues”.362 For instance, we heard that “the accuracy of face recognition technology depends on … the environment in which the technology is deployed” because “in real settings, images may be of suboptimal quality or environmental conditions may be inhibitory to realising the full accuracy of face recognition technology.”363 176. (extracted from223.txt)
371 Q 40 (Professor Colin Gavaghan) 372 Q 66 (Silkie Carlo), Q 46 (Dr Rosamunde van Brakel), and Q 4 (Professor Charles Raab) 373 Written evidence from Archie Drake and Perry Keller, Kings College London ( nTL0011 ) 65 THE ADVE nT OF n EW TECH nOLOGIES I n THE JUSTICE SYSTEM confirmed. (extracted from223.txt)
66 THE ADVE nT OF n EW TECH nOLOGIES I n THE JUSTICE SYSTEM was also critical of the Government’s justification for the increased use of polygraph testing—a technological solution with controversial scientific grounds. (extracted from223.txt)
David Spreadborough, a forensic analyst, told us that “a technology introduced in the judiciary system should be validated and approved by people technically competent on the matter.”389 BAE Systems agreed that a designated body could “develop a certificate of conformance (or ‘Kitemark’/CE label) for approved AI applications”.390 nCC Group concurred that it is “essential that clear processes are established to vet technologies before they are deployed”391, whereas Dr Liam Owens of technology provider Semantics21 told us about a “review” by “an intermediary”.392 388 Written evidence from the Information Commissioner’s Office ( nTL0016 ) 389 Written evidence from David Spreadborough ( nTL0015 ) 390 Written evidence from BAE Systems ( nTL0056 ) 391 Written evidence from n CC Group ( nTL0005 ) 392 Q 78 (Dr Liam Owens) 67 THE ADVE nT OF n EW TECH nOLOGIES I n THE JUSTICE SYSTEM Privacy International agreed with them and detailed mechanisms by which a technological solution should be “approved for use”.393 187. (extracted from223.txt)
The Ministry of Justice referred us to a peer-reviewed research study that evaluated the OASys Sexual reoffending Predictor (OSP) before this technological solution was approved by the Sexual Offending Management Board of Her Majesty’s Prison and Probation Service.394 Similarly, the Public Law Project drew our attention to the proposed AI Regulation in the European Union, which foresees central “certification indicating conformity to regulatory standards.”395 188. (extracted from223.txt)
Former Deputy Chief Constable David Lewis told us that “there probably should be more centralised procurement”, alluding to the success of “regional procurement hubs” bringing police forces together.398 BAE Systems agreed 393 Written evidence from Privacy International ( nTL0051 ) 394 Written evidence from the Ministry of Justice ( nTL0053 ) 395 Written evidence from Public Law Project ( nTL0046 ) 396 College of Policing, Fundamental review of the College of Policing : https://assets.college.police.uk/s3fspublic/2022–02/Fundamental- review-of-the-College-of-Policing.pdf [accessed 24 February 2022] 397 HMICFRS, Inspectorate with College standards letter (10 February 2022): https://www.justicein spectorates.gov.uk/hmicfrs/publication-html/ inspectorate-relationship-with-college-standards-letter/ [accessed 24 February 2022] 398 Q 77 (David Lewis) 68 THE ADVE nT OF n EW TECH nOLOGIES I n THE JUSTICE SYSTEM that they “would support some form of centralised AI procurement within policing and justice.”399 192. (extracted from223.txt)
The Ministry of Justice and HM Prison & Probation Service have developed several of them, such as: • the Offender Group Reconviction Score (OGRS), a “predictor of proven reoffending within one and two years of noncustodial sentence or discharge from custody”404 • the Offender Assessment System (OASys), which “aims to assess the risk of harm offenders pose to others and how likely an offender is to reoffend”405 • the Digital Categorisation Service (DCS), an algorithm used to support decisions on security categorisations in prisons. (extracted from223.txt)
405 Written evidence from Big Brother Watch ( nTL0037 ) 406 Written evidence from the Prison Reform Trust ( nTL0004 ) 69 THE ADVE nT OF n EW TECH nOLOGIES I n THE JUSTICE SYSTEM Police and the University College London.407 The n ational Data Analytics Solution, a nationwide project sponsored by the Home Office and led by West Midlands Police in partnership with the national Crime Agency and Accenture, also falls in this category.408 We were told that, in n ew Zealand, such partnerships were “by far the most common practice” when the government procures technological solutions.409 196. (extracted from223.txt)
415 Written evidence from the n CC Group ( nTL0005 ) 416 Written evidence from Dr Eilidh n oyes and Dr Reuben Moreton ( nTL0026 ) 417 Q 24 (Professor Charles Raab) 70 THE ADVE nT OF n EW TECH nOLOGIES I n THE JUSTICE SYSTEM has been overtaken by marketing” because of salespeople who “will take something they do not understand and shout a number that they do not understand” to make accuracy claims.418 200. (extracted from223.txt)
pdf [accessed 26 January 2022] 424 World Economic Forum, AI Procurement Guidelines (11 June 2020): https://www.weforum.org/reports/ ai-procurement-in-a-box/ai-government-procurement-guidelines [accessed 26 January 2022] 425 Q 73 (David Lewis) 71 THE ADVE nT OF n EW TECH nOLOGIES I n THE JUSTICE SYSTEM Chiefs’ Council, and the Police Digital Service confirmed that they “inform contract implementation and management”.426 203. (extracted from223.txt)
BAE Systems argued that the guidelines should be “more technical”, more “supplier-focused”, and more specific to “policing and the justice context”.427 Professor Wachter told us that these “vague” guidelines were “not good enough” because they were too soft to induce change in procurement practices.428 Dr Liam Owens agreed that the guidelines are “very broad” and “non-specific” and would need to be better tailored to address the needs of technology providers in the context of the application of the law.429 Furthermore, this document refers to ‘guidance’ as well as ‘guidelines’. (extracted from223.txt)
432 World Economic Forum, AI Procurement Guidelines 72 THE ADVE nT OF n EW TECH nOLOGIES I n THE JUSTICE SYSTEM used in the application of the law. (extracted from223.txt)
73 THE ADVE nT OF n EW TECH nOLOGIES I n THE JUSTICE SYSTEM The West Midlands Ethics Committee model 211. (extracted from223.txt)
These included: • A recruitment based on merit, 443; with independent membership444 with a range of expertise; 445 • A commitment to publish meetings papers, minutes and conclusions;446 • The Committee’s independence from the police force whose use of technology it is scrutinising; and447 • The Committee’s remit to consider technological solutions throughout their lifecycle.448 441 See, for instance, Q 2 (Professor Carole McCartney), Q 110 (Kit Malthouse MP), also written evidence from Archie Drake and Perry Keller ( nTL0011 ), and defenddigitalme ( nTL0044 ) 442 Written evidence from Association of Police and Crime Commissioners, national Police Chiefs’ Council and Police Digital Service ( nTL0049 ), see also Q 11 (Professor McCartney), Q 48 (Dr Rosamunde van Brakel), and written evidence from BAE Systems ( nTL0056 ) 443 Written evidence from Association of Police and Crime Commissioners, national Police Chiefs’ Council and Police Digital Service ( nTL0049 ) 444 Written evidence from Association of Police and Crime Commissioners, national Police Chiefs’ Council and Police Digital Service ( nTL0049 ) and BAE Systems ( nTL0056 ) 445 Q 11 (Professor McCartney), Q 48 (Dr Rosamunde van Brakel) and written evidence from BAE Systems ( nTL0056 ) 446 Q 11 (Professor McCartney), written evidence from Association of Police and Crime Commissioners, national Police Chiefs’ Council and Police Digital Service ( nTL0049 ), and BAE Systems ( nTL0056 ) 447 Written evidence from Association of Police and Crime Commissioners, n ational Police Chiefs’ Council and Police Digital Service ( nTL0049 ), Archie Drake and Perry Keller ( nTL0011 ) and BAE Systems ( nTL0056 ) 448 Written evidence from Association of Police and Crime Commissioners, national Police Chiefs’ Council and Police Digital Service ( nTL0049 ) and BAE Systems ( nTL0056 ) 74 THE ADVE nT OF n EW TECH nOLOGIES I n THE JUSTICE SYSTEM Box 14: The West Midlands Police Ethics Committee • The West Midlands Police and Crime Commissioner (PCC) and West Midlands Police (WMP) jointly established a specialist Ethics Committee in early 2019. (extracted from223.txt)
pdf ?x41638 [accessed 1 February 2022] 450 HL Deb, 3 n ovember 2021, cols 1301–1305 451 Written evidence from the Home Office ( nTL0055 ) 75 THE ADVE nT OF n EW TECH nOLOGIES I n THE JUSTICE SYSTEM which everything in society operates”. (extracted from223.txt)
76 THE ADVE nT OF n EW TECH nOLOGIES I n THE JUSTICE SYSTEM SUMMARY OF CONCLUSIONS AND RECOMMENDATIONS Legal and institutional frameworks 1. (extracted from223.txt)
(Paragraph 66) 77 THE ADVE nT OF n EW TECH nOLOGIES I n THE JUSTICE SYSTEM 11. (extracted from223.txt)
The Government should require that national guidance for the use of advanced technological tools in policing and criminal justice is drawn up and, as part of their response to this report, should outline concrete plans for this. (extracted from223.txt)
This risks undermining trust in the police, the justice system, and the rule of law. (extracted from223.txt)
(Paragraph 113) 78 THE ADVE nT OF n EW TECH nOLOGIES I n THE JUSTICE SYSTEM Human-technology interactions 21. (extracted from223.txt)
The Home Office should, in conjunction with the Ministry of Justice and the College of Policing, undertake or commission appropriate research to determine how the use of predictive algorithms affects decision making, and under what circumstances meaningful human interaction is most likely. (extracted from223.txt)
As part of continuing professional development, training should also be made available to lawyers, members of the Judiciary, and other professionals involved in the justice system. (extracted from223.txt)
(Paragraph 183) 79 THE ADVE nT OF n EW TECH nOLOGIES I n THE JUSTICE SYSTEM 30. (extracted from223.txt)
With the assurance brought by the certification process and the register of algorithms, police forces and other 80 THE ADVE nT OF n EW TECH nOLOGIES I n THE JUSTICE SYSTEM public bodies would remain free to procure the technological solutions of their choice, as long as the products have been certified. (extracted from223.txt)
(Paragraph 219) 81 THE ADVE nT OF n EW TECH nOLOGIES I n THE JUSTICE SYSTEM APPENDI x 1: LIST OF MEMBERS AND DECLARATIONS OF INTEREST Members Lord Blunkett Baroness Chakrabarti Lord Dholakia Baroness Hallett Baroness Hamwee (Chair) Lord Hunt of Wirral Baroness Kennedy of The Shaws Baroness Pidding Baroness Primarolo Lord Ricketts Baroness Sanderson of Welton Baroness of Shackleton of Belgravia Declarations of Interest Lord Blunkett Non- Financial: Non-executive Chairman, Cyber Essentials Direct Limited Directorship: Director and Chairman of the Board, University of Law Limited (subsidiary and affiliated institution of Global University Systems and Interactive Pro Limited) Baroness Chakrabarti No relevant interests to declare Lord Dholakia Trustee of the Police Foundation which produced a report on the Strategic Review of Policing in England and Wales on 8 March 2022 Baroness Hallett Retired judge Baroness Hamwee No relevant interests to declare Lord Hunt of Wirral Partner, DAC Beachcroft LLP (International commercial law firm) Honorary Bencher, Inner Temple Baroness Kennedy of The Shaws Member of Microsoft Technology and Human Rights Advisory Council Baroness Pidding No relevant interests to declare Baroness Primarolo Non-Executive Director on the Board of Thompson’s Solicitors LLP. (extracted from223.txt)
Lord Ricketts No relevant interests to declare Baroness Sanderson of Welton No relevant interests to declare Baroness Shackleton of Belgravia No relevant interests to declare other than those on the Register 82 THE ADVE nT OF n EW TECH nOLOGIES I n THE JUSTICE SYSTEM Specialist Adviser Dr Marion Oswald Associate Professor, Northumbria Law School Advisory Board member, Centre for Data Ethics and Innovation Senior Research Associate, The Alan Turing Institute Associate Fellow, Royal United Services Institute for Defence and Security Studies; Independent Chair, West Midlands Police & Crime Commissioner and West Midlands Police Data Ethics Committee; Member, New Zealand Police Expert Panel on Emergent Technologies; Advisory board member of the UKRI Trustworthy Autonomous Systems Hub; Member of the Royal Society Working Group on Privacy Enhancing Technologies (2018–19 and reconstituted 2021); Member of National Statistician’s Data Ethics Advisory Committee since its foundation (2016-date); Executive member, British & Irish Law, Education & Technology Association; Member, Arts and Humanities Research Council Peer Review College. (extracted from223.txt)
83 THE ADVE nT OF n EW TECH nOLOGIES I n THE JUSTICE SYSTEM APPENDI x 2: LIST OF WITNESSES Evidence is published online at https://committees .parliament.uk/committee/519/ justice-and-home-affairs-committee/publications / and available for inspection at the Parliamentary Archives (020 7219 3074). (extracted from223.txt)
Oral evidence in chronological order * Professor Sylvie Delacroix, Professor in Law and Ethics at University of BirminghamQQ 1–24 * Professor Carole McCartney, Professor of Law and Criminal Justice at n orthumbria UniversityQQ 1–24 ** Professor Charles Raab, Professorial Fellow, Politics and International Relations, School of Social and Political Science at The University of Edinburgh)QQ 1–24 * Dr David Leslie, Ethics Theme Lead at Alan Turing InstituteQQ 25–38 * Professor Michael Wooldridge, Head of Department of Computer Science, Professor of Computer Science at University of OxfordQQ 25–38 ** Professor Colin Gavaghan, Director n ew Zealand Law Foundation Centre for Law and Policy in Emerging Technologies at University of OtagoQQ 39–51 * Professor Elizabeth E Joh, Martin Luther King Jr. (extracted from223.txt)
Professor of Law at University of California, DavisQQ 39–51 * Dr Rosamunde Elise van Brakel, Co-Director, Surveillance Studies n etwork, Associate Professor Tilburg University/Vrije Universiteit Brussel (VUB)QQ 39–51 ** Silkie Carlo, Director, Big Brother Watch QQ 52–67 ** Peter Dawson, Director, Prison Reform Trust QQ 52–67 * Professor Karen Yeung, Interdisciplinary Professorial Fellow in Law, Ethics and Informatics, Birmingham Law School at The University of BirminghamQQ 52–67 * David Lewis, Former Deputy Chief Constable and former ethics lead n PCC at Dorset PoliceQQ 68–82 * Dr Liam Owens, Founder and Chief Executive Officer, Semantics 21QQ 68–82 ** Professor Sandra Wachter, Associate Professor and Senior Research Fellow at University of OxfordQQ 68–82 ** Professor Paul Taylor, Chief Scientific Adviser, national Police Chiefs’ CouncilQQ 83–98 84 THE ADVE nT OF n EW TECH nOLOGIES I n THE JUSTICE SYSTEM ** Alun Michael, Police and Crime Commissioner for South Wales and Joint Lead for Data and Bioethics, Association of Police and Crime CommissionersQQ 83–98 ** Darryl Preston, Police and Crime Commissioner for Cambridgeshire and Peterborough and Joint Lead for Data and Bioethics, Association of Police and Crime CommissionersQQ 83–98 * David Tucker, Faculty Lead on Crime and Criminal Justice, College of Policing.QQ 83–98 * The Rt Hon Kit Malthouse MP, Minister of State for Crime and Policing at the Home Office and Ministry of JusticeQQ 99–111 * Dr Christophe Prince, Director for Data and Identity, Home OfficeQQ 99–111 Alphabetical list of all witnesses Robin Allen QC, Barristers at A1 Law Consultancy/ Cloisters ChambersnTL0019 Dr Arianna Andreangeli, Senior Lecturer in Competition Law, Edinburgh Law School, University of EdinburghnTL0038 nTL0039 Dr Philip Avenell, Managing Director and Forensic Biologist at Forensic AccessnTL0024 Avon and Somerset Police nTL0052 BAE Systems nTL0056 Professor Melanie Bailey, Professor at University of SurreynTL0024 The Bar Council nTL0048 Dr Marcin Betkier, Lecturer in Law at Victoria University of WellingtonnTL0021 Dr Stephen Bleay, Senior Lecturer in Forensic Science at London South Bank UniversitynTL0024 Katy Bourne OBE, Sussex Police and Crime CommissionernTL0045 Dr Rebecca Brown, Research Fellow at University of OxfordnTL0030 Professor Dame Vicki Bruce DBE, Professor Emerita at newcastle UniversitynTL0012 Professor A Mike Burton, Professor of Psychology at University of YorknTL0012 Professor Liz Campbell, Professor and Francine V Mcniff Chair in Criminal Jurisprudence at Monash UniversitynTL0021 85 THE ADVE nT OF n EW TECH nOLOGIES I n THE JUSTICE SYSTEM ** Silkie Carlo, Director, Big Brother Watch (QQ 52–67 )nTL0037 Detective Sergeant Laurence Cartwright, Data Analytics lead at Sussex PolicenTL0040 Dr Jiahong Chen, Lecturer in Law at Sheffield Law School, University of SheffieldnTL0035 The Crown Prosecution Service nTL0018 Dr Benjamin Davies, Wellcome Trust Society & Ethics Research Fellow at University of OxfordnTL0030 ** Peter Dawson, Director, The Prison Reform Trust (QQ 52–67 )nTL0004 defenddigitalme nTL0044 Dr Delphine Defossez (Lecturer in Law), northumbria UniversitynTL0022 * Professor Sylvie Delacroix, Professor in Law and Ethics at University of Birmingham ( QQ 1–24 ) Professor Thomas Douglas, Professor of Applied Philosophy University of OxfordnTL0013 nTL0030 Archie Drake, Research Associate at Kings College, LondonnTL0011 Professor Gary Edmond, Professor of Law at U nSW SydneynTL0012 Professor Lilian Edwards, Professor of Law, Innovation and Society at n ewcastle Law School, newcastle UniversitynTL0035 Professor Seena Fazel, Professor of Forensic Psychiatry & Wellcome Trust Senior Research Fellow in Clinical Science at University of OxfordnTL0030 Dr Lisa Forsberg, British Academy Postdoctoral Fellow at University of OxfordnTL0013 nTL0030 Professor Simona Francese, Professor of Forensic and Bioanalytical Mass Spectrometry at Sheffield Hallam UniversitynTL0024 Professor Pete Fussey, Professor of Sociology, University of EssexnTL0017 Professor Angela Gallop, Professor of Practice/ Director of Forensic Science at University of Strathclyde/Forensic AccessnTL0024 ** Professor Colin Gavaghan, Director n ew Zealand Law Foundation Centre for Law and Policy in Emerging Technologies at University of Otago (QQ 39–51 )nTL0047 86 THE ADVE nT OF n EW TECH nOLOGIES I n THE JUSTICE SYSTEM Dr Jamie Grace, Senior Lecturer in Law at Sheffield Hallam UniversitynTL0001 Professor n igel Harvey, Professor of Judgment and Decision Research at UCL LondonnTL0025 Tobias Harvey, Student of Law at Kings College LondonnTL0025 Dr Binesh Hass, Research Fellow at University of OxfordnTL0030 The Home Office nTL0055 Independent Office for Police Conduct nTL0054 The Information Commissioner’s Office (ICO) nTL0016 Istanbul Bar Association nTL0028 * Professor Elizabeth E Joh, Martin Luther King Jr. (extracted from223.txt)
Professor of Law at University of California, Davis (QQ 39–51 ) Perry Keller, Reader in Media and Information Law, Director of Doctoral Studies at King’s College LondonnTL0011 Professor Paul Kelly, Professor of Inorganic Chemistry at University of LoughboroughnTL0024 Professor Richard I.Kemp, Professor of Psychology at UnSW SydneynTL0012 Dr Kyriakos n Kotsoglou, Senior Lecturer in Law, northumbria University/Research Fellow, University of LausannenTL0006 nTL0007 The Law Society of England and Wales nTL0023 Dr Christopher Lawless, Associate Professor at Durham UniversitynTL0029 * Dr David Leslie, Ethics Theme Lead at Alan Turing Institute ( QQ 25–38 ) * David Lewis, Former Deputy Chief Constable and former ethics lead n PCC at Dorset Police ( QQ 68– 82) Liberty nTL0020 Sjors Ligthart LLM PhD candidate at Tilburg UniversitynTL0013 Dr n essa Lynch, Associate Professor of Law at Victoria University of WellingtonnTL0021 * The Rt Hon Kit Malthouse MP, Minister of State for Crime and Policing at the Home Office and Ministry of Justice ( QQ 99–111 ) Stephen Mason, Associate Research Fellow, Institute of Advanced Legal StudiesnTL0002 87 THE ADVE nT OF n EW TECH nOLOGIES I n THE JUSTICE SYSTEM Dee Masters, Barristers at A1 Law Consultancy/ Cloisters ChambersnTL0019 Professor Derek McAuley, Director of Horizon Digital Economy Research Institute at University of nottinghamnTL0035 ** Professor Carole McCartney, Professor of Law and Criminal Justice at n orthumbria University (QQ 1–24 )nTL0022 medConfidential nTL0050 The Metropolitan Police Service nTL0031 Professor Gerben Meynen, Professor of Forensic Psychiatry and Bioethics at Utrecht University and VU University AmsterdamnTL0013 ** Alun Michael, Police and Crime Commissioner for South Wales and Joint Lead for Data and Bioethics, Association of Police and Crime Commissioners (QQ 83–98 )nTL0049 nTL0057 Migrants’ Rights n etwork nTL0042 The Ministry of Justice nTL0053 Abhishek Mishra, Doctoral Student at University of OxfordnTL0030 Dr Brent Mittelstadt of the Oxford Internet Institute (OII)nTL0058 Dr Reuben Moreton, Reli Ltd nTL0026 Professor Ruth Morgan, Professor of Crime and Forensic Sciences at University College LondonnTL0024 Dr Daragh Murray, Senior Lecturer in Human Rights at University of EssexnTL0017 nCC Group nTL0005 Dr Eilidh n oyes, University of Huddersfield nTL0026 * Dr Liam Owens, Founder and Chief Executive Officer, Semantics 21 ( QQ 68–82 ) Ms Angela Paul (PhD candidate in Law), northumbria UniversitynTL0022 Police Scotland nTL0043 Dr Susan Pope, D nA expert—chair of the Forensic Science Regulator D nA Specialist Group & is an assessor for the n etherlands Register of Court Experts at Principal Forensic ServicesnTL0024 ** Darryl Preston, Police and Crime Commissioner for Cambridgeshire and Peterborough and Joint Lead for Data and Bioethics, Association of Police and Crime Commissioners ( QQ 83–98 )nTL0049 nTL0057 88 THE ADVE nT OF n EW TECH nOLOGIES I n THE JUSTICE SYSTEM Dr Christophe Prince, Director for Data and Identity, Home Office ( QQ 99–111 ) Privacy International nTL0051 Public Law Project nTL0046 nTL0059 Gary Pugh, Forensic Science Regulator nTL0036 Dr Jonathan Pugh, Parfit Radcliffe Senior Research Fellow at University of OxfordnTL0030 Dr Joe Purshouse, Senior Lecturer in Criminal Law and Justice at University of SheffieldnTL0021 ** Professor Charles Raab, Professorial Fellow, School of Social and Political Science, University of Edinburgh Fellow, The Alan Turing Institute (QQ 1–24 )nTL0014 Dr Liam Ralph (Lecturer in Criminology and Policing) at the Centre for Crime and Policing & the Science and Justice Research, n orthumbria UniversitynTL0022 Dr Kay L. (extracted from223.txt)
Mehera San Roque, U nSW SydneynTL0012 SAS UK&I nTL0041 Professor Julian Savulescu, Professor of Practical Ethics at University of OxfordnTL0030 Serious Fraud Office nTL0034 Professor Ilina Singh, Professor of n euroscience and Society at University of OxfordnTL0030 David Spreadborough, CFVA, Forensic Analyst at Amped SoftwarenTL0015 Dr Amy Stevens, Senior Research Officer, Human Rights, Big Data and Technology Project at University of EssexnTL0017 Dr Clare Sutherland, Senior Lecturer at University of AberdeennTL0012 ** Professor Paul Taylor, Chief Scientific Adviser, national Police Chiefs’ Council ( QQ 83–98 )nTL0049 nTL0057 Dr Alice Towler, Research Fellow at U nSW Sydney nTL0012 ** David Tucker, Faculty Lead on Crime and Criminal Justice, College of Policing ( QQ 83–98 )nTL0057 89 THE ADVE nT OF n EW TECH nOLOGIES I n THE JUSTICE SYSTEM Professor Gillian Tully, Professor of Practice for Forensic Science Policy and Regulation at King’s College LondonnTL0024 UCL Centre for the Forensic Sciences nTL0010 Dr Lachlan Urquhart, Lecturer in Technology Law, and Co-Investigator of the UKRI Trustworthy Autonomous Systems n ode in Governance and Regulation at School of Law University of EdinburghnTL0035 * Dr Rosamunde Elise van Brakel, Co-Director, Surveillance Studies n etwork, Associate Professor Tilburg University/Vrije Universiteit Brussel (VUB) (QQ 39–51 ) ** Professor Sandra Wachter, Associate Professor and Senior Research Fellow at Oxford Internet Institute (OII), University of Oxford ( QQ 68–82 )nTL0058 Dr Adrian Weller, Principal Research Fellow in Machine Learning at the University of CambridgenTL0040 Dr David White, Senior Lecturer at U nSW Sydney nTL0012 Dr Matthias Wienroth (Vice-Chancellor’s Senior Fellow in Criminology/Sociology), n orthumbria UniversitynTL0022 Professor Kim Wolff, Director King’s Forensics at King’s College LondonnTL0024 * Professor Michael Woodridge, Head of Department of Computer Science, Professor of Computer Science at University of Oxford ( QQ 25–38 ) * Professor Karen Yeung, Interdisciplinary Professorial Fellow in Law, Ethics and Informatics, Birmingham Law School at The University of Birmingham (QQ 52–67 ) Dr Miri Zilka, Research Associate in Machine Learning at the University of CambridgenTL0040 90 THE ADVE nT OF n EW TECH nOLOGIES I n THE JUSTICE SYSTEM APPENDI x 3: CALL FOR EVIDENCE Scope of the inquiry The Committee seeks to explore the use of new technologies in the application of the law and the experience of people currently or previously engaged with them. (extracted from223.txt)
Are safeguards needed to ensure that 91 THE ADVE nT OF n EW TECH nOLOGIES I n THE JUSTICE SYSTEM technologies cannot be used to serve purposes incompatible with a democratic society? (extracted from223.txt)
92 THE ADVE nT OF n EW TECH nOLOGIES I n THE JUSTICE SYSTEM APPENDI x 4: ABBREVIATIONS, ACRONYMS AND TECHNICAL TERMS ADM Automated Decision Making AFR Automated Facial Recognition AI Artificial Intelligence Algorithm A series of instructions for performing a calculation or solving a problem, especially with a computer. (extracted from223.txt)
93 THE ADVE nT OF n EW TECH nOLOGIES I n THE JUSTICE SYSTEM ML Machine Learning nDAS national Data Analytics Solution (a national analytics capability being developed by West Midlands Police in conjunction with the Home Office). (extracted from223.txt)
RR\1215422EN.docx PE650.508v02-00 ENUnited in diversityENEuropean Parliament 2019-2024 Plenary sitting A9-0186/2020 8.10.2020 REPORT with recommendations to the Commission on a framework of ethical aspects of artificial intelligence, robotics and related technologies (2020/2012(INL)) Committee on Legal Affairs Rapporteur: Ibán García del Blanco Rapporteurs for the opinion (*): Urmas Paet, Committee on Foreign Affairs Alexandra Geese, Committee on Internal Market and Consumer Protection Valter Flego, Committee on Transport and Tourism Assita Kanko, Committee on Civil Liberties, Justice and Home Affairs (*) Associated committees – Rule 57 of the Rules of Procedure (Initiative – Rule 47 of the Rules of Procedure) PE650.508v02-00 2/130 RR\1215422EN.docx ENPR_INL CONTENTS Page MOTION FOR A EUROPEAN PARLIAMENT RESOLUTION............................................. (extracted from182.txt)
84 OPINION OF THE COMMITTEE ON CIVIL LIBERTIES, JUSTICE AND HOME AFFAIRS .................................................................................................................................. (extracted from182.txt)
RR\1215422EN.docx 5/130 PE650.508v02-00 ENRights and Fundamental Freedoms, and the European Charter for Regional or Minority Languages, – having regard to the OECD Council Recommendation on Artificial Intelligence adopted on 22 May 2019, – having regard to Rules 47 and 54 of its Rules of Procedure, – having regard to the opinions of the Committee on Foreign Affairs, the Committee on the Internal Market and Consumer Protection, the Committee on Transport and Tourism, the Committee on Civil Liberties, Justice and Home Affairs, the Committee on Employment and Social Affairs, the Committee on the Environment, Public Health and Food Safety and the Committee on Culture and Education, – having regard to the report of the Committee on Legal Affairs (A9-0186/2020), Introduction A. (extracted from182.txt)
Considers that technologies which can produce automated decisions, thus replacing decisions taken by public authorities, should be treated with the utmost precaution , notably in the area of justice and law enforcement; 68. (extracted from182.txt)
https://www.sae.org/standards/content/j3016_201806/ RR\1215422EN.docx 39/130 PE650.508v02-00 ENRights of the European Union (the ‘Charter’), settled case-law of the Court of Justice of the European Union, and other European and international instruments which apply in the Union. (extracted from182.txt)
Dalunde, Karima Delli, Anna Deparnay-Grunenberg, Tilly Metz 0 0 0 Key to symbols: + : in favour - : against 0 : abstention RR\1215422EN.docx 91/130 PE650.508v02-00 EN22.9.2020 OPINION OF THE COMMITTEE ON CIVIL LIBERTIES, JUSTICE AND HOME AFFAIRS for the Committee on Legal Affairs with recommendations to the Commission on the framework of ethical aspects of artificial intelligence, robotics and related technologies (2020/2012(INL)) Rapporteur for opinion (*): Assita Kanko (*) Associated committee – Rule 57 of the Rules of Procedure (Initiative – Rule 47 of the Rules of Procedure) SUGGESTIONS The Committee on Civil Liberties, Justice and Home Affairs calls on the Committee on Legal Affairs, as the committee responsible, to incorporate the following suggestions into its motion for a resolution: – having regard to Articles 2 and 3 of the Treaty on European Union (TEU), – having regard to Articles 10, 19, 21 and 167 of the Treaty on the Functioning of the European Union (TFEU), – having regard to the right to petition enshrined in Articles 20 and 227 of the TFEU and Article 44 of the Charter of Fundamental Rights of the European Union (EUCFR), – having regard to Articles 21 and 22 of the EUCFR, – having regard to the preamble to the TEU, – having regard to the Council of Europe’s Framework Convention for the Protection of National Minorities, Protocol No 12 to the Convention for the Protection of Human Rights and Fundamental Freedoms, and the European Charter for Regional or Minority Languages, – having regard to Council Directive 2000/43/EC of 29 June 2000 implementing the principle of equal treatment between persons irrespective of racial or ethnic origin1 1 OJ L 180, 19.7.2000, p. (extracted from182.txt)
(2021)2727 Open letter by 61 organisations calling for legal limits on AI risk assessment systems in the criminal justice context2819 affects the right to the right to a fair trial. (extracted from40.txt)
THE ADMINISTRATION OF JUSTICE AND DEMOCRATIC PROCESSES 31The use of AI in the administration of justice and democratic processes is particularly sensitive and should be approached with more nuance and scrutiny than it is done now. (extracted from40.txt)
Administration of justice and democratic processes: (a) AI systems intended to be used for or assist a judicial authority in researching and interpreting facts and the law and in applying the las to a concrete set of facts. (extracted from40.txt)
We do see areas for improvement though, in particular where it comes to inclusivity and multi-disciplinarity, complaints and redress, and the exclusion (for now) of the applicability of the AIA to ‘legacy high-risk AI’ and components of large scale European IT systems in the realm of “freedom, security and justice” already put into service before the application of the AIA. (extracted from40.txt)
The values of equality, tolerance, respect for others, and justice govern this principle. (extracted from54.txt)
This policy brief is based on the Dutch report Big Data in een vrije en veilige samenleving (Big Data in a Free and Secure Society), presented by wrr to Ard van der Steur, the Dutch Minister for Security and Justice, on 28 April 2016. (extracted from627.txt)
The gdpr will not be applicable to the police and justice sector, whose work will be regulated by national legislation to be based on the new eu Police and Criminal Justice Data Protection Directive.56 Fundamental rights and security exceptions The regulation of data protection and privacy is founded on fundamental rights that are enshrined in treaties such as the European Convention on Human Rights ( echr ) and the Charter of Fundamental Rights of the European Union. (extracted from627.txt)
Table 9.1 Legal frameworks Constitutional framework International Covenant on Civil and Political Rights ( iccpr ) European Convention for the Protection of Human Rights and Fundamental Freedoms ( echr ) Charter of Fundamental Rights of the European Union Constitution of the Netherlands Police and judiciary Code of Criminal Procedure (WvSv) Judicial Data and Criminal RecordsAct (Wjsg)Police Data Act (Wpg)Special Investigative Services Act(Wet bod) Police and Criminal Justice Data ProtectionDirective ( eu) 2016/680Intelligence and security services Intelligence and Security Services Act (Wiv 2002)Government (other agencies) Personal Data Protection Act (Wbp)Data Protection Directive (95/46/ ec) General Data Protection Regulation( eu) 2016/679 Law enforcement and public prosecuting authorities operate under their own legal framework, often with specific laws regulating the collection, exchange and use of data within the police organisation and the wider law enforcement community. (extracted from627.txt)
27 process to shift to the European Court of Human Rights and the European Court of Justice. (extracted from627.txt)
Regarding these rights, we focus on issues relating to the right to respect private life, human dignity, ownership, safety and liability, freedom of expression and the prohibition of discrimination as well as acc ess to justice and the right to a fair trial. (extracted from633.txt)
39 3.8 Access to justice and the right to a fair trial ................................ (extracted from633.txt)
The next three sections are dedicated to how automated decisions may affect freedom of expression, the prohibition of discrimination, access to justice and the right to a fair trial (sections 3.6, 3.7 and 3.8). (extracted from633.txt)
Intelligent artefacts may challenge different dimensions of human rights, for example in terms of freedoms (privacy and data protection, ownership, autonomy, personality), equality ( more specifically , non-discrimination) and justice (fair trial, access to justice). (extracted from633.txt)
Thus, the impact of intelligent artefacts is first discussed in the context of seven existing human rights: the right to respect for private and family life (3.2), the right to human dignity (3.3), the right to the peaceful enjoyment of possessions (3.4), tort rights and safety (3.5), the right to freedom of expression as well as the freedom of thought, conscience and religion (3.6), the prohibition of discrimination (3.7) and access to justice as well as the right to a fair trial (3.8). (extracted from633.txt)
18 Court of Justice of the European Union 19 October 2016, C -582/14 (Breyer ) and Court of Justice of the European Union 24 November 2011, C -70/10 (Scarlet/SABAM ), paragraph 51. (extracted from633.txt)
For instance, o n an international level, both the European Court of Human Rights and the Court of Justice of the European Union have voiced their concerns on numerous occasions regarding state surveillance activi ties and the impact on privacy rights.25 For 21 The Dutch Data Protection Authority imposed penalty payments on a company that co uld not demonstrate that Wi -Fi tracking in public spaces was necessary for a legitimate purpose. (extracted from633.txt)
21 instance when the EU Court of Justice declared the Data Retention Directive to be invalid.26 On a national level, the British Investigatory Powers Tribunal , for instance , ruled that during a time span of more than a decade, the British intelligence agencies illegally collected data about innocent citizens and illegally tracked their phone and web use.27 With regard to surveillance enacted by companies , the European Commission proposed the reform of EU data protect ion regulations , which ultimately led to the G eneral Data Protection Regulation . (extracted from633.txt)
Court of Justice of the European Union 8 April 2014, Joined Cases C -293/12 and C -594/12 ( Digital Rights Ireland and Seitlinger and Others ). (extracted from633.txt)
See also: Court of Justice of the European Union 21 December 2016, Joined Cases C -203/15 ( Tele2 Sverige AB v Post -och telestyrelsen ) and C -698/15 ( Secretary of State for the Home Department v Tom Watson and Others ). (extracted from633.txt)
28 For an overview of (a selec tion of) Court of Justice of the European Union and European Court of Human Rights cases, see Annex I of Article 29 Data Protection Working Party, Working Document 01/2016 on the justification of interferences with the fundamental rights to priva cy and dat a protection through surveillance measures when transferring personal data (European Essential Guarantees), 13 April 2016. (extracted from633.txt)
In addition, wi th regard to decisions of the Court of Justice of the European Union concerning data protection, see: L. (extracted from633.txt)
These letters are available at: http://ec.europa.e u/justice/data -protection/article -29/documentation/other -document/index_en.htm . (extracted from633.txt)
42 3.8 Access to justice and the right to a fair trial Access to justice and the right to a fair trial Article 6 (2) ECHR plays an important role with regard to predictive AI . (extracted from633.txt)
The increased use of risk -assessing algorithms in the American justice system raises accountability a nd transparency issues.100 It has been reported that software used to set bail , conditions for parole and sentencing decisions is biased against Afr ican Americans (Angwin et al. (extracted from633.txt)
To give another example, in response to worries by consumers about Wi -Fi tracking by shop owners, the former Dutch minister of economic affairs and the state secretary of security and justice stated that people should just turn off their smartphone if they do not want to be tracked.102 On the basis of this response , it seems that tracking and tracing people is a right which is deemed more important than peoples’ (privacy) rights. (extracted from633.txt)
Besides affecting the right to respect for private lif e in numerous ways, digiti sation, virtuali sation and roboti sation influence human dignity, the right to the peaceful enjoyment of possessions, safety and tort rights, the right to freedom of expression, the prohibition of discrimination, access to justice and the right to a fair trial. (extracted from633.txt)
Access to justice and the right to a fair trial (see subsection 3.8) The Council of Europe could establish a framework of minimum norms to be taken into account when a court uses AI. (extracted from633.txt)
(2007) E-Justice, beginselen van behoorlijke elektronische rechtspraak, The Hague: SDu Uitgevers. (extracted from633.txt)
Accept additional cookies Reject additional cookies View cookies Hide cookie message Skip to main content GOV.UK Navigation menu Menu Menu Services and information Benefits Births, death, marriages and care Business and self-employed Childcare and parenting Citizenship and living in the UK Crime, justice and the law Disabled people Driving and transport Education and learning Employing people Environment and countryside Housing and local services Money and tax Passports, travel and living abroad Visas and immigration Working, jobs and pensions Government activity Departments Departments, agencies and public bodies News News stories, speeches, letters and notices Guidance and regulation Detailed guidance, regulations and rules Research and statistics Reports, analysis and official statistics Policy papers and consultations Consultations and strategy Transparency Data, Freedom of Information releases and corporate reports Search GOV.UK × Search GOV.UK Search Search GOV.UK Search Home Government Government efficiency, transparency and accountability Data Ethics Framework GovernmentDigital Service Central Digital & Data Office Guidance Data Ethics Framework Updated 16 September 2020 Contents How to use the Data Ethics Framework Overarching principles Specific actions Next steps Print this page © Crown copyright 2020 This publication is licensed under the terms of the Open Government Licence v3.0 except where otherwise stated. (extracted from83.txt)
Cancel Services and information Benefits Births, death, marriages and care Business and self-employed Childcare and parenting Citizenship and living in the UK Crime, justice and the law Disabled people Driving and transport Education and learning Employing people Environment and countryside Housing and local services Money and tax Passports, travel and living abroad Visas and immigration Working, jobs and pensions Government activity Departments News Guidance and regulation Research and statistics Policy papers and consultations Transparency How government works Get involved Support links Help Privacy Cookies Accessibility statement Contact Terms and conditions Rhestr o Wasanaethau Cymraeg Government Digital Service All content is available under the Open Government Licence v3.0, except where otherwise stated © Crown copyright (extracted from83.txt)
Seven values underpin these ethical guidelines: well -being, autonomy, justice, privacy, knowledge, democracy and responsibility. (extracted from97.txt)
Accept additional cookies Reject additional cookies View cookies Hide cookie message Skip to main content GOV.UK Navigation menu Menu Menu Services and information Benefits Births, death, marriages and care Business and self-employed Childcare and parenting Citizenship and living in the UK Crime, justice and the law Disabled people Driving and transport Education and learning Employing people Environment and countryside Housing and local services Money and tax Passports, travel and living abroad Visas and immigration Working, jobs and pensions Government activity Departments Departments, agencies and public bodies News News stories, speeches, letters and notices Guidance and regulation Detailed guidance, regulations and rules Research and statistics Reports, analysis and official statistics Policy papers and consultations Consultations and strategy Transparency Data, Freedom of Information releases and corporate reports Search GOV.UK × Search GOV.UK Search Search GOV.UK Search Home Health and social care National Health Service Digital and data-driven health and care technology Departmentof Health &Social Care Guidance A guide to good practice for digital and data-driven health technologies Updated 19 January 2021 Contents Introduction 1. (extracted from380.txt)
Cancel Services and information Benefits Births, death, marriages and care Business and self-employed Childcare and parenting Citizenship and living in the UK Crime, justice and the law Disabled people Driving and transport Education and learning Employing people Environment and countryside Housing and local services Money and tax Passports, travel and living abroad Visas and immigration Working, jobs and pensions Government activity Departments News Guidance and regulation Research and statistics Policy papers and consultations Transparency How government works Get involved Support links Help Privacy Cookies Accessibility statement Contact Terms and conditions Rhestr o Wasanaethau Cymraeg Government Digital Service All content is available under the Open Government Licence v3.0, except where otherwise stated © Crown copyright (extracted from380.txt)
“Algorithms and Economic Justice: A Taxonomy of Harms and a Path Forward for the Federal Trade Commission. (extracted from394.txt)
Thinking about who products are being built for, who they might unintentionally exclude, how product use and product design can protect vulnerable populations, especially now that we’re in the middle of a racial justice crisis and a pandemic, which are disproportionately affecting people of colour and people of lower incomes. (extracted from431.txt)
First version of the Principles for Action The first version of these principles has been co-drafted through a multistakeholder process, while paying careful attention to the EU GDPR11 and the police and criminal justice directive,12 and has drawn inspiration from some of their principles. (extracted from430.txt)
“EU Data Protection Rules”, EU website, https://ec.europa.eu/commission/priorities/justice-and-fundamental-rights/ data-protection/2018- reform-eu-data-protection-rules/eu-data-protection-rules_en (link as of 28/1/20). (extracted from430.txt)
Several witnesses highlighted the growing use of AI within the US justice system, in particular the Correctional Offender Management Profiling for Alternative Sanctions (COMPAS) system, developed by Northpointe, and used across several US states to assign risk ratings to defendants, which help to assist judges in sentences and parole decisions. (extracted from381.txt)
144 Evidence from Sheena Urwin, Head of Criminal Justice at Durham Constabulary, emphasised the considerable lengths that Durham Constabulary have taken to ensure their use of these tools is open, fair and ethical, in particular the development of their ‘ALGO-CARE’ framework for the ethical use of algorithms in policing. (extracted from381.txt)
* Libby Kinsey, Co-founder, Project Juno ** Dr Mercedes Bunz, Senior Lecturer, Communications and Media Research Institute, University of WestminsterQQ 55–64 ** Elizabeth Denham, UK Information Commissioner, Information Commissioner’s Office * Dr Sandra Wachter, Postdoctoral Researcher in Data Ethics and Algorithms, Oxford Internet Institute * Olivier Thereaux, Head of Technology, The Open Data Institute QQ 65–75 * Javier Ruiz Diaz, Policy Director, The Open Rights Group ** Frederike Kaltheuner, Policy Officer, Privacy International ** Dr James Luke, Chief Technology Officer for the Public Sector, IBM QQ 76–84 ** Kriti Sharma, Vice President of Artificial Intelligence and Bots, Sage * Andrew de Rozairo, Vice President, Customer Innovation and Enterprise Platform, SAP * Colin Griffiths, Policy Manager, Citizens Advice QQ 85–94 ** Will Hayter, Project Director, Competition and Markets Authority ** Olly Buston, CEO and Founder, Future Advocacy QQ 95–104 * Professor Dame Henrietta Moore, Director, Institute for Global Prosperity, UCL ** Professor Richard Susskind OBE, IT Adviser to the Lord Chief Justice of England and Wales * Dr Mark Taylor, Global Strategy & Research Director, Dyson QQ 105–115 ** Dr Joseph Reger, Chief Technology Officer EMEIA, Fujitsu ** Paul Clarke, Chief Technology Officer, Ocado * Dr Julian Huppert, Chair, Independent Review Panel for DeepMind Health QQ 116–127 ** Dr Sobia Raza, Head of Science, PHG Foundation * Nicola Perrin, Head, Understanding Patient Data, Wellcome Trust ** Dr Hugh Harvey, Clinical Artificial Intelligence Researcher and Consultant Radiologist, Guy’s and St Thomas’ NHS Foundation Trust QQ 128–142 * Dame Fiona Caldicott, National Data Guardian for Health and Care, Office of the National Data Guardian 143 AI IN THE UK: READY, WILLING AND ABLE? (extracted from381.txt)
There were three overarching principles to this: • partnership with, not replacement of, humans; • putting human values at the centre of their applications; and • a strong focus on a wide-ranging set of ethical considerations, including the preservation of human autonomy, beneficence, non-maleficence, and justice. (extracted from381.txt)
for use in the criminal justice system, so defendants and their lawyers can understand and challenge evidence used against them); and • Tools for investigators and auditors for use when things go wrong. (extracted from381.txt)
www.nytimes.com/2017/06/13/opinion/howcomputers-are-harming-criminal-justice.html. (extracted from96.txt)
In 2019, Jobin and colleagues (2019) identified 84 published sets of ethical pr inciples for AI, which they concluded converged on five areas: transparency, justice and fairness, non-maleficence, responsibility and privacy. (extracted from82.txt)
Violations can prevent people from enjoying their rights, but they do not st op the rights existing.f Human rights are essential: They are essential for freedom, justice and peace.3.1.2. (extracted from82.txt)
Guidelines of the Committee of Ministers of the Council of Europe on child-friendly justice (2010), h ttps://rm.coe.int/16804b2cf3. (extracted from82.txt)
(2019), Artificial intelligence: human rights, social justice and development , Global Information Society Watch 2019, Association for Progressive Communications.R ehak R. (extracted from82.txt)
In addition to strong enforcement of the General Data Protection Regulation (GDPR) and safeguards such as human rights impacts assessments, software transparency and the availability of datasets for public scrutiny, it is vital that the upcoming regulatory proposal establishes in law clear limitations a s to what can be considered lawful uses of AI , to unequivocally address the following issues: • the enabling of biometric m ass surveillance and monitori ng of public spaces; • the exacerbation of structural discrimination , exclusion and collective harms; • the restriction of and discriminatory access to vital services such as health -care and social security; • the surveillance of workers and infringement of workers’ fundamental rights; • the impeding of fair access to justice and procedural rights; • the use of systems which make inferences and predictions about our most sensitive characteristics, beha viours and thoughts; • and, crucially, the manipulation or control of human behaviour and associated threats to human dignity, agency, and collective democracy. (extracted from154.txt)
Use of risk assessment tools in the criminal justice system and pre -trial context The use of algorithms in criminal justice matters to profile individuals within legal decision -making processes presents seve re threats to fundamental rights. (extracted from154.txt)
In addition, substantial evidence has shown that the introduction of such systems in criminal justice systems in Europe and elsewhere has resulted in unjust and discriminator y outcomes. (extracted from154.txt)
We argue that legal limits must be imposed on AI risk assessment systems in the criminal justice context. (extracted from154.txt)
Legal restrictions or legislative red -lines on the uses which contravene fundamental rights, including, but not limited to, uses of AI at the border, predictive policing, systems which restrict access to social rights and benefits, and risk -assessment tool s in the criminal justice context; 3. (extracted from154.txt)
Yours sincerely, European Digital Rights (EDRi), including: Access Now Bits of Freedom Chaos Computer Club D3 - Defesa dos Direitos Digitais Electronic Privacy Information Center (EPIC) Fitug Hermes Center Homo Digitalis IT-Pol Denmark Iuridicum Remedium Metamorphosis Foundation Panoptykon Foundation Privacy International Statewatch Other signatories: AI Now Institute, NYU Algorithm Watch Amnesty International App Drivers and Couriers Union (ADCU) Associazione Certi Diritti Associazione Luca Coscioni Associazione per gli Studi Giuridici sull'Immigrazione Big Brother Watch Center for Intersectional Justice (CIJ) Democratic Society Digitale Freiheit Dutch Section - International Commission of Jurists (NJCM) Each One Teach One (EOTO) e.V. (extracted from154.txt)
A shared statement of civil rights concerns’, signed by 119 civil rights organisations in the US’ (2018) http://civilrightsdocs.info/pdf/criminal -justice/Pretrial -Risk-Assessment Full.pdf accessed on 4 October 2018. (extracted from140.txt)
Binns et al., ‘‘It’s reducing a human being to a percentage ’; Perceptions of justice in algorithmic decisions ’, CHI 2018, April 21–26, 2018, Montréal, QC, Canada, https://dl.acm.org/citation.cfm?id=3173951 accessed 1 October 2018. (extracted from140.txt)
‘Intersectional discrimination in EU gender equality and non discrimination law, European network of legal ex perts in gender equality and non-discrimination’ (Report for European Commission, Directorate -General for Justice and Consumers), May 2016 https://publications.europa.eu/en/publication -detail/ -/publication/d73a9221 b7c3 -40f6-8414 -8a48a2157a2f/language -en accessed on 9 October 2018. (extracted from140.txt)
et al., ‘Blind justice: Fairness with encrypted sensitive attributes’ (2018) Proceedings of the 35th International Conference on Machine Learning (ICML 2018). (extracted from140.txt)
Rieke A, Robinson DG and Yu H, ‘Civil rights, big data, and our algorithmic future: A September 2014 report on social justice and technology (Version 1.2), Washington, DC: Upturn, PDF version, https://bigdata.fairness.io accessed 1 October 2018. (extracted from140.txt)
Taylor L, ‘What is data justice? (extracted from140.txt)
Accept additional cookies Reject additional cookies View cookies Hide cookie message Skip to main content GOV.UK Navigation menu Menu Menu Services and information Benefits Births, death, marriages and care Business and self-employed Childcare and parenting Citizenship and living in the UK Crime, justice and the law Disabled people Driving and transport Education and learning Employing people Environment and countryside Housing and local services Money and tax Passports, travel and living abroad Visas and immigration Working, jobs and pensions Government activity Departments Departments, agencies and public bodies News News stories, speeches, letters and notices Guidance and regulation Detailed guidance, regulations and rules Research and statistics Reports, analysis and official statistics Policy papers and consultations Consultations and strategy Transparency Data, Freedom of Information releases and corporate reports Search GOV.UK × Search GOV.UK Search Search GOV.UK Search Home Business and industry Press release New national innovation centre to put UK at forefront of big data A new £30m National Innovation Centre for Data (NICD) aims to see the next Google or Facebook started in the UK and help the country capitalise on a potential £40bn a year boost to the economy. (extracted from183.txt)
Cancel Services and information Benefits Births, death, marriages and care Business and self-employed Childcare and parenting Citizenship and living in the UK Crime, justice and the law Disabled people Driving and transport Education and learning Employing people Environment and countryside Housing and local services Money and tax Passports, travel and living abroad Visas and immigration Working, jobs and pensions Government activity Departments News Guidance and regulation Research and statistics Policy papers and consultations Transparency How government works Get involved Support links Help Privacy Cookies Accessibility statement Contact Terms and conditions Rhestr o Wasanaethau Cymraeg Government Digital Service All content is available under the Open Government Licence v3.0, except where otherwise stated © Crown copyright (extracted from183.txt)
Forexample, social network maps forservices such asTwitter candetermine credit-worthiness [5],opaque recidivism scores canshape criminal justice decisions inaracially disparate manner [6],and categorization based onzipcodes resulted inlessaccess toAmazon Prime same-day delivery service forAfrican-Americans inUnited States cities [7].These high-profile cases show that apparently neutral data canyield discriminatory outcomes, thereby compounding social inequities. (extracted from544.txt)
There areanumber ofgood models forinterdisciplinary ethics research, such asthe trainings offered bytheScience and Justice research center attheUniversity ofCalifornia, Santa Cruz [32] and Values inDesign curricula [33]. (extracted from544.txt)
Science &Justice Research Center (Collaboratio nsGroup. (extracted from544.txt)
Pah et al., How to Build a More Open Justice System, 369 Sci. (extracted from222.txt)
An example is given by the US criminal justice system, which is increasingly resorting to the use of artificial agents to ease the burden of managing such a large system. (extracted from236.txt)
4 ProPublica (23/05/2016) Machine Bias 5 The Telegraph (24/03/2016) Microsoft deletes 'teen girl' AI after it became a Hitler -loving sex robot within 24 hours 6 The Guardian (03/08/2014) The death of privacy 7 http://ec.europa.eu/justice/data -protection/ 24 Greencoat Place, London SW1P 1BE • t +44 (0) 20 7798 6040 • e info@ibe.org.uk • www.ibe.org.uk • Charity No. (extracted from236.txt)
1084014 Page 4 Business Ethics and Artificial Intelligence Issue 58 | January 2018 Fairness Fairness and justice, which are core issues in the stakeholder theory, remain paramount for ethical businesses when dealing with AI. (extracted from236.txt)
Skip to main content Skip to "About Canada.ca" Language selection Françaisfr Search CanadaBuys Search Canada.ca Menu Main Menu Home Jobs and the workplace Immigration and citizenship Travel and tourism Business and industry Benefits Health Taxes Environment and natural resources National security and defence Culture, history and sport Policing, justice and emergencies Transport and infrastructure Canada and the world Money and finances Science and innovation Mid Level Menu Account access CanadaBuys Menu Home Getting started Tender opportunities How procurement works Buyer’s Portal Support Account access You are here Canada.ca CanadaBuys home CanadaBuys CanadaBuys Welcome to the home for doing business with the Government of Canada and the broader Canadian public sector. (extracted from593.txt)
How Responsible AI can improve business and preserve value | PwC Skip to content Skip to footer Industries Services Issues About us Careers More Search Menu Industries Industries See all industries Consumer markets Energy, utilities and resources Financial services Government and public sector Health industries Industrials and services Private equity and principal investors Technology, media and telecommunications Menu Industries See all industries Menu Industries Consumer markets Consumer goods Hospitality and leisure Retail Transportation and logistics Menu Industries Energy, utilities and resources Chemicals Mining and metals Oil and gas Power and utilities Menu Industries Financial services Asset and wealth management Banking and capital markets Insurance Real estate Menu Industries Government and public sector Defence Education and skills Public safety, justice and security Menu Industries Health industries Health services Pharmaceuticals and life sciences Menu Industries Industrials and services Aerospace, defence and security Automotive Business services Engineering and construction Industrial manufacturing Space Menu Industries Private equity and principal investors Capital projects and infrastructure Menu Industries Technology, media and telecommunications Entertainment and media Technology Telecommunications Featured Value in motion Industry edge Engine by Starling: From launching a bank to launching a software business Menu Services Services See all services Alliances Artificial Intelligence Audit and assurance services Business model reinvention Business transformation Consulting Crisis management Deals Entrepreneurial and private business Family business Forensics Legal business solutions Managed Services Risk services Strategy Sustainability and climate change Tax Trust Workforce Menu Services See all services Menu Services Alliances Menu Services Artificial Intelligence Menu Services Audit and assurance services Actuarial services Capital markets Corporate reporting Financial audit IFRS Internal audit Next Generation Audit Risk assurance Sustainability assurance Menu Services Business model reinvention Menu Services Business transformation Menu Services Consulting Cloud transformation Digital operations Deals Finance transformation Forensics Front office transformation HR transformation Risk and regulation Strategy Technology Workforce Menu Services Crisis management Menu Services Deals Acquisitions Capital markets Corporate finance Deals strategy Joint ventures and alliances M&A legal M&A tax Performance and Restructuring​ Sovereign wealth funds Menu Services Entrepreneurial and private business Menu Services Family business Menu Services Forensics Menu Services Legal business solutions Employment Entity governance and compliance International business reorganisations Mergers and acquisitions NewLaw Menu Services Managed Services Menu Services Risk services Menu Services Strategy Menu Services Sustainability and climate change Climate risk, resilience and adaptation Energy solutions Impact management for sustainable business strategy Legal and sustainability Net zero transformation Social sustainability Sustainability assurance Sustainable capital Sustainability reporting Tax and sustainability Menu Services Tax Indirect taxes International tax services Mergers and acquisitions Sightline Tax code of conduct Tax controversy and dispute resolution Tax policy and administration Tax reporting and strategy Transfer pricing Menu Services Trust Menu Services Workforce Culture, leadership and change Inclusion Employment law Employment tax and payroll HR transformation and technology Organisational design People analytics and insights People in deals Retirement and pensions Reward and benefits Workforce risk Workforce strategy Featured Climate risk, resilience and adaptation Business transformation Sustainability assurance Menu Issues Issues See all issues Business model reinvention Business transformation C-suite insights Cybersecurity Climate and sustainability Megatrends Risk and regulation Technology Trust Upskilling Workforce Menu Issues See all issues Menu Issues Business model reinvention Menu Issues Business transformation Menu Issues C-suite insights Accelerating performance Global CEO Survey PwC at Davos strategy+business digital issue Take on Tomorrow: a strategy+business podcast The Leadership Agenda Menu Issues Cybersecurity Menu Issues Climate and sustainability Menu Issues Megatrends Menu Issues Risk and regulation Menu Issues Technology Menu Issues Trust Menu Issues Upskilling Menu Issues Workforce Featured Value in motion PwC’s 28th Annual Global CEO Survey The Fearless Future: 2025 Global AI Jobs Barometer Menu About us About us See more About Us Alumni Analyst relations Client case studies Ethics and compliance Committing to Net Zero Corporate sustainability Inclusion PwC's Global Annual Review Global regulatory affairs Human rights policy Network Leadership, Governance, and Structure New Ventures and Innovation News room Purpose, values and behaviours PwC office locations PwC's Code of Conduct Strategy& Tax Code of Conduct Third party code of conduct Transparency Report Menu About us See more About Us Menu About us Alumni Menu About us Analyst relations Menu About us Client case studies Menu About us Ethics and compliance Menu About us Committing to Net Zero Menu About us Corporate sustainability Menu About us Inclusion Menu About us PwC's Global Annual Review Menu About us Global regulatory affairs Menu About us Human rights policy Menu About us Network Leadership, Governance, and Structure Menu About us New Ventures and Innovation Menu About us News room Menu About us Purpose, values and behaviours Menu About us PwC office locations Menu About us PwC's Code of Conduct Menu About us Strategy& Menu About us Tax Code of Conduct Menu About us Third party code of conduct Menu About us Transparency Report Featured Value in motion Committing to Net Zero Global Transparency Report 2024 Menu Careers Careers Find out more about careers Search for a job Menu Careers Find out more about careers Menu Careers Search for a job Featured PwC’s Global Annual Review Committing to net zero The CEO’s ESG dilemma Loading Results No Match Found View All Results Designing, building and operating AI that delivers real-world impact Responsible AI Artificial intelligence is transforming business by streamlining activities, enhancing customer offerings, making workers more effective and speeding up innovation—prompting executives to deploy intelligent applications and agentic systems. (extracted from554.txt)
Other common principles include human oversight, explainability or interpretability, legal status of AI systems, and the equitable economic effect of AI.31 A separate analysis of 84 AI ethics documents done in 2019 found that there has been a global convergence around “transparency, justice and fairness, non-maleficence, responsibility and privacy.”32 While much progress has been made aligning on responsible AI, there remain differences—even among FCAI participants. (extracted from45.txt)
Rules, Standards, and R&D Projects: Key areas for collaboration | ⮌ contents 62the importance of cross-border data flows as well as challenges to privacy and other values.211 Work to advance data free with trust has continued through the World Economic Forum.212 The “Schrems II” judgment by the Court of Justice of the European Union (CJEU) in 2020, however, has been a seismic event for international transfers of personal information, the aftershocks of which are still reverberating and magnify the impact of the EU regime. (extracted from45.txt)
Kerry, ”The oracle at Luxembourg: The EU Court of Justice judges the world on surveillance and privacy,” The Brookings Institution, January 11, 2021, https://www.brookings.edu/ research/the-oracle-at-luxembourg-the-eu-court-of-justice-judges-the-world-on-surveillance-and-privacy/. (extracted from45.txt)
Kerry, ”The oracle at Luxembourg: The EU Court of Justice judges the world on surveillance and privacy,” The Brookings Institution, January 11, 2021, https://www.brookings.edu/research/the-oracle-at-luxembourg-the-eu-court-of-justice-judges-the-world-onsurveillance-and-privacy/. (extracted from45.txt)
AI tightly developed under human oversight and control), embed the principles of fairness and justice in algorithms, applied in all phases of AI systems’ design, implementation and testing, while following through bias complaints and other undesired effects reporting. (extracted from187.txt)
During the 71st session of the United Nations General Assembly, on 29 September 2016, UNICRI (United Nations International Crime and Justice research Institute) announced the opening of the first Centre on A I and Robotics KEY FINDINGS There is not yet robust evidence of AI applications used for addressing significant and wideranging real -life problems or societal challenges. (extracted from187.txt)
AI tightly developed under human oversight and control), embed the principles of fairness and justice in algorithms, applied in all phases of AI systems’ design, i mplementation and testing, while following through bias complaints and other undesired effects reporting. (extracted from187.txt)
P eace, inclusiv eness and justice, equity and interconnectedness should be promoted throughout the lifecycle of AI systems . (extracted from178.txt)
These include freedom, dignity and autonomy, privacy and data protection, non discrimination and equality, diversity, fairness, social justice, and internationally recognised labour rights'. (extracted from178.txt)
FAIR TRIAL AND DUE PROCESS The trend towards using automated processing techniques and algorithms in crime prevention and the criminal justice system is growing . (extracted from86.txt)
While it is unclear how prevalent such decisions created by algorithms are in the criminal justice system generally, the mere potential of their use raises serious concerns with regard to Article 6 of the ECHR and the principle of equal ity of arms and adversarial proceedings as established by the European Court of Human Rights.5 Furthermore, algorithms are increasingly used in the context of the civil and criminal justice systems where artificial intelligence is being developed to event ually support or replace decision -making by human judges. (extracted from86.txt)
51 See Laurel Eckhouse, “Big data may be reinforcing racial bias in the criminal justice system”, available at: https://www.washingtonpost.com/opinions/big -data-may-be-reinforcing -racial-bias-in-the-criminal -justice system/2017/02/10/d63de518 -ee3a-11e6-9973-c5efb7ccfb0d_story.html?utm_term=.720084735d73 (last visited on 25 September 2017) ; and ProPublica, Angwin, Julia, Surya Mattu, and Lauren Kirchner, “Machine Bias: There’s Softwar e Used Across the Country to Predict Future Criminals. (extracted from86.txt)
At present t he public sector in Europe is employing automated decision -making in areas as diverse as social security, taxation, health care and the justice system (van Haastert 2016; Tufekci et al. (extracted from86.txt)
Whether in the criminal justice, social media, healthcare, insurance or banking sector, to name just a few examples, each area will need specific regulatory responses to ensure greater transparency and accountability of automated data -processing and algorit hmic decision -making systems. (extracted from86.txt)
Laurel Eckhouse, “Big data may be reinforcing racial bias in the crim inal justice system”, available at: https://www .washingtonpost.com/opinions/big -data-may-be-reinforcing -racial -bias-in-thecriminal -justice -system/2017/02/10/d63de518 -ee3a-11e6-9973c5efb7ccfb0d_story.html?utm_term=.720084735d73 . (extracted from86.txt)
Moreover, a few specific cases, mentioned as running at the time of first gathering, were later found to have been discontinued because of various reasons , including significant criticism received from the general public , pressure from adversarial political forces or even executive orders from local courts of criminal justice. (extracted from144.txt)
— Fairness and justice : The development of AI should promote fairness and justice, the rights a nd interests of stakeholders and promote equality of opportunity. (extracted from144.txt)
Previous research had analyse d 84 ethical AI documents published by various business es, NGOs and (international) governmental organizations, highlight ing that some principles such as tra nsparency, fairness, justice, and responsibility were quite common in all. (extracted from144.txt)
The impact of using algorithms for managerial decisions on public employees’ procedural justice. (extracted from144.txt)
For example, the use of artificial intelligence tools by law enforcement and the criminal justice system could have an impact on an individual ’s right to be free from arbitrary arrest or to equality before the law; surveillance technologies c ould impact on the right to peaceful assembly; the use of social media platforms could impact the right to mental health; and property rental platforms could alter housing markets, possibly impacting the right to an adequate standard of living. (extracted from385.txt)
Hollywood, “Evaluation of the Shreveport Predictive Policing Experiment,” National Institute of Justice, 2014, https://nij.ojp.gov/topics/articles/evaluation-shreveport- predictive-policing-experiment . (extracted from420.txt)
4.2.6 Ethics and society It is necessary to guarantee that the uses of artificial intelligence are focused on humans’ well -being: artificial intelligenc e must be developed, applied and used with an ethical purpose based on fundamental rights, our social and cultural values, and the ethical principles of beneficence, autonomy of human beings, justice and the necessary explainability of their results . (extracted from352.txt)
Artificial intelligence should be used in a responsible, sensible and secure way and must include ethical reasoning, in terms of following and maintaining tradition and the European differentiating fact for everything that affects people and their development, as well as guarantee ing justice, transparency and lawfulness. (extracted from352.txt)
In 1890, future Supreme Court Justice Louis Brandeis took the first step in advocating for privacy protection when he co-authored an article with colleague Samuel Warren in the Harvard Law Review advocating “the right to be let alone.” The two argued that the development of “instantaneous photographs” and their circulation by newspapers for commercial gain had created the need to protect people with a new “right to privacy.” Technology today gives a new meaning to “instantaneous photographs” that Brandeis and Warren probably never imagined. (extracted from434.txt)
United States, Chief Justice John Roberts wrote for a majority of the court that an individual has a “legitimate expectation of privacy in the record of his physical movements” that are recorded in these cell site records. (extracted from434.txt)
Consider an appropriate mechanism for ensuring a coordinated approach by Irish regulators to Digital, including AI, as part of the forthcoming National Digital Strategy [DOT / DETE / DECC D/ Justice / DTCAGSM / DCEDIY] iii. (extracted from353.txt)
For example, the Company Law Review Group submitted a report on AI to the T ánaiste and Minister for Enterprise, Trade and Employment in December 2020, which analysed possible impacts of the increased use of AI in the context of company law and corporate governance matters.20 This work to address regulatory gaps spans a wide range of legal and regulatory regimes including data protection; justice; policing; intellectual property; transport and haulage; finance; health; human rights, export controls; consumer protection; competition law and company law.A Risk-Based Approach to Regulation Prohibited Permitted subject to compliance with AI requirements and ex-ante conformity assessment Permitted but subject to information/ transparency obligations Permitted with no restrictions HIGH RISK e.g. (extracted from353.txt)
31 In 2021, the ADAPT Centre, Science Gallery Dublin and other partners engaged in an innovative multidisciplinary collaboration between artists and technologists on the theme of bias, exploring AI, Ethics, Trust and Justice.32 In a Europe-wide first, a cutting-edge law and technology module has been rolled out for undergraduate students at Maynooth University. (extracted from353.txt)
Consider an appropriate mechanism for ensuring a coordinated approach by Irish regulators to Digital, including AI, as part of the forthcoming National Digital Strategy [DOT/ DETE / DECC / D/Justice/ DTCAGSM / DCEDIY] iii. (extracted from353.txt)
Funding has been directed at AI solutions across a number of industry sectors, including video production; criminal justice and security; patient empowerment; and decision support systems in medical care. (extracted from353.txt)
The EU AI High Level Expert Group (AI HLEG) identifies several areas of opportunity within the public sector where it considers the adoption of AI to be of utmost importance for the well-being of society, as well as for enhancing sustainable growth - these are: the e-Government domain; Justice and law enforcement; and the Healthcare sector.49 The European Commission is prioritising public sector dialogues on AI in healthcare, rural administrations and among public service operators,50 and it has outlined the creation of data spaces in health, agriculture and transport.51 Irish Revenue AI voicebot helps citizens handle tax clearance In early 2018, the Irish Revenue Commissioners initiated a pilot project to examine if AI-based Natural Language Processing (NLP) technologies could be used to deliver an improved customer service, reduce costs and increase efficiencies. (extracted from353.txt)
JUSTICE SECTOR The EU e-Justice Strategy and Action Plan 2019-2023 identifies the use of AI as a priority area in the justice field. (extracted from353.txt)
However, the use of AI within the justice sector also has considerable implications for ethics, human rights and the rule of law. (extracted from353.txt)
European Commission for the Efficiency of Justice (CEPEJ), European Ethical Charter on the Use of Artificial Intelligence in Judicial Systems and their environment, December 2018, accessed at: https://rm.coe.int/ethical-charter-en-for-publication-4-december-2018/16808f699c 55. (extracted from353.txt)
70 AI – HERE FOR GOOD A National Artificial Intelligence Strategy for IrelandAI Artificial Intelligence ALTAI Assessment List for Trustworthy Artificial Intelligence CAHAI Ad Hoc Committee on Artificial Intelligence CCPC Competition and Consumer Protection Commission CCI Cybersecurity and Cybercrime Investigation CeADAR Ireland’s Centre for Applied Artificial Intelligence CEN European Committee for Standardization CENELEC European Committee for Electrotechnical Standardization CEPEJ European Commission for the Efficiency of Justice CLRG Company Law Review Group CRT Centres for Research Training CSO Central Statistics Office DAFM Department of Agriculture, Food and the Marine DFA Department of Foreign Affairs DCEDIY Department of Children, Equality, Disability, Integration and Youth DCU Dublin City University DETE Department of Enterprise, Trade and Employment DECC Department of the Environment, Climate and Communications DESI Digital Economy and Society Index DFHERIS Department of Further and Higher Education, Research, Innovation and Science DFKI German Research Centre for Artificial Intelligence DIH Digital Innovation Hub DoE Department of Education DoJ Department of Justice DoH Department of Health DPAI Data Protection Impact Assessment DPER Department of Public Expenditure and Reform DTCAGSM Department of T ourism, Culture, Arts, Gaeltacht, Sport and Media DTIF Disruptive T echnologies Innovation Fund EDIHs European Digital Innovation Hubs EGFSN Expert Group on Future Skills Needs EI Enterprise Ireland EPA Environmental Protection Agency ESO European Standards Organizations ETSI European T elecommunications Standards Institute EU European Union FET Further Education and Training GCID Grand Canal Innovation District GDP Gross Domestic Product GDPR General Data Protection RegulationGlossary of Acronyms 71 AI – HERE FOR GOOD A National Artificial Intelligence Strategy for IrelandGGE on LAWS UN Group of Governmental Experts on Lethal Autonomous Weapons Systems GPAI Global Partnership on AI GSI Geological Survey Ireland HE Higher Education HEA Higher Education Authority HEI Higher Education Institutes HLEG EU AI High Level Expert Group HPC High Performance Computing HSE Health Service Executive IA Impact assessments ICHEC Irish Centre for High-end Computing ICT Information and Communications IEC International Electrotechnical Commission IF SFI Industry Fellowship IMR Irish Manufacturing Research IP Intellectual Property IPCEI Important Projects of Common European Interest IRC Irish Research Council ISO International Organization for Standardization ITI InterTrade Ireland MOOC Massive Open Online Course MNE Multinational Enterprise NSAI National Standards Authority of Ireland OECD Organisation for Economic Cooperation and Development OGP Office of Government Procurement ONE-AI OECD on Network of Experts on AI RCSI Royal College of Surgeons Ireland R&I Research and innovation REC Research Ethics Committee RTEF Reference T esting and Experimentation Facilities SEAI Sustainable Energy Authority of Ireland SFI Science Foundation Ireland SME Small and Medium-Sized Enterprises STEM Science, technology, engineering, and mathematics TUD T echnological University Dublin UCC University College Cork UCD University College Dublin UN United Nations UNGP UN Guiding Principles on Business and Human Rights WAI Women in AI WIPO World Intellectual Property Organisation 72 AI – HERE FOR GOOD A National Artificial Intelligence Strategy for IrelandList of Organisations Consulted The list below reflects the organisations that took part in a range of stakeholder engagements throughout the development of this Strategy. (extracted from353.txt)
The written submissions to the strategy are available on the Department of Enterprise, Trade and Employment’s website at: https://enterprise.gov.ie/en/Consultations/Public-Consultation-Development-of-a-National-Strategy-on-Artificial-Intelligence.html  30% Club  Accenture  AJH Emerging T echnology Intelligence  Allied Irish Banks  American Chamber of Commerce  Arvoia  Cainthus  CarTrawler  CeADAR Ireland’s Centre for Applied Artificial Intelligence  Central Bank  Chambers Ireland  Concern Worldwide  CONFIRM Centre (AIT)  CONSUS (Crop Optimisation through Sensing, Understanding & viSualisation)  CR Robotics  Data Protection Commission  Department of Agriculture, Food and the Marine  Department of Children, Equality, Disability, Integration and Youth  Department of Defence  Department of Education  Department of Enterprise, Trade and Employment  Department of Environment, Climate and Communication  Department of Finance  Department of Foreign Affairs  Department of Further and Higher Education, Research, Innovation and Science  Department of Health  Department of Housing, Local Government and Heritage  Department of Justice  Department of Public Expenditure and Reform  Department of Rural and Community Development  Department of Social Protection  Department of the T aoiseach  Department of T ourism, Culture, Arts, Gaeltacht, Sport and Media  Department of Transport  Digital Skills Global  Dublin Chamber of Commerce  Dublin City University, Business School  Dublin City University, School of Computing  Dublin City University, School of Electronic Engineering  Edgetier  Enable Ireland  Enterprise Ireland  Fotonation / Xperi  FourThereom  Freedomtech  FTI Consulting  Genesys  Health and Safety Authority  Ibec  IBM  ICT Skillnet  IDA Ireland  I-Form Advanced Manufacturing Research Centre  Industry Research and Development Group  Insight Centre for Data Analytics  Institute of Chartered Accountants Ireland  Insurance Ireland  Intel  InterTrade Ireland  Irish Centre for High-End Computing  Irish Computer Society/ICS Foundation  Irish Congress of Trade Unions  Irish Human Rights and Equality Commission  Irish Institute of Digital Business  Irish Manufacturing Research  Irish Marie Skłodowska-Curie Office  Irish Small and Medium Enterprises Association  Irish Universities Association  Jaguar Land Rover 73 AI – HERE FOR GOOD A National Artificial Intelligence Strategy for Ireland  Kerry Group  Law Society of Ireland  Learnovate  Letterkenny Institute of T echnology, Department of Computing  Lincoln Recruitment  Live tiles  Mason Hayes Curran  Mastercard Labs  Met Éireann  Microsoft  National Archives  National Standards Authority of Ireland  National University of Ireland Galway, School of Computer Science  National University of Ireland Maynooth, Department of Computer Science  National University of Ireland Maynooth, School of Business  Nokia Bell Labs  Office of the Government Chief Information Officer  Office of the Revenue Commissioners  Science Foundation Ireland  Science Foundation Ireland Centre for Research Training in Machine Learning  ServisBot  Skillnet Ireland  Small Firms Association  SOLAS  Swrve  T alent Garden  T ech Ireland  T echnological University Dublin, School of Computer Science  The ADAPT Centre  The Competition and Consumer Protection Commission  Trinity College Dublin, School of Computer Science and Statistics  Trinity College Dublin, School of Creative Arts  Truata  Ubotica  University College Cork, School of Computer Science & IT  University College Dublin, School of Computer Science  University College Dublin, School of Information and Communication Studies  University of Limerick LERO, Science Foundation Ireland Research Centre for Software  Valeo  Version1  Vodafone  Waterford Institute of T echnology, T elecommunications Software & Systems Group  Webio 74 AI – HERE FOR GOOD A National Artificial Intelligence Strategy for Ireland13 (extracted from353.txt)
about the purpose and aims of ECDEC, about children’s rights, democracy, about the importance of addressing diversity, equality, and social justice). (extracted from637.txt)
It is a crucial task to enable systematic encounters and democratic dialogue between all stakeholders in order to raise awareness of own and others’ values, and to work towards a shared orientation towards rights, equality, and social justice for all children and families. (extracted from637.txt)
UNESCO, World Bank) Own historiy, culture, values individual and group identity Local knowledge(s) and practices Resistance Creativity ‘Grassroots’ Competent Systems: for social justice, diversity and equalityForward planning Better understanding Evaluation Research and critical inquiryCreative InventionImplementation Interpretation 39 38 The Future of Work and Education for the Digital AgeCentro de Estudios Sociales (CES). (extracted from637.txt)
He works and publishes widely on questions of diversity and equality, social justice, evaluation and professionalism in working with young children, families and communities in diverse socio-cultural contexts. (extracted from637.txt)
FAIR TRIAL AND DUE PROCESS The trend towards using automated processing techniques and algorithms in crime prevention and the criminal justice system is growing . (extracted from145.txt)
While it is unclear how prevalent such decisions created by algorithms are in the criminal justice system generally, the mere potential of their use raises serious concerns with regard to Article 6 of the ECHR and the principle of equal ity of arms and adversarial proceedings as established by the European Court of Human Rights.5 Furthermore, algorithms are increasingly used in the context of the civil and criminal justice systems where artificial intelligence is being developed to event ually support or replace decision -making by human judges. (extracted from145.txt)
51 See Laurel Eckhouse, “Big data may be reinforcing racial bias in the criminal justice system”, available at: https://www.washingtonpost.com/opinions/big -data-may-be-reinforcing -racial-bias-in-the-criminal -justice system/2017/02/10/d63de518 -ee3a-11e6-9973-c5efb7ccfb0d_story.html?utm_term=.720084735d73 (last visited on 25 September 2017) ; and ProPublica, Angwin, Julia, Surya Mattu, and Lauren Kirchner, “Machine Bias: There’s Softwar e Used Across the Country to Predict Future Criminals. (extracted from145.txt)
At present t he public sector in Europe is employing automated decision -making in areas as diverse as social security, taxation, health care and the justice system (van Haastert 2016; Tufekci et al. (extracted from145.txt)
Whether in the criminal justice, social media, healthcare, insurance or banking sector, to name just a few examples, each area will need specific regulatory responses to ensure greater transparency and accountability of automated data -processing and algorit hmic decision -making systems. (extracted from145.txt)
Laurel Eckhouse, “Big data may be reinforcing racial bias in the crim inal justice system”, available at: https://www .washingtonpost.com/opinions/big -data-may-be-reinforcing -racial -bias-in-thecriminal -justice -system/2017/02/10/d63de518 -ee3a-11e6-9973c5efb7ccfb0d_story.html?utm_term=.720084735d73 . (extracted from145.txt)
The development of artificial intelligence should ensure fairness and justice, avoid bias or discrimination against specific groups or individuals, and avoid placing disadvantaged people in an even more unfavorable position.Article 4: Avoid harm. (extracted from93.txt)
At EU level, t he processing of biometric data has been actively encouraged and directly supported over the past years in the context of EU -level large -scale information technology ( IT) systems in the area of freedom, security and justice ( AFSJ ). (extracted from179.txt)
A review of this architecture and of the most relevant rules on biometrics and on automated decision -making in EU data protection la w, as well as of the most important case law in this area emanating from the Cour t of Justice of the EU (CJEU) and the European Court of Human Rights (ECtHR) , shows that ongoing technological developments are taking place amid – and possibly also somehow despite – existing rights and principles , which might thus possibly need to be reinforced, clarified, or at least fine -tuned. (extracted from179.txt)
The processing of biometric data has been actively supported at EU level23 in the context of EU -level large -scale IT systems in the Area of Freedom, Security and Justice (AFSJ). (extracted from179.txt)
I t also presents the most important case law in this area emanating from the Court of Justice of the EU (CJEU) and the European Court of Human Rights (ECtHR). (extracted from179.txt)
Legal aid shall be made available to those who lack sufficient resources in so far as such aid is necessary to ensure effective access to justice. (extracted from179.txt)
Fundamental rights case law Analysing the case law of the Court of Justice of the EU (CJEU) on biometrics, a series of points stand out. (extracted from179.txt)
As underlined in a report for the Committee on Equality and Non- Discrimination of the Council of Europe 's Parliamentary Assembly, certain flaws in a the criminal justice system can have ' far-reaching human rights consequences ' (Lacroix 2020 12). (extracted from179.txt)
Other areas mentioned as involving the qualification of an AI system as high risk are management and operation of critical infrastructure , educational and vocational training ; employment , workers management and acces s to self -employment; access to and enjoyment of essential private services and public services and benefits; law enforcement ; migration, asylum and border control management ; administration of justice and democratic processes . (extracted from179.txt)
' Under heading 8, on ' Administration of justice and democratic processes ', are mention ed: '(a) AI systems intended to assist a judicial authority in researching and interpreting facts and the law and in applying the law to a concrete set of facts. (extracted from179.txt)
European Union Agency for the Operatio nal Management of Large -Scale IT Systems in the Area of Freedom, Security and Justice (eu -LISA). (extracted from179.txt)
admission and grading ·Employment, worker management, and access to self-employment opportunities, including systems that make or inform decisions about hiring, firing, and task allocation ·Access to and enjoyment of essential private services and public services and benefits ·Specific uses of law enforcement ·Specific uses in migration, asylum, and border control management ·Administration of justice and democratic processes, in particular when used to research and establish facts or applying the law to some factsProviders of high-risk systems must perform a conformity assessment to make sure that they are compliant with requirements including: ·Risk management system ·Data requirements ·T echnical documentation ·Record-keeping ·T ransparency on the system’s functioning ·Human oversight ·Accuracy, robustness, and cybersecurity ·Post-market monitoringFines up to 4% of global revenue or 20mn euros, whichever is higher, for everything except the data requirements, where the same fines apply as for the prohibited systems LimitedRisk: Transparency Obligations (TitleIV)·AI systems interacting with natural persons ·Emotion recognition systems or biometric categorisation systems ·AI system that generates or manipulates image, audio, or video content that appears realNotify the user that they are engaging with an AI systemFines up to 4% of global revenue or 20mn euros, whichever is higher MinimalRisk: Voluntary Codesof Conduct (TitleIX)All AI systems that are not either prohibited or high- riskProviders can choose to comply with voluntary codes of conduct. (extracted from87.txt)
276European Commission and Directorate-General for Justice and Consumers, Liability for Artificial Intelligence and Other Emerging Digital Technologies (Publications Office of the European Union, 2019); European Commission, “InceptionImpactAssessment:ProposalforaDirectiveAdaptingLiability RulestotheDigitalAgeandArtificialIntelligence.” 277“The EU Product Liability Directive (PLD), that governs the responsibility for such defects, should be applied ‘without prejudice’ to the product safety regime” European Parliament, “Directive2001/95/ECoftheEuropeanParliamentandoftheCouncilof3December2001onGeneralProductSafety (TextwithEEARelevance),” CELEX number: 32001L0095, Official Journal of the European Union L 11, January 15, 2002, 4–17, art. (extracted from87.txt)
Schwartz, “Global Data Privacy: The EU Way.” 406Court of Justice of the European Union, “Judgment in Case C-362/14 Maximillian Schrems v Data Protection Commissioner: The Court of Justice DeclaresThattheCommission’sUSSafeHarbourDecisionIsInvalid,” Press Release 117/15 (Court of Justice of the European Union , October 6, 2015); European Commission, “EU-US Data Transfers: How Personal Data Transferred between the EU and US Is Protected,” European Commission, accessed July 14, 2022. (extracted from87.txt)
Both agreements were adopted even though the US data privacy standards were not equivalent to the EU, which is a requirement for data transmission agreements in both the DPD and GDPR.405Consequently, both data transmission agreements were declared invalid by the European Court of Justice (ECJ) in 2015 and 2020, respectively.406Since 2020, the US and the Commission have stated their intention to negotiate a new agreement. (extracted from87.txt)
Court of Justice of the European Union. (extracted from87.txt)
“ Judgment in Case C-362/14 Maximillian Schrems v Data Protection Commissioner: The Court of Justice Declares That the Commission’s US Safe Harbour Decision Is Invalid. (extracted from87.txt)
Court of Justice of the European Union , October 6, 2015. (extracted from87.txt)
https://ec.europa.eu/info/policies/justice-and-funda- THEBRUSSELSEFFECTANDARTIFICIALINTELLIGENCE•87 mental-rights/combatting-discrimination/racism-and-xenophobia/eu-code-conduct-countering-illegalhate-speech-online_en . (extracted from87.txt)
European Commission, and Directorate-General for Justice and Consumers. (extracted from87.txt)
Department of Commerce The following Federal departments and agencies are represented on the Subcommittee on Machine Learning and Artificial Intelligence and thro ugh it, work together to monitor the state of the art in machine learning (ML) and AI (within the Federal Government, in the private sector, and internationally), to watch for the arrival of important technology milestones in the development of AI, to coor dinate the use of and foster the sharing of knowledge and best practices about ML and AI by the Federal Government, and to consult in the development of Federal research and development priorities in AI: Department of Commerce (Co -Chair ) Depa rtment of Defense Department of Education Department of Energy Department of Health and Human Services Department of Homeland Security Department of Justice Department of Labor Department of State Department of Transportation Department of Treasury Department of Veterans Affairs United States Agency for International Development Central Intelligence Agency General Services Administration National Science Foundation National Security Agency National Aeronautics and Space Administration Office of the Director of National Intelligence Social Security Administration The following offices of the Executive Office of the President are also represented on the Subcommittee: Council of Economic Advisers Domestic Policy Council Office of Management and Budget Office of Science and Technology Policy (Co Chair) Office of the Vice President National Economic Council National Security Council PREPARING FOR THE FUTURE OF ARTIFICIAL INTELLIGENCE PREPARING FOR THE FUTURE OF ARTIFICIAL INTELLIGENCE Contents Executive Summary ................................ (extracted from192.txt)
30 Justice, Fairness, and Accountability ................................ (extracted from192.txt)
Public - and private sector investments in basic and applied R&D on AI have already begun reaping major benefits to the public in fields as diverse as health care, transportation, the environment, criminal justice, and economic inclusion . (extracted from192.txt)
Use of AI to make consequential decisions about people, often replacing decisions made by human -driven bureaucra tic processes , leads to concerns about how to ensure justice, fairness, and accountability —the same concerns voiced previously in the Administration’s Big Data: Seizing Opportunities, Preserving Values report of 2014,1 as well as the Report to the President on Big Data and Privacy: A Technological Perspective published by the President’s Council of Advisors on Science and Technology in 2014 .2 Transparency concerns focus not only on the data and algorithms involved, but also on the potential to have some form of explanat ion for any AI -based determination. (extracted from192.txt)
Many areas of public policy, from education and the economic safety net, to defense, envi ronmental preservation, and criminal justice, will see new opportunities and new challenges driven by the continued progress of AI. (extracted from192.txt)
The promise of AI has been compared to the transformative impacts of advances in mobile computing.21 Public - and private -sector investments in basic and applied R&D on AI have already begun reaping major benefits for the public in fields as diverse as health care, transportation, the environment, criminal justice, and economic inclusion .22 At Walter Reed Medical Center, the Department of Veteran Affairs is using AI to better predict medical complications and improve treatment of severe combat wounds, leading to better patient outcomes, faster healing, and lower costs.23 The same general approac h—predicting complications to enable preventive treatment —has also reduced hospital -acquired infections at Johns Hopkins University .24 Given the current transition to electronic health records, predictive analysis of health data may play a key role across m any health domains like precision medicine and cancer research. (extracted from192.txt)
Autonomous watercraft may be much cheaper to operate than manned ships, and may some day be used for enhanced weather prediction, climate monitoring, or policing illegal fishing.27 AI also has the potential to improve aspects of the criminal justice system, including crime reporting, policing, bail, sentencing, and parole decisions. (extracted from192.txt)
The Admini stration is exploring how AI can responsibly benefit current initiatives such as Data Driven Justice and the Police Data Initiative that seek to provide law enforcement and the public with data that can better inform decision -making in the criminal justice system, while also taking care to minimize the possibility that AI might introduce bias or inaccuracies due to deficiencies in the available data. (extracted from192.txt)
Social justice and public policy institutions that do not typically engage with advanced technologies and data science in their work should consider partnerships with AI researchers and practitioners that can help apply AI tactics to the broad social problems these institutions already address in other ways. (extracted from192.txt)
The u se of AI to make consequential decisions about people, often replacing decisions made by human actors and institutions , leads to concerns about how to ensure justice, fairness, and accountability —the same concerns voice d previously in the “ Big Data” context.62 The u se of AI to control physical -world equipment leads to concerns about safety, especially as systems are exposed to the full complexity of the human environment. (extracted from192.txt)
Justice, Fairness, and Accountability A common theme in the Law and Governance, AI for Social Good, and Social and Economic Impac ts workshops was the need to ensure that AI promotes justice and fairness, and that AI -based processes are accountab le to stakeholders . (extracted from192.txt)
In the criminal justice system, some of the biggest concerns with Big Data are the lack of data and the lack of quality data.65 AI needs good data. (extracted from192.txt)
It is important that anyone using AI in the criminal justice context is aware of the limitations of current data. (extracted from192.txt)
65 Matt Ford, “The Missing Statistics of Criminal Justice,” The Atlantic, May 31, 2015, http://www.theatlantic.com/politics/archive/2015/05/what -we-dont-know -about -mass -incarceration/394520/ 66 Julia Angwin, Jeff Larson, Surya Mattu, and Lauren Kirchner, “Machine Bias,” ProPublica, May 23, 2016, https://www.propublica.org/article/machine -bias-risk-assessments -in-criminal -sentencing. (extracted from192.txt)
Many areas of public policy, from education and the economic safety net, to defense , environmental prese rvation, and criminal justice, will see new opportunities and new challenges driven by the continued progress of AI. (extracted from192.txt)
Social justice and public policy institutions that do not typically engage with advanced t echnologies and data science in their work should consider partnerships with AI researchers and practitioners that can help apply AI tactics to the broad social problems these institutions already address in other ways. (extracted from192.txt)
Matt Ford, “The Missing Statistics of Criminal Justice,” The Atlantic, May 31, 2015, http://www.theatlantic.com/politics/archive/2015/05/what -we-dont-know-about -mass -incarceration/394520/ Jason Furman, “Is This Time Different? (extracted from192.txt)
Such tasks concern low -skilled as well as high ly-skilled personnel, for example in sectors such as banking, insurance or justice. (extracted from186.txt)
For the deployment of ADS , certification can be on either a voluntary basis (as encouraged by the GDPR) , or mandatory in certain areas such as justice and healthcare. (extracted from186.txt)
Decision -making algorithms are increasingly used in areas such as a ccess to information, e-commerce, recommendation systems, employment, health, justice, policing, banking and insurance. (extracted from186.txt)
Users Objectives Individuals Private sector Public sector Improvement of general Knowledge N/A Drugs discovery Climate Weather forecast Environment Healthcare Digital services Quantified -self Finance Note taking Smart home Recommendations Risk scoring Payment systems Targeting Personali sed services Predictive justice Predictive policing Hazard prediction Infrastructure development planning Physical systems Autonomous Cars Home Robots Security Personal assistants in the home Autonomous robots Autonomous weapons Defence Transport Smart cities Smart grids Understanding algorithmic decision -making: Opportunities and challenges 7 3. (extracted from186.txt)
' The fact that ADS can lead to discrimination has been documented in many areas , such as the justice system, targeted advertisements and employment. (extracted from186.txt)
Discrimination in justice : Another area that has raised much concern is the increasing reliance on ADS in the criminal justice system. (extracted from186.txt)
COMPAS scores can be used at different stages of the criminal justice system , e.g. (extracted from186.txt)
Several occurrences of this process have already been observed, not only in the field of justice with COMPAS, but also in education with the public debate raised by an algorithm called APB32 in Fr ance. (extracted from186.txt)
Understanding algorithmic decision -making: Opportunities and challenges 15 authors, 'perhaps even more problematic is the theory of justice impl icitly embedded in the algorithms' .48 The point is that most ADS used in this context are risk -assessment tools: based on a number of factors about the defendants ' criminal history, sociological data or demographic features, they provide an estimation of their risk of recidivism. (extracted from186.txt)
ADS are already in use in the medical sector and can potentially contribute to i mprove the decisions taken by practitioners and specialists in many ways: 48 Angèle Christin, Alex Rosenblat, Danah Boyd; Courts and predictive algorithms; Data & Civil Rights: A new era of poli cing and justice; 2015. (extracted from186.txt)
Public services ADS are currently being used by government and public agencies to provide new services or improve existing ones in many areas , such as energy, education, healthcare, transportation, justice systems and security . (extracted from186.txt)
These tasks concern low -skilled as well as high lyskilled personnel, for example in sectors like banking, insurance or justice. (extracted from186.txt)
ADS Fairness As ADS replace or support human decision -makers in a number of sensitive domains such as justice, health or education, it is important to ensure that they do not result in decisions that are considered unfair or discriminatory. (extracted from186.txt)
For example, it is well known that, in certain cities, there is a strong correlation between the reli gion or 141 Richard Berk, Hoda Heidari, Shahin Jabbari, Michael Kearns, Aaron Roth; Fairness in Criminal Justice Risk Assessments: The State of the Art; Sociological Methods & Research; 2018. (extracted from186.txt)
186 Richard Berk, Hoda Heidari, Shahin Jabbari, Michael Kearns, Aaron Roth; Fairness in Criminal Justice Risk Assessments: The State of the Art; Sociological Methods & Research; 2018. (extracted from186.txt)
187 Richard Berk, Hoda Heidari, Shahin Jabbari, Michael Kearns, Aaron Roth; Fairness in Criminal Justice Risk Assessments: The State of the Art; Sociological Methods & Research; 2018. (extracted from186.txt)
Ethical and political debate As illustrated in Chapter 3, ADS raise far reaching issues in many areas such as justice, policing, healthcare, democratic life, etc. (extracted from186.txt)
Another example of the systematic analysis of ethical issues that can be useful in this context is the EDPS Ethics Advisory Group Report,204 which proposes a list of ' foundational values to digital ethi cs': dignity, freedom, autonomy, solidarity, equality, democracy, justice and trust. (extracted from186.txt)
As seen previously, different approaches can be taken to 212 Rebecca Wexler; Life, liberty, and trade secrets : intellec tual pro perty in the criminal justice system; (70); Standford Law Review; (1343); 2018; https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2920883 . (extracted from186.txt)
[…] The trend towards using risk instruments in all sectors of the criminal justice system, therefore, merits further theoretical d eliberation and empirical study. (extracted from186.txt)
'221 • In the same vein, Chelsea Barabas and her colleagues argue: 'for a shift away from predictive technologies, towards diagnostic methods that will help us to understand the criminogenic effects of the criminal justice sys tem itself, as well as evaluate the effectiveness of interventions designed to interrupt cycles of crime. (extracted from186.txt)
221 Kelly Hannah -Moffat; Actuarial sentencing: an ' unsettled ' proposition; Justice Quarterly; (30,2); 2013. (extracted from186.txt)
Dillon Reisman and his colleagues have already advocated AIA as a ' practical framework for public agency accountability' in a recent AINow Institute report .226 Beyond a 'self-assessment of existing and proposed automated d ecision systems, evaluating potential impacts on fairness, justice, bias, or other concerns across affected communities ', they emphasi se the need for ' researcher review processes before the system has been acquired '. (extracted from186.txt)
For example, several studies have been conducted about the use of ADS in the area of justice, some of th em focusing on the risks of discrimination ,232 others on the benefits in improv ing judges' decisions.233 In addition, the benefit risk balance applies to both the primary functionalities of the ADS and to its transparency and explainability features. (extracted from186.txt)
ADS certification can be either on a voluntary basis (as encouraged by the GDPR) or mandatory in certain areas such as justice and healthcare. (extracted from186.txt)
Berk R., Heidari H., Jabbari S., Kearns M., Roth A.; Fairness in Criminal Justice Risk Assessments: The State of the Art; Sociological Methods & Research; 2018. (extracted from186.txt)
Christin A., Rosenblat A., Boyd D.; Courts and predictive algorithms; Workshop on Data & Civil Rights: A new era of policing and justice; 2015 . (extracted from186.txt)
Hannah -Moffat K.; Actuarial sentencing: an 'unsettled ' proposition; Justice Quarterly (30); 2013. (extracted from186.txt)
Canada-France Statement on Artificial Intelligence Skip to main content Skip to "About government" Language selection Français Government of Canada Search Search website Search Menu Main Menu Jobs and the workplace Immigration and citizenship Travel and tourism Business and industry Benefits Health Taxes Environment and natural resources National security and defence Culture, history and sport Policing, justice and emergencies Transport and infrastructure Canada and the world Money and finances Science and innovation You are here: Canada.ca Canada and the worldForeign policy and international relations Canada's partnerships and priorities by world regionCanada and EuropeCanada-France Statement on Artificial Intelligence Canada-France Statement on Artificial IntelligenceCanada and France affirm that artificial intelligence is a revolution whose impact is being felt more and more each day. (extracted from78.txt)
By the end of the year, the task force will submit a report on the implementation of the international study group, whose results will be shared within the G7.For the Government of CanadaNavdeep Singh BAINSMinister of Innovation, Science and Economic DevelopmentFor the Government of the French RepublicFrédérique VIDALMinister of Higher Education, Research and Innovation Date modified: 2018-06-07 About this site Government of Canada All contacts Departments and agencies About government Themes and topics Jobs and the workplace Immigration and citizenship Travel and tourism Business and industry Benefits Health Taxes Environment and natural resources National security and defence Culture, history and sport Policing, justice and emergencies Transport and infrastructure Canada and the world Money and finances Science and innovation Indigenous Peoples Veterans and military Youth About government Social media Mobile applications About Canada.ca Terms and conditions Privacy (extracted from78.txt)
China’s strategy, for example, proposes a system of public services underpinned by AI, from education, to healthcare, to justice, as well as AI having a key role in policy-making itself.15 Other countries are creating new institutions and networks to oversee AI research and implementation. (extracted from44.txt)
Digitalization, Anticorruption, Rule of Law and Competition PolicyBritish Embassy in Mexico City International Cooperation Carla Vazquez RUCAM Ministry of Economy Government Carlos Gershenson Researcher Metro CDMX/ UNAM Academia CDMX Carlos López Franco Researcher UDG/CUCEI Jal Academia Chris WallHead of Human Rights, Security and Justice at the British Embassy in Mexico CityBritish Embassy in Mexico City International Cooperation Claudia Araujo GálvezCoordinator of Development of Industrial Technological PlatformsMinistry of Innovation, Science and TechnologyJal Govt Claudia Pando Programme Manager, Future Cities British Embassy in Mexico City International Cooperation Cristina Cardenas General Coordinator @prende.mx SEP Government David Bates Social Innovation Programmes CoordinatorMinistry of Innovation, Science and TechnologyJal Govt Edgar Nelson Sánchez CamperosResearcher CINVESTAV Jal Academia Eduardo Barbosa Emerging Technologies Director Ciudad Creativa Digital Jal Govt Eduardo Clark Public Innovation Deputy Director CEDN Government Eduardo Farina BluemessagingCEO Bluemessaging Start-up Eduardo Morales AI Phd INAOE Puebla Academia Elsa Ayala General Director of Normatividad Mercantil Ministry of Economy Government Enrique Jaime Herrera LópezResearcher CIATEJ Jal Academia Enrique Sucar Senior Research Scientist INAOEP Academia Enrique Zapata General Director for Open Data CEDN Government Fernando CotaTechnical Secretary of the Urban Development Commission Senate Government Francisco Búrquez SenatorMember of the Science and Technology CommissionLegislative branch Gabriella Gomez Mont Director Laboratorio de la Ciudad CDMX Govt Gerardo Rodríguez BarbaDirector of Technological Platforms Development and PromotionMinistry of Innovation, Science and TechnologyJal Govt Gustavo Carreon Researcher Metro CDMX/ UNAM Academia CDMX Gustavo Pares Nearshore CEO Nearshore Start-up Isaac Avila Coordinator CANIETI Occidente GDL Industry Ivan Millan General Director Jalisco Talent Land GDL Javier Mata Yalo CEO Yalo Start-up Jessica Paola Avila Open Data Director SEPAF Gobierno Jalisco Jal Govt Appendix 1: List of participants who contributed to this report TOWARDS AN AI STRATEGY IN MEXICO: Harnessing the AI Revolution 40 Name Job Title Organisation Sector Jesus Cepeda CEO One Smart City MTY Civil Society/Consultancy José Cantoral Researcher CIATEQ Jal Academia Jose Franco General CoordinatorForo Consultivo Científico y TecnológicoAcademia / Civil Society Juan Pablo Escobar Director Civica Digital MTY Civil Society/Consultancy Katie Allan Associate Oxford Insights Consultancy Laura Caccia Consultant Oxford Insights Consultancy Lorenzo ValleDirector - Center for Business Development in IT and Big Data InitiativeTEC Academia Lorenzo Valle Garcilazo ITESMBig Data Center Coordinator ITESM Academia Luis Cadena General Administrator of Communications and ICT SAT, SHCP Government Luis Valtierra President IJALTI Cluster Manuel Avalos IBM Storage Cloud and Solutions for IBM WW IBM/ Watson Jal Industry Margarita SolisGeneral Director of Social Innovation and Entrepreneurship Ministry of Innovation, Science and TechnologyJal Govt María de Lourdes Martínez Villaseñor, SMIAVocal SMIA Academia Marian UrizarFuture Cities Programme and Policy Officer– Programme TeamBritish Embassy in Mexico City International Cooperation Mario Angel Siller Gonzalez Researcher CINVESTAV Jal Academia Martha González PérezSandiDirector Cognitive Solutions, IBM Mexico Industry Matt Pasiensky VP of International Operations Wizeline GDL Industry Miguel Gonzalez President & Researcher SMIA, ITESM Academia Miguel González Mendoza President Mexico's AI Society Academia / Civil Society Miguel Salazar Director Ejecutivo Codeando Civil Society GDL Miriam Díaz Rodríguez Researcher/ Professor Tecnológico Mario Molina Jal Academia Morris Schwarzblat y KatzGeneral Director of Science and Technological DevelopmentMinistry of Innovation, Science and TechnologyJal Govt Nancy Guadalupe Arana Daniel Director, Systems Control & AI Center UDG/CUCEI Jal Academia Neil Hernández Gress ITESMResearcher ITESM Academia Olivia Barron AI PhD UDEM MTY Academia Oliver Rice Associate Oxford Insights Consultancy Omar Gonzalez KYSE Agritech UNAM CDMX Startup Raymundo Vazquez SW Engineering Manager INTEL Jal Industry Ricardo Reyes Data Wuki & Quantum LabsCEO Data Wuki and Quantum Labs Start-up Rodrigo FélixHead of Anti-corruption, Competition, Digitalisation and Rule Of Law PolicyBritish Embassy in Mexico City International Cooperation Saiph Savage Coordinator BanFakeNews Bot CDMX Sebastian Sposito Public Policy and Government Affairs Advisor Google Industry Sissi de la Peña Regional Markets CEDN Government Sophie Marment Head of Prosperity Fund FCO Government Tania Cruz Digital Government Services CEDN Government Victor Gutierrez Industry CCE Industry Yamin Ruiz Global Proteus CEO Global Proteus Start-up Yolanda Martinez Coordinator National Digital Strategy Government TOWARDS AN AI STRATEGY IN MEXICO: Harnessing the AI Revolution 41 Appendix 2: Innovation in Mexico’s Regions Jalisco, CDMX, and Nuevo Leon are leading the country’s science and technology scene. (extracted from44.txt)
Access to justice and effective redress are key elements of building consumer trust and thus are an important part of Trustworthy AI. (extracted from233.txt)
AI can find broad applications in such fields as education, medical treatment, eldercare, environmental protection, city operation, and justice services, and it is able to greatly improve the precision level of public services and increase the quality of people’s lives across the board. (extracted from582.txt)
These values are common to the societ ies of all Member States in which pluralism, non -discrimination, tolerance, justice, solidarity and equality prevail. (extracted from219.txt)
The third principle, justice, considers who gets chosen to be a research subject and what population segments stand to benefit — or be harmed — by the research results. (extracted from225.txt)
JoaAnne Stonier, Chief Data Officer at MasterCard, acknowledged another challenge to data management, which is the urge to “do good” with data when a company is approached by an academic researcher or non-profit with a social justice mission. (extracted from225.txt)
on democracy, the rule of law and distributive justice , or on the human mind itself .) Adopt adequate measures to mitigate these risks when appropriate, and proportionately to the magnitude of the risk. (extracted from231.txt)
10 by reference to dignity, freedoms, equality and solidarity, citizens’ rights and justice. (extracted from231.txt)
Respect for democracy, justice and the rule of law . (extracted from231.txt)
Additionally, fairness implies that AI practitioners should respect the principle of proportionality between means and ends, and consider carefully how to Fairness is closel y linked to the rights to Non -discrimination, Solidarity and Justice (reflected in Articles 21 and following). (extracted from231.txt)
Explicability and Responsibility are closely linked to the rights relating to Justice (as reflected in Article 47). (extracted from231.txt)
on democracy, the rule of law and distributive justice, or on the human mind itself.) Adopt adequate measu res to mitigate these risks when appropriate, and proportionately to the magnitude of the risk. (extracted from231.txt)
In general terms , it deals with questions like “What is a good action?”, “What is the value of a human life?”, “What is justice?”, or “What is the good life?”. (extracted from231.txt)
It has litigated or intervened in cases implicating the right to privacy in the courts of the United States, the UK, and Europe, including the European Court of Human Rights and the European Court of Justice. (extracted from557.txt)
Four of them are core principles commonly used in bioethics: beneficence, non-maleficence, autonomy, and justice. (extracted from0.txt)
In the ensuing discussion, we note the limitations and assess the implications of this ethical framework for future efforts to create laws, rules, technical standards, and best practices for ethical AI in a wide range of contexts.Keywords: accountability, autonomy, artificial intelligence, beneficence, ethics, explicability, fairness, intelligibility, justice, non-maleficence.1. (extracted from0.txt)
This convergence can most clearly be shown by comparing the sets of principles with the four core principles commonly used in bioethics: beneficence, non-maleficence, autonomy, and justice (Beauchamp & Childress, 2012). (extracted from0.txt)
Justice: Promoting Prosperity, Preserving Solidarity, Avoiding UnfairnessThe decision to make or delegate decisions does not take place in a vacuum. (extracted from0.txt)
The consequences of this disparity in autonomy are addressed in the principle of justice. (extracted from0.txt)
The importance of ‘justice’ is explicitly cited in the Montreal Declaration, which argues that “the development of AI should promote justice and seek to eliminate all types of discrimination,” while the Asilomar Principles include the need for both “shared benefit” and “shared prosperity” from AI. (extracted from0.txt)
Under its principle named “Justice, equity and solidarity,” the EGE argues that AI should “contribute to global justice and equal access to the benefits” of AI technologies. (extracted from0.txt)
It also warns against the risk of bias in datasets used to train AI systems, and – unique among the documents – argues for the need to defend against threats to “solidarity,” including “systems of mutual assistance such as in social insurance and healthcare.” Elsewhere ‘justice’ has still other meanings (especially in the sense of fairness), variously relating to the use of AI to correct past wrongs such as eliminating unfair discrimination, promoting diversity, and preventing the rise of new threats to justice. (extracted from0.txt)
The diverse ways in which justice is characterised hints at a broader lack of clarity over AI as a human-made reservoir of ‘smart agency.’ Put simply, are we (humans) the patient, receiving the ‘treatment’ of AI, the doctor prescribing it? (extracted from0.txt)
This list was designed by consensus of a large diverse interdisciplinary committee to give the public something better than Asimov’s Laws (which covered beneficence & justice), but extended to five in order to bring in transparency and accountability. (extracted from0.txt)
Fairness and non discrimination Safety and Security Privacy Justice and fairness Non- maleficence Privacy OFFICIAL DSTG -TR-3786 OFFICIAL 50 Supply Chain Test & Evaluation Misuse and risks Authority pathway Data subjects result in unfair discrimination agains t individuals, communities or groups Privacy protection and security: Throughout their lifecycle, AI systems should respect and uphold privacy rights and data protection, and ensure the security of data Contestability: When an AI system significantly im pacts a person, community, group or environment, there should be a timely process to allow people to challenge the use or output of the AI system have control over their identity Awareness of Misuse: A/IS creators shall guard against all potential misuses and risks of A/IS in operation. (extracted from190.txt)
States that in line with strict liability systems of the Member States, the proposed Regulation should cover violations of the important legally protected rights to life, health, physical integrity and property, and should set out the amounts and extent of compensation, as well as the limitation period; is of the opinion that the proposed Regulation should also incorporate significant immaterial harm that results in a verifiable economic loss above a threshold harmonised in Union liability law, that balances the access to justice of affected persons and the interests of other involved persons; urges the Commission to re-evaluate and to align the thresholds for damages in Union law; is of the opinion that the Commission should analyse in depth the legal traditions in all Member States and their existing national laws that grant compensation for immaterial harm, in order to evaluate if the inclusion of immaterial harm in AIspecific legislative acts is necessary and if it contradicts the existing Union legal framework or undermines the national law of the Member States; 20. (extracted from184.txt)
This follows from general and widely accepted liability concepts of justice, according to which the person that creates or maintains a risk for the public is liable if that risk causes harm or damage, and thus should ex-ante minimise or ex-post compensate that risk. (extracted from184.txt)
(16) This Regulation should cover harm or damage to life, health, physical integrity, property and significant immaterial harm that results in a verifiable economic loss above a threshold, harmonised in Union liability law, that balances the access to justice of affected persons with the interests of other involved persons. (extracted from184.txt)
ArticleSCIENCE + TECHNOLOGYWith great power comes great responsibility – but will tech companies accept it?23 Nov 18 DebateIQ2IQ2 Debate: Humanity is Designing its Own Demise28 Oct 17 ArticleSCIENCE + TECHNOLOGYThe “good enough” ethical setting for self-driving cars19 Jul 16 Say Hello GET IN TOUCH SUBSCRIBE FOLLOW US With respect for the people of our First Nations and the justice of their claims, The Ethics Centre acknowledge their unbroken care for country, since time immemorial. (extracted from609.txt)
Khoo said that racial justice activists, whom she and her colleagues talked to in the context of the research conducted for the Citizen Lab report, consider the use of algorithmic technologies by police t o be 21st -century state violence: before it was done with pen and paper, now it is done with computers and algorithms.20 Ms. (extracted from91.txt)
The RCMP also confirmed that no ethics review was done before using Clearview AI’s FRT.26 Roch Séguin , director of the Strategic Services Branch, Technical Operations, said that the RCMP had approached the Department of Justice regarding the use of FRTs in its investigations only once, but it was internal to the RCMP . (extracted from91.txt)
17 (Justice La Forest). (extracted from91.txt)
In April 2022, the Scottish Commissioner published a draft code of practice on the acquisition, retention, use and destruction of biometric data for criminal justice and police purposes in Scotland. (extracted from91.txt)
Society may reasonably conclude that justice requires decision-making processes to be designed so that human judgment can intervene where needed to achieve fair and reasonable outcomes for each person, informed by individual evidence. (extracted from85.txt)
For example, the 2017 Lammy Review81 found that BAME individuals faced bias, including overt discrimination, in parts of the justice system. (extracted from85.txt)
Whilst there is no current evidence of police algorithms in the UK being racially biased, one can certainly see the risks of algorithms entrenching and amplifying widely documented human biases and prejudices, in particular against BAME individuals, in the criminal justice system. (extracted from85.txt)
94 CDEI have published a research paper on facial recognition technology, which covers police use of live facial recognition technology, along with other uses; https://www.gov.uk/ government/publications/cdei-publishes-briefing-paper-on-facial-recognition-technology/snapshot-paper-facial-recognition-technology 95 See for example: Richardson, Rashida and Schultz, Jason and Crawford, Kate, ‘Dirty Data, Bad Predictions: How Civil Rights Violations Impact Police Data, Predictive Policing Systems, and Justice’, AI Now Institute, https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3333423; Kearns, Ian and Rick Muir, ‘Data Driven Policing and Public Value’, The Police Foundation, 2019; http://www.police-foundation.org.uk/2017/wp-content/uploads/2010/10/data_driven_policing_final.pdf; ‘Policing By Machine’, Liberty, 2019; https://www.libertyhumanrights.org.uk/issue/ policing-by-machine/ 96 RUSI sent Freedom of Information requests to all police forces in England and Wales, interviewed over 60 people from police forces, technology providers, academia, civil society, government, and regulation, and ran roundtables, jointly with CDEI and TechUK. (extracted from85.txt)
Review into bias in algorithmic decision-making: Policing Centre for Data Ethics and Innovation675.2 Findings 99 CDEI ‘Call for evidence summary of responses - Review into bias in algorithmic decision-making’, 2019; https://assets.publishing.service.gov.uk/government/uploads/system/uploads/attachment_data/ file/838426/CDEI-Call-for-Evidence-Bias-Summary-of-responses-October2019.pdf 100 Couchman, Hannah; ‘Policing by machine’, Liberty, 2019; https://www.gov.uk/government/publications/responses-to-cdei-call-for-evidence/cdei-bias-review-call-for-evidence-summary-of-responses 101 Robin Moore, ‘A Compendium of Research and Analysis on the Offender Assessment System (OASys) 2009–2013’, National Offender Management Service, Ministry of Justice Analytical Series, 2015; https://assets.publishing.service.gov.uk/government/uploads/system/uploads/attachment_data/file/449357/research-analysis-offender-assessment-system.pdf 102 The Guardian, ‘Met uses software that can be deployed to see if ethnic groups ‘specialise’ in areas of crime’, 2020; https://www.theguardian.com/uk-news/2020/jul/27/met-police-use-software-ethnicgroups-specialise-profileAlgorithms are in develo pment and use across some police forces in England and Wales but the picture i s varied From the responses we received to our call for evidence99 and wider research, we know there are challenges in defining what is meant by an algorithmic tool and consequently understanding the extent and scale of adoption. (extracted from85.txt)
In England and Wales, police forces are currently taking a variety of different approaches to their development of algorithmic systems, ethical safeguards, community engagement and data science expertise.Review into bias in algorithmic decision-making: Policing Centre for Data Ethics and Innovation68103 See for example: Richardson, Rashida and Schultz, Jason and Crawford, Kate, ‘Dirty Data, Bad Predictions: How Civil Rights Violations Impact Police Data, Predictive Policing Systems, and Justice’, AI Now Institute, https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3333423 104 Home Office, ‘Police powers and procedures, England and Wales, year ending 31 March 2019’; https://assets.publishing.service.gov.uk/government/uploads/system/uploads/attachment_ data/file/841408/police-powers-procedures-mar19-hosb2519.pdf 105 Nesta, ‘Decision-making in the Age of the Algorithm’, 2019; https://media.nesta.org.uk/documents/Decision-making_in_the_age_of_the_algorithm.pdfMitigating bias and ensuring fairness requires looking at the entire decision-making process As set out earlier in the report, we think it is crucial to take a broad view of the whole decision-making process when considering the different ways bias can enter a system and how this might impact on fairness. (extracted from85.txt)
It could also influence how high or low risk certain crimes or areas are deemed by a data analytics tool and potentially perpetuate or exacerbate biased criminal justice outcomes for certain groups or individuals. (extracted from85.txt)
Research by the RSA and DeepMind113 highlights that people feel more strongly against the use of automated decision systems in the criminal justice system (60 percent of people oppose or strongly oppose its use in this domain) than other sectors such as financial services. (extracted from85.txt)
Moreover, people are least familiar with the use of automated decision-making systems in the criminal justice system; 83 percent were either not very familiar or not at all familiar with its use. (extracted from85.txt)
60 percent of people oppose or strongly oppose the use of automated decision systems in the criminal justice system.113 110 Police Foundation, ‘Data-Driven Policing and Public Value’, 2019; http://www.police-foundation.org.uk/2017/wp-content/uploads/2010/10/data_driven_policing_final.pdf 111 Quote from Rick Muir, Director of Police Foundation, in, Strategic View of Policing, ‘Sir Michael Barber to head major review of the police service’, 2019; https://policingreview.org.uk/ hello-world/ 112 https://www.gov.uk/government/publications/policing-by-consent/definition-of-policing-by-consent 113 Royal Society for the encouragement of Arts, Manufactures and Commerce, ‘Artificial Intelligence: Real Public Engagement’, 2018; https://www.thersa.org/globalassets/pdfs/reports/ rsa_artificial-intelligence---real-public-engagement.pdf 114 ‘Ipsos MORI Veracity Index 2019’; 76% survey respondents trust the police to tell the truth - an increase of 15ppt since 1983. (extracted from85.txt)
et al., ‘Data Scores as Governance: Investigating uses of citizen scoring in public services’, Data Justice Lab, Cardiff University, 2018; https://datajustice.files.wordpress. (extracted from85.txt)
It is difficult to map how widespread algorithmic decision-making is in local government There have been multiple attempts to map the usage of algorithmic decision-making tools across local authorities but many researchers have found this challenging.120 An investigation by The Guardian found that, at a minimum, 140 councils out of 408 have invested in software contracts that cover identifying benefit fraud, identifying children at risk and allocating school places.121 However this did not include additional use cases found in a report by the Data Justice Lab, a research group based in Cardiff University. (extracted from85.txt)
The Data Justice Lab used Freedom of Information requests to learn which tools are being used and how frequently. (extracted from85.txt)
et al., ‘Data Scores as Governance: Investigating uses of citizen scoring in public services’, Data Justice Lab, Cardiff University, 2018; https://datajustice.files.wordpress. (extracted from85.txt)
et al., ‘Data Scores as Governance: Investigating uses of citizen scoring in public services’, Data Justice Lab, Cardiff University, 2018; https://datajustice.files.wordpress.com/2018/12/data-scores-as-governance-project-report2.pdfReview into bias in algorithmic decision-making: Local government Centre for Data Ethics and Innovation 77Third-party providers offer specialist data science expertise that is likely not available to most local authorities and are likely to have valuable experience from previous work with other local authorities. (extracted from85.txt)
et al., ‘Data Scores as Governance: Investigating uses of citizen scoring in public services’, Data Justice Lab, Cardiff University, 2018, p34; https://datajustice.files.wordpress. (extracted from85.txt)
P.; and Weller, A.; ‘Blind Justice: Fairness with Encrypted Sensitive Attributes’. (extracted from85.txt)
P., and Weller, A., ‘Blind Justice: Fairness with Encrypted Sensitive Attributes’. (extracted from85.txt)
Review into bias in algorithmic decision-making: Enabling fair innovation Centre for Data Ethics and Innovation96 Centre for Data Ethics and Innovation 96160 House of Commons Justice Committee, ‘Court and Tribunal reforms, Second Report of Session 2019’; https://publications.parliament.uk/pa/cm201919/cmselect/cmjust/190/190.pdf 161 Lord Chancellor; Lord Chief Justice; Senior President of Tribunals, ‘Transforming Our Justice System’, 2016; https://assets.publishing.service.gov.uk/government/uploads/system/uploads/ attachment_data/file/553261/joint-vision-statement.pdf 162 Administrative Data Research UK, ‘Data First, Harnessing the Potential of linked administrative data for the justice system’, ongoing; https://www.adruk.org/our-work/browse-all-projects/ data-first-harnessing-the-potential-of-linked-administrative-data-for-the-justice-system-169/ 163 Ibid.Case study: Monitoring for bias in digital transformation of the courts Accessing protected characteristic data to monitor outcomes is not only necessary when introducing algorithmic decisionmaking, but also when making other major changes to significant decision-making processes. (extracted from85.txt)
They have worked with the Government Statistical Service’s Harmonisation Team and academic researchers to rebuild their data architecture to support this.163 The resulting information is intended to both be valuable to the Ministry of Justice for designing fair interventions in the functioning of the courts, but also eventually to be made available for independent academic research (via Administrative Data Research UK and the Office for National Statistics). (extracted from85.txt)
Ultimately, humans must choose which notions of fairness an algorithm will work to, taking wider notions and considerations into account, and recognising that there will always be aspects of fairness outside of any statistical definition.Review into bias in algorithmic decision-making: Enabling fair innovation Centre for Data Ethics and Innovation99An example of how these different definitions play out in practice can be seen in the US criminal justice system, as per the following case study.Group Individual • Demographic parity - outcomes for different protected groups are equally distributed, and statistically independent. (extracted from85.txt)
Review into bias in algorithmic decision-making: The regulatory environment Centre for Data Ethics and Innovation113186 Lord Sales, Justice of the UK Supreme Court; ‘Algorithms, Artificial Intelligence and the Law’, The Sir Henry Brooke Lecture for BAILI, 2019; https://www.supremecourt.uk/ docsspeech-191112.pdf In support of this legislation, there are two primary cross-cutting regulators: the Equality and Human Rights Commission (EHRC, for the Equality Act and Human Rights Act) and the Information Commissioner’s Office (ICO, for the Data Protection Act and the GDPR). (extracted from85.txt)
For example, the ruling by the European Court of Justice in the Test-Achats case made it unlawful for insurers to charge different rates based on sex or gender.200 UK car insurance providers had routinely charged higher premiums for men, based on their higher expected claims profile. (extracted from85.txt)
Instead the implications of new technologies for the justice system, transport provision and decisionmaking in the workplace are captured within those specific programmes. (extracted from85.txt)
It also recently completed an inquiry into the experiences of people with disabilities in the criminal justice system, including the challenges arising from a move towards digital justice, and has undertaken research into the potential for discrimination in using AI in recruitment. (extracted from85.txt)
For example, an investigation by The Guardian last year (https://www.theguardian.com/society/2019/oct/15/councils-using-algorithms-make-welfaredecisions-benefits) showed some 140 of 408 councils in the UK are using privately-developed algorithmic ‘risk assessment’ tools, particularly to determine eligibility for benefits and to calculate entitlements; the New Statesman (https://www.newstatesman.com/science-tech/technology/2019/07/revealed-how-citizen-scoring-algorithms-are-being-used-local) revealed that Experian secured £2m from British councils in 2018; and Data Justice Lab research in late 2018 (https://datajustice.files.wordpress.com/2018/12/data-scores-as-governance-project-report2.pdf) showed 53 out of 96 local authorities and about a quarter of police authorities are now using algorithms for prediction, risk assessment and assistance in decision-making. (extracted from85.txt)
These could include social care, criminal justice or benefits allocation. (extracted from85.txt)
• Manage the overhead of responding to large numbers of similar reactive requests.Review into bias in algorithmic decision-making: Transparency in the public sector Centre for Data Ethics and Innovation139253 House of Commons Science and Technology Committee, ‘Algorithms in decision-making, Fourth Report of Session 2017-19’; https://publications.parliament.uk/pa/cm201719/cmselect/ cmsctech/351/351.pdf 254 The Law Society, ‘Algorithms in the criminal justice system’, 2019; https://www.lawsociety.org.uk/support-services/research-trends/algorithm-use-in-the-criminal-justice-system-report/ 255 Guidance: Gender pay gap reporting: overview; https://www.gov.uk/guidance/gender-pay-gap-reporting-overviewManaging the process of transparency The House of Lords Science and Technology Select Committee and the Law Society have both recently recommended that parts of the public sector should maintain a register of algorithms in development or use. (extracted from85.txt)
the Government should produce, publish, and maintain a list of where algorithms with significant impacts are being used within Central Government, along with projects underway or planned for public service algorithms, to aid not just private sector involvement but also transparency.” - House of Lords Science and Technology Select Committee253 Quote “A National Register of Algorithmic Systems should be created as a crucial initial scaffold for further openness, cross-sector learning and scrutiny.” - The Law Society ‘Algorithms in the Criminal Justice System’254 CDEI agrees that there are some significant advantages both to government and citizens in some central coordination around this transparency. (extracted from85.txt)
Targeting interventions for ‘at risk’ groups Some of the most vulnerable individuals in society are often dealing with issues that cut across public services, including housing, health, and justice. (extracted from635.txt)
To further ensure trustworthy AI, most governments are adopting new legislative frameworks for AI technologies , which are often sector specific and cover areas such as autonomous driving, healthcare and e -justice. (extracted from147.txt)
The Federal Ministry also highlights the use of AI for evaluating judicial proceedings, as mentioned in its National e -justice strategy . (extracted from147.txt)
ensuring that citizens can make informed and independent decisions) and to human dignity, equality and justice (i.e. (extracted from147.txt)
In addition, public sector operators should be secured sufficient resources and i ncentives to engage in such development, paying particular attention to sort out the rights of the outcomes of co -development;  The Ministry of Justice and the Ministry of Finance are currently examining national regulation of automated decision -making. (extracted from147.txt)
5.13.4 Regulation Hungary’s nationa l AI strategy aims to ensure a responsible, reliable and human -centred utilisation of AI technologies by means of the following policies:  Creating an ethical framework : developing an AI code of conduct by the first half of 2021 in collaboration between the Ministry of Justice, the Ministry for Innovation and Technology, AI Innovation Hub and the Central Statistical Office ;  Setting up an Artificial Intelligence Regulation and Ethics Knowledge Centre : the aim is to create and coordinate an extensive pool of experts to help resolve legal issues and matters of ethics relating to the regulation of AI and the implementation of the strategy ;  Establishing a regulatory framework for AI: t he obje ctive is to amen d the current regulatory system to suit AI and to align it to EU regulations ;  Building data management regulation : the objective is to set up regulations for the use and exchange of public and private data and to define rules regarding d ata monetisation. (extracted from147.txt)
The Latvian strategy identifies priority sectors with a high potential for AI applicatio ns in the country, such as transport (Intelligent transport systems), culture, justice (AI as support for decision making and drafting legislation), agriculture (automated control), and translation. (extracted from147.txt)
Regarding ethics, the government adopts the European ethical guidelines outlined by the European Commission for the Efficiency of Justice. (extracted from147.txt)
66 1) International bodies and AI, 2) Swiss intelligence of interests in the European AI (Digital Europe Programme) activities, 3) Changes in the world of work, 4) AI in industry and services, 5) AI in education, 6) A pplication of AI in science and research, 7) AI in cybersecurity and security policy, 8) AI, Media & Public, 9) Automated mobility and AI, 10) AI in healthcare, 11) AI in the financial secto r, 12) AI in agriculture, 13) Energy, climate, environment and AI, 14) AI in administration, 15) Further development of the general legal framework on AI, 16) AI in justice, 17) AI, data and intellectual property law. (extracted from147.txt)
While the establishment of the legal basis is ensured by a wide range of institutions, the FDFA will specifically focus on the following policies to further develop the general legal framework on AI:  Examining the emergence of AI -specific international law and its impac t on Switzerland;  Following -up developments with regard to the visibility of AI systems in interaction with consumers;  Monitoring developments in AI -based decision -making in the justice system (predictive justice). (extracted from147.txt)
The specific human rights implications for AI systems can be viewed through provisions of the European Convention of Human Rights (ECHR) and the European Social Charter (ESC), including its specific guarantees regarding liberty and justice, privacy, freedom of expression, equality and non-discrimination, and social and economic rights. (extracted from351.txt)
Liberty and Justice: AI can adversely affect the liberty and justice of individuals, particularly when implemented in high impact contexts such as criminal justice. (extracted from351.txt)
-Member States are obliged to ensure that the AI systems they deploy do not result in unlawful discrimination, harmful stereotypes (including but not limited to gender stereotypes), and wider social inequality, and should therefore apply the highest level of scrutiny when using or promoting the use of AI systems in sensitive public policy areas, including but not limited to law enforcement, justice, asylum and migration, health, social security, and employment. (extracted from351.txt)
This should also include the possibility of receiving insight into and challenging AIinformed decisions in the context of law enforcement or justice, including the right to review of such decision by a human. (extracted from351.txt)
13 ECHR), also in case of unlawful harm or breach an individual’s human rights in the context of AI systems.-Member States must ensure that AI systems used in the field of justice and law enforcement are in line with the essential requirements of the right to a fair trial. (extracted from351.txt)
Such information must especially be provided when AI systems are used in the field of justice and law enforcement, both as concerns the role of AI systems within the process, and the right to challenge the decisions informed or made thereby. (extracted from351.txt)
We hope that, taken together, this material can function as a kind of launching pad for meaningful reflection on the prospects for a principles-based legal framework for governing AI research and innovation in accordance with the Council of Europe's stewardship of fundamental rights and freedoms, justice, and democratic values. (extracted from351.txt)
Policy makers, scholars, and activists are tasked with proposing and critiquing strategies and actions aimed at promoting general well-being and social justice. (extracted from351.txt)
Work in the field of justice 4 3European Committee on Democracy and Governance (CDDG) Currently preparing a study on the impact of digital transformation on democracy and governance Venice Commission: Principles for a fundamental rights-compliant use of digital technologies in electoral processes (2020) Emphasised the need for a human rights-compliant approach to eight principles involving the use of digital technologies in elections The eight principles are described in greater detail in the document, but they are outlined below and have been taken directly from the original document 1. (extracted from351.txt)
https://www.justice.gc.ca/eng/csj-sjc/pl/charter-charte/c27_1.html#:~:text=In%20 addition%2C%20the%20Artificial%20Intelligence,with%20other%20government%20 entities%20specified 12. (extracted from345.txt)
Importance of careful management of AI challenges 'How important is it for companies and gove rnments to car efully manage this challenge?'% Low importance %Moderate importance % High importance Cyber Attack 3 9 88 Autonomous Vehicles 4 10 86 Misaligned with Human Values 4 10 86 Data Privacy 3 12 85 Disease Misdiagnosis 3 12 85 Fake Online Content 3 13 84 Critical AI Failur es 4 12 84 Surveillance 4 12 84 Criminal Justice Bias 4 12 84 Autonomous W eapons 5 12 83 HR Bias 5 12 83 Technological Unemployment 4 13 83 Low importance = 'Not at all important' or 'Slightly important' Moderate importance = 'Moderately important' High importance = 'Very important' or 'Extr emely important' ©2021 The University of Queensland ©2021 KPMG, an Australian partnership and a member firm of the KPMG global organisation of independent member firms affiliated with KPMG International Limited, a private English company limited by guarantee. (extracted from423.txt)
Likelihood of AI challenges impacting large numbers of citizens 'In the next 10 years, how likely do you think it is that this challenge will impact la rge numbers of the people in your country?'% Unlikely % Equally likely as unlikely % Likely Surveillance 17 22 61 Fake Online Content 17 23 60 Cyber Attack 19 21 60 Data Privacy 19 22 59 Disease Misdiagnosis 25 25 50 HR Bias 26 25 49 Technological Unemployment30 25 45 Critical AI Failur es 29 26 45 Misaligned with Human Values30 25 45 Autonomous Vehicles 31 24 45 Criminal Justice Bias 29 28 43 Autonomous W eapons 42 22 36 Unlikely = 'Very unlikely (<5% chance'), 'Unlikely (5-20% chance)' or 'Somewhat unlikely (20-40% chance)' Equally likely as unlikely = 40-60% chance Likely = 'Somewhat likely (60-80% chance)', 'Likely (80-95% chance)' or 'Very likely (>95% chance)' ©2021 The University of Queensland ©2021 KPMG, an Australian partnership and a member firm of the KPMG global organisation of independent member firms affiliated with KPMG International Limited, a private English company limited by guarantee. (extracted from423.txt)
22 Exceptions were autonomous vehicles and criminal justice bias, where Australia was higher than only select countries, not all. (extracted from423.txt)
In addition to this “top down” -Initia tive, the Montreal AI players have compiled the Montreal Declaration for Responsible AI, which aims to stimulate the discussion between the public, the private sector and the state.12 It proposes a framework and a set of values, such as well-being, autonomy, justice and privacy, which must be assessed and observed in the development or imple mentation of AI technologies. (extracted from422.txt)
In it, the Ministry of Justice proposes an exception in the Copy Right Act which allows copying of copyrighted documents and data for the purpose of data analysis.331 Politicians are expected to agree to this proposal.332 Business support and startup promotion: The conditions for startups in the country are already very good. (extracted from422.txt)
• Utilise AI innovation s pro-socially so as to enable bonds of interpersonal solidarity to form and individuals to be socialised and recognised by each other • Use AI technologies to foster this capacity to connect so as to reinforce the edifice of trust, empathy, reciprocal responsibility, and mutual understanding upon which all ethically well founded social orders rest → CARE for the wellbeing of each and all : • Design and deploy AI systems to foster and to cultivate the welfare of all stakeholders whose interests are affected by their use • Do no harm with these technologies and minimise the risks of their misuse or abuse Understanding Artificial Intelligence Ethics and Safety 11 • Prioritise the safety and the mental and physical integri ty of people when scanning horizon s of technological possibility and when conceiving of and deploying AI applications → PROTECT the priorities of social values, justice, and the public interest : • Treat all individuals equally and protect social equity • Use digital technologies as a n essential support for the protection of fair and equal treatment under the law • Prioritise social welfare, public interest, and the consideration of the social and ethical impacts of innovation in determining the legitimacy and desirability of AI technologies • Use AI to empower and to advance the interests and well -being of as many individuals as possible • Think big -picture about the wider impacts of the AI technologies you are conceiving and developing. (extracted from350.txt)
Ethical considerations about looking after patient wellbeing and clinical safety are paramount and wider justice c oncerns about improving healthcare for all and health equity factor in as well. (extracted from350.txt)
I am also incredibly grateful for the impact that our interactions with the Ministry of Justice (MoJ)’s Data Science Hub has had on developing the framing for this guide. (extracted from350.txt)
'It's reducing a human being to a percentage': Perceptions of justice in algorithmic decisions. (extracted from350.txt)
Optimized scoring systems: Toward trust in machine learning for healthcare and criminal justice. (extracted from350.txt)
Criminal Justice and Behavior , 46(2), 185 -209. (extracted from350.txt)
Data Justice Lab. (extracted from350.txt)
Dirty data, bad predictions: How civil rights violations impact police data, predictive policing systems, and justice. (extracted from350.txt)
Copyright © United Nations Interregional Crime and Justice Research Institute (UNICRI), 2020 Viale Maestri del Lavoro,10, 10127 Torino – Italy Tel: +39 011-6537 111 / Fax: +39 011-6313 368 Website: www.unicri.it E-mail: unicri.publicinfo@un.org © The International Criminal Police Organization (INTERPOL), 2020 200, Quai Charles de Gaulle, 69006 Lyon – France Tel: +33 4 72 44 70 00 / Fax: +33 4 72 44 71 63 Website: www.interpol.int E-mail: edgci-ic@interpol.int 2 FOREWORD Crime is not stagnant. (extracted from387.txt)
We have strived to shape this forum, giving it meaning and purpose, and positioning it to grow into a global platform for cooperation and collaboration amongst law enforcement on AI This report on AI for law enforcement is the most recent product of the collaboration on AI between the Innovation Centre of the International Criminal Police Organization (INTERPOL) and the United Nations Interregional Crime and Justice Research Institute’s (UNICRI) Centre for AI and Robotics. (extracted from387.txt)
The increasing interest and attention these meetings are receiving is both a reward for INTERPOL and UNICRI and reveals the growing relevance of AI for the criminal justice community. (extracted from387.txt)
Human rights, civil liberties and even the fundamental principles of law upon which our criminal justice system is based may be unacceptably exposed, or even irreparably compromised, if we do not navigate this route with extreme caution. (extracted from387.txt)
The chapter be gins by presenting the general principles that law enforcement should endeavour to adhere to, namely the respect for human rights, democracy, justice and rule of law, as well as the related requirements of fairness, accountability, transparency and explainability that should be adopted in order for law en forcement to meet these principles. (extracted from387.txt)
Equally, it is a valuable exercise for policy- and deci sion-makers in the broader criminal justice community to, from a legal and ethical perspective, prepare frameworks for the eventual integration of such technologies into law enforcement. (extracted from387.txt)
The seminal 2019 white paper AI and Ethics at the Police by Leiden University and TU Delft16 suggests that, from a legal perspective, to act responsibly means “to accept moral integrity and authenticity as ideals and to deploy reasonable effort toward achieving them.”17 Striving for moral integrity, in turn, implies “adhering to the values of freedom, equality, and solidarity.”18 For the purposes of this report, however, a more straightforward understanding will be adopted and the term ‘responsible’ will be framed in line with the Oxford Dictionary, which defines ‘responsibly’ as acting “in a sensible or trustworthy manner.”19 In this context, the responsible use of AI by law enforcement should be understood as use that enshrines the general principles of respect for human rights, democracy, justice and the rule of law. (extracted from387.txt)
Justice for Hedgehogs . (extracted from387.txt)
It has been heralded for its role in building “an area of freedom, security and justice with a high level of data protection, in accordance with the EU Charter of Fundamental Rights.” Aiming at protecting individuals’ personal data, while guaran teeing a high level of public security, the LED provides rights for data subjects, as well as obligations for “competent authorities” when processing data for “law enforcement purposes”, i.e., prevention, investi gation, detection, prosecution of criminal offences or the execution of criminal penalties, including the safeguarding against and the prevention of threats to public security. (extracted from387.txt)
The development of trustworthy AI systems should be based upon established fundamental values, such as the respect for human dignity, democracy, justice and rule of law, while, at the same time, guaranteeing the freedom of the individual and citizens’ rights in order to ensure equality and non-dis crimination. (extracted from387.txt)
45 RECOMMENDED ACTIONS Define a statement of principles on the use of AI in law enforcement that will guide law enforcement to ensure respect for human rights, democracy, justice and the rule of law and support it to prioritize the key requirements of fairness, accountability, transparen cy and explainability, as well as safety and robustness; ›Develop guidance for law enforcement on the implementation of new technology to support and encourage law enforcement agencies to explore and invest in new AI opportunities and to develop training in new AI applications and disseminate best practices; ›Create a knowledge-base with the law enforcement community on the requiremen ts for the adoption of AI, such as what kinds of problems AI is capable of tackling, the current or inherent limitations and the resources (tools, data, expertise, com puting power) required to implement AI solutions; ›Develop guidance for law enforcement on the admissibility of AI in court that as sesses the impact and results of the specific use of AI in courts, while ensuring the respect for human rights and rule of law; ›Create an expert advisory committee that can provide guidance to law enforcement in terms of legislation and serve as a forum for discussing appropriate legislative models with legal experts and other key stakeholders; ›Identify an external global body to provide advisory support to law enforcement on ethical issues and to provide support in carrying out audits to check whether a system is responsible and complies with legal requirements; ›Foster a community and organize training courses and workshops to attract and connect different stakeholders from law enforcement, industry, academia, civil society and international bodies with the diverse backgrounds and essential per spectives to gather and synthesize views from cross-sections of society, in order to provide a balanced and facts-based picture of the opportunities and challenges of the use of AI and to highlight the application of AI to law enforcement and provide hands-on support. (extracted from387.txt)
52 ANNEX II LIST OF ABBREVIATIONS ADM Automated Decision-Making AFP Australian Federal Police AI Artificial intelligence AI-HLEG High-Level Expert Group on AI AiLECS Artificial Intelligence for Law Enforcement of Community Safety CCTV Closed-Circuit Television EU European Union FATE Fairness, Accountability, Transparency and Explainability GDPR General Data Protection Regulation GPS Global Positioning Services IC INTERPOL’s Innovation Centre IEDs Improvised Explosive Devices IGCI INTERPOL’s Global Complex for Innovation INTERPOL International Criminal Police Organization LED Law Enforcement Directive 2016/680 MAS Monetary Authority of Singapore NLP Natural Language Processing non-POI non-Person of Interest NPA Japan National Police Agency OECD Organisation for Economic Co-operation and Development R&D Research and Development UAV Unmanned Aerial Vehicles UNICRI United Nations Interregional Crime and Justice Research Institute ZITiS Central Office for Information Technology in the Security Sector 53 ABOUT INTERPOL INTERPOL is the world’s largest international police organiza tion. (extracted from387.txt)
ABOUT UNICRI The United Nations Interregional Crime and Justice Research Institute was established in 1968. (extracted from387.txt)
Within the broad scope of its mandate, the Institute contributes, through research, training, field activities and the collection, exchange and dissemination of information, to the formulation and implementation of improved policies in the field of crime prevention, justice and emerging security threats, due regard being paid to the integration of such policies within broader policies for socio-economic change and development, and to the protection of human rights. (extracted from387.txt)
In 2017, UNICRI opened its Centre for Artificial Intelligence and Robotics in The Hague, the Netherlands, with a view towards advancing understanding of artificial intelligence, robotics and related technologies vis-à-vis crime prevention, criminal justice, the rule of law and security. (extracted from387.txt)
At a global level, the UNESCO Global Recommendations on the Ethics of AI in November 2021 forefronts the principles of human dignity, inclusive growth and social justice. (extracted from393.txt)
justice, health, transport, etc). (extracted from620.txt)
To further ensure trustworthy AI, most governments are adopting new legislative frameworks for AI technologies , which are often sector specific and cover areas such as autonomous driving, healthcare and e -justice. (extracted from146.txt)
The Federal Ministry also highlights the use of AI for evaluating judicial proceedings, as mentioned in its National e -justice strategy . (extracted from146.txt)
ensuring that citizens can make informed and independent decisions) and to human dignity, equality and justice (i.e. (extracted from146.txt)
In addition, public sector operators should be secured sufficient resources and i ncentives to engage in such development, paying particular attention to sort out the rights of the outcomes of co -development;  The Ministry of Justice and the Ministry of Finance are currently examining national regulation of automated decision -making. (extracted from146.txt)
5.13.4 Regulation Hungary’s nationa l AI strategy aims to ensure a responsible, reliable and human -centred utilisation of AI technologies by means of the following policies:  Creating an ethical framework : developing an AI code of conduct by the first half of 2021 in collaboration between the Ministry of Justice, the Ministry for Innovation and Technology, AI Innovation Hub and the Central Statistical Office ;  Setting up an Artificial Intelligence Regulation and Ethics Knowledge Centre : the aim is to create and coordinate an extensive pool of experts to help resolve legal issues and matters of ethics relating to the regulation of AI and the implementation of the strategy ;  Establishing a regulatory framework for AI: t he obje ctive is to amen d the current regulatory system to suit AI and to align it to EU regulations ;  Building data management regulation : the objective is to set up regulations for the use and exchange of public and private data and to define rules regarding d ata monetisation. (extracted from146.txt)
The Latvian strategy identifies priority sectors with a high potential for AI applicatio ns in the country, such as transport (Intelligent transport systems), culture, justice (AI as support for decision making and drafting legislation), agriculture (automated control), and translation. (extracted from146.txt)
Regarding ethics, the government adopts the European ethical guidelines outlined by the European Commission for the Efficiency of Justice. (extracted from146.txt)
66 1) International bodies and AI, 2) Swiss intelligence of interests in the European AI (Digital Europe Programme) activities, 3) Changes in the world of work, 4) AI in industry and services, 5) AI in education, 6) A pplication of AI in science and research, 7) AI in cybersecurity and security policy, 8) AI, Media & Public, 9) Automated mobility and AI, 10) AI in healthcare, 11) AI in the financial secto r, 12) AI in agriculture, 13) Energy, climate, environment and AI, 14) AI in administration, 15) Further development of the general legal framework on AI, 16) AI in justice, 17) AI, data and intellectual property law. (extracted from146.txt)
While the establishment of the legal basis is ensured by a wide range of institutions, the FDFA will specifically focus on the following policies to further develop the general legal framework on AI:  Examining the emergence of AI -specific international law and its impac t on Switzerland;  Following -up developments with regard to the visibility of AI systems in interaction with consumers;  Monitoring developments in AI -based decision -making in the justice system (predictive justice). (extracted from146.txt)
Stephen Quest, Director-General, JRC Joost Korte, Director-General, Employment, Social Affairs and Inclusion Gwenole Cozigou, Director for Sustainable Industry and Mobility, DG GROW Roberto Viola, Director-General, Communications Networks, Content and Technology Paul Nemitz, Principal Advisor in the Directorate General for Justice and Consumers GETTING IN TOUCH WITH THE EU In person All over the European Union there are hundreds of Europe Direct information centres. (extracted from152.txt)
Regarding these rights, we focus on issues relating to the right to respect private life, human dignity, ownership, safety and liability, freedom of expression and the prohibition of discrimination as well as acc ess to justice and the right to a fair trial. (extracted from634.txt)
39 3.8 Access to justice and the right to a fair trial ................................ (extracted from634.txt)
The next three sections are dedicated to how automated decisions may affect freedom of expression, the prohibition of discrimination, access to justice and the right to a fair trial (sections 3.6, 3.7 and 3.8). (extracted from634.txt)
Intelligent artefacts may challenge different dimensions of human rights, for example in terms of freedoms (privacy and data protection, ownership, autonomy, personality), equality ( more specifically , non-discrimination) and justice (fair trial, access to justice). (extracted from634.txt)
Thus, the impact of intelligent artefacts is first discussed in the context of seven existing human rights: the right to respect for private and family life (3.2), the right to human dignity (3.3), the right to the peaceful enjoyment of possessions (3.4), tort rights and safety (3.5), the right to freedom of expression as well as the freedom of thought, conscience and religion (3.6), the prohibition of discrimination (3.7) and access to justice as well as the right to a fair trial (3.8). (extracted from634.txt)
18 Court of Justice of the European Union 19 October 2016, C -582/14 (Breyer ) and Court of Justice of the European Union 24 November 2011, C -70/10 (Scarlet/SABAM ), paragraph 51. (extracted from634.txt)
For instance, o n an international level, both the European Court of Human Rights and the Court of Justice of the European Union have voiced their concerns on numerous occasions regarding state surveillance activi ties and the impact on privacy rights.25 For 21 The Dutch Data Protection Authority imposed penalty payments on a company that co uld not demonstrate that Wi -Fi tracking in public spaces was necessary for a legitimate purpose. (extracted from634.txt)
21 instance when the EU Court of Justice declared the Data Retention Directive to be invalid.26 On a national level, the British Investigatory Powers Tribunal , for instance , ruled that during a time span of more than a decade, the British intelligence agencies illegally collected data about innocent citizens and illegally tracked their phone and web use.27 With regard to surveillance enacted by companies , the European Commission proposed the reform of EU data protect ion regulations , which ultimately led to the G eneral Data Protection Regulation . (extracted from634.txt)
Court of Justice of the European Union 8 April 2014, Joined Cases C -293/12 and C -594/12 ( Digital Rights Ireland and Seitlinger and Others ). (extracted from634.txt)
See also: Court of Justice of the European Union 21 December 2016, Joined Cases C -203/15 ( Tele2 Sverige AB v Post -och telestyrelsen ) and C -698/15 ( Secretary of State for the Home Department v Tom Watson and Others ). (extracted from634.txt)
28 For an overview of (a selec tion of) Court of Justice of the European Union and European Court of Human Rights cases, see Annex I of Article 29 Data Protection Working Party, Working Document 01/2016 on the justification of interferences with the fundamental rights to priva cy and dat a protection through surveillance measures when transferring personal data (European Essential Guarantees), 13 April 2016. (extracted from634.txt)
In addition, wi th regard to decisions of the Court of Justice of the European Union concerning data protection, see: L. (extracted from634.txt)
These letters are available at: http://ec.europa.e u/justice/data -protection/article -29/documentation/other -document/index_en.htm . (extracted from634.txt)
42 3.8 Access to justice and the right to a fair trial Access to justice and the right to a fair trial Article 6 (2) ECHR plays an important role with regard to predictive AI . (extracted from634.txt)
The increased use of risk -assessing algorithms in the American justice system raises accountability a nd transparency issues.100 It has been reported that software used to set bail , conditions for parole and sentencing decisions is biased against Afr ican Americans (Angwin et al. (extracted from634.txt)
To give another example, in response to worries by consumers about Wi -Fi tracking by shop owners, the former Dutch minister of economic affairs and the state secretary of security and justice stated that people should just turn off their smartphone if they do not want to be tracked.102 On the basis of this response , it seems that tracking and tracing people is a right which is deemed more important than peoples’ (privacy) rights. (extracted from634.txt)
Besides affecting the right to respect for private lif e in numerous ways, digiti sation, virtuali sation and roboti sation influence human dignity, the right to the peaceful enjoyment of possessions, safety and tort rights, the right to freedom of expression, the prohibition of discrimination, access to justice and the right to a fair trial. (extracted from634.txt)
Access to justice and the right to a fair trial (see subsection 3.8) The Council of Europe could establish a framework of minimum norms to be taken into account when a court uses AI. (extracted from634.txt)
(2007) E-Justice, beginselen van behoorlijke elektronische rechtspraak, The Hague: SDu Uitgevers. (extracted from634.txt)
Accept additional cookies Reject additional cookies View cookies Hide cookie message Skip to main content GOV.UK Navigation menu Menu Menu Services and information Benefits Births, death, marriages and care Business and self-employed Childcare and parenting Citizenship and living in the UK Crime, justice and the law Disabled people Driving and transport Education and learning Employing people Environment and countryside Housing and local services Money and tax Passports, travel and living abroad Visas and immigration Working, jobs and pensions Government activity Departments Departments, agencies and public bodies News News stories, speeches, letters and notices Guidance and regulation Detailed guidance, regulations and rules Research and statistics Reports, analysis and official statistics Policy papers and consultations Consultations and strategy Transparency Data, Freedom of Information releases and corporate reports Search GOV.UK × Search GOV.UK Search Search GOV.UK Search Home Independent report COVID-19 repository and public attitudes retrospective The CDEI has published new research on the use of AI and data-driven technology in the UK’s COVID-19 response, highlighting insights into public attitudes, as well as trends it has identified. (extracted from84.txt)
Cancel Services and information Benefits Births, death, marriages and care Business and self-employed Childcare and parenting Citizenship and living in the UK Crime, justice and the law Disabled people Driving and transport Education and learning Employing people Environment and countryside Housing and local services Money and tax Passports, travel and living abroad Visas and immigration Working, jobs and pensions Government activity Departments News Guidance and regulation Research and statistics Policy papers and consultations Transparency How government works Get involved Support links Help Privacy Cookies Accessibility statement Contact Terms and conditions Rhestr o Wasanaethau Cymraeg Government Digital Service All content is available under the Open Government Licence v3.0, except where otherwise stated © Crown copyright (extracted from84.txt)
Accept additional cookies Reject additional cookies View cookies Hide cookie message Skip to main content GOV.UK Navigation menu Menu Menu Services and information Benefits Births, death, marriages and care Business and self-employed Childcare and parenting Citizenship and living in the UK Crime, justice and the law Disabled people Driving and transport Education and learning Employing people Environment and countryside Housing and local services Money and tax Passports, travel and living abroad Visas and immigration Working, jobs and pensions Government activity Departments Departments, agencies and public bodies News News stories, speeches, letters and notices Guidance and regulation Detailed guidance, regulations and rules Research and statistics Reports, analysis and official statistics Policy papers and consultations Consultations and strategy Transparency Data, Freedom of Information releases and corporate reports Search GOV.UK × Search GOV.UK Search Search GOV.UK Search Home Business and industry Science and innovation Artificial intelligence National AI Strategy Department forScience, Innovation& Technology Office for Artificial Intelligence Department forDigital, Culture,Media & Sport Department forBusiness, Energy& Industrial Strategy Guidance National AI Strategy - HTML version Updated 18 December 2022 Contents Our ten-year plan to make Britain a global AI superpower Executive summary Summary of key actions Introduction Pillar 1: Investing in the long-term needs of the AI ecosystem Pillar 2: Ensuring AI benefits all sectors and regions Pillar 3: Governing AI effectively Next steps Print this page © Crown copyright 2022 This publication is licensed under the terms of the Open Government Licence v3.0 except where otherwise stated. (extracted from185.txt)
Cancel Services and information Benefits Births, death, marriages and care Business and self-employed Childcare and parenting Citizenship and living in the UK Crime, justice and the law Disabled people Driving and transport Education and learning Employing people Environment and countryside Housing and local services Money and tax Passports, travel and living abroad Visas and immigration Working, jobs and pensions Government activity Departments News Guidance and regulation Research and statistics Policy papers and consultations Transparency How government works Get involved Support links Help Privacy Cookies Accessibility statement Contact Terms and conditions Rhestr o Wasanaethau Cymraeg Government Digital Service All content is available under the Open Government Licence v3.0, except where otherwise stated © Crown copyright (extracted from185.txt)
Assessment List for Trustworthy AI (ALTAI) 27 Fairness: Fairness refers to a variety of ideas known as equity, impartiality, egalitarianism, non-discrimination and justice. (extracted from230.txt)
For instance, even if a machine's impact is only ever good, the distribution of that good (concerns for justice) might be in question”. (extracted from224.txt)
European Parliament 2014-2019 TEXTS ADOPTED P8_TA(2019)0081 A comprehensive European industrial policy on artificial intelligence and robotics European Parliament resolution of 12 February 2019 on a comprehensive European industrial policy on artificial intelligence and robotics (2018/2088(INI)) The European Parliament, – having regard to its resolution of 16 February 2017 with recommendations to the Commission on Civil Law Rules on Robotics1, – having regard to its resolution of 1 June 2017 on digitising European industry2, – having regard to its resolution of 12 September 2018 on autonomous weapon systems3, – having regard to its resolution of 11 September 2018 on language equality in the digital age4, – having regard to the Commission proposal of 6 June 2018 establishing the Digital Europe programme for the period 2021-2027 (COM(2018)0434), – having regard to Council Regulation (EU) 2018/1488 of 28 September 2018 establishing the European High Performance Computing Joint Undertaking5, – having regard to Rule 52 of its Rules of Procedure, – having regard to the report of the Committee on Industry, Research and Energy and the opinions of the Committee on the Internal Market and Consumer Protection, the Committee on Legal Affairs, the Committee on Civil Liberties, Justice and Home Affairs and the Committee on the Environment, Public Health and Food Safety (A80019/2019), A. (extracted from280.txt)
Points out that the guiding ethical framework should be based on the principles of beneficence, non-maleficence, autonomy and justice, on the principles and values enshrined in Article 2 of the Treaty on European Union and in the Charter of Fundamental Rights, such as human dignity, equality, justice and equity, nondiscrimination, informed consent, private and family life and data protection, as well as on other underlying principles and values of Union law, such as non-stigmatisation, transparency, autonomy, individual responsibility and social responsibility, and on existing ethical practices and codes; 148. (extracted from280.txt)
Texts adopted - Civil Law Rules on Robotics - Thursday, 16 February 2017 Choisissez la langue de votre document : bg - български es - español cs - čeština da - dansk de - Deutsch et - eesti keel el - ελληνικά en - English fr - français ga - Gaeilge hr - hrvatski it - italiano lv - latviešu valoda lt - lietuvių kalba hu - magyar mt - Malti nl - Nederlands pl - polski pt - português ro - română sk - slovenčina sl - slovenščina fi - suomi sv - svenska Index Previous Next Full text Procedure : 2015/2103(INL)Document stages in plenaryDocument selected : A8-0005/2017Texts tabled : A8-0005/2017 Debates : PV 15/02/2017 - 14 CRE 15/02/2017 - 14 Votes : PV 16/02/2017 - 6.9 Explanations of votes Texts adopted : P8_TA(2017)0051 Texts adopted 253k 74k Thursday, 16 February 2017 - Strasbourg Civil Law Rules on Robotics P8_TA(2017)0051A8-0005/2017 Resolution Annex European Parliament resolution of 16 February 2017 with recommendations to the Commission on Civil Law Rules on Robotics (2015/2103(INL)) The European Parliament, – having regard to Article 225 of the Treaty on the Functioning of the European Union, – having regard to Council Directive 85/374/EEC(1), – having regard to the study on Ethical Aspects of Cyber-Physical Systems carried out on behalf of the Parliament's Science and Technology Options Assessment (STOA) Panel and managed by the Scientific Foresight Unit (STOA), European Parliamentary Research Service; – having regard to Rules 46 and 52 of its Rules of Procedure, – having regard to the report of the Committee on Legal Affairs and the opinions of the Committee on Transport and Tourism, the Committee on Civil Liberties, Justice and Home Affairs, the Committee on Employment and Social Affairs, the Committee on the Environment, Public Health and Food Safety, the Committee on Industry, Research and Energy and the Committee on the Internal Market and Consumer Protection (A8-0005/2017), Introduction A. (extracted from294.txt)
Points out that the guiding ethical framework should be based on the principles of beneficence, non-maleficence, autonomy and justice, on the principles and values enshrined in Article 2 of the Treaty on European Union and in the Charter of Fundamental Rights, such as human dignity, equality, justice and equity, non-discrimination, informed consent, private and family life and data protection, as well as on other underlying principles and values of the Union law, such as non-stigmatisation, transparency, autonomy, individual responsibility and social responsibility, and on existing ethical practices and codes; 14. (extracted from294.txt)
Researchers in the field of robotics should commit themselves to the highest ethical and professional conduct and abide by the following principles: Beneficence – robots should act in the best interests of humans; Non-maleficence – the doctrine of ‘first, do no harm’, whereby robots should not harm a human; Autonomy – the capacity to make an informed, un-coerced decision about the terms of interaction with robots; Justice – fair distribution of the benefits associated with robotics and affordability of homecare and healthcare robots in particular. (extracted from294.txt)
LICENCE FOR DESIGNERS – You should take into account the European values of dignity, autonomy and self-determination, freedom and justice before, during and after the process of design, development and delivery of such technologies including the need not to harm, injure, deceive or exploit (vulnerable) users. (extracted from294.txt)
To further ensure trustworthy AI, most governments are adopting new legislative frameworks for AI technologies , which are often sector specific and cover areas such as autonomous driving, healthcare and e -justice. (extracted from243.txt)
The Federal Ministry also highlights the use of AI for evaluating judicial proceedings, as mentioned in its National e -justice strategy . (extracted from243.txt)
ensuring that citizens can make informed and independent decisions) and to human dignity, equality and justice (i.e. (extracted from243.txt)
In addition, public sector operators should be secured sufficient resources and i ncentives to engage in such development, paying particular attention to sort out the rights of the outcomes of co -development;  The Ministry of Justice and the Ministry of Finance are currently examining national regulation of automated decision -making. (extracted from243.txt)
5.13.4 Regulation Hungary’s nationa l AI strategy aims to ensure a responsible, reliable and human -centred utilisation of AI technologies by means of the following policies:  Creating an ethical framework : developing an AI code of conduct by the first half of 2021 in collaboration between the Ministry of Justice, the Ministry for Innovation and Technology, AI Innovation Hub and the Central Statistical Office ;  Setting up an Artificial Intelligence Regulation and Ethics Knowledge Centre : the aim is to create and coordinate an extensive pool of experts to help resolve legal issues and matters of ethics relating to the regulation of AI and the implementation of the strategy ;  Establishing a regulatory framework for AI: t he obje ctive is to amen d the current regulatory system to suit AI and to align it to EU regulations ;  Building data management regulation : the objective is to set up regulations for the use and exchange of public and private data and to define rules regarding d ata monetisation. (extracted from243.txt)
The Latvian strategy identifies priority sectors with a high potential for AI applicatio ns in the country, such as transport (Intelligent transport systems), culture, justice (AI as support for decision making and drafting legislation), agriculture (automated control), and translation. (extracted from243.txt)
Regarding ethics, the government adopts the European ethical guidelines outlined by the European Commission for the Efficiency of Justice. (extracted from243.txt)
66 1) International bodies and AI, 2) Swiss intelligence of interests in the European AI (Digital Europe Programme) activities, 3) Changes in the world of work, 4) AI in industry and services, 5) AI in education, 6) A pplication of AI in science and research, 7) AI in cybersecurity and security policy, 8) AI, Media & Public, 9) Automated mobility and AI, 10) AI in healthcare, 11) AI in the financial secto r, 12) AI in agriculture, 13) Energy, climate, environment and AI, 14) AI in administration, 15) Further development of the general legal framework on AI, 16) AI in justice, 17) AI, data and intellectual property law. (extracted from243.txt)
While the establishment of the legal basis is ensured by a wide range of institutions, the FDFA will specifically focus on the following policies to further develop the general legal framework on AI:  Examining the emergence of AI -specific international law and its impac t on Switzerland;  Following -up developments with regard to the visibility of AI systems in interaction with consumers;  Monitoring developments in AI -based decision -making in the justice system (predictive justice). (extracted from243.txt)
President: Pascal Pichonnaz First Vice-President: Lord John Thomas Second Vice-President: Anne Birgitte Gammeljord Treasurer: Pietro Sirena Speaker of the Senate: Reinhard Zimmermann Secretary-General: Vanessa Wilcox Scientific Director: Christiane Wendehorst ISBN: 978-3-9505192-7-3 © European Law Institute 2022 Cover image: ShutterstockThe European Law Institute This publication was co-funded by the European Union’s Justice Programme. (extracted from257.txt)
21 Guiding Principle 8: No limitations to the exercise of rights and access to justice Automation shall not prevent, limit, or render unfeasible the exercise of rights and access to justice by affected persons. (extracted from257.txt)
The provision of public services or the exercise of public functions likely to materially impact citizens’ rights and liberties should be subject to special regulatory scrutiny.10 Likewise, legislators may consider it unacceptable to admit fully automated dispute resolution as access to justice would thus be deprived of human intervention. (extracted from257.txt)
Should damage or personal injuries be caused by an accident, a collision with buildings or windows, or by a drone crashing in a garden, the university, as the operator, bears the risk, without prejudice to the liability of the producer, if damage is caused by a defect of the product.Guiding Principle 8: No limitations to the exercise of rights and access to justice Automation shall not prevent, limit, or render unfeasible the exercise of rights and access to justice by affected persons. (extracted from257.txt)
As a specific application of Guiding Principle 1, this Principle focuses on the risk that the exercise of rights by the affected person and effective access to justice may be prevented, hampered or limited by the inadequate use of automation. (extracted from257.txt)
Second, where the affected person is deprived of the possibility of exercising a right or access to justice solely on the grounds that the contested decision was made by ADM. (extracted from257.txt)
Stop Explaining Black Box Machine Learning Models for High Stakes Decisions and Use Interpretable Models Instead Cynthia Rudin Duke University cynthia@cs.duke.edu Abstract Black box machine learning models are currently being used for high stakes decision-making throughout society, causing problems throughout healthcare, criminal justice, and in other domains. (extracted from690.txt)
This manuscript clariﬁes the chasm between explaining black boxes and using inherently interpretable models, outlines several key reasons why explainable black boxes should be avoided in high-stakes decisions, identiﬁes challenges to interpretable machine learning, and provides several example applications where interpretable models could potentially replace black box models in criminal justice, healthcare, and computer vision. (extracted from690.txt)
1 Introduction There has been an increasing trend in healthcare and criminal justice to leverage machine learning (ML) for high-stakes prediction applications that deeply impact human lives. (extracted from690.txt)
The lack of transparency and accountability of predictive models can have (and has already had) severe consequences; there have been cases of people incorrectly denied parole [ 1], poor bail decisions leading to the release of dangerous criminals, ML-based pollution models stating that highly polluted air was safe to breathe [ 2], and generally poor use of limited valuable resources in criminal justice, medicine, energy reliability, ﬁnance, and in other domains [3]. (extracted from690.txt)
As of yet, I have not encountered such an application, despite having worked on numerous applications in healthcare and criminal justice [e.g., 21], energy reliability [e.g., 20], and ﬁnancial risk assessment [e.g., 22]. (extracted from690.txt)
Justice system for parole and bail decisions. (extracted from690.txt)
Justice System for recidivism risk prediction does not depend on the seriousness of the current crime [ 27,29]. (extracted from690.txt)
Justice System for predicting the probability that someone will be arrested after their release [29]. (extracted from690.txt)
This evidence, however, has not changed the momentum of the justice system towards proprietary models. (extracted from690.txt)
Justice System Table 1: Comparison of COMPAS and CORELS models. (extracted from690.txt)
Perhaps we could prevent some of the poor decisions in criminal justice and medicine that are caused by problems with using black box models. (extracted from690.txt)
This may continue to lead to poor decisions throughout our criminal justice system, incorrect safety guidance for air quality disasters, incomprehensible loan decisions, and other widespread societal problems. (extracted from690.txt)
When a Computer Program Keeps You in Jail: How Computers are Harming Criminal Justice. (extracted from690.txt)
Optimized Scoring Systems: Toward Trust in Machine Learning for Healthcare and Criminal Justice. (extracted from690.txt)
Criminal Justice and Behavior. (extracted from690.txt)
The benefit of establishing clear, common rules of criminal liability will benefit a proper administration o f justice. (extracted from121.txt)
In order to maintain good co-operation in criminal matters among the members of the Council of Europe several issues should be addressed including the question of how different approaches in testing and using auto mated vehicles can translate into “permissible risks” not criminali sed in domestic law (like the different uses o f technologies in cars) as well as the question of whether an auto mated vehicle may eventually have to answer the law as an e -person (similar to corporations as legal persons) or whether criminal justice is for “human persons ” only. (extracted from121.txt)
It is foreseeable that any process to set regulatory standards in this area will also require input from a range of stakeholders, including, but not limited to: - Criminal justice system : o Prosecutors and investigators , o Trial courts , o Ministry of Justice / central administrations . (extracted from121.txt)
Accept additional cookies Reject additional cookies View cookies Hide cookie message Skip to main content GOV.UK Navigation menu Menu Menu Services and information Benefits Births, death, marriages and care Business and self-employed Childcare and parenting Citizenship and living in the UK Crime, justice and the law Disabled people Driving and transport Education and learning Employing people Environment and countryside Housing and local services Money and tax Passports, travel and living abroad Visas and immigration Working, jobs and pensions Government activity Departments Departments, agencies and public bodies News News stories, speeches, letters and notices Guidance and regulation Detailed guidance, regulations and rules Research and statistics Reports, analysis and official statistics Policy papers and consultations Consultations and strategy Transparency Data, Freedom of Information releases and corporate reports Search GOV.UK × Search GOV.UK Search Search GOV.UK Search Home Business and industry Science and innovation Artificial intelligence National AI Strategy Department forScience, Innovation& Technology Office for Artificial Intelligence Department forDigital, Culture,Media & Sport Department forBusiness, Energy& Industrial Strategy Guidance National AI Strategy - HTML version Updated 18 December 2022 Contents Our ten-year plan to make Britain a global AI superpower Executive summary Summary of key actions Introduction Pillar 1: Investing in the long-term needs of the AI ecosystem Pillar 2: Ensuring AI benefits all sectors and regions Pillar 3: Governing AI effectively Next steps Print this page © Crown copyright 2022 This publication is licensed under the terms of the Open Government Licence v3.0 except where otherwise stated. (extracted from109.txt)
Cancel Services and information Benefits Births, death, marriages and care Business and self-employed Childcare and parenting Citizenship and living in the UK Crime, justice and the law Disabled people Driving and transport Education and learning Employing people Environment and countryside Housing and local services Money and tax Passports, travel and living abroad Visas and immigration Working, jobs and pensions Government activity Departments News Guidance and regulation Research and statistics Policy papers and consultations Transparency How government works Get involved Support links Help Privacy Cookies Accessibility statement Contact Terms and conditions Rhestr o Wasanaethau Cymraeg Government Digital Service All content is available under the Open Government Licence v3.0, except where otherwise stated © Crown copyright (extracted from109.txt)
Department of Commerce Subcommittee on Networking and Information Technology Research and Development Co-Chair Bryan Biegel Director, National C oordination Office for Networking and Information Technology Research and Development Co-Chair James Kurose Assistant Director, Computer and Information Science and Engineering National Science Foundation Networking and Information Technology Research and Development Task Force on Artificial Intelligence Co-Chair Lynne Parker Division Director Information and Intelligent System s National Science Foundation Co-Chair Jason Matheny Director Intelligence Advanced Research Projects Activity Members Milton Corn National Institutes of Health Nikunj Oza National Aeronautics and Space Administration William Ford National Institute of Justice Robinson Pino Department of Energy Michael Garris National Institute of Standards and Technology Gregory Shannon Office of Science and Technology Policy Steven Knox National Security Agency Scott Tousley Department of Homeland Security viii John Launchbury Defense Advanced Research Projects Agency Faisal D’Souza Technical Coordinator National Coordination Office for Networking and Information Technology Research and Development Richard Linderman Office of the Secretary of Defense NATIONAL ARTIFICIAL INTELLIGENCE RESEARCH AND DEVELOPMENT STRATEGIC PLAN 1 Contents About the National Science and Technology Council ................................ (extracted from486.txt)
The Federal government should therefore emphasize AI investments in areas of strong societal importance that are not aimed at consumer markets —areas such as AI for public health, urban systems and smart communiti es, social welfare, criminal justice, environmental sustainability, and national security, as well as long -term research that accelerates the production of AI knowledge and technologies . (extracted from486.txt)
Scientists must also study to what extent justice and fairness considerations can be designed into the system, and how to accomplish this within the bounds of current engineering techniques . (extracted from486.txt)
NATIONAL ARTIFICIAL INTELLIGENCE RESEARCH AND DEVELOPMENT STRATEGIC PLAN 27 Building e thical AI Beyond fundamental assumptions of justice and fairness are other concerns about whether AI systems can exhibit behavior that a bides by general ethical principles. (extracted from486.txt)
1 Statement by the French Presidency of the Committee of Ministers of the Council of Europe at the Conference of Ministers of Justice on “Digital Challenges for Justice in Europe” (15 October 2019) For 70 years, the Council of Europe and its member States have been committed to promoting and protecting human rights, democracy and the rule of law , values that are enshrined in the European Convention on Human Rights. (extracted from323.txt)
The development of digital technology is both a n opportunity and a challenge for justice systems in Europe, which are founded on the primacy of law , respect for human rights and the principles of judicial independence, impartiality and efficiency. (extracted from323.txt)
In particular, new technologies have important implications for citizens seeking to access law and justice, as well as the suppression of crimes and offenc es they may be victims of. (extracted from323.txt)
It is essential that the Organisation continue its work to ensure that justice systems can take greater advantage of digital technology while preventing any harmful effects that it may have on our shared values. (extracted from323.txt)
Encourages ongoing dialogue and coop eration among Council of Europe member States on the use of digital tools in the justice field; 2. (extracted from323.txt)
Recalls the need to take into account the following principles when developing Council of Europe tools and instruments for justice and digital technology: i. (extracted from323.txt)
Digital access to law and justice should supplement non -digital access. (extracted from323.txt)
The use of digital tools and algorithms in the justice field must not have a ny discriminatory effect on individuals , and must guarantee respect for privacy and the right to data protection. (extracted from323.txt)
Establish a data protection framework with legal backing : The work being done by Justice Srikrishna Committee on data protection law is very opportune and timely. (extracted from479.txt)
23In considering the applications of AI in areas such as criminal justice and health care, organizations should design, build and deploy AI systems that leverage human judgment and responsibility where they are needed. (extracted from478.txt)
Suggested lead: Department of Commerce, Department of State, Department of Justice. (extracted from478.txt)
Suggested lead: Department of Commerce, Department of State, Department of Justice. (extracted from478.txt)
In particular, in m edicine, CNOM refers in its report to the applicability to digital technologies of the four principles of medical ethics: beneficence, non-maleficence, auton omy, and justice. (extracted from322.txt)
Regulation and soft law (self -compliance and volu ntary certification) The ruling on December 7 last by the European Court of Justice notably circumscribed national capacity for regulation on d igital innovation in the healthcare sphere.41 This Community framework therefore opens the door to methods of regulation that do not entail enforceabl e law. (extracted from322.txt)
41 The Court of Justice of the European Union (CJEU) was called upon to rule on a prejudicial question regarding whether a software program for p rescription support meets the definition of a medical device, if that program provides at le ast one function that can be used to process data specific to the patient for the purpose of hel ping the doctor to establish a prescription, in particular by detecting contraindications, drug int eractions, and overdoses, although it does not of itself act in or upon the human body (CE, Ju ne 8, 2016, n° 387156). (extracted from322.txt)
Court of Appeals for the Second Circuit and Justice David H. (extracted from336.txt)
Mariano-Florentino Cuéllar is a Justice on the Supreme Court of California, the Herman Phleger Visiting Professor of Law at Stanford University, and a faculty affiliate at the Stanford Center for AI Safety. (extracted from336.txt)
We also appreciate superb editorial assistance and dedicated research support from three members of Justice Cuéllar’s staff: Ryan Azad, Alexandra Havrylyshyn, and Mikayla Hardisty. (extracted from336.txt)
A large number of use cases fell under health- and law-enforcement-focused subagencies such as the Food and Drug Administration, the Office of Justice Programs, and the Transportation Safety Administration and Customs and Border Protection. (extracted from336.txt)
As a result, the Department of Health and Human Services, the Department of Justice, and the Department of Homeland Security account for a collective 51 use cases. (extracted from336.txt)
TOP TEN AGENCIES AND SUBAGENCIES BY NUMBER OF USE CASES Agency NameNumber of Use Cases Office of Justice Programs 12 Securities and Exchange Commission 10 National Aeronautics and Space Administration 9 Food and Drug Administration 8 United States Geological Survey 8 United States Postal Service 8 Social Security Administration 7 United States Patent and Trademark Office 6 Bureau of Labor Statistics 5 Customs and Border Protection 4 Table 2: The above list excludes overarching department-level agencies. (extracted from336.txt)
For example, the Department of Health and Human Services (19 use cases), the Department of Justice (16 use cases), and the Department of Homeland Security (16 use cases) have been refactored into respective sub-agencies (e.g., the Food and Drug Administration, the Office of Justice Programs, and Customs and Border Protection). (extracted from336.txt)
Others, such as the Federal Bureau of Investigation Terrorist Screening Database and the Department of Justice National Crime Information Center, come from peer agencies. (extracted from336.txt)
While the foregoing cannot possibly do justice to ongoing debate about the proper role of technology-enabled surveillance, it is a crucial debate to have. (extracted from336.txt)
Loomis, the Wisconsin Supreme Court did not find a due process violation when gender was used in a criminal risk assessment score, finding that the “use of gender promotes accuracy that ultimately inures to the benefit of the justice system.”101 Due to the doctrinal uncertainty, states and localities using criminal risk assessment scores remain split in whether they rely on gender.102 To the extent that the machine learning literature calls for awareness of protected attributes to promote fairness, it is on a collision course with equal protection doctrine. (extracted from336.txt)
2013), https://www.governing.com/topics/public-justice-safety/gov-social-media-transforms-chicago-policing.html. (extracted from336.txt)
Dep’t of Justice, Exec. (extracted from336.txt)
68 Linesight: Advanc ed Targeting Analytics Solution for Border Security, Unisys, https://www.unisys.com/offerings/industry-solutions/public-sector-industry-solutions/justice-law-enforcement-and-border-security-solutions/linesight (last visited Apr 7, 2019). (extracted from336.txt)
Mashaw, Bureaucratic Justice: Managing Social Security Disabilit y Claims (1985); David Ames, Cassandra Handan-Nader, Daniel E. (extracted from336.txt)
11 Some 390 Immigra tion Judges work for the Executive Office of Immigration Review in the Department of Justice and decide immigration cases. (extracted from336.txt)
Immigration Judge, Dep’t of Justice (June 9, 2017), https://www.justice.gov/legal-careers/job/immigration-judge; 4 Executive Off. (extracted from336.txt)
Dep’t of Justice, Executive Off. (extracted from336.txt)
One of the more interesting arguments for “internal” constraints on algorithmic decision-making is that, while marquee uses of algorithmic decisions systems—e.g., the criminal justice context—will draw certain judicial scrutiny. (extracted from336.txt)
Mashaw, Bureaucratic Justice: Managing Social Security (1985). (extracted from336.txt)
101, 109 (2019); Rebecca Wexler, Life, Liberty, and Trade Secrets: Intellectual Property in the Criminal Justice System, 70 Stan. (extracted from336.txt)
T yler, What Is Procedural Justice?: Criteria Used by Citizens to Assess the Fairness of Legal Procedures 22 L. (extracted from336.txt)
Mashaw, Bureaucratic Justice: Managing Social Security Disabilit y Claims (1983). (extracted from336.txt)
124 Eugene V olokh, Chief Justice Robots, 68 Duke L.J. (extracted from336.txt)
They don’t connect our ideas about what’s good or just to the practices that create goodness and justice. (extracted from493.txt)
46 ODR and access to justice (article 6 and 13) ................................ (extracted from120.txt)
(5) The use of online courts has the ability revolutionise access to justice for litigants. (extracted from120.txt)
The development of new procedures to resolve disputes online can revolutionise access to justice to persons who would usually be unable to understand court procedures without hiring a lawyer. (extracted from120.txt)
It could improve the justice system to make it more accessible for those who live far from legal centres or who struggle to afford the costs of seeking justice, by providing cheaper, alternative means to resolving disputes. (extracted from120.txt)
This is a massive improvement of access to justice for litigants. (extracted from120.txt)
(7) An issue with ODR and access to justice is that those who are computer illiterate or have no access to technology might be side -lined in the process. (extracted from120.txt)
Requiring parties to use technology to resolve disputes could inhibit access to justice if there is a great discrepancy between the parties and their access to technology. (extracted from120.txt)
The move to online and virtual justice also threatens to significantly increase the number of unrepresented defendants, to further discriminate against vulnerable defendants, to CDCJ(2018) 5 7 inhibit the relationship between defence lawyers and their clients, and, as some argue, make justice less open. (extracted from120.txt)
(8) If some litigants do not have access to, or the ability to use, technology and the internet, these litigants will be excluded from the administration of justice. (extracted from120.txt)
Traditionally members of the public have been granted physical access to the court building, but many countries in Europe do not allow public broadcasting of trials on TV for the reason that this may influence advocates and judges who then ” play” to populist sentiments of crowd watching which may not lead to better justice. (extracted from120.txt)
This would correspond to the pyramid model of dispute resolution which integrates legal advic e (the parties informing themselves about their legal rights and their legal position through the use of expert systems/artificial intelligence), negotiation and conflict resolution techniques (restorative justice), facilitated negotiation and mediation, a nd adjudication (potentially several stages, including the possibility of review and appeal). (extracted from120.txt)
(17) Generally speaking, like in other areas of digiti zation, ODR may have a negative impact on data protection and privacy in that online justice is likely to generate a much greater wealth of data (including metadata, for example who accessed a particular court record when and from where), increases the pos sibilities of data processing, searching, data mining and the use of artificial intelligence (which is the other side of the coin of increased access to justice) and online data (including court data) may be more mobile (easy online transfer), sticky (in t he sense that data remains on storage devices until erased) and vulnerable to unauthorised, remote access (computer hacking from anywhere in the world). (extracted from120.txt)
If ODR is to be implemented into public justice systems, these systems mu st be designed in such a way that there is equality of arms between the litigants. (extracted from120.txt)
(25) Given the pressure of high caseloads and insufficient resources from which most justice systems suffer, there is a danger that support systems based on artificial intelligence are inappropriately us ed by judges to “delegate” decisions to technological systems that were not developed for that purpose and are perceived as being more ‘objective' even when this is not the case. (extracted from120.txt)
A second expert was also appointed, Petra Jurina who is Head of Service for Civil Procedure Law, Commercial Law and Alternative Dispute Resolution at the Ministry of Justice in Croatia who helped to produce the Dra ft Questionnaire . (extracted from120.txt)
To analyse the compatibility of Online Dispute Resolution with the right to a fair trial both in terms of the challenges to the right of a fair trial as well as opportunities afforded by Online Dispute Resolution to provide greater access to justice and enha nced due process. (extracted from120.txt)
3 J Hornle Cross -border Internet Dispute Resolution (Cambridge University Press 2009) 4 Dory Reiling, ‘E -justice: experiences with court IT in Europe’ at pg. (extracted from120.txt)
However such processes are only part of the Study to the extent that they are part of the civil or administrative procedure of the country concerned ( ie mandatory or ordered by a judge or someho w annexed or integrated into the official civil /administrative justice system). (extracted from120.txt)
Q.A.3: under Belgian Law family disputes are dealt with by the Family Tribunal and the Justice of the Peace deals with neighbourhood disputes and other minor disputes - but there are no specific ODR processes/techniques for these special tribunals other than those used in the ordinary civil courts. (extracted from120.txt)
If any criminal or civil proceedings, the Court may – if it considers that justice requires so – to allow a witness who is abroad, to give his/her testimony via videoconference. (extracted from120.txt)
The “ePodatelna“ system can be used (https://epodatelna.justice.cz/ePodatelna/epo1200new/form.do ). (extracted from120.txt)
This is regulated in the Administration of Justice Act, chapter 26 and 27. (extracted from120.txt)
France Q.A.1 and Q.A.2 A 2016 Act (La loi n° 2016 -1547 du 18 novembre 2016 de modernisation de la justice du XXIème siècle) has introduced into French Law an obligation to attempt mediation/conciliatio n led by a judicial mediator, when the submission of the case to the trial court is envisaged by statement to the court registry. (extracted from120.txt)
Q.A.4 In 2013 the Ministry of Security and Justice in cooperation with the Council for the Judiciary started a modernization program for the judiciary. (extracted from120.txt)
The Portugues e Ministry of Justice has developed an online platform – named ‘Citius’, accessible at www.citius.mj.pt – to dematerialise proceedings by treating electronically all information belonging to the proceedings (such as claims, counterclaims, responses and rel ated documents), thus reducing their physical form to a minimum. (extracted from120.txt)
While ‘Citius’ is managed by the Ministry of Justice, enforcement officers use an application managed by their professional association (‘Ordem dos Solicitadores e dos Agentes de Execução’) which interconnects with ‘Citius’. (extracted from120.txt)
As for civil courts, the Portuguese Ministry of Justice has developed an online platform designed for administrative courts – named ‘SITAF and accessible at www.taf.mj.pt – to dematerialise proceedings by treating electronically all information belonging to the proceedings (such as claims, counterclaims, responses and related documents), thus reducing their physical form to a mi nimum. (extracted from120.txt)
The last report drawn up by the mediator at the end of the mediation negotiations is submitted to the Department of Mediation of the M inistry of Justice Directorate General for Civil Affairs. (extracted from120.txt)
This is a challenge which this report must attemp t to tackle: how to ensure that, if ODR mechanisms are developed and implemented into the court systems of EU Member States, these ODR systems are compliant with the right to a fair tri al: a right that imposes procedural safeguards onto the justice system, and which would impose procedural safeguards onto the ODR systems. (extracted from120.txt)
This must be done in a way that does not destroy the efficiency savings and access to justice gained through ODR. (extracted from120.txt)
The fair trial standards to be developed foc us on the following challenges i ) due process in a narrow sense (equality of arms, impartiality, transparency etc), ii) access to justice and the digital divide, iii) issues inherent in the technology itself, for example prejudice issues embedded in artificial intelligence and iv) cybersecurity (authenticity, identification and integrity) and data protection. (extracted from120.txt)
The Civil Justice Council formed an Online Dispute Resolution Advisory Group reporting in February 2015 and recommending that there should be a new, Int ernet based , court service, known as the HM Online Court. (extracted from120.txt)
Lord Justice Briggs has been tasked in July 2015 to review the structure of the civil courts in England & Wales with a view to their modernisation and is currently consulting on the possibility of online courts for lower value disputes. (extracted from120.txt)
In 2012, the British Columbia government passed the Civil Resolution Tribunal Act, “with the goal of using technology and ADR to increase access to justice for British Columbians with small claims and condominium property disputes.”17 36. (extracted from120.txt)
The Civil Resolution Tribunal (CRT) is Canada’s first online tribunal, and currently the only ODR system in the world that is fully integrated into the justice system. (extracted from120.txt)
A key design feature of the CRT is that wherever possible, a user should only have to enter information once, and the system should carry this information forward to other stages of the CRT 17 Shan non Salter, ‘ODR and Justice System Integration: B.C.’s Civil Resolution Tribunal’ (2017) at pg. (extracted from120.txt)
18 Shannon Salter, ‘ODR and Justice System Integration: B.C.’s Civil Resolution Tribunal’ (2017) at pg. (extracted from120.txt)
(2017) 34 Windsor Y B Access Just 19 Taken from Shannon Salter, ‘ODR and Justice System Integration: B.C.’s C ivil Resolution Tribunal’ (2017) 34 Windsor Y B Access Just CDCJ(2018) 5 46 process. (extracted from120.txt)
From beginning to end, the CRT process is intended to take 60 to 90 days for most cases, and the average total cost to the parties is roughly the same as in Small Claims Court, or about $200.21 ODR AND ACCESS TO JUSTICE (ARTICLE 6 AND 13) 39. (extracted from120.txt)
As has been noted above, ODR has the ability to revolutionise the public justice system. (extracted from120.txt)
If designed and implemented c orrectly, it has the ability to drastically enhance 20 Shannon Salter, ‘ODR and Justice System Integration: B.C.’s Civil Resolution Tribunal’ (2017) at pg. (extracted from120.txt)
CDCJ(2018) 5 47 access to justice for persons who would ordinarily be outcasts in the justice system. (extracted from120.txt)
However, if developed in the wrong hands it may have an opposite effect and actually reduce access to justice by placi ng technological barriers to persons who do not ordinarily have the capacity to use technology. (extracted from120.txt)
If ODR is the future of justice, which it seems is the case, then it is of fundamental importance to develop standards for ODR; standards by which ODR can be d eveloped and implemented and under which justice can be carried out. (extracted from120.txt)
Judgment shall be pronounced publicly but the press and public may be excluded from all or part of the trial in the interests of morals, public order or national security in a democratic society, where the interests of juveniles or the protection of the private life of the parties so require, or to the extent strictly necessary in the opinion of the cou rt in special circumstances where publicity would prejudice the interests of justice .” 43. (extracted from120.txt)
To realise these elements of a fair trial, it is quite obviously essential that persons have the right of access to court, for without access to justice there cannot be justice. (extracted from120.txt)
Moreover, ODR can have various implications for the right of access to court (or access to justice). (extracted from120.txt)
The use of online courts has the ability revolutionise access to justice for litigants. (extracted from120.txt)
The development of new procedures to resolve disputes online can revolutionise access to justice to persons who would usually be unable to understand court procedures without hiring a lawyer.26 The use of ODR could level the playing field of parties who would ordinarily find it hard to access courts. (extracted from120.txt)
It could improve the justice system to make it more accessible for those who liv e far from legal centres or who struggle to afford the costs of seeking justice,27 by providing cheaper, alternative means to resolving disputes. (extracted from120.txt)
ODR processes may be able to facilitate access to justice in that, if designed and developed correctly, ODR s ystems can be economically viable, efficient, fast and flexible.28 This is quite an obvious characteristic of online systems. (extracted from120.txt)
This is a massive improvement of access to justice for litigants. (extracted from120.txt)
Increased high internet access reflects social and generational change of how people now lead their lives, but what of the vulnerable users and those without access?29 Requiring parties to use technology to resolve disputes could inhibit access to justice if there is a great discrepancy between the parties and their access to technology.30 The move to online and virtual justice also threatens to significantly increase the number of unrepresented defendants, to further discriminate against vulnerable defendants, to inhibit the relationship between defence lawyers and their clients, and , as some argue, make justice less open.31 65. (extracted from120.txt)
Given the discussion about advantages and disadvantages above, it is important to note that parties must always have the right of access to justice. (extracted from120.txt)
Although ADR is permissible and parties are allowed to waive their right to access a court, when certain means of resolving a d ispute become compulsory, it must be guaranteed that such means give the parties the right to access justice. (extracted from120.txt)
Thus, if ODR is implemented into the court system or used in a compulsory ADR process, the right of the parties to access justice cannot be violat ed. (extracted from120.txt)
If some litigants do not have access to, or the ability to use, technology and the internet, these litigants will be exclu ded from the administration of justice. (extracted from120.txt)
Therefore, if ODR is implemented, there should (a) either be an alternative paper -based traditional means of having a dispute resolved for parties who do not have this access to technology and the internet or (b) a c omprehensive system of legal 26 J Rozenberg QC, ‘Justice Online: Just as Good?’ (2017). (extracted from120.txt)
27 L Tickle, The Guardian, ‘Online Justice: why co urts should explore emerging digital possibilities (2017). (extracted from120.txt)
29 Robert Thomas, ‘Current Developments in UK Tribunals: Challe nges for Administrative Justice’ (2016). (extracted from120.txt)
30 O Bowcott, The Guardian, ‘Government’s £1bn plan for online courts ‘challenges open justice’ (2017). (extracted from120.txt)
31 O Bowcott, The Guardian, ‘Government’s £1bn plan for online courts ‘challenges open justice’ (2017). (extracted from120.txt)
This may have an impact on the way in which ODR systems are designed and there may be undue influenc e placed on the administrators of the ODR system to retrieve such information at the expense of the distribution of justice. (extracted from120.txt)
One author argues that the move to online and virtual justice threatens to significantly increase the n umber of unrepresented defendants, to further discriminate against vulnerable defendants, to inhibit the relationship between defence lawyers and their clients, and to make justice less open.39 This is most certainly a point that deserves attention. (extracted from120.txt)
If tech nology is used (or allowed to be used) in a way that does not address concerns relating to access to justice and fairness, then those persons who are in a vulnerable position and who have less access to resources could be even more prejudiced than they alr eady are, at the hands of technology. (extracted from120.txt)
“The Court reiterates that the public character of proc eedings before the judicial bodies referred to in Article 6 § 1 protects litigants against the administration of justice in secret with no public scrutiny; it is also one of the means whereby confidence in the courts, superior and inferior, can be maintain ed.”50 “By rendering the administration of justice visible, publicity contributes to the achievement of the aim of Article 6 § 1, namely a fair trial, the guarantee of which is one of the fundamental principles of any democratic society, within the meaning of the Convention.”51 46 Werner v Austria (1998) 26 EHRR 310,349 ; Scarth v UK (1999) 27 E.H.R.R. (extracted from120.txt)
Trust in a justice system is vital to its legitimacy and success.55 If the public cannot scrutinise cases and decisions that are ma de, then there may no public approval of the justice system, which leads to it illegitimacy and failure. (extracted from120.txt)
55 L Tickle, The Guardian, ‘Online Justice: why courts should explore emerging digital possibilities (2017 ). (extracted from120.txt)
CDCJ(2018) 5 58 not lead to better justice. (extracted from120.txt)
This would correspond to the pyramid mod el of dispute resolution which integrates legal advice (the parties informing themselves about their legal rights and their legal position) , negotiation and conflict resolution techniques (restorative justice), facilitated negotiation and mediation, and ad judication (potentially several stages, including the possibility of review and appeal). (extracted from120.txt)
The idea behind this tiered model of dispute resolution is that most disputes are solved at the lower levels, thus being cost effective while at the same time giving more disputants access to justice. (extracted from120.txt)
A detailed discussion of data protection law and its application to civil and administrative justice systems is beyond the remit of this research. (extracted from120.txt)
Instead we only point to some pertinent issues of ODR, civil and administrative justice and data protection. (extracted from120.txt)
Data protection law in the EU provides for the lawfulness of processing of personal data where “processing is necessary for the performance of a task carried out in the public interest or in the exercise of official authority vested in the controller” .77 Thus in a nutshell, provided the processing of personal data is necessary (not excessive, proportionality test) for providing justice to litigants and running a justice system it is lawful under data protection law.78 115. (extracted from120.txt)
Generally speaking, like in other areas of digitization, ODR may have a negative impact on data protection and privacy in that online justice is likely to generate a much 75 J. (extracted from120.txt)
May 2018 in the Member States of the EU 78 To the extent that the data processing falls outside the scope of EU law there may also be an argument that data protection law is not applicable see Art 2 CDCJ(2018) 5 61 greater wealth of data (including metadata, for example who accessed a particular court record when and from where), increas es the possibilities of data processing, searching, data mining and the use of artificial intelligence (which is the other side of the coin of increased access to justice) and online data (including court data) may be more mobile (easy online transfer), st icky (in the sense that data remains on storage devices until erased) and vulnerable to unauthorised, remote access (computer hacking from anywhere in the world). (extracted from120.txt)
Furthermore as far as cybersecurity is concerned, digitalised courts are not only vulnerable to hacking (data protection & privacy implication s) but also vulnerable to other forms of malicious attack, affecting the integrity of data and the functioning of the justice system (one only needs to remember the large -scale Ransomware Wannacry attack of last year as an example of the impact79). (extracted from120.txt)
There is no doubt that while the application of these standards may be necessary to ensure that the development and implementation of an ODR system complies with the right to a fair trial, they may nonetheless prove burdensome to the development and implementation of an ODR system within the justice system. (extracted from120.txt)
It is a broadly phrased concept that, for the purposes of this Report, entails the idea that the use of technology, particularly online technology, to resolve disputes might disenfranchise certain portions of the population from access to justice. (extracted from120.txt)
If one is to devel op a public justice system that improves access to justice then such a system cannot come to be implemented if it will effectively deny certain persons access to justice. (extracted from120.txt)
A similar argument about literacy could be made as follows: not everyone within every society can read and write but that does not mean go vernments prevent people from accessing justice through written proceedings. (extracted from120.txt)
The digital divide also applies to the difference in access to justice between developed and developing states. (extracted from120.txt)
If ODR is to be implemented into public justice systems, these systems must be designed in such a way that there is equality of arms between the litigants. (extracted from120.txt)
This raises the question of why AI is problematic from the viewpoint of Article 6 ECHR and more generally for the idea of justice. (extracted from120.txt)
However the use of AI in policing (for example Predpol - used to prioritize police resources based on existing crime spots) or within the criminal justice sys tem (for example to decide on matters of whether a suspect is released on bail) have been criticized for being (racially) prejudiced and creating new types of discrimination.85 The concern here is that computer based decisions are inflexible and are not abl e to create exceptions or use human discretion. (extracted from120.txt)
This may go to the very core of our notion of access to justice and lead to a denial of justice. (extracted from120.txt)
AI is increasingly used in the context of the civil and criminal justice systems where artificial intelligenc e is being developed to eventually support or replace decision -making by human judges. (extracted from120.txt)
87ibid CDCJ(2018) 5 67 insufficient resources from which most justice systems suffer, there is a danger that support systems based on artificial intelligence are inappropriately used by judges to “delegate” decisions to technological systems that were not developed fo r that purpose and are perceived as being more ‘objective' even when this is not the case. (extracted from120.txt)
(5) The use of o nline courts has the ability revolutionise access to justice for litigants. (extracted from120.txt)
The development of new procedures to resolve disputes online can revolutionise access to justice to persons who would usually be unable to understand court procedures without hirin g a lawyer. (extracted from120.txt)
It could improve the justice system to make it more accessible for those who live far from legal centres or who struggle to afford the costs of seeking justice, by providing cheaper, alternative means to resolving disputes. (extracted from120.txt)
This is a massive improvemen t of access to justice for litigants. (extracted from120.txt)
(7) An issue with ODR and access to justice is that those who are computer illiterate or have no access to technology might be side -lined in the process. (extracted from120.txt)
Requiring parties to use technology to resolve disputes could inhibit access to justice if there is a great discrepancy between the parties and their acces s to technology. (extracted from120.txt)
The move to online and virtual justice also threatens to significantly increase the number of unrepresented defendants, to further discriminate against vulnerable defendants, to inhibit the relationship between defence lawyers and their c lients, and, as some argue, make justice less open. (extracted from120.txt)
(8) If some litigants do not have access to, or the ability to use, technology and the internet, these litigants will be excluded from the administration of justice. (extracted from120.txt)
Traditionally members of the public have been granted physical access to the court buildi ng, but many countr ies in Europe do not allow public broadcasting of trials on TV for the reason that this may influence advocates and judges who then ”play” to populist sentiments of crowd watching which may not lead to better justice. (extracted from120.txt)
This would correspond to the pyramid model of dispute resolution which integrates legal advice (the parties informing CDCJ(2018) 5 70 themselves about their legal rights and their legal position through the use of expert systems/ar tificial intelligence ), negotiation and conflict resolution techniques (restorative justice), facilitated negotiation and mediation, and adjudication (potentially several stages, including the possibility of review and appeal). (extracted from120.txt)
The idea behind this tiered model of dispute resolution is tha t most disputes are solved at the lower levels, thus being cost -effective while at the same time giving more disputants access to justice. (extracted from120.txt)
(17) Generally speaking, like in other areas of digitization, ODR may have a negative impact on data protection and privacy in that online justice is likely to generate a much greater wealth of data (including metadata, for example who accessed a particular court record when and from where), increases the possibilities of data processing, searching, data mining and the use of artificial intelligence (which is the other side of the coin of increased access to justice) and online data (including court data) may be more mobile (easy online transfer), sticky (in the sense that data remains on storage devices until erased) and vulnerable to unauthorised, remote access (computer hackin g from anywhere in the world). (extracted from120.txt)
If ODR is to be implemented into public justice systems, these systems must be designed in such a way that there is equality of arms between the litigants. (extracted from120.txt)
(25) Given the pressure of high caseloads and insufficient resources from which most justice systems suffer, there i s a danger that support systems based on artificial intelligence are inappropriately used by judges to “delegate” decisions to technological systems that were not developed for that purpose and are perceived as being more ‘objective' even when this is not the case. (extracted from120.txt)
CDCJ(2018) 5 74 THE COUNCIL OF EUROPE ACTIVITY The activity is a study with the following aims: - to analyse the compatibility of Online Dispute Resolution with the right to a fair tr ial both in terms of the challenges to the right of a fair trial as well as opportunities afforded by Online Dispute Resolution to provide greater access to justice and enhanced due process; - to examine whether online dispute resolution could open up new avenues of redress for infringements of ECHR rights (Article 13). (extracted from120.txt)
It will improve access to justice and accordingly fair trial. (extracted from120.txt)
Do you think that ODR processes and procedures could work within the civil justice system? (extracted from120.txt)
CDCJ(2018) 5 88 Pablo Cortes , Prof in Civil Justice at Leicester Law School. (extracted from120.txt)
Pablo is a chair in Civil Justice at Leicester Law School. (extracted from120.txt)
He is a fellow of the National Centre for Technology and Dispute Resolutions (University of Massachusetts) and a member of the Online Dispute Resolution (ODR) Taskforce of the International Mediation Ins titute and of the ODR Advisory Group of the Civil Justice Council. (extracted from120.txt)
Definitely, it will e nhance access to justice in case the mechanism has been designed properly. (extracted from120.txt)
Do you think that ODR processes and procedures could work within the civil justice system? (extracted from120.txt)
In general, it ’s not a problem, it’s an access to justice. (extracted from120.txt)
In e -justice Czech Republic is in a low position. (extracted from120.txt)
We have to give access to justice online to people in villages. (extracted from120.txt)
Do you think that ODR processes and procedures could work within the civil justice CDCJ(2018) 5 92 system? (extracted from120.txt)
There was a project in Soviet Union aimed to create machine justice – computer that was supposed to resolve disputes. (extracted from120.txt)
In 2001 the Minister of Justice had a plan – Phoenix system – online dispute resolution. (extracted from120.txt)
But this week, the Minister of Justice announced the new system – JustOn. (extracted from120.txt)
Unique system for the consumer and civil justice system. (extracted from120.txt)
Do you think that ODR processes and procedur es could work within the civil justice system? (extracted from120.txt)
Do you see risks that certain individuals (with physical or mental disabilities/sight problems/ socially disadvantaged/the elderly) will find it more difficult to obtain access to justice? (extracted from120.txt)
For instance, the Minister of Justice presented the plan to introduce AI court for dispute in regard to fines for illegal parking. (extracted from120.txt)
Looking at civil litigation and the administrative courts (not the criminal courts and criminal justice system) Two aspects: 1) use of ODR techniques in the COURTS and 2) possible interface between the courts and Alternative Dispute Resolution (ADR)/ Online Dispute Resolution (ODR). (extracted from120.txt)
I have been personally involved and have led multiple ODR initiatives in BC, including:  Development of Consumer Protection BC ODR consumer pilot in 2011 with Modria  Development of Property Assessment Appeal Board (an administrative tribunal) ODR initiative beginning in 2011 with Modria  Development of the Civil Resolution Tribunal (CRT) (an online administrative tribunal with civil jurisdiction) in 2011  Instructed the drafters on legislation to make ODR part of the body of law in BC and to create the authority for the CDCJ(2018) 5 98 CRT in 2011 -2012  Participated in the design for the technology and processes for the CRT beginning in 2012 (ongoing)  Drafted the CRT rules of procedure 2015 -2016  Led the knowledge engineering work for the CRT Solution Explorer ODR expert system (similar to phase 1 of the proposed online courts for Eng & W)  Former CDN delegate to th e United Nations Commission on International Trade Law ODR working group  Instructed Legal Information Technology law school courses at the University of Victoria and Osgoode Hall (York U) law schools, including a multi -week ODR simulation involving student s from Canada, the US and England  Currently co -instructing knowledge engineering courses at Thompson Rivers University and University of Ottawa Law Schools  Participated in one of the Civil Justice Council ODR Working Group meetings in London, provided on a nd off consultation with individual members, HMCS representatives and judiciary. (extracted from120.txt)
 Consulted with Lord Justice Briggs leading up to his reports on ODR  Part of a team developing new ODR initiatives for BC that touch on other areas for administrative tribunal s, family justice other administration of justice issues More: http://darinthompson.ca/about/ II. (extracted from120.txt)
The CRT may be the only public system based ODR tribunal of its kind in the world - Ontario is planning to create a new “Condo Authority” that is apparently going to include ODR - There is also a low volume small claims O DR pilot led by the Vancouver -based Justice Education Society - The Vancouver -based Legal Services Society currently offers “MyLawBC” which is based on the Rechtwijzer and Modria platforms. (extracted from120.txt)
Are you aware of any ODR processes and procedures which have been integrated into the civil and administrative justice procedure? (extracted from120.txt)
- The Property Assessment Appeal Board and the CRT, each mentioned above, are both deeply integrated into the civil/admin justice context. (extracted from120.txt)
- I continue to be amazed at the inaction among lead ing justice stakeholders when it comes to conceptualizing and piloting ODR initiatives, let alone implementing them. (extracted from120.txt)
I have taken several from the concept phase to the implementation phase, covering the tech, legislation, rules, workflows, user -focused design work, evaluation and more, and am firmly convinced that ODR is an effective response to many of the challenges facing justice systems everywhere. (extracted from120.txt)
- I believe the “digital divide” is narrower than the gap in access to justice for most people. (extracted from120.txt)
Do you see risks that certain individuals (with physical or mental disabilities/sight problems/ socially disadvantaged/the elderly) will find it more difficult to obtain access to justice? (extracted from120.txt)
What impact will ODR and automation have on access to justice? (extracted from120.txt)
While ODR is hard to define, it should be used more within the justice system. (extracted from120.txt)
The current system is burdensome, and we should do more to ensure that there is access to justice and fairne ss. (extracted from120.txt)
Does the person who has access to technology and the skills to use it ha ve better access to justice than another person who does not have access to such technology? (extracted from120.txt)
Information might not be a good steward for justice. (extracted from120.txt)
We could find out 10 years down the line, after an ODR system has been implemented, that companies who develop ed the platforms and software have used the system to gain an advantage through their control over information, and which has negative implications for the CDCJ(2018) 5 105 justice system. (extracted from120.txt)
Justice based kiosks. (extracted from120.txt)
The introduction of technology into the justice system will not happen within the next 10 years, particularly in the USA. (extracted from120.txt)
There is a culture in the US that disputes should be resolved in person – this is the understanding of justice. (extracted from120.txt)
Face -to-face in is imbedded in the justice culture. (extracted from120.txt)
Arbitration provides a good opportunity to create own justice system. (extracted from120.txt)
In justice system? (extracted from120.txt)
Otherwise issues may arise regarding access to justice. (extracted from120.txt)
Technology must make it easier but not to hinder access to justice. (extracted from120.txt)
Benefit to society – speedy justice . (extracted from120.txt)
ODR and change to the current justice system Impression on civil law jurisdiction embracing change to justice system by implementing ODR? (extracted from120.txt)
There is a need to come with support from all stakeholders – judges, attorneys, politicians – have their say – is it good for justice? (extracted from120.txt)
Not against access to justice – adhere to this, not obliged to arbitrate can mediate. (extracted from120.txt)
He used aspects of Game Theory and notions of fairness and justice to develop these concepts about ODR. (extracted from120.txt)
Access to Justice ODR can definitely enhance access to justice. (extracted from120.txt)
Technology can provide people with access to justice who would not ordinarily or traditionally have had it. (extracted from120.txt)
Summary John’s view is that ODR is a very useful tool to enhancing access to justice, ensuring efficiency and effectiveness in justice system. (extracted from120.txt)
It can be used, if implemented correctly and ethically, to enhance access to justice and t o ensure justice is carried out effectively and efficiently. (extracted from120.txt)
It is a fundamental aspect of justice that parties meet face -to-face when assessing each other’s cases, assessing ev idence and arguing their respective cases before court. (extracted from120.txt)
Shannon was initially involved with law from an administrative law perspective but was attracted to the potential use of ODR to address access to justice issues. (extracted from120.txt)
Bring the justice to where the system is. (extracted from120.txt)
There is no need to replicate the traditional court system which is not conducive to fairness and access to justice. (extracted from120.txt)
Access to justice – easier online. (extracted from120.txt)
Such committees may play an especially important role in areas where transparency and stakeholder engagement can be more difficult due to competing interests and rights , such as in the field s of predictive justice, crime prevention and detection . (extracted from134.txt)
Such disparities are often sterilized by well-intentioned names (e.g., “disproportionate contact” in criminal justice or the “achievement gap” in education) that hide the social consequence of structural racism: that, as a group, Black, Indigenous, and people of color in America have worse outcomes in many human service system outcome measures regardless of socioeconomic status.6 And yet, many agency solutions and data initiatives are largely disconnected from this root cause, and the “hunt for more data is [often] a barrier for acting on what we already know.”7 3 Racial Equity Tools (n.d.), Core Concepts: Racism 4 We intentionally use the acronym BIPoC (Black, Indigenous, people of color) as a term that seeks to recognize the unique experience of Black and Indigenous People within the United States. (extracted from35.txt)
The BDC consists of Broward County Public Schools, Broward County Human Services Department, Broward Behavioral Health Coalition, Florida Departments of Children and Families and Juvenile Justice, Early Learning Coalition of Broward, and the Children’s Services Council of Broward County (which acts as the BDC backbone organization). (extracted from35.txt)
Working with the Kirwan Institute of Race and Social Justice at The Ohio State University, the City of Tacoma created an Equity Index to measure social mobility in the city. (extracted from35.txt)
So when racial justice doesn’t have a critique of patriarchy and homophobia, the particular way that racism is experienced and exacerbated by heterosexism, classism, etc., falls outside of our political organizing. (extracted from35.txt)
It means that significant numbers of people in our communities aren’t being served by social justice frames because they don’t address the particular ways that they’re experiencing discrimination.” — Kimberlé Williams Crenshaw13 13 Quoted in Guobadia, O. (extracted from35.txt)
Members of the group were all previously involved in efforts to promote algorithmic justice and had been accumulating knowledge on NYC-specific ADSs since fall of 2018. (extracted from35.txt)
Provide a plan to the group that defines harm, and outline processes for repair and restoring justice. (extracted from35.txt)
Other useful resources for understanding accountability and repairing harm include: Tools for Addressing Chapter Conflict from Black Lives Matter37 Challenging Neutrality, Examining Privilege and Encouraging Practitioner Self-Reflexivity: A Social Justice Approach to ADR38 Celebrate community and the effort you are putting toward growth and change. (extracted from35.txt)
(n.d.) 40 Future of Privacy Forum & AISP (2018) 41 King County Office of Equity and Social Justice (2015) 42 Gorski, P. (extracted from35.txt)
Community Reconciliation Through Facilitated Dialogue & Restorative Justice . (extracted from35.txt)
How schools are using restorative justice to remedy racial disparities in discipline . (extracted from35.txt)
The Little Book of Race and Restorative Justice: Black Lives, Healing, and US Social Transformation . (extracted from35.txt)
The numbers don’t speak for themselves: Racial Disparities and the persistence of inequality in the criminal justice system . (extracted from35.txt)
https://dl.acm.org/doi/abs/10.1145/3351095.3372874 King County Office of Equity and Social Justice. (extracted from35.txt)
No Small Matters: Reimagining the Use of Research Evidence From A Racial Justice Perspective . (extracted from35.txt)
How Philanthropy Can Help Lead on Data Justice . (extracted from35.txt)
Challenging Neutrality, Examining Privilege, and Encouraging Practitioner Self-Reflexivity: A Social Justice Approach to ADR. (extracted from35.txt)
Just mercy: A story of justice and redemption . (extracted from35.txt)
Racial Equity in Planning: WORK IN ACTION Broward Data Collaborative by Sue Gallagher Who: Broward Data Collaborative, Children’s Services Council of Broward County Where: Broward County, Florida Organization Type: Government Agency, Community-Based Organization Domains: Child welfare, behavioral health, juvenile justice, early learning, school, human services, prevention programs. (extracted from35.txt)
The BDC seeks to improve the outcomes of residents in Broward County by integrating child-serving data from child welfare, behavioral health, juvenile justice, schools, early learning systems, county human services, and prevention programs. (extracted from35.txt)
One of the BDC’s desired outcomes is for the people whose data are in the IDS (e.g., youth aging out of the child welfare system, youth in the juvenile justice system) to be integrally involved in the use, interpretation, and evaluation of their data. (extracted from35.txt)
Recognizing that every child-serving system (e.g., child welfare, juvenile justice, education) produces racially disparate outcomes, the CPAR work is supported by training partners on the history and structures of racism (both nationally and locally) to build a common language and framework for system participants and system professionals. (extracted from35.txt)
It centers on themes of equity, opportunity, partnerships, and accountability, and specifically reflects community members’ desire for racial fairness and social justice across all public programs. (extracted from35.txt)
Allegheny County Department of Human Services, Office of Analytics, Technology and Planning by AISP with contributions from Samantha Loaney, Brian Bell, & Jamaal Davis Location: Allegheny County, Pennsylvania Organization Types Represented: Government Agency Domains Represented: Child welfare, family support, housing/income support, behavioral health, criminal justice, aging and intellectual disabilities Goal: To provide clients and providers access to the robust data held by the county’s Department of Human Services, allowing clients to understand what data is collected and providers to give more holistic care. (extracted from35.txt)
53 The client portal will contain data from local school districts, family support programs, housing, income support programs, child welfare agencies, drug and alcohol treatment, the criminal justice system, mental health providers, and more. (extracted from35.txt)
Racial Equity in Algorithms / Use of Statistical Tools: WORK IN ACTION Allegheny County Child Welfare Algorithm by AISP with contributions from Katy Collins Location: Allegheny County, Pennsylvania Organization Types Represented: Government Agency Domains Represented: Criminal justice system, behavioral health, public assistance, child welfare, education Goal: To use predictive analytics to allow caseworkers to engage in data-driven decision making that creates a more equitable, efficient, and successful child welfare system. (extracted from35.txt)
Members of the group were all previously involved in efforts to promote algorithmic justice and had been accumulating knowledge on NYC-specific ADS since fall of 2018. (extracted from35.txt)
In an effort to ensure that an analysis of these indicators felt useful to both participating agencies and to community members that are impacted by these systems, the Educational Services Division of Youth & Family Justice formed a community engagement committee. (extracted from35.txt)
One of the co-chairs of the committee, 62 APPENDIX I: WORK IN ACTION THROUGHOUT THE DATA LIFE CYCLESarah Zeller-Berkman, Director of Youth Studies Programs at the City University of New York (CUNY), who is a critical participatory action researcher, proposed that sustained engagement with a group of young people who had been in ACS care (preventive or juvenile justice) or young people who had fallen behind in middle school could generate data from other young people about the lived experience of being enmeshed in these systems. (extracted from35.txt)
Most youth team members had either fallen behind in school or had been involved with ACS through foster care or juvenile justice, while others were simply committed to making positive change using research. (extracted from35.txt)
See http://racialequitytools.org/glossary#structural-racism Social Justice: The proactive reinforcement of policies, practices, attitudes, and actions that produce equitable power, access, opportunities, treatment, impacts, and outcomes for all. (extracted from35.txt)
How Philanthropy Can Help Lead on Data Justice , Stanford Social Innovation Review . (extracted from35.txt)
No Small Matters: Reimagining the Use of Research Evidence From A Racial Justice Perspective . (extracted from35.txt)
http://wtgrantfoundation.org/digest/no-smallmatters-reimagining-the-use-of-research-evidence-from-a-racial-justice-perspective Actionable Intelligence for Social Policy University of Pennsylvania 3701 Locust Walk, Philadelphia, PA 19104 215.573.5827 www.aisp.upenn.edu (extracted from35.txt)
2020 AI for social good Receiver-contextualized intervention with respect to autonomyNon-maleficence Justice & situational fairnessReceiver-contextualized explanation and transparent purposesPrivacy protection and data subject consent Grote et al. (extracted from691.txt)
Such tasks concern low -skilled as well as high ly-skilled personnel, for example in sectors such as banking, insurance or justice. (extracted from295.txt)
For the deployment of ADS , certification can be on either a voluntary basis (as encouraged by the GDPR) , or mandatory in certain areas such as justice and healthcare. (extracted from295.txt)
Decision -making algorithms are increasingly used in areas such as a ccess to information, e-commerce, recommendation systems, employment, health, justice, policing, banking and insurance. (extracted from295.txt)
Users Objectives Individuals Private sector Public sector Improvement of general Knowledge N/A Drugs discovery Climate Weather forecast Environment Healthcare Digital services Quantified -self Finance Note taking Smart home Recommendations Risk scoring Payment systems Targeting Personali sed services Predictive justice Predictive policing Hazard prediction Infrastructure development planning Physical systems Autonomous Cars Home Robots Security Personal assistants in the home Autonomous robots Autonomous weapons Defence Transport Smart cities Smart grids Understanding algorithmic decision -making: Opportunities and challenges 7 3. (extracted from295.txt)
' The fact that ADS can lead to discrimination has been documented in many areas , such as the justice system, targeted advertisements and employment. (extracted from295.txt)
Discrimination in justice : Another area that has raised much concern is the increasing reliance on ADS in the criminal justice system. (extracted from295.txt)
COMPAS scores can be used at different stages of the criminal justice system , e.g. (extracted from295.txt)
Several occurrences of this process have already been observed, not only in the field of justice with COMPAS, but also in education with the public debate raised by an algorithm called APB32 in Fr ance. (extracted from295.txt)
Understanding algorithmic decision -making: Opportunities and challenges 15 authors, 'perhaps even more problematic is the theory of justice impl icitly embedded in the algorithms' .48 The point is that most ADS used in this context are risk -assessment tools: based on a number of factors about the defendants ' criminal history, sociological data or demographic features, they provide an estimation of their risk of recidivism. (extracted from295.txt)
ADS are already in use in the medical sector and can potentially contribute to i mprove the decisions taken by practitioners and specialists in many ways: 48 Angèle Christin, Alex Rosenblat, Danah Boyd; Courts and predictive algorithms; Data & Civil Rights: A new era of poli cing and justice; 2015. (extracted from295.txt)
Public services ADS are currently being used by government and public agencies to provide new services or improve existing ones in many areas , such as energy, education, healthcare, transportation, justice systems and security . (extracted from295.txt)
These tasks concern low -skilled as well as high lyskilled personnel, for example in sectors like banking, insurance or justice. (extracted from295.txt)
ADS Fairness As ADS replace or support human decision -makers in a number of sensitive domains such as justice, health or education, it is important to ensure that they do not result in decisions that are considered unfair or discriminatory. (extracted from295.txt)
For example, it is well known that, in certain cities, there is a strong correlation between the reli gion or 141 Richard Berk, Hoda Heidari, Shahin Jabbari, Michael Kearns, Aaron Roth; Fairness in Criminal Justice Risk Assessments: The State of the Art; Sociological Methods & Research; 2018. (extracted from295.txt)
186 Richard Berk, Hoda Heidari, Shahin Jabbari, Michael Kearns, Aaron Roth; Fairness in Criminal Justice Risk Assessments: The State of the Art; Sociological Methods & Research; 2018. (extracted from295.txt)
187 Richard Berk, Hoda Heidari, Shahin Jabbari, Michael Kearns, Aaron Roth; Fairness in Criminal Justice Risk Assessments: The State of the Art; Sociological Methods & Research; 2018. (extracted from295.txt)
Ethical and political debate As illustrated in Chapter 3, ADS raise far reaching issues in many areas such as justice, policing, healthcare, democratic life, etc. (extracted from295.txt)
Another example of the systematic analysis of ethical issues that can be useful in this context is the EDPS Ethics Advisory Group Report,204 which proposes a list of ' foundational values to digital ethi cs': dignity, freedom, autonomy, solidarity, equality, democracy, justice and trust. (extracted from295.txt)
As seen previously, different approaches can be taken to 212 Rebecca Wexler; Life, liberty, and trade secrets : intellec tual pro perty in the criminal justice system; (70); Standford Law Review; (1343); 2018; https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2920883 . (extracted from295.txt)
[…] The trend towards using risk instruments in all sectors of the criminal justice system, therefore, merits further theoretical d eliberation and empirical study. (extracted from295.txt)
'221 • In the same vein, Chelsea Barabas and her colleagues argue: 'for a shift away from predictive technologies, towards diagnostic methods that will help us to understand the criminogenic effects of the criminal justice sys tem itself, as well as evaluate the effectiveness of interventions designed to interrupt cycles of crime. (extracted from295.txt)
221 Kelly Hannah -Moffat; Actuarial sentencing: an ' unsettled ' proposition; Justice Quarterly; (30,2); 2013. (extracted from295.txt)
Dillon Reisman and his colleagues have already advocated AIA as a ' practical framework for public agency accountability' in a recent AINow Institute report .226 Beyond a 'self-assessment of existing and proposed automated d ecision systems, evaluating potential impacts on fairness, justice, bias, or other concerns across affected communities ', they emphasi se the need for ' researcher review processes before the system has been acquired '. (extracted from295.txt)
For example, several studies have been conducted about the use of ADS in the area of justice, some of th em focusing on the risks of discrimination ,232 others on the benefits in improv ing judges' decisions.233 In addition, the benefit risk balance applies to both the primary functionalities of the ADS and to its transparency and explainability features. (extracted from295.txt)
ADS certification can be either on a voluntary basis (as encouraged by the GDPR) or mandatory in certain areas such as justice and healthcare. (extracted from295.txt)
Berk R., Heidari H., Jabbari S., Kearns M., Roth A.; Fairness in Criminal Justice Risk Assessments: The State of the Art; Sociological Methods & Research; 2018. (extracted from295.txt)
Christin A., Rosenblat A., Boyd D.; Courts and predictive algorithms; Workshop on Data & Civil Rights: A new era of policing and justice; 2015 . (extracted from295.txt)
Hannah -Moffat K.; Actuarial sentencing: an 'unsettled ' proposition; Justice Quarterly (30); 2013. (extracted from295.txt)
STUDY Panel for the Future of Science and Technology EPRS | European Parliamentary Research Service Scientific Foresight Un it (STOA) PE 72 9.533 – July 2022 EN Governing data and artificial intelligence for all Models for sustainable and just data governance Governing data and artificial intelligence for all Models for sustainable and just data governance With a particular focus on artificial intelligence (AI), t his study identif ies and examines policy options for the EU' s data governance framework that align with a data justice perspective. (extracted from281.txt)
A data justice approach is one that centres on equity, recognition and representation of plural interests, and the creation and preservation of public goods as its principal goals. (extracted from281.txt)
Four benchmarks for good data governance are proposed, in line with the principles of justice: preserving and strengthening public infrastructure and public goods, inclusiveness, contestability and accountability, and global responsibility. (extracted from281.txt)
STOA | Panel for the Future of Science and Technology II AUTHOR S This study was written by Joan Lopez Solano, Aaron Martin, Siddharth de Souza and Linnet Taylor of the Global Data Justice project , Tilburg University , at the request of the Panel for the Future of Science and Technology (STOA) and managed by the Scientific Foresight Unit , within the Directorate -General for Parliamentary Research Services (EPRS) of the Secretariat of the European Parliament. (extracted from281.txt)
The Global Data Justice project would like to acknowledge valuable contributions to the analysis in this report from: Maria Anagnostu, Shweta Degalahal, Paula Ferreira Vidal, Yash Kaushal, Andrew Key, Janne Joosten, Alexis Manus, Franklyn Ohai , Gargi Sharma and Zsuzsanna Véghné Ujj. (extracted from281.txt)
533 ISBN: 978- 92-846- 9623- 9 doi: 10.2861/915401 QA-05-22-170- EN-N http://www.europarl.europa.eu/stoa (STOA website) http://www.eprs.ep.parl.union.eu (intranet) http://www.europarl.europa.eu/thinktank (internet) http://epthinktank.eu (blog) Governing data and artificial intelligence for all III Executive s ummary With particular regard to artificial intelligence (AI), t his study aims to identify and examine policy options for Europe' s data governance framework that align with a data justice perspective. (extracted from281.txt)
A data justice approach is one that centres on equity, the recognition and representation of plural interests, and the creation and preservation of public goods as its principal goals. (extracted from281.txt)
A data justice perspective is a particularly appropriate tool for this analysis , because AI is not a bottom -up class of technology , in terms of either development or use. (extracted from281.txt)
Policy options and alternatives: a data justice analysis ______________________________ 58 6.1. (extracted from281.txt)
An important component of that representation can be framed as 'data justice ' - the view that data governance should not only seek to do no harm, but should positively contribute to people 's autonomy and to their ability to particip ate in society and make claims about their needs, on a more general level. (extracted from281.txt)
1 Linnet Taylor, 'What Is Data Justice? (extracted from281.txt)
Technology, https://www.theguardian.com/technology/2017/dec/20/uber -european -court -of-justice -ruling -barcelona-taxi -drivers ecj-eu. (extracted from281.txt)
Lessons from Cybersecurity Vulnerability Disclosure for Algorithmic Harms Discovery, Disclosure, and Redress ' (Algorithmic Justice League, January 2022), https://www.ajl.org/bugs . (extracted from281.txt)
One such proposal from India is the creation of an Interoperable Criminal Justice Database. (extracted from281.txt)
59 High Court of Tripura, ' Interoperable Criminal Justice System,' accessed April 26, 2022, https://thc.nic.in/user%20manual/ICJS -manual.pdf . (extracted from281.txt)
62 It was found for instance that the creation of registers of repeat offenders had an underlying caste bias, and rather than challenging the existence of such registers, they would now be part of a centralised data base where their use would be further cemented.The Indian Express, ' The Dangers of a Centralised Database for Justice System,' May 28, 2021, https://indianexpress.com/article/opinion/columns/the -dangers -of-a-centralised -database -for-justice -system -7333252/ . (extracted from281.txt)
For instance in the Dutch government 's misuse of data to predict fraud among 63 Linnet Taylor, ' What Is Data Justice? (extracted from281.txt)
These values relating to the economic growth and wel lbeing of EU Member States potentially stand in tension with the rights - and justice -based orientation of the other core value statements to do with digital strategy, for instance in the statement that the innovation principle is legislatively as important as the precautionary principle, which underlies much thinking about digital rights and data protection. (extracted from281.txt)
One way to arbitrate between these differing visions is to address them through a justice lens, which asserts that there are tests we can apply to statements about good governance and related models, to see whether they align with the core as sumptions that make governance functional for people, and place it at the service of the public rather than in the interests of the most powerful and privileged. (extracted from281.txt)
In 2017, the Global Data Justice project (the authors of this report, based in the Netherlands ) asserted three fundamental pillars of 'good ' data governance.159 These pillars offer a tool for understanding how public values can be incorporated in governance frameworks. (extracted from281.txt)
If it has the effect of channelling power and profit toward the best- resourced and most powerful actors, whether governmental or corporate, a governance model is not in line with principles of social justice and requires reorienting. (extracted from281.txt)
This justice -based reasoning leads to a set of core benchmarks for good governance of data: 159 Linnet Taylor, ' What Is Data Justice? (extracted from281.txt)
In terms of the first of these two, these infrastructure- related goods include the social safety net, scientific knowledge,163 public education and healthcare, access to justice, electoral processes and law enforcement. (extracted from281.txt)
' Speech January 8th 2021 https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=&cad=rja&uact=8&ved=2ahUKEwiavsCj5_D4AhV LaQKHQzEA64QFnoECAUQAQ&url=https%3A%2F%2Fwww.uva.nl%2Fbinaries%2Fcontent%2Fassets%2Fuva%2Fnl%2Fo ver-de-uva%2Fspeech -karen -maex ---dies -2021.pdf&usg=AOvVaw2V5_UZK8444_8hFUU2XU9s 164 See the work of the Global Data Justice project on pandemic -related commercialisation of public goods: https://globaldatajustice.org/sphere -trans/ 165 For further explanation, see for example Timo Meynhardt, ' Public Value Inside: What Is Public Value Creation?, ' International Journal of Public Administration 32, no. (extracted from281.txt)
Data regulation tends to cite both human rights and market efficiency, the former of which aligns well with a social justice -oriented governance model. (extracted from281.txt)
In contrast, a social justice perspective starts from the assumption that all societies are unequal playing fields where people' s circumstances give them different degrees of agency and ability to claim their rights. (extracted from281.txt)
A social justice -oriented data governance model suggests additions to our way of conceptualising vulnerability in relation to technology. (extracted from281.txt)
Beyond the notion that people are more easily harmed if they have certain minority or sensitive attributes, a social justice view would add another facet to these considerations: that of unequally distributed power and agency. (extracted from281.txt)
A data justice perspective demands that we broaden our conceptualisation of contestability beyond claims based on competition and consumer protection regulation. (extracted from281.txt)
Taking a justice perspective, our analysis explores the extent to which these interests productively interact, where governance becomes skewed toward economic priorities at the expense of people, and how the plural interests of the EU 's diverse populations can be recognised and represented through the process of governing technologies. (extracted from281.txt)
' Abnormal justice. (extracted from281.txt)
In our discussion so far on thinking about data governance from a social justice standpoint, there are few considerations that have emerged that require further discussion. (extracted from281.txt)
Policy options and alternatives: a data justice analysis In this section we will analyse policy options first in relation to the European Strategy for Data, and then in relation to the principal legislative files currently under development. (extracted from281.txt)
233 Sohel Sarkar and Amay Korjan, eds., A Digital New Deal: Visions of Justice in a Post -Covid World (IT for Change and Just Net Coalition, 2021). (extracted from281.txt)
Starting from a definition of governance as arbitration between different interests with regard to publ ic and private goods, we have offered a justice- based analysis of the legislative context with regard to artificial intelligence and contributing data technologies and argue that along with considerations of its economic benefits, a strategy as to how Euro pean AI generates public value could be central to the EU 's policy aims. (extracted from281.txt)
We then conducted an analysis of the core set of legislative files relating to AI, and explored how they could be align ed with these justice -based benchmarks for good governance. (extracted from281.txt)
With a particular focus on artificial intelligence ( AI), th is study identifies and examines policy options for the EU' s data governance framework that align with a data justice perspective. (extracted from281.txt)
A data justice approach is one that centres on equity, recognition and representation of plural interests, and the creation and preservation of public goods as its principal goals. (extracted from281.txt)
We propose four benchmarks for good data governance according to principles of justice: pre serving and strengthening public infrastructures and public goods, inclusiveness, contestability and accountability, and global responsibility. (extracted from281.txt)
Mladenovic, Milos N./McPherson, Tristram : Engineering social justice into traffic control for self-driving vehicles? (extracted from532.txt)
In the liberal tradition – a key influence on both pub lic debate and the administration of justice in Ger many and the European Union – privacy is generally regarded as a condition for enabling autonomy and as its expression, in the sense of being able to think and act independently. (extracted from37.txt)
Collective goods, rather like the terms “common good” and “justice”, cannot be defined in terms of their sub stance. (extracted from37.txt)
Government of Canada Strategic Plan for Information Management and Information Technology 2017 to 2021 - Canada.ca Skip to main content Skip to "About government" Language selection Français fr / Gouvernement du Canada Search Search Canada.ca Search Menu Main Menu Jobs and the workplace Immigration and citizenship Travel and tourism Business and industry Benefits Health Taxes Environment and natural resources National security and defence Culture, history and sport Policing, justice and emergencies Transport and infrastructure Canada and the world Money and finances Science and innovation Manage life events You are here: Canada.ca About government Government oversight Information, technology and project management Government of Canada Strategic Plan for Information Management and Information Technology 2017 to 2021 From Treasury Board of Canada Secretariat On this page About this document / what’s new Message from the Government of Canada Chief Information Officer Introduction Citizens’ expectations Workplace and workforce evolution Privacy and security The enterprise approach IM-IT sustainability and aging IT The vision Digital services Open and accessible Security Enterprise management Community Workplace Mission statement Guiding principles Principle 1: client and service-centred design Principle 2: open Principle 3: enterprise first Principle 4: secure Principle 5: cloud first approach Principle 6: enable a modern workplace: anywhere, anytime with anyone Strategic goals Strategic goal 1: service Strategic goal 2: value Strategic goal 3: security Strategic goal 4: agility Strategic actions Service Service management Cloud first Technology modernization Information and data sharing Manage Governance Enterprise architecture alignment and practices Agility and innovation Sustainability Secure Defence in depth Trusted solutions and services Awareness and understanding Community IM-IT workforce Modern workplace Digital collaboration The way forward Implementing the Strategic Plan Risks and mitigation strategies Measuring progress Staying evergreen Appendix A: Government of Canada IM/IT modernization priorities Appendix B: implementation roadmap Appendix C: key performance indicators Appendix D: roles and responsibilities Appendix E: definitions Appendix F: draft digital principles About this document / what’s new This is the first update to the Government of Canada Information Technology Strategic Plan, published in June 2016. (extracted from644.txt)
Return to footnote 1 referrer © Her Majesty the Queen in Right of Canada, represented by the President of the Treasury Board, 2017,ISBN: 978-0-660-24007-7 Page details Date modified: 2020-07-27 About this site Treasury Board of Canada Secretariat (TBS) Contact us Forms Government of Canada All contacts Departments and agencies About government Themes and topics Jobs Immigration and citizenship Travel and tourism Business Benefits Health Taxes Environment and natural resources National security and defence Culture, history and sport Policing, justice and emergencies Transport and infrastructure Canada and the world Money and finances Science and innovation Indigenous Peoples Veterans and military Youth Manage life events Government of Canada Corporate Social media Mobile applications About Canada.ca Terms and conditions Privacy (extracted from644.txt)
Several witnesses highlighted the growing use of AI within the US justice system, in particular the Correctional Offender Management Profiling for Alternative Sanctions (COMPAS) system, developed by Northpointe, and used across several US states to assign risk ratings to defendants, which help to assist judges in sentences and parole decisions. (extracted from650.txt)
144 Evidence from Sheena Urwin, Head of Criminal Justice at Durham Constabulary, emphasised the considerable lengths that Durham Constabulary have taken to ensure their use of these tools is open, fair and ethical, in particular the development of their ‘ALGO-CARE’ framework for the ethical use of algorithms in policing. (extracted from650.txt)
* Libby Kinsey, Co-founder, Project Juno ** Dr Mercedes Bunz, Senior Lecturer, Communications and Media Research Institute, University of WestminsterQQ 55–64 ** Elizabeth Denham, UK Information Commissioner, Information Commissioner’s Office * Dr Sandra Wachter, Postdoctoral Researcher in Data Ethics and Algorithms, Oxford Internet Institute * Olivier Thereaux, Head of Technology, The Open Data Institute QQ 65–75 * Javier Ruiz Diaz, Policy Director, The Open Rights Group ** Frederike Kaltheuner, Policy Officer, Privacy International ** Dr James Luke, Chief Technology Officer for the Public Sector, IBM QQ 76–84 ** Kriti Sharma, Vice President of Artificial Intelligence and Bots, Sage * Andrew de Rozairo, Vice President, Customer Innovation and Enterprise Platform, SAP * Colin Griffiths, Policy Manager, Citizens Advice QQ 85–94 ** Will Hayter, Project Director, Competition and Markets Authority ** Olly Buston, CEO and Founder, Future Advocacy QQ 95–104 * Professor Dame Henrietta Moore, Director, Institute for Global Prosperity, UCL ** Professor Richard Susskind OBE, IT Adviser to the Lord Chief Justice of England and Wales * Dr Mark Taylor, Global Strategy & Research Director, Dyson QQ 105–115 ** Dr Joseph Reger, Chief Technology Officer EMEIA, Fujitsu ** Paul Clarke, Chief Technology Officer, Ocado * Dr Julian Huppert, Chair, Independent Review Panel for DeepMind Health QQ 116–127 ** Dr Sobia Raza, Head of Science, PHG Foundation * Nicola Perrin, Head, Understanding Patient Data, Wellcome Trust ** Dr Hugh Harvey, Clinical Artificial Intelligence Researcher and Consultant Radiologist, Guy’s and St Thomas’ NHS Foundation Trust QQ 128–142 * Dame Fiona Caldicott, National Data Guardian for Health and Care, Office of the National Data Guardian 143 AI IN THE UK: READY, WILLING AND ABLE? (extracted from650.txt)
There were three overarching principles to this: • partnership with, not replacement of, humans; • putting human values at the centre of their applications; and • a strong focus on a wide-ranging set of ethical considerations, including the preservation of human autonomy, beneficence, non-maleficence, and justice. (extracted from650.txt)
for use in the criminal justice system, so defendants and their lawyers can understand and challenge evidence used against them); and • Tools for investigators and auditors for use when things go wrong. (extracted from650.txt)
Department of Commerce Subcommittee on Networking and Information Technology Research and Development Co-Chair Bryan Biegel Director, National C oordination Office for Networking and Information Technology Research and Development Co-Chair James Kurose Assistant Director, Computer and Information Science and Engineering National Science Foundation Networking and Information Technology Research and Development Task Force on Artificial Intelligence Co-Chair Lynne Parker Division Director Information and Intelligent System s National Science Foundation Co-Chair Jason Matheny Director Intelligence Advanced Research Projects Activity Members Milton Corn National Institutes of Health Nikunj Oza National Aeronautics and Space Administration William Ford National Institute of Justice Robinson Pino Department of Energy Michael Garris National Institute of Standards and Technology Gregory Shannon Office of Science and Technology Policy Steven Knox National Security Agency Scott Tousley Department of Homeland Security viii John Launchbury Defense Advanced Research Projects Agency Faisal D’Souza Technical Coordinator National Coordination Office for Networking and Information Technology Research and Development Richard Linderman Office of the Secretary of Defense NATIONAL ARTIFICIAL INTELLIGENCE RESEARCH AND DEVELOPMENT STRATEGIC PLAN 1 Contents About the National Science and Technology Council ................................ (extracted from485.txt)
The Federal government should therefore emphasize AI investments in areas of strong societal importance that are not aimed at consumer markets —areas such as AI for public health, urban systems and smart communiti es, social welfare, criminal justice, environmental sustainability, and national security, as well as long -term research that accelerates the production of AI knowledge and technologies . (extracted from485.txt)
Scientists must also study to what extent justice and fairness considerations can be designed into the system, and how to accomplish this within the bounds of current engineering techniques . (extracted from485.txt)
NATIONAL ARTIFICIAL INTELLIGENCE RESEARCH AND DEVELOPMENT STRATEGIC PLAN 27 Building e thical AI Beyond fundamental assumptions of justice and fairness are other concerns about whether AI systems can exhibit behavior that a bides by general ethical principles. (extracted from485.txt)
Article 29 Data Protection Working Party, Opinion 1/2008 on data protection issues related to search engines, dated 4 April 2008, http://ec.europa.eu/justice/policies/privacy/workinggroup/wpdocs/index_en.htm , S. (extracted from309.txt)
And it was in this mindset that the CNIL set a collective approach in motion, for several months overseeing a public debate with the help of partners from various sectorial fields (health, justice...). (extracted from321.txt)
Among them, we could mention the “Ligue de l’Enseignement” (associa tion that focused on education concerns), French Insu rance Federation (FFA), French Ministry of Culture (DG-MIC), Open Law (association that reflects on the justice system) as well as trade unions such as CFE-CFC and FO Cadres (for recruitment and HR), etc.An innovative approach to crafting a collective and pluralist ethical thought process Ethical thinking concerns decisive societal choices. (extracted from321.txt)
The views of the diverse stakeholders (trade unions, associations, businesses, researchers, citizens, etc.) across a wide range of sectors (from insurance to education, justice and healthcare) thus informed the writing of this report, which provides an overview of the ethical matters raised by algorithms and artificial intel ligence in their current applications and their potential uses in the relatively short term. (extracted from321.txt)
THE ETHICAL MATTERS RAISED BY ALGORITHMS AND ARTIFICIAL INTELLIGENCE ALGORITHMS AND ARTIFICIAL INTELLIGENCE TODAY 22 Education Justice Health Security Work, HR Culture Other Generating knowledgeBetter identify learners’ abilitiesReveal the different ways judgments are handed down between regionsTap into the vast amount of scientific publicationsIdentify unsuspected links for solving gendarmerie-led investigations Understand social phenomena in the workplaceCreate cultural showpieces (painting, music)Fine-tune an insurance company customer’s risk profile MatchingAllocate higher education places to candidates (APB)Allocate patients for participation in a clinical trialMatch a list of applicants to a job vacancyMatch “compatible” profiles on dating apps, etc. (extracted from321.txt)
The next step of what some refer to as “predictive justice” would involve entrusting systems with the task of making decisions based on a cross-analysis of the data pertaining to a certain case, with case-law data.Delegating tasks to algorithms: contrasting situations What immediately becomes clear is that concern over the potential ethical and social implications of automated systems varies depending on the tasks being delegated to the latter and the very conditions shaping this delegation. (extracted from321.txt)
In the report it submitted to the CNIL, the Conseil National des Barreaux, the national institution that represents all practising lawyers in France, highlighted that “care must be taken to ensure that the obsession for effectiveness and predictability behind the use of algorithms does not lead to us designing legal rules and categories no longer on the grounds of our ideal of justice, but so that they are more readily ‘codable’”. (extracted from321.txt)
A question of scale: the massive delegation of non-critical decisions Should ethical thinking on algorithms and artificial intel-ligence be limited to crucial decisions, sectors where the impact on humans is undeniable, such as medicine, justice, educational guidance, and even the automotive sector with its implications in terms of safety? (extracted from321.txt)
Practical examples of algorithms being used by the authorities as well as the example of predictive justice give a clearer idea of this ambivalence, between the optimising and diminishing of processes stripped of their spatial dimension. (extracted from321.txt)
Similarly, at the symposium on predictive justice orga nised on 19 May 2017 by the Lille Bar, Law Department of Université catholique de Lille and the Douai Court of Appeal, certain participants stressed that “knowledge of judgments given by the other neighbouring jurisdictions or by the other magistrates would contribute towards a certain consistency and prevent that the outcome of a dispute depends on knowing whether it is heard in a city or another”. (extracted from321.txt)
The same line of thinking could be applied to the idea of a predictive justice. (extracted from321.txt)
Predictive justice applications are being subjected to particularly close public scrutiny as regards their Management Profile for Alternative Sanction) tool designed to come up with a recidivism risk score for prisoners or defendants on trial. (extracted from321.txt)
Medicine and justice are other sectors where this question might be asked. (extracted from321.txt)
IIt is crucial to guard against excessive trust by raising awar eness of the ethical dimensions of a decision-making process that must not exclude human intervention and by honing critical thinking in some particularly sensitive sec tors, such as medicine, recruitment, justice and perhaps now marketing above all, where the antisemitic categories recently generated by Facebook’s machine learning algorithms are a stark wakeup call to the sharpness of the risks. (extracted from321.txt)
The partners in the public debate • Allistene’s research committee on ethics (CERNA) • Bordeaux’s Cognitique Institute (ENSC) • Bordeaux University • Caisse des dépôts et consignations (CDC) • Club des Juristes (thinktank) • Collège des Bernardins • Complex Systems Institute of Paris Ile-de-France (ISC-PIF) • Confédération française de l’encadrement – Confédération générale des cadres (CFE-CGC, trade union) • Communication Publique • Conseil National des Barreaux (national institution that represents all practising lawyers in France/CNB) • Conseil Supérieur de l’Audiovisuel (independent authority to protect audiovisual communication freedom/CSA) • Conservatoire National des Arts et Métiers (leading higher education and research institution dedicated to adult continuing education/CNAM) • Douai court of appeal • ESCP Europe, IoT Chair • Etalab(body that works in France on data sharing in the public sector) • “Familles rurales” association • Federal University of Toulouse • French Association for Artificial intelligence (AFIA) • French Association for Employment Law (AFDT) • French Development Agency(AFD) • French governmental advisory council on bioethics issues (CCNE) • French Insurance Federation (FFA) • French National Center for Scientific Research (CNRS)’s ethics committee (COMETS) • FO-Cadres (trade union) • Fondation Internet Nouvelle Génération (FING) • Fotonower • Génotoul societal (bioscience and ethics platform) • Groupe VYV (MGEN – ISTYA – Harmonie) • Imagine Institute on genetic diseases • INNOvation Ouverte par Ordinateur (INNOOO)• Institut Mines-Télécom (IMT), Research Chair “Values and Politics of Personal Information” • Laboratory for Collective and Artificial Intelligence (LICA) • Law Department of Université Catholique de Lille, Centre of research on relations between risk and law • Law Department of Université Catholique de Lyon • Ligue des Droits de l’Homme (Human Rights League/LDH) • Ligue de l’Enseignement (Education League) • Lille 2 University • Lille Association of Lawyers • Lyon’s administrative court of appeal • Microsoft • Ministry of Culture, via the General Directorate of Media and Cultural Industries (DGMIC) • Ministry of National Education, via the Directorate of Digital Technology for Education (DNE) and its Numéri’lab • National Academy of Technologies of France • National Institute of Higher Studies on Defence (IHEDN) • National Institute of Higher Studies on Security and Justice (INHESJ) • National Institute of Applied Sciences (INSA) • Necker Hospital • OpenLaw (association) • Paris II University • Randstad • Research Centre of the National Gendarmerie School of Officers (CREOGN) • Rhône Département -level Council of the Medical Association • Renaissance Numérique (thinktank) • School of Advanced Studies in the Social Sciences (EHESS) • Sciences Po Lille • Sciences Po Paris • Société informatique de France (association devoted to computer science/SIF) • The Future Society at Harvard Kennedy School, AI Initiative • Universcience • Visions d’Europe (association) The other contributors • Arbre des connaissances (association) • Autorité de contrôle prudentiel et de résolution (French authority responsible for the supervision of the banking and insurance sectors/ACPR) • Autorité des marchés financiers (authority which regulates participants and products in France’s financial markets/AMF) • Montpellier Méditerranée Métropole and its President, Philippe Saurel • City of MontpellierThe 37 citizens who took part in the public consultation organised in Montpellier on 14 October 2017. (extracted from321.txt)
LAUNCH EVENT ROUNDTABLE SESSIONS “Algorithms and humans” and “Fairness, transparency and plurality of algorithms” > CNIL SYMPOSIUM “Towards new forms of humanity?” > Universcience CONFERENCE “Algorithms and law” > Lille II University CONFERENCE “The choice in the age of Big Data” > Sciences Po Lille and Visions d’Europe DEBATE “The governance of emerging technosciences” > German American Conference at Harvard University DEBATE “Transatlantic perspectives on: AI in the age of social media; privacy, security and the future of political campaigning” > The Future Society at Harvard Kennedy School ROUNDTABLE SESSIONS “Big Data, human resources: algorithms on the agenda” > FO-Cadres CONFERENCE “Fairness of algorithmic decision-making” > Toulouse III – Paul Sabatier University DEBATE “Will digital technology spell the end of the rule of law?” > Collège des Bernadins SYMPOSIUM “Predictive justice” > Douai Court of Appeal, Lille Association of Lawyers and Law Department of Université Catholique de Lille WORKSHOPS “Fairness of algorithmic decision-making and processing” > LabEx International Centre for Mathematics and Computer Science in Toulouse23/01/2017 23/03/2017 25/03/2017 31/03/2017 06/04/201708/04/201718/04/2017 18/04/2017 04/05/201716/05/201719/05/201702/06/2017 DEBATE “Algorithms in healthcare: what ethics?” > Groupe VYV (MGEN – ISTYA – Harmonie) ROUNDTABLE SESSION “Artificial intelligence: ethics, at the intersection of HR and Big Data” > Confédération française de l’encadrement – Confédération générale des cadres (CFE-CGC) DEBATE “Algorithms, employment and ethics” > French Association for Employment Law (AFDT) DAY “Ethical algorithms, a moral requirement and competitive advantage” > Allistene’s CERNA and Société Informatique de France (SIF) SYMPOSIUM “Human, non-human in the age of artificial intelligence” > Paris II University SYMPOSIUM “Artificial intelligence: autonomy, delegation and accountability” > Bordeaux’s Cognitique Institute (ENSC) WORKSHOP “Ethics of algorithms: implications for healthcare” > Genotoul (bioscience and ethics platform) CROWDSOURCING WORKSHOP “Artificial intelligence and law” > OpenLaw SYMPOSIUM “ The many dimensions of data ” > Institut Mines-Télécom, Values and Politics of Personal Information Research Chair SYMPOSIUM “Security and justice, the challenge of the algorithm” > National Institute of Higher Studies of Security and Justice (INHESJ) MOCK TRIAL AND ROUNDTABLE SESSION “ Ethique, algorithmes and justice ” > Law Department of Université Catholique de Lyon and Lyon’s Administrative Court of Appeal STUDY DAY “Admission Post-bac, textbook case of public algorithms” > Fondation Internet Nouvelle Génération (FING) and Etalab DAY “Algorithms and digital sovereignty” > Allistene’s CERNA DAY “Ethics and artificial intelligence” > French National Center for Scientific Research (CNRS)’s ethics committee (COMETS) and French Association for AI (AFIA) DEBATES on algorithms in education. (extracted from321.txt)
AI in the Nordic-Baltic region | Nordic cooperation Skip to main content Service menu Press Career Contact EN language dropdown DAFIISSVNO Main navigation Nordic Council of Ministers Nordic Council Mega menu EN language dropdown DAFIISSVNO Nordic Council of Ministers News from the Nordic Council of Ministers Our Vision 2030 Politiske prioriteter 2025-2030 The Presidency of the Nordic Council of Ministers Presidency programme for the Nordic Council of Ministers 2025 Organisation The Secretary General Secretariat Governing documents The Nordic Councils of Ministers Ministers for Co-operation Labour Environment and Climate Sustainable Growth Digitalisation Fisheries, Aquaculture, Agriculture, Food and Forestry Justice Culture Gender Equality and LGBTI Social and Health Affairs Education and Research Economic and Fiscal Policy Meetings and protocols Institutions Nordic Culture Point Nordic House in Reykjavik Nordic House in the Faroe Islands The Nordic Institute in Greenland Nordic Institute on Åland Nordic Welfare Centre NordForsk NordGen Nordic Innovation Nordic Energy Research Nordregio NIVA Education About the Nordic Council of Ministers History of the Nordic Council of Ministers Budget Nordic Council News from the Nordic Council Parliamentary co-operation Nordic Council cases Nordic Council’s international co-operation The Presidency of the Nordic Council Presidency programme for the Nordic Council 2025 Organisation Secretary General Secretariat Governing documents Nordic Council committees and members All members The Presidium Committee for Knowledge and Culture in the Nordic Region Committee for a Sustainable Nordic Region Committee for Growth and Development in the Nordic Region Committee for Welfare in the Nordic Region Control Committee Election Committee Sessions and meetings Protocols and upcoming meetings Session 2025 Theme Session 2025 2024 Session Past sessions About the Nordic Council The history of the Nordic Council Annual report News All news Events Current initiatives 2025 Session of the Nordic Council Arctic Circle 2025 EXPO 2025 Press Nordic image bank Career Knowledge and services Living, working and studying in the Nordic Region Publications Release a publication Facts and statistics Facts about the Nordic Region Nordic statistics Funding opportunities About funding from the Nordic Council of Ministers Public procurements Border database About the Nordic co-operation The history of Nordic co-operation Nordic Council prizes Environment Prize Children and Young People's Literature Prize Literature Prize Film Prize Music Prize Policy areas Environment and climate Culture Legislation and Justice Sustainable development Digitalisation and innovation Children and young people See all policy areas Service menu Press Career Contact Search Search Menu Breadcrumb Home AI in the Nordic-Baltic region 14.05.18 | Declaration Artificial intelligence (AI) can help solve major societal challenges and provide significant benefits in a variety of areas. (extracted from490.txt)
Department of Commerce The following Federal departments and agencies are represented on the Subcommittee on Machine Learning and Artificial Intelligence and thro ugh it, work together to monitor the state of the art in machine learning (ML) and AI (within the Federal Government, in the private sector, and internationally), to watch for the arrival of important technology milestones in the development of AI, to coor dinate the use of and foster the sharing of knowledge and best practices about ML and AI by the Federal Government, and to consult in the development of Federal research and development priorities in AI: Department of Commerce (Co -Chair ) Depa rtment of Defense Department of Education Department of Energy Department of Health and Human Services Department of Homeland Security Department of Justice Department of Labor Department of State Department of Transportation Department of Treasury Department of Veterans Affairs United States Agency for International Development Central Intelligence Agency General Services Administration National Science Foundation National Security Agency National Aeronautics and Space Administration Office of the Director of National Intelligence Social Security Administration The following offices of the Executive Office of the President are also represented on the Subcommittee: Council of Economic Advisers Domestic Policy Council Office of Management and Budget Office of Science and Technology Policy (Co Chair) Office of the Vice President National Economic Council National Security Council PREPARING FOR THE FUTURE OF ARTIFICIAL INTELLIGENCE PREPARING FOR THE FUTURE OF ARTIFICIAL INTELLIGENCE Contents Executive Summary ................................ (extracted from484.txt)
30 Justice, Fairness, and Accountability ................................ (extracted from484.txt)
Public - and private sector investments in basic and applied R&D on AI have already begun reaping major benefits to the public in fields as diverse as health care, transportation, the environment, criminal justice, and economic inclusion . (extracted from484.txt)
Use of AI to make consequential decisions about people, often replacing decisions made by human -driven bureaucra tic processes , leads to concerns about how to ensure justice, fairness, and accountability —the same concerns voiced previously in the Administration’s Big Data: Seizing Opportunities, Preserving Values report of 2014,1 as well as the Report to the President on Big Data and Privacy: A Technological Perspective published by the President’s Council of Advisors on Science and Technology in 2014 .2 Transparency concerns focus not only on the data and algorithms involved, but also on the potential to have some form of explanat ion for any AI -based determination. (extracted from484.txt)
Many areas of public policy, from education and the economic safety net, to defense, envi ronmental preservation, and criminal justice, will see new opportunities and new challenges driven by the continued progress of AI. (extracted from484.txt)
The promise of AI has been compared to the transformative impacts of advances in mobile computing.21 Public - and private -sector investments in basic and applied R&D on AI have already begun reaping major benefits for the public in fields as diverse as health care, transportation, the environment, criminal justice, and economic inclusion .22 At Walter Reed Medical Center, the Department of Veteran Affairs is using AI to better predict medical complications and improve treatment of severe combat wounds, leading to better patient outcomes, faster healing, and lower costs.23 The same general approac h—predicting complications to enable preventive treatment —has also reduced hospital -acquired infections at Johns Hopkins University .24 Given the current transition to electronic health records, predictive analysis of health data may play a key role across m any health domains like precision medicine and cancer research. (extracted from484.txt)
Autonomous watercraft may be much cheaper to operate than manned ships, and may some day be used for enhanced weather prediction, climate monitoring, or policing illegal fishing.27 AI also has the potential to improve aspects of the criminal justice system, including crime reporting, policing, bail, sentencing, and parole decisions. (extracted from484.txt)
The Admini stration is exploring how AI can responsibly benefit current initiatives such as Data Driven Justice and the Police Data Initiative that seek to provide law enforcement and the public with data that can better inform decision -making in the criminal justice system, while also taking care to minimize the possibility that AI might introduce bias or inaccuracies due to deficiencies in the available data. (extracted from484.txt)
Social justice and public policy institutions that do not typically engage with advanced technologies and data science in their work should consider partnerships with AI researchers and practitioners that can help apply AI tactics to the broad social problems these institutions already address in other ways. (extracted from484.txt)
The u se of AI to make consequential decisions about people, often replacing decisions made by human actors and institutions , leads to concerns about how to ensure justice, fairness, and accountability —the same concerns voice d previously in the “ Big Data” context.62 The u se of AI to control physical -world equipment leads to concerns about safety, especially as systems are exposed to the full complexity of the human environment. (extracted from484.txt)
Justice, Fairness, and Accountability A common theme in the Law and Governance, AI for Social Good, and Social and Economic Impac ts workshops was the need to ensure that AI promotes justice and fairness, and that AI -based processes are accountab le to stakeholders . (extracted from484.txt)
In the criminal justice system, some of the biggest concerns with Big Data are the lack of data and the lack of quality data.65 AI needs good data. (extracted from484.txt)
It is important that anyone using AI in the criminal justice context is aware of the limitations of current data. (extracted from484.txt)
65 Matt Ford, “The Missing Statistics of Criminal Justice,” The Atlantic, May 31, 2015, http://www.theatlantic.com/politics/archive/2015/05/what -we-dont-know -about -mass -incarceration/394520/ 66 Julia Angwin, Jeff Larson, Surya Mattu, and Lauren Kirchner, “Machine Bias,” ProPublica, May 23, 2016, https://www.propublica.org/article/machine -bias-risk-assessments -in-criminal -sentencing. (extracted from484.txt)
Many areas of public policy, from education and the economic safety net, to defense , environmental prese rvation, and criminal justice, will see new opportunities and new challenges driven by the continued progress of AI. (extracted from484.txt)
Social justice and public policy institutions that do not typically engage with advanced t echnologies and data science in their work should consider partnerships with AI researchers and practitioners that can help apply AI tactics to the broad social problems these institutions already address in other ways. (extracted from484.txt)
Matt Ford, “The Missing Statistics of Criminal Justice,” The Atlantic, May 31, 2015, http://www.theatlantic.com/politics/archive/2015/05/what -we-dont-know-about -mass -incarceration/394520/ Jason Furman, “Is This Time Different? (extracted from484.txt)
In a digitally connected world, the question of how to respect, protect and implement human rights and access to environmental justice is becoming paramount. (extracted from679.txt)
Human rights and access to environmental justice need to be safeguarded in the development, implementation, legislation, 23and governance of digital technologies. (extracted from679.txt)
Accept additional cookies Reject additional cookies View cookies Hide cookie message Skip to main content GOV.UK Navigation menu Menu Menu Services and information Benefits Births, death, marriages and care Business and self-employed Childcare and parenting Citizenship and living in the UK Crime, justice and the law Disabled people Driving and transport Education and learning Employing people Environment and countryside Housing and local services Money and tax Passports, travel and living abroad Visas and immigration Working, jobs and pensions Government activity Departments Departments, agencies and public bodies News News stories, speeches, letters and notices Guidance and regulation Detailed guidance, regulations and rules Research and statistics Reports, analysis and official statistics Policy papers and consultations Consultations and strategy Transparency Data, Freedom of Information releases and corporate reports Search GOV.UK × Search GOV.UK Search Search GOV.UK Search Home Defence and armed forces Ambitious, safe, responsible: our approach to the delivery of AI-enabled capability in Defence Ministryof Defence Policy paper Ambitious, safe, responsible: our approach to the delivery of AI-enabled capability in Defence Published 15 June 2022 This was published under the 2019 to 2022 Johnson Conservative government Contents Executive Summary Ambitious delivery of capability Our approach and AI-enabled weapons Key challenges to Defence AI Adoption Using AI Safely Using AI Legally Using AI Ethically Partnerships and Consultation Governance Implementation – building justified trust Annex A: Ethical Principles for AI in Defence Annex B: The Ministry of Defence AI Ethics Advisory Panel ANNEX C: Lethal Autonomous Weapon Systems (LAWS) Print this page © Crown copyright 2022 This publication is licensed under the terms of the Open Government Licence v3.0 except where otherwise stated. (extracted from651.txt)
Cancel Services and information Benefits Births, death, marriages and care Business and self-employed Childcare and parenting Citizenship and living in the UK Crime, justice and the law Disabled people Driving and transport Education and learning Employing people Environment and countryside Housing and local services Money and tax Passports, travel and living abroad Visas and immigration Working, jobs and pensions Government activity Departments News Guidance and regulation Research and statistics Policy papers and consultations Transparency How government works Get involved Support links Help Privacy Cookies Accessibility statement Contact Terms and conditions Rhestr o Wasanaethau Cymraeg Government Digital Service All content is available under the Open Government Licence v3.0, except where otherwise stated © Crown copyright (extracted from651.txt)
Finally, AI raises many different sector -specific issues concerning the various AI fields of application (labour, justice administration, crime control, contract relationships , etc.) and the consequences of AI use (e.g. (extracted from137.txt)
Where societal issues are significant , legal, ethical or sociological expertise , as well as domain specific knowledge , will be essential .46 Such committees may play an even more important role in areas where transparency and stakeholders’ engagement are difficult to achieve , such as predictive justice, crime detection or predictive policing . (extracted from137.txt)
predictive policing, justice, precision medicine, marketing , political propaganda) . (extracted from137.txt)
“It’s Reducing a Human Being to a Percentage”; Perceptions of Justice in Algorithmic Decisions. (extracted from137.txt)
8 December 2020 CEPEJ(2020) 15Rev EUROPEAN COMMISSION FOR THE EFFIC IENCY OF JUSTICE (CEPEJ) Possible introduction of a mechanism for certif ying artificial intelligence tools and services in the sphere of justice and the judiciary : Feasibility S tudy In December 2018, the European Commission for the Efficiency of Justice (CEPEJ) adopt ed the Ethical Charter on the use of artificial intelligence in judicial systems and their environment . (extracted from123.txt)
31 3 Feasibility st udy on the possible introduction of a mechanism for certif ying artificial intelligence tools and services Introduction Justice is not currently the preferred focus of companies that are innova ting in the field of artificial intelligence . (extracted from123.txt)
The in -depth s tudy on the use of AI in judicial systems, notably AI applications processing judicial decisions and data , reprodu ced i n Appendix I to the CEPEJ Charte r6, defines the main categories of artificial intelligence in the sphere of justice for illustrative purposes as follows : - Advanced case -law search engines - Online dispute resolution , - Assistance in drafting deeds - Analysis (predictive, scales ), - Categorisation of contracts according to different criteria and detection of divergent or incompatible contractual clauses, - "Chatbots " to inform litigants or support them in their legal proceedings . (extracted from123.txt)
Two additional categories with strong ethics implications could also be discussed, namely algorithmic justice, which could be seen as comparable to the aforementioned category of "online dispute resolution", and tools for enhanced judges' decision -making , which could be linked to the "Analys is" category, still at an experimental stage chiefly geared to aid for determining damages and sanctions. (extracted from123.txt)
Besides these considerations l inked to the categories of artificial intelligence and its state of technological advancement in terms of contextual learning, we should distinguish between AI applications in the justice sphere according to their functions and purposes . (extracted from123.txt)
Specifically in the judicial field where artificial intelligence tools provide aid for decision -making for judges, if, for example, the machine propose s a detention measure rather than a security measure, the defendant and society are entitled to demand an explanation for that choice if it guide s an official decision of justice. (extracted from123.txt)
The reasoning of decisions of justice demands a certain level of explainability. (extracted from123.txt)
The justice sphere has not been included in the areas listed for experimentation33. (extracted from123.txt)
The sole reference to artificial intelligence in the justice sphere in this document is to be seen in a survey finding that only half of the respondents are comfortable with 26 Article 6, Regulation (EC) No 1980/2000 of the European Parliament and of the Council of 17 July 2000 on a revised Community eco -label award scheme. (extracted from123.txt)
9 AI in the area of justice, compared to 80% in the area of transport ation34, which appears to have been the reason why it was exclu ded from the pilot experiments . (extracted from123.txt)
- Proportion ate processing of person al data (priv acy) and clear purpose s - Anonymisation of the parties and particip ants (physi cal individual s) and their counsels o Checking by consultation of data sets - Absence of evalu ation and class ification of physi cal individuals or legal entities on the basis of judicial decisions o Checking by consultation of the interface - Anonymisation of the name of the judge and the location of the court in decisions u sed for predictive justice ( with th e aim of avoiding forum shopping ) o Checking by consultation of data sets - Hermetic separation of artificial intelligence services having different purpos es (such as dissoci ating the search engine service from the aid for decision -making service) o Checking of databases and data sources used by each system - Right of access to the judge and the right to a fair trial - Presence of clear information indicating, where applicable, th at a report generated by an artificial intelligence system is not explainable . (extracted from123.txt)
If successfully applied to artificial intelligence in the sphere of justice, the human rights by design approach could be applied more widely to artificial intelligence systems used in other fields and could also potentially be tes ted in legislative processes, after enactment of legislation, to limit the need for the review of compatibility of domestic legislation with international conventions and treaties before courts of law. (extracted from123.txt)
The use of artificial intelligence in the legal and judicial sphere represents an important societal challenge, particularly in the field of algorithmic justice. (extracted from123.txt)
While technological advances may bring improvements to the judicial system, facilitate the work of legal professionals and improve access to justice and information for defendant s/litigants , increased vigilance is necessary in this sector which the European Commission descri bes as a high -risk sector. (extracted from123.txt)
28 Appendixes Summary table of indicators and certification criteria Objective Criteria Assessment method Target AI category Proportionate processing of personal data Anonymisation of the parties and participants (physical individuals) and their counsels Consultation of data sets Unprocessed data All Absence of evaluation and classification of physical individuals or legal entities on the basis of judicial decisions Checking of interface Interface Connectionist Checking of database Limit forum shopping Anonymisation of the judge and the court ’s location in decisions used for predictive justice Consultation of data sets Unprocessed data Connectionist Clear purposes for processing Hermetic separation of artificial intelligence services Checking of databases and data sources used by each system Databases All Fair trial Information indicating to the judge and the defendant, if relevant that a report generated by an artificial intelligence system is not explainable (See also below : Defendant ’s/litigant's right to opt out of the use of artificial intelligence ) Checking of AI category and checking of existence and clarity of information (See also below : Checking of a notification system for the defendant ’s/litigant's decision and for effective redirection to conventional proceedings before a court within the meaning of A rticle 6 of the ECHR ) Learning model and interface Connectionist Judges’ independence in their decision making process Safeguard against the profiling of judges A/B testing checking of search results Search engine and processed data All Match between the criterion displayed and the actual pattern of classification of search results Auditing of search results Search engine All Transparency of weighting of criteria for multicriteria searches Checking of the existence of explanatory information and auditing of search results Interface and search engine All Verification by auditing of search results Checking of the existence of Interface and search engine All 29 Summary table of indicators and certification criteria Transparency of criteria used for searches by “relevance” explanatory information and auditing of search results Verification by auditing of search results Ethics and Human rights by design No human rights violation Report presenting decision trees and explaining how fundamental rights and freedoms are taken into account Report Symbolic No human rights violation Report presenting training data and methods and explaining how fundamental rights and freedoms are taken into account Report Connectionist Avoiding discrimination based on sensitive data Elimination of the tags that could be linked to parties ’ sensitive data (home address, income, family situation, registered capital) Checking by consultation of data sets Unprocessed data Not under public authority control A/B testing using information and tags that could be linked to sensitive data by changing, where applicable, one of the following parameters during each test: name, home address, income, family situation, registered capital, relevant specific contextual information, etc. (extracted from123.txt)
Responsible use of artificial intelligence in government - Canada.ca Skip to main content Skip to "About government" Language selection Français fr / Gouvernement du Canada Search Search Canada.ca Search Menu Main Menu Jobs and the workplace Immigration and citizenship Travel and tourism Business and industry Benefits Health Taxes Environment and natural resources National security and defence Culture, history and sport Policing, justice and emergencies Transport and infrastructure Canada and the world Money and finances Science and innovation Manage life events You are here: Canada.ca About government Government in a digital age Digital government innovation Responsible use of artificial intelligence in government Artificial intelligence (AI) technologies offer promise for improving how the Government of Canada provides digital services. (extracted from645.txt)
Features (Article) Driving forward: navigating the AI landscape in the Government of Canada LinkedIn article by Stephen Burt, Chief Data Officer, Government of Canada (Video) AI procurement for a digital world Overview of the AI procurement process (Video) Algorithmic Impact Assessment Introduction to the Algorithmic Impact Assessment From: Treasury Board of Canada Secretariat Page details Date modified: 2025-03-04 About this site Government of Canada All contacts Departments and agencies About government Themes and topics Jobs Immigration and citizenship Travel and tourism Business Benefits Health Taxes Environment and natural resources National security and defence Culture, history and sport Policing, justice and emergencies Transport and infrastructure Canada and the world Money and finances Science and innovation Indigenous Peoples Veterans and military Youth Manage life events Government of Canada Corporate Social media Mobile applications About Canada.ca Terms and conditions Privacy (extracted from645.txt)
This can most clearly be shown by comparing the sets of principles with the set of four core principles commonly used in bioethics: beneficence, non-maleficence, autonomy, and justice. (extracted from22.txt)
4.4 Justice: Promoting Prosperity and Preserving Solidarity The last of the four classic bioethics principles is justice, which is typically invoked in relation to the distribution of resources, such as new and experimental treatment options or simply the general availability of conventional healthcare. (extracted from22.txt)
The importance of “justice” is explicitly cited in the Montreal Declaration, which argues that “the development of AI should promote justice and seek to eliminate all 699 1 3AI4People—An Ethical Framework for a Good AI Society:… types of discrimination”, while the Asilomar Principles include the need for both “shared benefit” and “shared prosperity” from AI. (extracted from22.txt)
Under its principle named “Jus-tice, equity and solidarity”, the EGE argues that AI should “contribute to global justice and equal access to the benefits” of AI technologies. (extracted from22.txt)
As with the other principles already discussed, these interpretations of what justice means as an ethical principle in the context of AI are broadly similar, yet con-tain subtle distinctions. (extracted from22.txt)
Across the documents, justice variously relates to (a) Using AI to correct past wrongs such as eliminating unfair discrimination; (b) Ensuring that the use of AI creates benefits that are shared (or at least shareable); and (c) Preventing the creation of new harms, such as the undermining of existing social structures. (extracted from22.txt)
Notable also are the different ways in which the position of AI, vis-à-vis people, is characterised in relation to justice. (extracted from22.txt)
In Asilomar and EGE respectively, it is AI technologies themselves that “should benefit and empower as many people as pos-sible” and “contribute to global justice”, whereas in Montreal, it is “the develop-ment of AI” that “should promote justice” (italics added). (extracted from22.txt)
Develop appropriate legal procedures and improve the IT infrastructure of the justice system to permit the scrutiny of algorithmic decisions in court. (extracted from22.txt)
European Parliament 2019-2024 TEXTS ADOPTED P9_TA(2021)0009 Artificial intelligence: questions of interpretation and application of international law European Parliament resolution of 20 January 2021 on artificial intelligence: questions of interpretation and application of international law in so far as the EU is affected in the areas of civil and military uses and of state authority outside the scope of criminal justice (2020/2013(INI)) The European Parliament, – having regard to the preamble to the Treaty on European Union, and to Articles 2, 3, 10, 19, 20, 21, 114,167, 218, 225 and 227 thereof, – having regard to the right to petition enshrined in Articles 20 and 227 of the Treaty on the Functioning of the European Union, – having regard to the Charter of Fundamental Rights of the European Union, – having regard to Council Directive 2000/43/EC of 29 June 2000 implementing the principle of equal treatment between persons irrespective of racial or ethnic origin1 (Racial Equality Directive), – having regard to Council Directive 2000/78/EC of 27 November 2000 establishing a general framework for equal treatment in employment and occupation2 (Equal Treatment in Employment Directive), – having regard to Regulation (EU) 2016/679 of the European Parliament and of the Council of 27 April 2016 on the protection of natural persons with regard to the processing of personal data and on the free movement of such data, and repealing Directive 95/46/EC (General Data Protection Regulation)3 (GDPR), and to Directive (EU) 2016/680 of the European Parliament and of the Council of 27 April 2016 on the protection of natural persons with regard to the processing of personal data by competent authorities for the purposes of the prevention, investigation, detection or prosecution of criminal offences or the execution of criminal penalties, and on the free 1 OJ L 180, 19.7.2000, p. (extracted from269.txt)
Languages, – having regard to the European ethical Charter on the use of Artificial Intelligence in judicial systems and their environment adopted by the Council of Europe Working Group on quality of justice (CEPEJ-GT-QUAL) in December 2018, – having regard to the OECD Council Recommendation on Artificial Intelligence adopted on 22 May 2019, – having regard to Rule 54 of its Rules of Procedure, – having regard to the opinions of the Committee on Foreign Affairs, the Committee on the Internal Market and Consumer Protection, the Committee on Transport and Tourism and the Committee on Civil Liberties, Justice and Home Affairs, – having regard to the report of the Committee on Legal Affairs (A9-0001/2021), Introduction A. (extracted from269.txt)
whereas this particular responsibility implies a need to examine questions of interpretation and application of international law related to the active participation of the EU in international negotiations, in so far as the EU is affected by the civil and military uses of this kind of AI, robotics and related technologies, and questions of state authority over such technologies lie outside the scope of criminal justice; I. (extracted from269.txt)
Recalls that according to the Advisory Opinion of the International Court of Justice of 8 July 1996, the principle of originality cannot be cited in support of any derogation regarding compliance with current norms of international humanitarian law; 13. (extracted from269.txt)
Insists on the importance of investing in human skills, including digital skills, in order to adapt to scientific progress involving AI-driven solutions, for individuals exercising regulated professions, including activities connected with the exercise of state authority, such as the administration of justice; calls on the Member States and the Commission to duly take this into account as part of the implementation of Directive 2005/36/EC1; 25. (extracted from269.txt)
Believes that an effective mechanism for enforcing the rules on non-proliferation of LAWS and any future offensive AI-enabled technologies is of paramount importance for global security; State authority: examples from civil areas, including health and justice 51. (extracted from269.txt)
Urges the Member States to assess the risks related to AI-driven technologies before automating activities connected with the exercise of state authority, such as the administration of justice; calls on the Member States to consider the need to provide for safeguards such as supervision by a qualified professional and strict rules on professional ethics; 54. (extracted from269.txt)
Notes that AI is increasingly being used in the field of justice in order to take decisions which are more rational, more in keeping with the law in force, and quicker; welcomes the fact that the use of AI is expected to speed up judicial proceedings; 68. (extracted from269.txt)
Stresses that the use of AI in justice could improve the analysis and collection of data and the protection of victims, and that this could be explored in research and development and accompanied by impact assessments, in particular regarding safeguards for due process and against bias and discrimination, with the precautionary principle being applied; recalls, however, that this is no substitute for human involvement in sentencing or decision-making; 70. (extracted from269.txt)
Recalls the importance of the principles of governance, transparency, impartiality, accountability, fairness and intellectual integrity in the use of AI in criminal justice; 71. (extracted from269.txt)
Urges the Member States to assess the risks related to AI-driven technologies before automating activities connected with the exercise of state authority, especially in the area of justice; calls on them to consider the need to provide safeguards, such as supervision by a qualified professional and rules on professional ethics; 72. (extracted from269.txt)
Requests that the public is kept informed about the use of AI in the field of justice, and that such uses do not give rise to discrimination resulting from programming biases; stresses that the right of every individual to have access to a public official must be respected, as well as the right of the responsible official to personally take the decision and deviate from the information received from the AI when they deem it necessary in the light of the details of the matter in question; highlights the right of the defendant to appeal the decision in accordance with national legislation, without ever eliminating the final responsibility of the judiciary; 74. (extracted from269.txt)
Moreover, a few specific cases, mentioned as running at the time of first gathering, were later found to have been discontinued because of various reasons , including significant criticism received from the general public , pressure from adversarial political forces or even executive orders from local courts of criminal justice. (extracted from241.txt)
— Fairness and justice : The development of AI should promote fairness and justice, the rights a nd interests of stakeholders and promote equality of opportunity. (extracted from241.txt)
Previous research had analyse d 84 ethical AI documents published by various business es, NGOs and (international) governmental organizations, highlight ing that some principles such as tra nsparency, fairness, justice, and responsibility were quite common in all. (extracted from241.txt)
The impact of using algorithms for managerial decisions on public employees’ procedural justice. (extracted from241.txt)
It might also compound further the pro blems with international transfer of data after that the US Safe Harbour and later the EU -US Privacy Shield were invalidated by the Court of Justice (Schrems I and II case- law, C -362/14 and C -311/18), as illustrated by Kiner (2020). (extracted from282.txt)
The Schrems II judgment of the Court of Justice and the future of data transfer regulation. (extracted from282.txt)
European Law Blog: https://europeanlawblog.eu/2020/07/17/the -schrems -iijudgment -of-the-court -of-justice -and -the-future -of-data -transfer- regulation/ . (extracted from282.txt)
AI tightly developed under human oversight and control), embed the principles of fairness and justice in algorithms, applied in all phases of AI systems’ design, implementation and testing, while following through bias complaints and other undesired effects reporting. (extracted from296.txt)
During the 71st session of the United Nations General Assembly, on 29 September 2016, UNICRI (United Nations International Crime and Justice research Institute) announced the opening of the first Centre on A I and Robotics KEY FINDINGS There is not yet robust evidence of AI applications used for addressing significant and wideranging real -life problems or societal challenges. (extracted from296.txt)
AI tightly developed under human oversight and control), embed the principles of fairness and justice in algorithms, applied in all phases of AI systems’ design, i mplementation and testing, while following through bias complaints and other undesired effects reporting. (extracted from296.txt)
Texts adopted - Artificial intelligence in criminal law and its use by the police and judicial authorities in criminal matters - Wednesday, 6 October 2021 Choisissez la langue de votre document : bg - български es - español cs - čeština da - dansk de - Deutsch et - eesti keel el - ελληνικά en - English fr - français ga - Gaeilge hr - hrvatski it - italiano lv - latviešu valoda lt - lietuvių kalba hu - magyar mt - Malti nl - Nederlands pl - polski pt - português ro - română sk - slovenčina sl - slovenščina fi - suomi sv - svenska Index Previous Next Full text Procedure : 2020/2016(INI)Document stages in plenaryDocument selected : A9-0232/2021Texts tabled : A9-0232/2021 Debates : PV 04/10/2021 - 13 CRE 04/10/2021 - 13 Votes : PV 05/10/2021 - 9 PV 06/10/2021 - 2 Texts adopted : P9_TA(2021)0405 Texts adopted 168k 65k Wednesday, 6 October 2021 - Strasbourg Artificial intelligence in criminal law and its use by the police and judicial authorities in criminal matters P9_TA(2021)0405A9-0232/2021 European Parliament resolution of 6 October 2021 on artificial intelligence in criminal law and its use by the police and judicial authorities in criminal matters (2020/2016(INI)) The European Parliament, – having regard to the Treaty on European Union, in particular Articles 2 and 6 thereof, and to the Treaty on the Functioning of the European Union, in particular Article 16 thereof, – having regard to the Charter of Fundamental Rights of the European Union (the “Charter”), in particular Articles 6, 7, 8, 11, 12, 13, 20, 21, 24 and 47 thereof, – having regard to the Convention for the Protection of Human Rights and Fundamental Freedoms, – having regard to the Council of Europe Convention for the Protection of Individuals with regard to Automatic Processing of Personal Data (ETS 108), and its amending protocol (Convention 108+), – having regard to the European Ethical Charter on the use of artificial intelligence in judicial systems and their environment of the European Commission for the Efficiency of Justice (CEPEJ) of the Council of Europe, – having regard to the Commission communication of 8 April 2019 entitled ‘Building Trust in Human-Centric Artificial Intelligence’ (COM(2019)0168), – having regard to the Ethics Guidelines for Trustworthy AI published by the Commission’s High-Level Expert Group on Artificial Intelligence on 8 April 2019, – having regard to the Commission white paper of 19 February 2020 entitled ‘Artificial Intelligence – A European approach to excellence and trust’ (COM(2020)0065), – having regard to the Commission communication of 19 February 2020 entitled ‘A European strategy for data’ (COM(2020)0066), – having regard to Regulation (EU) 2016/679 of the European Parliament and of the Council of 27 April 2016 on the protection of natural persons with regard to the processing of personal data and on the free movement of such data, and repealing Directive 95/46/EC (General Data Protection Regulation)(1), – having regard to Directive (EU) 2016/680 of the European Parliament and of the Council of 27 April 2016 on the protection of natural persons with regard to the processing of personal data by competent authorities for the purposes of the prevention, investigation, detection or prosecution of criminal offences or the execution of criminal penalties, and on the free movement of such data, and repealing Council Framework Decision 2008/977/JHA(2), – having regard to Regulation (EU) 2018/1725 of the European Parliament and of the Council of 23 October 2018 on the protection of natural persons with regard to the processing of personal data by the Union institutions, bodies, offices and agencies and on the free movement of such data, and repealing Regulation (EC) No 45/2001 and Decision No 1247/2002/EC(3), – having regard to Directive 2002/58/EC of the European Parliament and of the Council of 12 July 2002 concerning the processing of personal data and the protection of privacy in the electronic communications sector (Directive on privacy and electronic communications)(4), – having regard to Regulation (EU) 2016/794 of the European Parliament and of the Council of 11 May 2016 on the European Union Agency for Law Enforcement Cooperation (Europol) and replacing and repealing Council Decisions 2009/371/JHA, 2009/934/JHA, 2009/935/JHA, 2009/936/JHA and 2009/968/JHA(5), – having regard to its resolution of 19 June 2020 on the anti-racism protests following the death of George Floyd(6), – having regard to its resolution of 14 March 2017 on fundamental rights implications of big data: privacy, data protection, non-discrimination, security and law-enforcement(7), – having regard to the hearing in the Committee on Civil Liberties, Justice and Home Affairs (LIBE) on 20 February 2020 on artificial intelligence in criminal law and its use by the police and judicial authorities in criminal matters, – having regard to the report of the LIBE mission to the United States in February 2020, – having regard to Rule 54 of its Rules of Procedure, – having regard to the opinions of the Committee on the Internal Market and Consumer Protection and the Committee on Legal Affairs, – having regard to the report of the Committee on Civil Liberties, Justice and Home Affairs (A9-0232/2021), A. (extracted from292.txt)
whereas the Union together with the Member States bears a critical responsibility for ensuring that decisions surrounding the life cycle and use of AI applications in the field of the judiciary and law enforcement are made in a transparent manner, fully safeguard fundamental rights, and in particular do not perpetuate discrimination, biases or prejudices where they exist; whereas the relevant policy choices should respect the principles of necessity and proportionality in order to guarantee constitutionality and a fair and humane justice system; H. (extracted from292.txt)
whereas the relationship between protecting fundamental rights and effective policing must always be an essential element in the discussions on whether and how AI should be used by the law enforcement sector, where decisions may have long-lasting consequences on the life and freedom of individuals; whereas this is particularly important as AI has the potential to be a permanent part of our criminal justice ecosystem providing investigative analysis and assistance; M. (extracted from292.txt)
gunshot detection algorithms), autonomous research and analysis of identified databases, forecasting (predictive policing and crime hotspot analytics), behaviour detection tools, advanced virtual autopsy tools to help determine cause of death, autonomous tools to identify financial fraud and terrorist financing, social media monitoring (scraping and data harvesting for mining connections), and automated surveillance systems incorporating different detection capabilities (such as heartbeat detection and thermal cameras); whereas the aforementioned applications, alongside other potential or future applications of AI technology in law enforcement, can have vastly varying degrees of reliability and accuracy and impact on the protection of fundamental rights and on the dynamics of criminal justice systems; whereas many of these tools are used in non-EU countries but would be illegal under the Union data protection aquis and case law; whereas the routine deployment of algorithms, even with a small false positive rate, can result in false alerts outnumbering correct alerts by far; N. (extracted from292.txt)
whereas, the deployment of AI in the field of law enforcement and the judiciary should not be seen as a mere technical feasibility, but rather a political decision concerning the design and the objectives of law enforcement and of criminal justice systems; whereas modern criminal law is based on the idea that authorities react to an offence after it has been committed, without assuming that all people are dangerous and need to be constantly monitored in order to prevent potential wrongdoing; whereas AI-based surveillance techniques deeply challenge this approach and render it urgent that legislators worldwide thoroughly assess the consequences of allowing the deployment of technologies that diminish the role of human beings in law enforcement and adjudication; 1. (extracted from292.txt)
Considers it essential, both for the effectiveness of the exercise of defence rights and for the transparency of national criminal justice systems, that a specific, clear and precise legal framework regulates the conditions, modalities and consequences of the use of AI tools in the field of law enforcement and the judiciary, as well as the rights of targeted persons, and effective and easily available complaint and redress procedures, including judicial redress; underlines the right of the parties to a criminal proceeding to have access to the data collection process and the related assessments made by or obtained through the use of AI applications; underlines the need for executing authorities involved in judicial cooperation, when deciding on a request for extradition (or surrender) to another Member State or non-EU country, to assess whether the use of AI tools in the requesting country might manifestly compromise the fundamental right to a fair trial; calls on the Commission to issue guidelines on how to conduct such an assessment in the context of judicial cooperation in criminal matters; insists that Member States, in accordance with applicable laws, should ensure that individuals are informed when they are subject to the use of AI applications by law enforcement authorities or the judiciary; 15. (extracted from292.txt)
Points out that if humans only rely on the data, profiles and recommendations generated by machines, they will not be able to conduct an independent assessment; highlights the potentially grave adverse consequences, specifically in the area of law enforcement and justice, when individuals overly trust in the seemingly objective and scientific nature of AI tools and fail to consider the possibility of their results being incorrect, incomplete, irrelevant or discriminatory; emphasises that over-reliance on the results provided by AI systems should be avoided, and stresses the need for authorities to build confidence and knowledge to question or override an algorithmic recommendation; considers it important to have realistic expectations on such technological solutions and not to promise perfect law enforcement solutions and detection of all offences committed; 16. (extracted from292.txt)
Stresses that only robust European AI governance with independent evaluation can enable the necessary operationalisation of fundamental rights principles; calls for periodic mandatory auditing of all AI systems used by law enforcement and the judiciary where there is the potential to significantly affect the lives of individuals, by an independent authority, to test and evaluate algorithmic systems, their context, purpose, accuracy, performance and scale, and, once they are in operation, in order to detect, investigate, diagnose and rectify any unwanted and adverse effects and to ensure the AI systems are performing as intended; calls therefore for a clear institutional framework for this purpose, including proper regulatory and supervisory oversight, to ensure full implementation and to guarantee a fully informed democratic debate on the necessity and proportionality of AI in the field of criminal justice; underlines that the results of these audits should be made available in public registers so that citizens know the AI systems being deployed and which measures are taken to remedy any violation of fundamental rights; 22. (extracted from292.txt)
Highlights further that adequate accountability, responsibility, and liability require significant specialised training with regard to the ethical provisions, potential dangers, limitations, and proper use of AI technology, especially for police and judiciary personnel; emphasises that suitable professional training and qualifications should ensure that decision-makers are trained about the potential for bias, as the data sets may be based on discriminatory and prejudiced data; supports the establishment of awareness-raising and educational initiatives to ensure that individuals working in law enforcement and the judiciary are aware of and understand the limitations, capabilities and risks that the use of AI systems entails, including the risk of automation bias; recalls that the inclusion in AI training data sets of instances of racism by police forces in fulfilling their duties will inevitably lead to racist bias in AI-generated findings, scores, and recommendations; reiterates its call on Member States, therefore, to promote anti-discrimination policies and to develop national action plans against racism in the field of policing and the justice system; 24. (extracted from292.txt)
At EU level, t he processing of biometric data has been actively encouraged and directly supported over the past years in the context of EU -level large -scale information technology ( IT) systems in the area of freedom, security and justice ( AFSJ ). (extracted from286.txt)
A review of this architecture and of the most relevant rules on biometrics and on automated decision -making in EU data protection la w, as well as of the most important case law in this area emanating from the Cour t of Justice of the EU (CJEU) and the European Court of Human Rights (ECtHR) , shows that ongoing technological developments are taking place amid – and possibly also somehow despite – existing rights and principles , which might thus possibly need to be reinforced, clarified, or at least fine -tuned. (extracted from286.txt)
The processing of biometric data has been actively supported at EU level23 in the context of EU -level large -scale IT systems in the Area of Freedom, Security and Justice (AFSJ). (extracted from286.txt)
I t also presents the most important case law in this area emanating from the Court of Justice of the EU (CJEU) and the European Court of Human Rights (ECtHR). (extracted from286.txt)
Legal aid shall be made available to those who lack sufficient resources in so far as such aid is necessary to ensure effective access to justice. (extracted from286.txt)
Fundamental rights case law Analysing the case law of the Court of Justice of the EU (CJEU) on biometrics, a series of points stand out. (extracted from286.txt)
As underlined in a report for the Committee on Equality and Non- Discrimination of the Council of Europe 's Parliamentary Assembly, certain flaws in a the criminal justice system can have ' far-reaching human rights consequences ' (Lacroix 2020 12). (extracted from286.txt)
Other areas mentioned as involving the qualification of an AI system as high risk are management and operation of critical infrastructure , educational and vocational training ; employment , workers management and acces s to self -employment; access to and enjoyment of essential private services and public services and benefits; law enforcement ; migration, asylum and border control management ; administration of justice and democratic processes . (extracted from286.txt)
' Under heading 8, on ' Administration of justice and democratic processes ', are mention ed: '(a) AI systems intended to assist a judicial authority in researching and interpreting facts and the law and in applying the law to a concrete set of facts. (extracted from286.txt)
European Union Agency for the Operatio nal Management of Large -Scale IT Systems in the Area of Freedom, Security and Justice (eu -LISA). (extracted from286.txt)
AI tightly developed under human oversight and control), embed the principles of fairness and justice in algorithms, applied in all phases of AI systems’ design, implementation and testing, while following through bias complaints and other undesired effects reporting. (extracted from279.txt)
During the 71st session of the United Nations General Assembly, on 29 September 2016, UNICRI (United Nations International Crime and Justice research Institute) announced the opening of the first Centre on A I and Robotics KEY FINDINGS There is not yet robust evidence of AI applications used for addressing significant and wideranging real -life problems or societal challenges. (extracted from279.txt)
AI tightly developed under human oversight and control), embed the principles of fairness and justice in algorithms, applied in all phases of AI systems’ design, i mplementation and testing, while following through bias complaints and other undesired effects reporting. (extracted from279.txt)
These i nclude freedom, dignity and autonomy, privacy and data protection, non -discrimination and equality, diversity, fairness, social justice, and internationally recognized labor rights. (extracted from523.txt)
Some interventions might aim to improve completion and contr ibute to social justice, fairness and non -discri mination, in line with the G20 P rinciples. (extracted from523.txt)
Criminal justice, public housing, welfare and health provision are examples of areas where “black box” systems should not be developed or used. (extracted from721.txt)
29 In “Fairness in Criminal Justice Risk Assessments: The State of the Art” Berk et al, 2017 provide a through review of the technical pathways towards promoting fairness in machine learning. (extracted from721.txt)
Berk et al, 2017, Fairness in Criminal Justice Risk Assessments: The State of the Art https://arxiv.org/ abs/1703.0920730 Ai Now Institute 2017 Report 13 How to Prevent Discriminatory Outcomes in Machine LearningBringing principles of non-discrimination to life: Human rights due diligence for machine learning Companies developing and using ML systems must integrate these principles of non-discrimination into their human rights due diligence – a process by which businesses take ongoing, proactive, and reactive steps to ensure that they uphold people’s dignity and do not cause or contribute to human rights abuses. (extracted from721.txt)
AI Now produces interdisciplinary research to help ensure that AI systems are accountable to the communities and contexts they are meant to serve, and that they are applied in ways that promote justice and equity. (extracted from26.txt)
Our workshop on Immigration, Data, and Automation in the Trump Era , co-hosted with the Brennan Center for Justice and the Center for Privacy and Technology at Georgetown Law, focused on the Trump Administration’s use of data harvesting, predictive analytics, and machine learning to target immigrant communities. (extracted from26.txt)
Domains like health, education, criminal justice, and welfare all have their own histories, regulatory frameworks, and hazards. (extracted from26.txt)
Facial recognition technology poses its own dangers, reinforcing skewed and potentially discriminatory practices, from criminal justice to education to employment, and presents risks to human rights and civil liberties in multiple countries. (extracted from26.txt)
Yet, without a framework that accounts for social and political contexts and histories, these mathematical formulas for fairness will almost inevitably miss key factors, and can serve to paper over deeper problems in ways that ultimately increase harm or ignore justice. (extracted from26.txt)
Broadening perspectives and expanding research into AI fairness and bias beyond the merely mathematical is critical to ensuring we are capable of addressing the core issues and moving the focus from parity to justice. (extracted from26.txt)
The following report develops these themes in detail, reﬂecting on the latest academic research, and outlines seven strategies for moving forward: 1.Expanding AI fairness research beyond a focus on mathematical parity and statistical fairness toward issues of justice 2.Studying and tracking the full stack of infrastructure needed to create AI, including accounting for material supply chains 3.Accounting for the many forms of labor required to create and maintain AI systems 4.Committing to deeper interdisciplinarity in AI 5.Analyzing race, gender, and power in AI 6.Developing new policy interventions and strategic litigation 7.Building coalitions between researchers, civil society, and organizers within the technology sector These approaches are designed to positively recast the AI ﬁeld and address the growing power imbalance that currently favors those who develop and proﬁt from AI systems at the expense of the populations most likely to be harmed. (extracted from26.txt)
1 There have been major movements from both inside and outside technology companies pushing for greater accountability and justice. (extracted from26.txt)
Facial recognition ampliﬁes civil rights concerns Concerns are intensifying that facial recognition increases racial discrimination and other biases in the criminal justice system. (extracted from26.txt)
53 In its response to the ACLU, Amazon acknowledged that “the Rekognition results can be signiﬁcantly skewed by using a facial database that is not appropriately representative.” 54 Given the deep and historical racial biases in the criminal justice system, most law enforcement databases are unlikely to be “appropriately representative.” 55 Despite these serious ﬂaws, ongoing pressure from civil rights groups, and protests from Amazon employees over the potential for misuse of these technologies, Amazon Web Services CEO Andrew Jassy recently told employees that “we feel really great and really strongly about the value that Amazon Rekognition is providing our customers of all sizes and all types of industries in law enforcement and out of law enforcement.” 56 Nor is Amazon alone in implementing facial recognition technologies in unaccountable ways. (extracted from26.txt)
17 1.2 The Risks of Automated Decision Systems in Government Over the past year, we have seen a substantial increase in the adoption of Automated Decision Systems (ADS) across government domains, including criminal justice, child welfare, education, and immigration. (extracted from26.txt)
For years, criminal justice advocates and researchers have pushed for the elimination of cash bail, which has been shown to disproportionately harm individuals based on race and socioeconomic status while at the same time failing to enhance public safety. (extracted from26.txt)
90 The shift from policies such as cash bail to automated systems and risk assessment scoring is still relatively new, and is proceeding even without substantial research examining the potential to amplify discrimination within the criminal justice system. (extracted from26.txt)
91 Similarly, when California’s legislation passed earlier this year, many of the criminal justice advocates who pushed for the end of cash bail, and supported an earlier version of the bill, opposed its ﬁnal version due to the risk assessment requirement. (extracted from26.txt)
The National Association for the Advancement of Colored People (NAACP) and the Lawyers’ Committee for Civil Rights and 21 Economic Justice opposed the plan because of the school district’s failure to appreciate that parents of color and lower-income parents often rely on jobs that lack work schedule ﬂexibility and may not be able to afford additional child care. (extracted from26.txt)
3.1 From Fairness to Justice Any debate about bias and fairness should approach issues of power and hierarchy, looking at who is in a position to produce and proﬁt from these systems, whose values are embedded in these systems, who sets their “objective functions,” and which contexts they are intended to work within. (extracted from26.txt)
For example, making a facial recognition system perform equally on people with light and dark skin may be a type of technical progress in terms of parity, but if that technology is disproportionately used on people of color and low-income communities, is it really “fair?” This is why deﬁnitions of fairness face a hard limit if they remain purely contained within the technical domain: in short, “parity is not justice.” 169 32 3.2 Infrastructural Thinking In order to better understand and track the complexities of AI systems, we need to look beyond the technology and the hype to account for the broader context of how AI is shaping and shaped by social and material forces. (extracted from26.txt)
192 Such initiatives are critical: as AI becomes more deeply embedded in areas like healthcare, criminal justice, hiring, housing, and educational systems, experts from these domains are essential if we are to ensure AI works as envisioned. (extracted from26.txt)
234 The last year revealed many of the hardest challenges for accountability and justice as AI systems moved deeper into the social world. (extracted from26.txt)
45 21.Natalie Ram, “Innovating Criminal Justice,” Northwestern University Law Review 112, no. (extracted from26.txt)
165.For a more general description of justice as fairness, see: John Rawls, Justice as Fairness: A Restatement , ed. (extracted from26.txt)
167.Ben Green, “‘Fair’ Risk Assessments: A Precarious Approach for Criminal Justice Reform” (5th Workshop on Fairness, Accountability, and Transparency in Machine Learning, Stockholm, 2018), https://scholar.harvard.edu/ﬁles/bgreen/ﬁles/18-fatml.pdf . (extracted from26.txt)
This can most clearly be shown by comparing the sets of principles with the set of four core principles commonly used in bioethics: beneficence, non-maleficence, autonomy, and justice. (extracted from32.txt)
4.4 Justice: Promoting Prosperity and Preserving Solidarity The last of the four classic bioethics principles is justice, which is typically invoked in relation to the distribution of resources, such as new and experimental treatment options or simply the general availability of conventional healthcare. (extracted from32.txt)
The importance of “justice” is explicitly cited in the Montreal Declaration, which argues that “the development of AI should promote justice and seek to eliminate all 699 1 3AI4People—An Ethical Framework for a Good AI Society:… types of discrimination”, while the Asilomar Principles include the need for both “shared benefit” and “shared prosperity” from AI. (extracted from32.txt)
Under its principle named “Jus-tice, equity and solidarity”, the EGE argues that AI should “contribute to global justice and equal access to the benefits” of AI technologies. (extracted from32.txt)
As with the other principles already discussed, these interpretations of what justice means as an ethical principle in the context of AI are broadly similar, yet con-tain subtle distinctions. (extracted from32.txt)
Across the documents, justice variously relates to (a) Using AI to correct past wrongs such as eliminating unfair discrimination; (b) Ensuring that the use of AI creates benefits that are shared (or at least shareable); and (c) Preventing the creation of new harms, such as the undermining of existing social structures. (extracted from32.txt)
Notable also are the different ways in which the position of AI, vis-à-vis people, is characterised in relation to justice. (extracted from32.txt)
In Asilomar and EGE respectively, it is AI technologies themselves that “should benefit and empower as many people as pos-sible” and “contribute to global justice”, whereas in Montreal, it is “the develop-ment of AI” that “should promote justice” (italics added). (extracted from32.txt)
Develop appropriate legal procedures and improve the IT infrastructure of the justice system to permit the scrutiny of algorithmic decisions in court. (extracted from32.txt)
Perceived fairness of web‐ based applicant screening procedures: Weighing the rules of justice and the role of individual differences. (extracted from641.txt)
Fairness and Justice The development of AI should promote fairness and justice, protect the rights and interests of all stakeholders, and promote equal opportunities. (extracted from480.txt)
(2019), ‘Dirty data, bad predictions: How civil rights violations impact police data, predictive policing systems, and justice’, NYU Law Review, Vol. (extracted from325.txt)
Beyond that, gender identity is mentioned only in Recital 9 of the Victims’ Rights Directive75 in the context of criminal law.76 According to the Court of Justice of the European Union, gender identity is only partly covered by the principle of equal treatment between men and women.77 Legal protection against discrimination based on religion is currently also limited under EU law.78 Nevertheless, one may argue that many comments referring to people who identify as lesbian, gay, bisexual, transgender and intersex (LGBTI), Jewish or Muslim fall under either the Racial Equality Directive or the Gender Goods and Services Directive, because discrimination based on sexual orientation, gender identity or religion predominantly affects a specific race or gender. (extracted from325.txt)
88 The use of algorithms may further increase the opacity of content moderation and further increase challenges linked to fairness and justice.89 Without proper safeguards, such tools can lead to censorship and biased enforcement of laws and platforms’ terms and conditions.90 A potential increase in discrimination is just one of the challenges when using algorithms to support speech detection for content moderation purposes. (extracted from325.txt)
Article 29 Working Party (2017a), Opinion on some key issues of the Law Enforcement Directive (EU 2016/680), WP 258, Brussels, European Commission Directorate-General Justice and Consumers. (extracted from325.txt)
Article 29 Working Party (2017b), Guidelines on automated individual decisionmaking and profiling for the purposes of Regulation 2016/679, WP251rev.01, Brussels, European Commission Directorate-General Justice, p. (extracted from325.txt)
(2019), ‘ CS224n: Natural language processing with deep learning 1 – Lecture notes: Part I ’, course instructor: Manning, C., Winter 2021.CJEU (Court of Justice of the European Union) (1991), C-184/89, Helga Nimz v. (extracted from325.txt)
(2021), Automating Injustice – The use of artificial intelligence and automated decision-making systems in criminal justice in Europe, London, Fair Trials.Finck, M. (extracted from325.txt)
307–335.FRA (2016), Ensuring justice for hate crime victims: Professional perspectives, Luxembourg, Publications Office.FRA (2017), Second European Union Minorities and Discrimination Survey – Main results, Luxembourg, Publications Office.FRA (2018a), #BigData: Discrimination in data-supported decision making, Luxembourg, Publications Office. (extracted from325.txt)
(2019), ‘ Dirty data, bad predictions: How civil rights violations impact police data, predictive policing systems, and justice’, NYU Law Review, Vol. (extracted from325.txt)
354–365.The Law Society (2019), Algorithms in the criminal justice system , London, The Law Society, p. (extracted from325.txt)
(2017) “Social justice, epidemiology and health inequalities”, European Journal of Epidemiology, available at https://doi.org/10.1007/ s10654-017-0286-3 134. (extracted from330.txt)
Jillian Hastings Ward , Chair of the Participant Panel for the 100,000 Genomes Project Professor Sabine Hauert , Assistant Professor in Robotics at the University of Bristol Eleonora Harwich , Head of Digital and Technological Innovation at Reform Mr Iain Hennessey , Theme Lead for Paediatric Surgical Technologies at the National Institute for Health Research (NIHR), and Clinical Director of Innovation at Alder Hey Children’s Hospital Imogen Heywood , Engagement Manager at the Centre for Information Sharing Matthew Honeyman , Policy Researcher at The King’s Fund Nigel Houlden , Head of Technology Policy at the Information Commissioner’s Ofﬁce Dr Julian Huppert , Director of the Intellectual Forum at Jesus College, Cambridge and Chair of DeepMind Health’s Independent Review Panel Dr Mona Johnson , Senior Clinical Lead, Doman A: Self-care & Prevention at NHS Digital Professor Jeffrey Kahn , Director of the Johns Hopkins Berman Institute of Bioethics Dr Pearse Keane, NIHR Clinician Scientist, Institute of Ophthalmology, UCL and Moorﬁelds Eye Hospital NHS Foundation Trust Dr Dominic King , Clinical Lead at DeepMind Health and Honorary Clinical Lecturer in Surgery at Imperial College London Jacob Lant, Head of Policy and Public Affairs at Healthwatch England Dr Geraint Lewis, Chief Data Ofﬁcer at NHS England and Honorary Clinical Senior Lecturer at University College London Dr Harry Longman , Founder and Chief Executive of GP Access Maxine Mackintosh , PhD candidate at University College London’s Farr Institute of Health Informatics and Co-founder of One HealthTech Professor Eduardo Magrani , Professor of Law and Technology at Fundação Getulio Vargas Law School Christopher Markou , PhD candidate in the Faculty of Law at the University of Cambridge Dr Ben Maruthappu , Co-founder and CEO of Cera Care Dr Debra Mathews , Assistant Director for Science Programs for the Johns Hopkins Berman Institute of Bioethics, and Associate Professor in the Department of Pediatrics, Johns Hopkins University School of Medicine Dr Brent Mittelstadt , Research Fellow and British Academy Postdoctoral Fellow at the Oxford Internet Institute Ben Moody , Head of Health and Social Care at techUK Dr Bertie Müller, Senior Lecturer in Computing at the University of South Wales Michaela Muruianu , Innovation Co-ordinator at Digital Catapult 50ETHICAL, SOCIAL, AND POLITICAL CHALLENGES OF ARTIFICIAL INTELLIGENCE IN HEALTH Dr Luke Oakden-Rayner , Radiologist and PhD candidate with the School of Public Health at the University of Adelaide Dr Claudia Pagliari, Senior Lecturer in Primary Care and Informatics and Director of Global eHealth at the University of Edinburgh Imogen Parker , Head of Justice, Citizens and Digital Society Programmes at The Nufﬁeld Foundation Dr Ali Parsa , Founder and CEO of Babylon Health Bakul Patel , Associate Center Director for Digital Health at the Food and Drug Administration (FDA) Nicola Perrin , Head of Understanding Patient Data Carol Platt , Innovation Associate at Alder Hey Children’s Hospital Professor Nasir Rajpoot , Professor in Computational Pathology at the Department of Computer Science, University of Warwick Professor Daniel Ray, Director of Data at NHS Digital Professor Geraint Rees , Dean of the UCL Faculty of Life Sciences and Professor of Cognitive Neurology at University College London Dr Travis Rieder , Assistant Director for Education Initiatives, Director of the Master of Bioethics degree program and Research Scholar at the Berman Institute of Bioethics Professor Renato Rocha Souza , Professor at the Applied Mathematics School, Fundação Getulio Vargas Professor Ferdinando Rodriguez y Baena , Professor of Medical Robotics in the Department of Mechanical Engineering at Imperial College London Dr Caroline Rubin , Vice-President for Clinical Radiology at the Royal College of Radiologists and Consultant Radiologist at the University Hospital Southampton NHS Foundation Trust Dr Benedict Rumbold , Research Fellow in the Department of Philosophy at University College London Professor Burkhard Schafer , Professor of Computational Legal Theory at the University of Edinburgh’s School of Law Professor Stefan Schulz, Professor of Medical Informatics at Medical University Graz, Austria Allan Tucker , Senior Lecturer of Computer Science at Brunel University Professor Rhema Vaithianathan , Co-Director of the Centre for Social Data Analytics at the University of Auckland Jenny Westaway , Head of the Ofﬁce of the National Data Guardian Hugh Whittall , Director of the Nufﬁeld Council on Bioethics John Wilkinson , Director of Devices at the Medicines and Healthcare products Regulatory Agency (MHRA) Professor Stephen Wilkinson , Professor of Bioethics 51ETHICAL, SOCIAL, AND POLITICAL CHALLENGES OF ARTIFICIAL INTELLIGENCE IN HEALTH D: Patients and members of the public who contributed to this report Alex Brownrigg Mariana Campos Ann Cawley Annabel Dawson Ruth Day Eric Deeson Fran Husson Elaine Manna John Marsh Richard Melville Ballerand Dave McCormick Kath Pollock Bob Ruane Edward Sherley-Price Chris Warner Marney Williams 52ETHICAL, SOCIAL, AND POLITICAL CHALLENGES OF ARTIFICIAL INTELLIGENCE IN HEALTH Professor Richard Ashcroft , Professor of Bioethics at Queen Mary University of London Shirley Cramer CBE , Chief Executive of the Royal Society for Public Health Professor Bobbie Farsides , Professor of Professor of Clinical and Biomedical Ethics at the University of Sussex Professor John Fox , Professor at the Department of Engineering Science at the University of Oxford Professor Nina Hallowell , Associate Professor at the Nuffield Department of Public Health, University of Oxford Dr Hugh Harvey, Clinical Lead for Kheiron Medical and Royal College of Radiologists Informatics Committee Member Eleonora Harwich , Head of Digital and Technological Innovation at Reform Dr Geraint Lewis , Chief Data Officer at NHS England and an Honorary Clinical Senior Lecturer at University College London Maxine Mackintosh , PhD candidate at University College London’s Farr Institute of Health Informatics and co-founder of One HealthTech Dr Benedict Rumbold , Research Fellow in the Department of Philosophy at University College London Professor Ilina Singh , Professor of Neuroscience & Society at the Department of Psychiatry at the University of Oxford and Co-Director of the Wellcome Trust Centre for Ethics Dr Nicola Strickland , President of the Royal College of Radiologists and Consultant Radiologist at the Imperial College Healthcare NHS Trust Professor Stephen Wilkinson , Professor of Bioethics E: List of attendees at expert roundtable 53ETHICAL, SOCIAL, AND POLITICAL CHALLENGES OF ARTIFICIAL INTELLIGENCE IN HEALTH F: Methodology by which patient/public contributors were recruited Patients and members of the public that were interviewed or that participated in our roundtable on the 22nd February 2018 were recruited via one of two methods. (extracted from330.txt)
It points out that a guiding ethical framework should be “based on […] the principles and values enshrined in Article 2 of the Treaty on European Union and in the Charter of Fundamental Rights, such as human dignity, equality, justice and equity, non ­discrimination, informed consent, private and family life and data protection”, among other principles.** * European Parliament (2017a). (extracted from324.txt)
In this regard, consideration could be given to establishing public bodies that have the right and power to investigate the use of predictive systems, which are used to make decisions that affect peo ­ ple’s lives.24 One example of a study on bias in algorithms was carried out by journalists who investigated if there is racial bias in the risk scores used in the US crimi ­ nal justice system. (extracted from324.txt)
For example, the use of artificial intelligence tools by law enforcement and the criminal justice system could have an impact on an individual ’s right to be free from arbitrary arrest or to equality before the law; surveillance technologies c ould impact on the right to peaceful assembly; the use of social media platforms could impact the right to mental health; and property rental platforms could alter housing markets, possibly impacting the right to an adequate standard of living. (extracted from654.txt)
Artificial intelligence constitutes a major form of scientific and technological progress, which can generate considerable social benefits by improving living conditions and health, facilitating justice, creating wealth, bolstering public safety, and mitigating the impact of human activities on the environment and the climate. (extracted from683.txt)
AIS must avoid using acquired data to lock individuals into a user profile, fix their personal identity, or confine them to a filtering bubble, which would restrict and confine their possibilities for personal development — especially in fields such as education, justice, or business. (extracted from683.txt)
From Principles to Practice An interdisciplinary framework to operationalise AI ethics AI Ethics Impact Group led by From Principles to Practice An interdisciplinary framework to operationalise AI ethics 4 5 EXECUTIVE SUMMARY 6 1 INTRODUCTION 8 1.1 Challenges of practically implementing AI ethics 10 1.2 Multimethod framework as solution 12 1.3 Handling AI ethics in practice 14 2 VALUES, CRITERIA, INDICATORS, OBSERVABLES (VCIO) AND THE AI ETHICS LABEL IN DETAIL 15 2.1 How to apply VCIO to AI ethics: Three illustrated examples 17 2.1.1 Applying the VCIO approach to transparency as a value 20 2.1.2 Applying the VCIO approach to justice as a value 22 2.1.3 Applying the VCIO approach to accountability as a value 24 2.2 Values constituting the AI ethics rating 26 2.2.1 Transparency 26 2.2.2 Accountability 27 2.2.3 Privacy 28 2.2.4 Justice 28 2.2.5 Reliability 29 2.2.6 Environmental sustainability 30 2.3 How VCIO underpins the ratings in the AI Ethics Label 31 3 CLASSIFYING AN AI’S APPLICATION CONTEXT 35 3.1 The risk matrix 35 3.2 Dimensions of the risk matrix 37 3.2.1 Intensity of potential harm (x-axis) 37 3.2.2 Dependence on the decision (y-axis) 38 3.3 Recommendation for classes 38 4 CONCLUSION AND WHERE TO GO FROM HERE 41 4.1 Putting it all together 41 4.2 Next steps 42 5 BIBLIOGRAPHY 45 6 ABOUT THE AUTHORS 48 Imprint 54CONTENTS 6 EXECUTIVE SUMMARY Artificial intelligence (AI) increasingly pervades all areas of life. (extracted from33.txt)
In chapter two, we present the VCIO model (values, criteria, indicators, and observables) for the operationalisation and measurement of otherwise abstract principles and demonstrate the functioning of the model for the values of transparency, justice and accountability. (extracted from33.txt)
7EXECUTIVE SUMMARY For the proposed AI Ethics Label, we carefully suggest six values, namely justice, environmental sustainability, accountability, transparency, privacy, and reliability, based on contemporary discourse and operability. (extracted from33.txt)
2 See the Guidelines of the European High-Level Expert Group.Implementation challenge Solution framework1 INTRODUCTION 9INTRODUCTION • We offer practical examples for applying the VCIO model to selected values such as transparency, accountability and justice. (extracted from33.txt)
So how we implement and prioritise values such as justice (here includes fairness or non-discrimination) and transparency in practice, depends to some extent on the field of application and the cultural context an AI system operates in. (extracted from33.txt)
A system used in the justice sector must necessarily exhibit higher levels of privacy and fairness than a system used in the organisation of industrial production. (extracted from33.txt)
The implementation of values such as justice and transparency requires multiple measures throughout the complex development and implementation process of AI systems. (extracted from33.txt)
with regards to justice, accountability, transparency) independent of the system’s application context (see VCIO approach, chapter 2). (extracted from33.txt)
For example, it can become both a template for the work of regulatory bodies commissioned with the enforcement of regulation and provide orientation to AI developers and users and citizens and consumers.Framework addresses all main challenges 13INTRODUCTION Transparency Accountability Privacy Justice Reliability Environmental SustainabilityFIGURE 1 The AI Ethics Label with six selected valuesTaking the energy efficiency label as a guide, a label showing a rating of an AI system’s ethical characteristics could then look as follows: 14INTRODUCTION 1.3 Handling AI ethics in practice Taken together, our approach for the operationalisation of general principles (VCIO), the context-independent rating of ethical characteristics, the proposal of the introduction of an AI ethics label and the classification of different application contexts through a risk matrix provides a framework for bringing AI ethics from principles to practice. (extracted from33.txt)
For application fields that are classified in one of the higher risk levels, they may demand that an AI system (1) must carry an ethics label that shows the rating for values such as transparency, robustness, or justice and (2) satisfy minimum levels within the rating. (extracted from33.txt)
For example, the demand that algorithms should not discriminate finds consensus; the debate, however, begins with the question of what is understood by discrimination (justice), how to check whether it exists, and how to deal with conflicts between different values. (extracted from33.txt)
They are defined at the highest level (as justice or transparency, for example). (extracted from33.txt)
2.1 How to apply VCIO to AI ethics: Three illustrated examples In the following, we illustrate how to apply the VCIO model by focusing on three values, namely transparency, justice, and accountability.7 The findings from the Algo.Rules project, an initiative by the Bertelsmann Stiftung and the iRights.Lab, have been essential for the development of the framework and this chapter in particular. (extracted from33.txt)
In practice, where values such as privacy, reliability or justice come into conflict with each other, option and legacy values can act as arbitrator values. (extracted from33.txt)
2.1.1 Applying the VCIO approach to transparency as a value (page 20/21) Justice The criteria subsumed under the value of justice in this example pertain to classic aspects of algorithmic fairness such as bias prevention and assessment but emphasise a process perspective to include a broader set of ethical considerations. (extracted from33.txt)
These aspects are, for example, inclusion, represented by criteria such as participatory procedures, or social justice considerations, and a criterion for the assessment of trade-offs generated by the employment of the AI system in question. (extracted from33.txt)
In this sense, justice refers to a broader set of ethical considerations than the often-used term fairness, which mostly focuses on algorithmic outcomes themselves. (extracted from33.txt)
2.1.2 Applying the VCIO approach to justice as a value (page 22/23) Accountability The value of accountability refers to problems that arise in connection with the complex allocation or clarification of responsibility relationships in the use of AI. (extracted from33.txt)
2.1.3 Applying the VCIO approach to accountability as a value (page 24/25)Transparency as explainability and interpretability Justice with aspects of algorithmic fairness and inclusion Accountability refers to questions of assigning responsibility 20VALUES, CRITERIA, INDICATORS, OBSERVABLES (VCIO) AND THE AI ETHICS LABEL IN DETAIL Value TRANSPARENCY TRANSPARENCY Value Criteria Disclosure of origin of data sets Disclosure of properties of algorithm/model used Accessibility Criteria IndicatorsIs the data’s origin documented?Is it plausible for each purpose, which data is being used?Are the training data set’s characteristics documented and disclosed? (extracted from33.txt)
22VALUES, CRITERIA, INDICATORS, OBSERVABLES (VCIO) AND THE AI ETHICS LABEL IN DETAIL ValueJUSTICE JUSTICE ValueCriteriaIdentifying and assessing trade-offsAssessment of different sources of potential biases to ensure fairness1Social justice considerationsDetection and prevention of biases to ensure fairness Participatory procedures CriteriaIndicatorsHave tradeoffs been identified and assessed?Has the training data been analysed for potential biases?Has the input design (sensors, user interface) and input data been reviewed for potential biases?Have the requirements, goals and task definitions been examined for implicit and explicit discriminatory effects?Were possible selfreinforcing processes considered?Has due care been taken with regard to discriminatory effects caused by the design of the data output?Have the applied methods (e.g. (extracted from33.txt)
2.1.2 Applying the VCIO approach to justice as a value 23VALUES, CRITERIA, INDICATORS, OBSERVABLES (VCIO) AND THE AI ETHICS LABEL IN DETAIL ValueJUSTICE JUSTICE ValueCriteriaIdentifying and assessing trade-offsAssessment of different sources of potential biases to ensure fairness1Social justice considerationsDetection and prevention of biases to ensure fairness Participatory procedures CriteriaIndicatorsHave tradeoffs been identified and assessed?Has the training data been analysed for potential biases?Has the input design (sensors, user interface) and input data been reviewed for potential biases?Have the requirements, goals and task definitions been examined for implicit and explicit discriminatory effects?Were possible selfreinforcing processes considered?Has due care been taken with regard to discriminatory effects caused by the design of the data output?Have the applied methods (e.g. (extracted from33.txt)
Building on a meta-analysis of relevant publications, we settled on six values: transparency, accountability, privacy, justice, reliability, and environmental sustainability. (extracted from33.txt)
2.2.4 Justice Questions of justice include problems of equal treatment and the fair distribution of certain goods. (extracted from33.txt)
This includes aspects of social justice, in particular, “hidden” work, which is essential for the operation of AI systems. (extracted from33.txt)
9 Rössler 2004, Arendt 2006, Fried 1984, Stahl 2016To ensure privacy AI ethics must consider several methods Informational privacy as data is being used for specific purposes, after explicit consent, and with a right to delete or rectify Differential privacy and privacy by design Justice as algorithmic nondiscrimination and question of fair working conditions 29VALUES, CRITERIA, INDICATORS, OBSERVABLES (VCIO) AND THE AI ETHICS LABEL IN DETAIL Concerning discrimination by AI algorithms, reasons mostly lie in the reproduction of existing discrimination patterns that are introduced via the training data, in the (unintended) bias of software engineers, in the absorption of biases via presuppositions in labels, or the implementation of biases due to particular contexts of use. (extracted from33.txt)
The aspects of social justice that the VCIO model adds to this debate focus on the said “hidden” work that goes into the operation of AI systems. (extracted from33.txt)
While confidentiality means that no unauthorised party has access Algorithmic discrimination through biases Click work as an aspect of social justice Reliability as a precondition for trust Predictability and safety as robustness and resilience Cybersecurity as confidentiality, integrity and availability 30VALUES, CRITERIA, INDICATORS, OBSERVABLES (VCIO) AND THE AI ETHICS LABEL IN DETAIL to the information, integrity covers aspects such as that information cannot be altered, that changes to the information are transparent and traceable, as well as the protection of the authenticity of the information. (extracted from33.txt)
2.2.6 Environmental sustainability Environmental sustainability is a form of intergenerational justice and describes the obligation towards future generations to ensure and preserve their living conditions. (extracted from33.txt)
Those are just a few of many examples that show how various AI tools can be explicitly used to foster sustainability goals when taking the context into account.Resource-saving infrastructures to ensure intergenerational justice A right to repair Positive effects of AI systems on the environment 31VALUES, CRITERIA, INDICATORS, OBSERVABLES (VCIO) AND THE AI ETHICS LABEL IN DETAIL 2.3 How VCIO underpins the ratings in the AI Ethics Label By discussing the VCIO model in more depth, we have set the foundation for a comprehensive approach towards handling AI ethics. (extracted from33.txt)
Rating with 5–7 levels, indicated with letters A–GTransparency Accountability Privacy Justice Reliability Environmental SustainabilityFIGURE 3 The AI Ethics Label and the elements of the system rating VALUE 1 Criterion 1.1 Criterion 1.2 Indicator 1.1.1Indicator 1.1.2Indicator 1.2.1Indicator 1.2.2 Observables 1 Observables 2 Observables 3 Observables 4 CA DBA DCBA CBA DFIGURE 4 System rating and operationalisation of a value using minimum requirements and aggregation 33VALUES, CRITERIA, INDICATORS, OBSERVABLES (VCIO) AND THE AI ETHICS LABEL IN DETAIL However, one drawback of a system using minimum requirements may be that it gives few incentives to strive for individual indicator ratings that go beyond the minimum requirements. (extracted from33.txt)
We describe the proposed methodology to classify the application context as captured in the risk matrix in the following chapter 3.Ensuring incentives for higher ratings despite aggregation AI system evaluation requires analysis of application context 34VALUES, CRITERIA, INDICATORS, OBSERVABLES (VCIO) AND THE AI ETHICS LABEL IN DETAIL FIGURE 5 Illustration of the composition of the whole system rating using minimum requirements Observables CA DObservables BA D Observables CA D Observables GA Observables CA FObservables BA GC Observables CA D Observables BA CIndicator 1.1.1 Indicator 1.1.2 Indicator 1.2.1 Indicator 1.2.2 Indicator 2.1.1 Indicator 2.1.2 Indicator 2.2.1 Indicator 2.2.2Criterion 1.1 Criterion 1.2 Criterion 2.1 Criterion 2.2 Indicator 3.1.1 Indicator 3.1.2Criterium 3.1 Observables A Observables CA D Transparency Accountability Privacy Justice Reliability Environmental Sustainability 35 The AI Ethics Label provides at a glance information about the ethically relevant characteristics of an AI system. (extracted from33.txt)
To assess this, the following issues must be regarded: • Impact on fundamental rights, equality or social justice: Does an AI have a negative impact on a natural, legal persons’ fundamental rights or are social justice mechanisms (e.g. (extracted from33.txt)
–Domain: some domains – such as health, justice and immigration – are very sensitive. (extracted from720.txt)
justice) in which the AI solution will be applied, machine learning/data science, data engineering, technology (software and hardware), procurement, ethics and human rights.7 –Ensure that you have a diverse team. (extracted from720.txt)
To further ensure trustworthy AI, most governments are adopting new legislative frameworks for AI technologies , which are often sector specific and cover areas such as autonomous driving, healthcare and e -justice. (extracted from244.txt)
The Federal Ministry also highlights the use of AI for evaluating judicial proceedings, as mentioned in its National e -justice strategy . (extracted from244.txt)
ensuring that citizens can make informed and independent decisions) and to human dignity, equality and justice (i.e. (extracted from244.txt)
In addition, public sector operators should be secured sufficient resources and i ncentives to engage in such development, paying particular attention to sort out the rights of the outcomes of co -development;  The Ministry of Justice and the Ministry of Finance are currently examining national regulation of automated decision -making. (extracted from244.txt)
5.13.4 Regulation Hungary’s nationa l AI strategy aims to ensure a responsible, reliable and human -centred utilisation of AI technologies by means of the following policies:  Creating an ethical framework : developing an AI code of conduct by the first half of 2021 in collaboration between the Ministry of Justice, the Ministry for Innovation and Technology, AI Innovation Hub and the Central Statistical Office ;  Setting up an Artificial Intelligence Regulation and Ethics Knowledge Centre : the aim is to create and coordinate an extensive pool of experts to help resolve legal issues and matters of ethics relating to the regulation of AI and the implementation of the strategy ;  Establishing a regulatory framework for AI: t he obje ctive is to amen d the current regulatory system to suit AI and to align it to EU regulations ;  Building data management regulation : the objective is to set up regulations for the use and exchange of public and private data and to define rules regarding d ata monetisation. (extracted from244.txt)
The Latvian strategy identifies priority sectors with a high potential for AI applicatio ns in the country, such as transport (Intelligent transport systems), culture, justice (AI as support for decision making and drafting legislation), agriculture (automated control), and translation. (extracted from244.txt)
Regarding ethics, the government adopts the European ethical guidelines outlined by the European Commission for the Efficiency of Justice. (extracted from244.txt)
66 1) International bodies and AI, 2) Swiss intelligence of interests in the European AI (Digital Europe Programme) activities, 3) Changes in the world of work, 4) AI in industry and services, 5) AI in education, 6) A pplication of AI in science and research, 7) AI in cybersecurity and security policy, 8) AI, Media & Public, 9) Automated mobility and AI, 10) AI in healthcare, 11) AI in the financial secto r, 12) AI in agriculture, 13) Energy, climate, environment and AI, 14) AI in administration, 15) Further development of the general legal framework on AI, 16) AI in justice, 17) AI, data and intellectual property law. (extracted from244.txt)
While the establishment of the legal basis is ensured by a wide range of institutions, the FDFA will specifically focus on the following policies to further develop the general legal framework on AI:  Examining the emergence of AI -specific international law and its impac t on Switzerland;  Following -up developments with regard to the visibility of AI systems in interaction with consumers;  Monitoring developments in AI -based decision -making in the justice system (predictive justice). (extracted from244.txt)
Proposal for a regulation of the European Parliament and of the Council laying down harmonised rules on artificial intelligence (artificial intelligence act) and amending certain Union legislative acts Committees responsible: Rapporteurs: Shadow rapporteurs: Internal Market and Consumer Protection (IMCO) and Civil Liberties, Justice and Home Affairs (LIBE) (jointly under Rule 58) Brando Benifei (S&D, Italy) and Dragoş Tudorache (Renew, Romania) Deirdre Clune, Axel Voss (EPP); Petar Vitanov (S&D); Svenja Hahn, (Renew); Sergey Lagodinsky, Kim Van Sparrentak (Greens/EFA); Rob Rooken, Kosma Złotowski (ECR ); Jean -Lin Lacapelle, Jaak Madison (ID); Cornelia Ernst, Kateřina Konecna (The Left) COM(2021)206 21.4.2021 2021/0106(COD) Ordinary legislative procedure (COD) (Parliament and Council on equal footing – formerly 'co-decision') Procedure completed. (extracted from278.txt)
In Parliament , the file was assigned jointl y (under Rule 58) to the Committee on Internal Market and Consumer Protection (IMCO) and the Committee on Civil Liberties, Justice and Home Affairs (LIBE), with Brando Benifei (S&D, Italy) and Dragoş Tudorache, Renew, Romania) appointed as rapporteurs. (extracted from278.txt)
creditworthiness evaluation ), law enfo rcement, border control, administration of justice and democratic processes , biometric identification, categorisation and emotion recognition systems (outside the prohibited categories) . (extracted from278.txt)
Texts adopted - Fundamental rights implications of big data - Tuesday, 14 March 2017 Choisissez la langue de votre document : bg - български es - español cs - čeština da - dansk de - Deutsch et - eesti keel el - ελληνικά en - English fr - français ga - Gaeilge hr - hrvatski it - italiano lv - latviešu valoda lt - lietuvių kalba hu - magyar mt - Malti nl - Nederlands pl - polski pt - português ro - română sk - slovenčina sl - slovenščina fi - suomi sv - svenska Index Previous Next Full text Procedure : 2016/2225(INI)Document stages in plenaryDocument selected : A8-0044/2017Texts tabled : A8-0044/2017 Debates : PV 13/03/2017 - 15 CRE 13/03/2017 - 15 Votes : PV 14/03/2017 - 6.12 Explanations of votes Texts adopted : P8_TA(2017)0076 Texts adopted 203k 61k Tuesday, 14 March 2017 - Strasbourg Fundamental rights implications of big data P8_TA(2017)0076A8-0044/2017 European Parliament resolution of 14 March 2017 on fundamental rights implications of big data: privacy, data protection, non-discrimination, security and law-enforcement (2016/2225(INI)) The European Parliament, – having regard to Article 16 of the Treaty on the Functioning of the European Union, – having regard to Articles 1, 7, 8, 11, 14, 21, 47 and 52 of the Charter of Fundamental Rights of the European Union, – having regard to the guidelines for the regulation of computerised personal data files of the United Nations General Assembly in its Resolution 45/95 of 14 December 1990, – having regard to Regulation (EU) 2016/679 of the European Parliament and of the Council of 27 April 2016 on the protection of natural persons with regard to the processing of personal data and on the free movement of such data, and repealing Directive 95/46/EC (General Data Protection Regulation)(1) (GDPR), and to Directive (EU) 2016/680 of the European Parliament and of the Council of 27 April 2016 on the protection of natural persons with regard to the processing of personal data by competent authorities for the purposes of the prevention, investigation, detection or prosecution of criminal offences or the execution of criminal penalties, and on the free movement of such data, and repealing Council Framework Decision 2008/977/JHA(2), – having regard to the Communication from the Commission to the European Parliament, the Council, the European Economic and Social Committee and the Committee of the Regions entitled ‘A Digital Single Market Strategy for Europe’ of 6 May 2015 (COM(2015)0192), – having regard to the Council of Europe Convention for the protection of individuals with regard to automatic processing of personal data of 28 January 1981 (ETS No 108) and its Additional Protocol of 8 November 2001 (ETS No 181)(3), – having regard to Recommendation CM/Rec(2010)13 of the Committee of Ministers of the Council of Europe to Member States on the protection of individuals with regard to automatic processing of personal data in the context of profiling of 23 November 2010(4), – having regard to Opinion 7/2015 of the European Data Protection Supervisor of 19 November 2015 entitled ‘Meeting the challenges of big data – A call for transparency, user control, data protection by design and accountability’(5), – having regard to Opinion 8/2016 of the European Data Protection Supervisor of 23 September 2016 entitled ‘EDPS Opinion on coherent enforcement of fundamental rights in the age of big data’(6), – having regard to the statement of the Article 29 Data Protection Working Party on the impact of the development of big data on the protection of individuals with regard to the processing of their personal data in the EU of 16 September 2014(7), – having regard to Rule 52 of its Rules of Procedure, – having regard to the report of the Committee on Civil Liberties, Justice and Home Affairs (A8-0044/2017), A. (extracted from293.txt)
Recalls that in accordance with Article 15 of Directive 2000/31/EC, Member States shall neither impose a general obligation on the providers of transmission, storage and hosting services to monitor the information which they transmit or store, nor a general obligation to actively seek facts or circumstances suggesting illegal activity; reiterates in particular that the Court of Justice of the European Union, in the cases C-360/10 and C-70/10, rejected measures for the ‘active monitoring’ of almost all users of the services concerned (internet access providers in one case, a social network in the other) and specified that any injunction requiring a hosting services provider to undertake general monitoring shall be precluded; Non- discrimination 19. (extracted from293.txt)
(3) http://www.coe.int/en/web/conventions/full-list/-/conventions/treaty/108 (4) https://search.coe.int/cm/Pages/result_details.aspx?ObjectID=09000016805cdd00 (5) https://secure.edps.europa.eu/EDPSWEB/webdav/site/mySite/shared/Documents/Consultation/Opinions/2015/15-11-19_Big_Data_EN.pdf (6) https://secure.edps.europa.eu/EDPSWEB/webdav/site/mySite/shared/Documents/Consultation/Opinions/2016/16-09-23_BigData_opinion_EN.pdf (7) http://ec.europa.eu/justice/data-protection/article-29/documentation/opinion-recommendation/files/2014/wp221_en.pdf (8) Opinion 8/2016 of the European Data Protection Supervisor of 23 September 2016, p. (extracted from293.txt)
P eace, inclusiv eness and justice, equity and interconnectedness should be promoted throughout the lifecycle of AI systems . (extracted from285.txt)
These include freedom, dignity and autonomy, privacy and data protection, non discrimination and equality, diversity, fairness, social justice, and internationally recognised labour rights'. (extracted from285.txt)
States that in line with strict liability systems of the Member States, the proposed Regulation should cover violations of the important legally protected rights to life, health, physical integrity and property, and should set out the amounts and extent of compensation, as well as the limitation period; is of the opinion that the proposed Regulation should also incorporate significant immaterial harm that results in a verifiable economic loss above a threshold harmonised in Union liability law, that balances the access to justice of affected persons and the interests of other involved persons; urges the Commission to re-evaluate and to align the thresholds for damages in Union law; is of the opinion that the Commission should analyse in depth the legal traditions in all Member States and their existing national laws that grant compensation for immaterial harm, in order to evaluate if the inclusion of immaterial harm in AIspecific legislative acts is necessary and if it contradicts the existing Union legal framework or undermines the national law of the Member States; 20. (extracted from291.txt)
This follows from general and widely accepted liability concepts of justice, according to which the person that creates or maintains a risk for the public is liable if that risk causes harm or damage, and thus should ex-ante minimise or ex-post compensate that risk. (extracted from291.txt)
(16) This Regulation should cover harm or damage to life, health, physical integrity, property and significant immaterial harm that results in a verifiable economic loss above a threshold, harmonised in Union liability law, that balances the access to justice of affected persons with the interests of other involved persons. (extracted from291.txt)
49 http://ec.europa.eu/justice/citizen/document/files/2015_public_consultation_booklet_en.pdf , p. (extracted from252.txt)
149 of Consumer protection policies, strategies and statistics http://ec.europa.eu/justice/consumer -marketing/files/ucp_guidance_en.pdf . (extracted from252.txt)
Article 6 of the Directive, http://ec.europa.eu/justice/consumer marketing/files/ucp_guidance_en.pdf , p. (extracted from252.txt)
122 http://ec.europa.eu/justice/consumer -marketing/files/ucp_guidance_en.pdf , p. (extracted from252.txt)
http://ec.europa.eu/justice/dataprotection/files/factsheets/factsheet_data_protection_eurobarometer_240615_en.p df . (extracted from252.txt)
The Court of Justice enshrined the right to effective remedy in its judgment of 15 May 1986 as a general principle of Union law (Case 222/84 Johnston [1986] ECR 1651; see also judgment of 15 October 1987, Case 222/86 Heylens [1987] ECR 4097 and judgment of 3 December 1992, Case C -97/91 Borelli [1992] ECR I -6313). (extracted from252.txt)
Notably, in the Recommendation on common principles for injunctive and compensatory collective redress mechanisms in the Member States concerning violations of rights granted under Union Law, the Commission determined that the recourse to opt -out collective redress mecha nisms may be justified “by reasons of sound administration of justice”, see Article 21, , Commission Recommendation of 11.06.2013 on common principles for injunctive and compensatory collective redress mechanisms in the Member States concerning violations of rights granted under Union Law (2013/396/EU) http://eur -lex.europa.eu/legal content/EN/TXT/?uri=OJ:JOL_2013_201_R_NS0013 . (extracted from252.txt)
First version of the Principles for Action The first version of these principles has been co-drafted through a multistakeholder process, while paying careful attention to the EU GDPR11 and the police and criminal justice directive,12 and has drawn inspiration from some of their principles. (extracted from722.txt)
“EU Data Protection Rules”, EU website, https://ec.europa.eu/commission/priorities/justice-and-fundamental-rights/ data-protection/2018- reform-eu-data-protection-rules/eu-data-protection-rules_en (link as of 28/1/20). (extracted from722.txt)
1.Core​ ​public​ ​agencies,​ ​such​ ​as​ ​those​ ​responsible​ ​for​ ​criminal​ ​justice,​ ​healthcare, welfare,​ ​and ​ ​education ​ ​(e.g​ ​“high ​ ​stakes”​ ​domains) ​ ​should ​ ​no ​ ​longer​ ​use​ ​“black​ ​box” AI​ ​and ​ ​algorithmic​ ​systems.​​ ​This​ ​includes​ ​the​ ​unreviewed​ ​or​ ​unvalidated​ ​use ​ ​of pre-trained​ ​models,​ ​AI​ ​systems​ ​licensed​ ​from​ ​third​ ​party​ ​vendors, ​ ​and​ ​algorithmic processes​ ​created​ ​in-house.​ ​The ​ ​use​ ​of​ ​such​ ​systems​ ​by​ ​public​ ​agencies​ ​raises​ ​serious due​ ​process​ ​concerns, ​ ​and​ ​at​ ​a​ ​minimum​ ​they​ ​should​ ​be ​ ​available​ ​for​ ​public​ ​auditing, testing,​ ​and​ ​review, ​ ​and​ ​subject​ ​to​ ​accountability​ ​standards. (extracted from25.txt)
​ ​ Within each ​ ​ domain ​ ​ – ​ ​ such ​ ​ as ​ ​ education, ​ ​ healthcare ​ ​ or ​ ​ criminal ​ ​ justice ​ ​ – ​ ​ legacies ​ ​ of ​ ​ bias ​ ​ and movements ​ ​ toward ​ ​ equality ​ ​ have ​ ​ their ​ ​ own ​ ​ histories ​ ​ and ​ ​ practices. (extracted from25.txt)
​ ​ The ​ ​ third section, ​ ​ on ​ ​​ Rights ​ ​ and ​ ​ Liberties ​ , ​ ​ begins ​ ​ by ​ ​ recognizing ​ ​ the ​ ​ recent ​ ​ rise ​ ​ of ​ ​ political authoritarianism, ​ ​ and ​ ​ asks ​ ​ about ​ ​ the ​ ​ role ​ ​ of ​ ​ AI ​ ​ systems ​ ​ in ​ ​ either ​ ​ supporting ​ ​ or ​ ​ eroding citizens’ ​ ​ rights ​ ​ and ​ ​ liberties ​ ​ in ​ ​ areas ​ ​ like ​ ​ criminal ​ ​ justice, ​ ​ law ​ ​ enforcement, ​ ​ housing, ​ ​ hiring, lending ​ ​ and ​ ​ other ​ ​ domains. (extracted from25.txt)
The ​ ​ danger ​ ​ of ​ ​ bias ​ ​ increases ​ ​ when ​ ​ these ​ ​ systems ​ ​ are ​ ​ applied, ​ ​ often ​ ​ in ​ ​ non-transparent ways, ​ ​ to ​ ​ critical ​ ​ institutions ​ ​ like ​ ​ criminal ​ ​ justice ​ ​ and ​ ​ healthcare. (extracted from25.txt)
​ ​ How ​ ​ will ​ ​ AI’s ​ ​ use ​ ​ in ​ ​ the ​ ​ criminal ​ ​ justice ​ ​ system affect ​ ​ our ​ ​ understanding ​ ​ of ​ ​ due ​ ​ process ​ ​ and ​ ​ the ​ ​ principle ​ ​ of ​ ​ equal ​ ​ justice ​ ​ under ​ ​ the ​ ​ law? (extracted from25.txt)
​ ​ As ​ ​ AI ​ ​ systems ​ ​ promise ​ ​ new ​ ​ forms ​ ​ of technical ​ ​ efficiency ​ ​ in ​ ​ the ​ ​ service ​ ​ of ​ ​ safety, ​ ​ we ​ ​ may ​ ​ need ​ ​ to ​ ​ confront ​ ​ a ​ ​ fundamental tension ​ ​ between ​ ​ technological ​ ​ efficiency ​ ​ and ​ ​ a ​ ​ commitment ​ ​ to ​ ​ ideals ​ ​ of ​ ​ justice. (extracted from25.txt)
Scholars ​ ​ like ​ ​ Kate ​ ​ Crawford ​ ​ and ​ ​ Jason ​ ​ Schultz ​ ​ have ​ ​ identified ​ ​ a ​ ​ series ​ ​ of ​ ​ conflicts ​ ​ between AI ​ ​ techniques ​ ​ and ​ ​ constitutional ​ ​ due ​ ​ process ​ ​ requirements, ​ ​ such ​ ​ as ​ ​ how ​ ​ AI ​ ​ techniques 123 affect ​ ​ procedural ​ ​ considerations ​ ​ and ​ ​ equal ​ ​ justice ​ ​ under ​ ​ the ​ ​ law. (extracted from25.txt)
130 The ​ ​ criminal ​ ​ justice ​ ​ system’s ​ ​ implementation ​ ​ of ​ ​ risk ​ ​ assessment ​ ​ algorithms ​ ​ provides ​ ​ an example ​ ​ of ​ ​ the ​ ​ legal ​ ​ system’s ​ ​ use ​ ​ of ​ ​ AI ​ ​ and ​ ​ its ​ ​ attendant ​ ​ risks. (extracted from25.txt)
135 Rebecca ​ ​ Wexler, ​ ​ “Life, ​ ​ Liberty, ​ ​ and ​ ​ Trade ​ ​ Secrets: ​ ​ Intellectual ​ ​ Property ​ ​ in ​ ​ the ​ ​ Criminal ​ ​ Justice ​ ​ System,” ​ ​ SSRN ​ ​ preprint: https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2920883 ​ . (extracted from25.txt)
137 Richard ​ ​ Berk, ​ ​ Hoda ​ ​ Heidari, ​ ​ Shahin ​ ​ Jabbari, ​ ​ Michael ​ ​ Kearns ​ ​ and ​ ​ Aaron ​ ​ Roth, ​ ​ “Fairness ​ ​ in ​ ​ Criminal ​ ​ Justice ​ ​ Risk ​ ​ Assessments: The ​ ​ State ​ ​ of ​ ​ the ​ ​ Art,” ​ ​ arXiv:1703.09207, ​ ​ March ​ ​ 27, ​ ​ 2017. (extracted from25.txt)
Justice ................................ (extracted from118.txt)
The Council of Europe has committed to framing their scope and implications in most of its specialised areas of activity, such as justice, data protection, equality and non -discrimination 3. (extracted from118.txt)
3 European Commission for the Efficiency of Justice (CEPEJ), European Ethical Charter for the Use of Judicial Intelligence in Judicial Systems and their Environment ; Consultative Committee of the Convention for the Protection of Individuals with regard to the Processing of Personal Data (Convention 108), Gui delines on Artificial Intelligence and Data Protection ; European Commission against Racism and Intolerance (ECRI), Study, Discrimination, Artificial Intelligence and Algorithmic Decisions . (extracted from118.txt)
These applications have found their way into sectors such as law enforcement, justice, human resource management, financial services, transport, healthcare, public services, etc. (extracted from118.txt)
The European Commission f or the Efficiency of Justice already in 2018 outlined 5 principles for the use of AI in the judiciary in the “European Ethical Charter on the use of AI in the judicial systems and their environment”. (extracted from118.txt)
Redress in l ight of AI impact on human rights entails access to justice and effective remedy. (extracted from118.txt)
As far as access to justice goes, it might be too soon to determine whether this is sufficiently guaranteed when it comes to AI and human rights impact. (extracted from118.txt)
More importantly however, access to justice is challenged when many AI -applications are deve loped and deployed by only a handful of large private actors. (extracted from118.txt)
In this respect, AI might serve as a good opportunity and think of a structure that would legally oblige private actors to comply with human rights and to grant access to justice if they fail to do so.58 The basic question is whether to a) accept the private power of AI companies and to make sure they use it responsibly, or to b) challenge it and try to reassert the power of the state. (extracted from118.txt)
➢ We found growing agreement around the following ethical principles: transparency, justice, non-maleficence, responsibility, and privacy. (extracted from118.txt)
➢ The principles of privacy, justice and fairness showed the least variation across CoE- member countries, CoE -observer countries and the rest of the world, hence the highest degree of cross - geographical and cross -cultural stability. (extracted from118.txt)
➢ The convergence of current soft law instruments around five generic ethical principles such as transparency, justice, non-maleficence, responsibility, and privacy reveals five priority areas of oversight and possible intervention by mandatory governance authorities at both the governmental and intergovernmental level. (extracted from118.txt)
These are, by decreasing order of frequency of the sources in which they were featured: transparency, justice and fairness, non -maleficence, responsibility, privacy, beneficence, freedom and autonomy, trust, dignity, sustainability, and solidarity. (extracted from118.txt)
49 Ethical principle Number of documents Included codes Transparency, explainability, explicability, understandability, Transparency 101/116 interpretability, communication, disclosure, showing Justice, fairness, consistency, inclusion, equality, equity, (non -)bias, (non-)discrimination, diversity, Justice and fairness 97/116 plurality, accessibility, reversibility, remedy, redress, challenge, access and distribution, impartiality Non-maleficence, security, safety, harm, protection, precaution, Non-maleficence 84/116 prevention, integrity (bodily or mental), non - subversion Responsibility, accountability, Responsibility 79/116 liability, acting with integrity Privacy, personal or private Privacy 74/116 information, confidentiality Benefits, beneficence, well - Beneficence 58/116 being, peace, social good, common good Freedom, autonomy, consent, Freedom and autonomy 48/116 choice, self -determination, liberty, empowerment Trustworthiness 41/116 Trust, trustworthiness Sustainability, environment (nature), Sustainability 20/116 energy, resources (energy) Dignity 20/116 Dignity Solidarity 10/116 Solidarity, social security, cohesion Table 2 - Frequency of ethical themes and associated codes 50 No single ethical principle appears to be common to the entire corpus of documents, although there is an emerging convergence around the following principles: transparency, justice and fairness, non - maleficence, responsibility and privacy. (extracted from118.txt)
In contrast, they appear to refer more sporadically to the principles of justice, beneficence, and dignity. (extracted from118.txt)
The principles of privacy, justice and fairness showed the least variation, hence the highest degree of cross -geographical and cross -cultural stability. (extracted from118.txt)
Justice, fairness, and equity : Justice is mainly expressed in terms of fairness and prevention (or mitigation) of algorithmic biases that can lead to discrimination. (extracted from118.txt)
Documents disagree on how to achieve justice and fairne ss in AI. (extracted from118.txt)
We identified five main non - mutually - exclusive implementation strategies for preserving and promoting justice and fairness in AI: i. (extracted from118.txt)
Our analysis shows the emergence of an apparent cross -stakeholder convergence on promoting the ethical principles of transparency, justice, non -maleficence, responsibility, and privacy. (extracted from118.txt)
Although no single ethical principle is explicitly endorsed by all existing guidelines, transparency, justice and fairness, non -maleficence, responsibility and privacy are each referenced in more than half of all guidelines. (extracted from118.txt)
In particular, the prevalence of calls for tr ansparency, justice and fairness points to an emerging moral priority to require transparent processes throughout the entire AI continuum (from transparency in the development and design of algorithms to transparent practices for AI use), and to caution the global community against the risk that AI might increase inequality if justice and fairness considerations are not adequately addressed. (extracted from118.txt)
Both these themes appear to be strongly intertwined with the theme of responsibility, as the promotion of b oth transparency and justice seems to postulate increased responsibility and accountability on the side of AI makers and deployers. (extracted from118.txt)
The convergence of current soft law instruments around five generic ethical principles such as transparency, justice, non-maleficence, responsibility, and privacy reveals five priority areas of oversight and possible intervention by mandatory governance authorities at both the governmental and intergovernmental level. (extracted from118.txt)
After an init ial sector -specific analysis to map and identify key guiding principles in four core areas (data protection, health, democracy and justice), these principles are contextualised in the light of the changes to society produced by AI. (extracted from118.txt)
The fourth and the fifth sections are centred on democracy and justice . (extracted from118.txt)
69 European Commission for the Efficiency of Justice (CEPEJ). (extracted from118.txt)
In this regard four key areas have been selected: data, health, democracy and justice. (extracted from118.txt)
The following two tables provide a first example of this mapping exercise based on a preliminary overview of the data protection and justice realms to identify the guiding principles for future regulation of AI. (extracted from118.txt)
2018 Guiding principles and legal values Accountability Risk-based approach Precautionary principle Data quality & security Transparency Fairness Contextual approach Role of experts Participation/Inclusiveness Freedom of choice/Autonomy Human control/oversight Awareness Literacy Responsible innovation Cooperation between supervisory authorities 66 Figure 2: Justice Binding instruments Universal Declaration of Human Rights International Covenant on Civil and Political Rights International Convention on the Elimination of All Forms of Racial Discrimination Convention on the Elimination of All Forms of Discrimination against Women Convention for the Protection of Human Rights and Fundamental Freedoms Charter of Fundamental Rights of the European Union Impacted areas Processing of judicial decisions and data Predictive policing Related non - binding instruments CEPEJ. (extracted from118.txt)
Figure 4: Common guiding values in the field of data protection and justice 68 III. (extracted from118.txt)
The fourth and the fifth sections are centred on democracy and justice. (extracted from118.txt)
The result of this analysis made it possible to group the guiding principles and values around a number of key elements which emerged in terms of distribution (frequency): Non-discrimination (15) Diversity, inclusion and pluralism (13) Privacy and Data Protection (11) Transparency (9) Equality (8) Access to justice, fair trial (7) Human control (7) Impartiality (6) Access to information (5) Security (5) 74 See Council of Europe -Committee of experts on internet intermediaries (MSI -NET), 2018. (extracted from118.txt)
Such committees may play an especially important role in areas where transparency and stakeholder engagement can be more difficult due to competing interests and rights, such as in the fields of predictive justice, crime prevention and detection.94 Appropriate mechanisms should be put in place to ensure the independence of these committees of experts.95 88 See GAI, para. (extracted from118.txt)
above Fig ure 4: Common guiding values in the field of data protection and justice. (extracted from118.txt)
Justice As in the previous section, the field of justice is a broad domain and analysing the whole spectru m of the consequences of AI on justice and its related effects on democracy would be too ambitious. (extracted from118.txt)
Justice differs from data protection and health in the absence of specific and dedicated binding instruments, such as Convention 108+ and the Oviedo Convention. (extracted from118.txt)
This exercise is facilitated by the European Ethical Charter on the use of artificial intelligence (AI) in judicial systems and th eir environment, adopted by the CEPEJ in 2019, which directly addresses the relationship between justice and AI. (extracted from118.txt)
Guiding principles for the development of AI in the field of justice can be derived from the following binding instruments: the Universal Declaration of Human Rights, the International Covenant on Civil and Political Rights, the Convention for the Protection of Human Rights and Fundamental Freedoms, the International Convention on the Elimination of All Forms of Racial Discrimination, the Convention on the Elimination of All Forms of Discrimination against Women, and the Conventio n for the Protection of Human Rights and Fundamental Freedoms.188 Given the range of types and purposes of operations in this field and the various professional figures and procedures involved, this section makes a functional distinction between two areas: (i) judicial decisions and alternative dispute resolutions (ADRs) and (ii) crime prevention/prediction. (extracted from118.txt)
Given the textual nature of legal documents, natural language processing (NLP) plays an important role in AI applications for the justice sphere. (extracted from118.txt)
All these constraints suggest a careful and more critical adoption of AI in the field of justice than in other do mains and, with regard to court decisions and ARDs, suggest following a distinction between cases characterised by routinely and fact -based evaluations and cases characterised by a significant margin for legal reasoning and discretion.192 Court decisions an d ADRs Several so-called Legal Tech AI products do not have a direct impact on the decision - making processes in courts or alternative dispute resolutions (ADRs), but rather facilitate content and knowledge management, organisational management, and performance measurement.193 These applications include, for example, tools for contracts categorisation, detection of divergent or incompatible contractual clauses, e -discovery, drafting assistance, law provision retrieval, assisted compliance review. (extracted from118.txt)
192 See the following Section on the dist inction between codified justice and equitable justice. (extracted from118.txt)
193 See European Commission for the Efficiency of Justice (CEPEJ). (extracted from118.txt)
Here, the distinction between codified justice and equitable justice196 suggests that AI should be circumscribed for decision -making purposes to cases characterised by routine and fact -based evaluations. (extracted from118.txt)
This entails the importance to carry out further research on the classification of the different kind of decisional processes to identify those routinised applications of legal reasoning that can be demanded to AI, preserving in any case human overview that also guarantees legal creativity of decision makers.197 Regarding equitable justice, as the literature points out,198 its logic is more complicated than the simple outcome of individual cases. (extracted from118.txt)
The Universal Declaration of Human Rights (Articles 7 and 10), the ICCPR 195 See also European Commission for the Efficiency of Justice (CEPEJ). (extracted from118.txt)
196 See Re and Solow -Niederman, 2019, 252 -254 (“Equitable just ice entails both reflection on the values set in place by the legal system and the reasoned application of those values, in context […] Codified justice refers to the routinized applicati on of standardized procedures to a set of facts […] In short, codifie d justice sees the vices of discretion, whereas equitable justice sees its virtues”). (extracted from118.txt)
199 See European Commission for the Efficiency of Justice (C EPEJ). (extracted from118.txt)
As stated by the European Commission for the Efficiency of Justice, “the neutrality of algorithms is a myth, as their creators consciously or unintentionally transfer their own value systems into them”. (extracted from118.txt)
Many cases of biases regarding AI applications confirm that these systems too often – albeit in many cases unintentionally – provide a partial representation of society and individual cases, whic h is not compatible with the principles of equal treatment before the law and non-discrimination .200 Data quality and other forms of quality assessment (impact assessment, audits, etc.) can reduce this risk201but, given the degree of potentially affected interests in the event of biased decisions, the risks remain high in the case of equitable justice and seem disproportionate to the benefits largely in terms of efficiency for the justice system.202 Further concerns affect the principles of fair tr ial and of equality of arms ,203 when court decisions are based on the results of proprietary algorithms whose training data and structure are not publicly available.204 A broad notion of transparency might address these issues in relation to the use of AI in judicial decisions, but the transparency of AI – a challenging goal in itself – cannot address the other structural and functional objections cited above. (extracted from118.txt)
This is the case, for instance, of the independence of the judges or the principles of fair trial and of equality of arms, which concern justice alone.220 Second, some guiding principles are the same in different areas, but with different nuances in each context. (extracted from118.txt)
Then again, in the context of justice, transparency has a more complex significance being vital to safeguard fundame ntal rights and freedoms (e.g. (extracted from118.txt)
For instance, in the field of data protection there are several provisions implementing these principles with a significant degree of detail, whereas in the case of democracy and justice these principles are less developed with regard to data intensive applications such as AI. (extracted from118.txt)
These mainly concern broad areas, such as democracy and justice, where different options and interpretations are available, depending on the political and societal vision of the future relationship between humans and machines. (extracted from118.txt)
Artificial Intelligence in the Context of Crime and Criminal Justice. (extracted from118.txt)
https://www.cyberjustice.ca/publications/lintelligence -artificielle -dans -le-contexte -de- lacriminalite -et-de-la-justice -penale/ , accessed 30.05.2020. (extracted from118.txt)
European Commission for the Efficiency of Justice (CEPEJ). (extracted from118.txt)
Developing Artificially Intelligent Justice. (extracted from118.txt)
Algorithmic Justice: Algorithms and Big Data in Criminal Justice Settings. (extracted from118.txt)
Justice - Universal Declaration of Human Rights - International Covenant on Civil and Political Rights CEPEJ. (extracted from118.txt)
In addition, there are many social rights (and Charter) aspects related to subjects covered by a wide range of other areas of CoE work: Some examples: - CM Rec(93)1 on effective access to the law and to justice to the very poor; - social rights aspects of the prison rules (health care, living conditions, employment, education, family rights,…); - etc. (extracted from118.txt)
job applications, welfare/social benefits, access to goods and services, such as bank loans, insurance) • Predictive policing (which holds high ris k of racial profiling) • Predictive justice • Facial recognition • Behavioural prediction technologies such as emotional recognition and AI -based lie detection • Personal assistance tools (e.g. (extracted from118.txt)
- Automated patching of vulnerabilities Justice sector • Processing of judicial decisions and data: - to support judicial decision -making or judicial research) - On-line dispute resolution - Provision of legal advice to litigants • Predictive policing Congress of Local and Regional Authorities • Provision of local public services. (extracted from118.txt)
• Predictive justice (ex VAW) • Predictive health based on gender -biased data (ex some diseases characterised as “female” or “male”) • Inherited biases in machine -led content moderation (high tolerance for sexism, sexist hate speech & VAW) • Gendered virtual assistants / robots perpetuating gender stereotypes • Gendered marketing perpetuating gender stereotypes • Differential pricing ba sed on sex/gender Positive impacts • Use of GPS tracking devices to ensure respect of protection orders in cases of VAW • Use of AI by law enforcement agencies to conduct risk assessment in DV cases • Use of AI to identify and track gender bias and being able to quarantine or eliminate the spreading of (sexist) hate speech on platforms • Developments of Apps to support and inform victims of VAW • Use of AI -based tools to analyse content and track gender bias / analyse representation (ex in movies or other media) Culture, Creativity and Heritage • Access and participation in public / cultural life; • FoE (incl. (extracted from118.txt)
Including, but not limited to: many aspects of employment (including but not limited to monitoring and surveillance, job screening and work in the platform economy, etc); ditto different aspects of health (the right to enjoy the highest standard of health attainable); ditto education; equally for social protection, integration and participation; let alone non-discrimination; housing and protection from social exclusion; For example: justice (both as regards the administration of justice, and criminal justice and prisons; trafficking in human beings (forced labour and exploitation, …); migration and refugees; gender equality, plus violence against women; children and youth, plus education; bioethics; non-discrimination, Roma and Travellers, SOGI ; drug policy; participation and culture; sport; 108 Annex 3 . (extracted from118.txt)
Problem of evidence in the cloud versus territorial enforcement jurisdiction for criminal justice (to be addressed in the 2nd Additional Protocol to the Budapest Convention). (extracted from118.txt)
Justice sector Non-discrimination Data quality & security Transparency Impartiality Fairness Freedom of choice/ Independence of judges (decision -making process) Human control/oversight Guarantees of the right of access to the judge Guarantees of the right to a fair trial Precautionary principle for applications missing fundamental transparency requirements Congress Transparency Human control (oversight) Impartiality Right to privacy Data security Cyber security Non-discrimination Inclusive cities Financial sustainability Monitoring safety Service efficiency Digital literacy Democracy and participation – Deep fakes, Microtargeting and propaganda in the framework of electoral processes 109 Democracy and participation Right to free elections Freedom of expression Right of individuals to access the internet Right to private life; Data protection Equality of opportunity for parties and candidates Requirement of a neutral attitude by state authorities with regard to the election campaign, to coverage by the media, and to public funding of parties and campaigns Requirement of a minimum access to privately owned audio -visual media, with regard to the election campaign and to advertising, for all participants in elections Transparency in campaign funding Prevention of improper influence on political decisions through financial donations Responsible, accurate and f air media coverage of electoral campaigns; right of reply, modalities of disseminating opinion polls, transparency requirements on paid advertising content; media pluralism Network neutrality Protection of individuals with regard to the collection and proc essing of personal data on information highways Non-discrimination Data quality & security Transparency Impartiality Fairness Freedom of choice/ Independence of judges (decision -making process) Human control/oversight Guarantees of the right of access to the judge Guarantees of the right to a fair trial Balance between sometimes conflicting rights such as e.g. (extracted from118.txt)
Democracy (excluding issues relating to elections and electoral cycle) Transparency Impartiality Fairness Freedom of choice Freedom of expression Freedom of assembly and association Access to information Human control/oversight Diversity Equality Non-discrimination Data quality & security Data protection Independence - Role of intermediaries - Tech & digital literacy - Question of who owns the data - Democratic oversight - Open data and open government - Risk assessment Good Governance - Non-discrimination - Data quality & security - Impartiality - Fairness - Participation, Representation Fair Conduct of Elections - Responsiveness - Efficiency and Effectiveness - Openness and Transparency - Rule of Law - Ethical Conduct - Competence and Capacity - Innovation and Openness to Change - Sustainability and Long -term Orientation - Sound Financial Management - Human rights, Cultural Diversity and Social Cohesion - Accountability - Redress mechanisms - Access to remedy - Independence - Democratic oversight - Access to remedy and redress mechanisms in case of automated and algorithmic decisions making by public officials - Role of intermediaries - Tech literacy & competences - Questions of who actually owns the data - Open data and open government - Civil and criminal liability - Risk assessments and risk management 111 Gender equality including violence against women Equality and non -discrimination Integrity / Elimination of violence (against women) Equal access to justice Guarantees of the right to a fair trial and to redress Un-biased data (Gender) inclusiveness of AI as a sector AI as an employment sector respecting labour and social rights Data quality & security Transparency & explainability Accountability Impartiality Fairness Human control/oversight Digital literacy and closing existing digital (gender) gaps, essential with regards to right to redress – if citizens & consumers do not understand AI, they will not be able to claim their rights Precautionary principle for applications missing fundamental transparency requirements Ethical principles such as “do no harm” are not respected because some of the spyware apps are developed and advertised for the sole purpose of “knowing what your wife is up to”. (extracted from118.txt)
Such committees may play an especially important role in areas where transparency and stakeholder engagement can be more difficult due to competing interests and rights, such as in the fields of predictive justice, crime prevention and detection. (extracted from118.txt)
Acknowledgments The authors would like to thank Cedric Sabbah, Director of International Cybersecurity & IT Law at the Office of the Deputy Attorney General (International Law), within Israel's Ministry of Justice, Prof. (extracted from118.txt)
In the development of each of those projects, Digital Israel has worked with in -house counsel and Ministry of Justice constitutional counsel, to ensure that the development an d deployment of each project complies with applicable constitutional and administrative law limitations Digital health The project has ambitious goals, including:242 • Customized treatment: promoting research, development and implementation of tools that allow the patient to receive the best and most personalized treatment; • Promoting health and patient prevention through use of digital tools in a way that shifts the focu s from patient care to preventive medicine; • Sustainable health: promoting the development and implementation of systems that increase the operational and managerial effectiveness of the health system, in a way that frees up existing resources; • Development and implementation of digital tools that streamline communication between the Ministry of Health and those it serves; • Delivery of emergency treatment services through an appointment management system and an application informing the patient on the progress of the treatment. (extracted from118.txt)
Human rights: A path for solutions Topics Instruments and mechanisms Publications News UN Human Rights Office Press releases 29 September 2025 Madagascar: UN Human Rights Chief shocked by violent response to electricity and water protests Press releases 26 September 2025 South Sudan South Sudan: Türk alarmed by deteriorating human rights situation amid rising violence and political tensions Press releases 24 September 2025 Attacks on Gaza-bound flotilla defy belief, accountability a must View more news View speeches and statements Independent Experts and Committees Special Procedures Press releases 01 October 2025 UN experts call for reparatory justice for enslavement, the trade in enslaved Africans and colonialism Press releases 01 October 2025 China UN experts urge China to end repression of Uyghur and cultural expression of minorities Press releases 30 September 2025 Democratic Republic of Congo: UN expert warns of abuses against women human rights defenders and their families in South Kivu View all Special Procedures News Treaty Bodies Press releases 30 September 2025 Economic, social and cultural rights Development cannot be achieved on dying planet, UN committee issues new guidance Press releases 29 September 2025 UN Committee on Economic, Social and Cultural Rights publishes findings on Chile, Colombia, Laos, Netherlands, Russia, and Zimbabwe Media advisories 19 September 2025 Enforced and involuntary disappearances UN Committee on Enforced Disappearances to review Montenegro, Benin, and Sri Lanka View all Treaty Bodies News Human Rights Council Press releases 24 September 2025 People of African descent UN experts: Systemic racism against Africans and people of African descent in criminal justice systems is pervasive Press releases 24 September 2025 Sudan: UN Fact-Finding Mission urges immediate action after deadly mosque strike in El Fasher Statements and speeches 23 September 2025 Oral Update by Mr. (extracted from656.txt)
Mechanisms and processes in place should ensure that access to remedies is speedy and child-fr iendly, and provides appropriate redress to children.69 .S tates should ensure that in all cases access to courts or judicial review of administr ative remedies and other procedures are available, in line with the principles set out in the Guidelines of the Committee of Ministers of the Council of Europe on child-fr iendly justice (2010).7 0.S tates should, where appropriate, also provide children and/or their parents or legal representatives with non-judicial mechanisms, administrative or other means t o seek remedy, such as through ombudspersons for children and other national human rights institutions and data-protection authorities. (extracted from130.txt)
Algorithmic Impact Assessment tool - Canada.ca Skip to main content Skip to "About government" Language selection Français fr / Gouvernement du Canada Search Search Canada.ca Search Menu Main Menu Jobs and the workplace Immigration and citizenship Travel and tourism Business and industry Benefits Health Taxes Environment and natural resources National security and defence Culture, history and sport Policing, justice and emergencies Transport and infrastructure Canada and the world Money and finances Science and innovation Manage life events You are here: Canada.ca About government Government in a digital age Digital government innovation Responsible use of artificial intelligence in government Algorithmic Impact Assessment tool On this page 1. (extracted from642.txt)
Page details Date modified: 2025-06-24 About this site Government of Canada All contacts Departments and agencies About government Themes and topics Jobs Immigration and citizenship Travel and tourism Business Benefits Health Taxes Environment and natural resources National security and defence Culture, history and sport Policing, justice and emergencies Transport and infrastructure Canada and the world Money and finances Science and innovation Indigenous Peoples Veterans and military Youth Manage life events Government of Canada Corporate Social media Mobile applications About Canada.ca Terms and conditions Privacy (extracted from642.txt)
C’est typiquement le raisonnement utilisé par la justice pour adapter la jurisprudence à une nouvelle situation . (extracted from497.txt)
En particulier, ce premier rapport se conclut sur l ’idée que les technologies d’intelligence artificielle font émerg er un potentiel d’amélioration de la vie des citoyens en ouvrant de nouveaux marchés et de nouvelles opportunités permettant de résoudre ce rtains des grands enjeux sociétaux : la santé, les transports, l’éducation, l’énergie , l’environnement, la justice, la sécurité ou encore l’efficacité du gouvernement. (extracted from497.txt)
5 Article « Le droit à l’épreuve de l’intelligence artificielle » du 28 novembre 2016 paru dans la revue Village de la Justice. (extracted from497.txt)
Lors d’un colloque à New York sur les défis posés p ar l’émergence de l’intelligence artificielle, organisé le 14 octo bre 2015 par l’Institut de recherche sur la criminalité et la justice des Nati ons Unies (UNICRI), Max Tegmark était invité avec un autre expert 3 à s’exprimer devant quelques 1 L’Institut a été fondé en mars 2014 par Max Tegmar k cosmologiste au MIT, Jaan Tallinn cofondateur de Skype, Anthony Aguirre physicien à l’U CSC et deux étudiants (Viktoriya Krakovna et Meia Chita-Tegmark), figurent à son conseil consult atif l’informaticien Stuart J. (extracted from497.txt)
In fact, without being able to explain decisions taken by autonomous systems, it is difficult to justify them: it would seem inconceivable to accept what cannot be The accountability of systems based on machine learning constitutes a real scientifi c challenge 116 justified in areas as crucial to the life of an individual as acce ss to credit, employment, accommodation, justice and health. (extracted from468.txt)
Several of the measures that were adopted in recent years have widened and improved group access to litigation; in particular, the French Act for the Modernization of Justice in the Twenty -First Century introduced the ‘personal data’ class action which allows associations of consumer protection to act when infringements to existing legislation occur. (extracted from468.txt)
Other rights are also affected; for example, an automated system used in the justice system, which is based on poor quality data, can negatively impact on the right to a fair trial and effective remedy, as well as on the principle of good administration. (extracted from326.txt)
7 Committee on Civil Liberties, Justice and Home Affairs (2018). (extracted from326.txt)
If one gender is under-repre sented or if sexist behaviour is represented in the training data28, AI can increase inequality between men and women.29 Access to a fair trial and effective remedies (Article 47 of the Charter) can also be impacted, particularly if algorithms are used in the area of crime preven tion and the criminal justice system. (extracted from326.txt)
The use of AI and algorithms in the area of justice needs testing, as highlighted in the CEPEJ European ethical char ter.30 One potential problem might be the use of biased data for automated systems.31 In addition, to enjoy access to a fair trial and effective reme dies, in cases where someone claims to have been mistreated by an AI-system or wants to challenge a decision based on an algorithm, information on how the system or algorithm works is essential. (extracted from326.txt)
17.Committee on Civil Liberties, Justice and Home Affairs (2018), Opinion of the Committee on Civil Liberties, Justice and Home Affairs for the Committee on Industry, Research and Energy on a comprehensive European industrial policy on artificial intelligence and robotics , (2018/2088(INI)), 11 December 2018. (extracted from326.txt)
(forthcoming), ‘Dirty Data, Bad Predictions: How Civil Rights Violations Impact Police Data, Predictive Policing Systems, and Justice’ , New York University Law Review Online . (extracted from326.txt)
FRA – EUROPEAN UNION AGENCY FOR FUNDAMENTAL RIGHTS Schwarzenbergplatz 11 – 1040 Vienna – Austria Tel: +43 158030-0 – Fax: +43 158030-699 fra.europa.eu facebook.com/fundamentalrights linkedin.com/company/eu-fundamental-rights-agency twitter.com/EURightsAgency© European Union Agency for Fundamental Rights, 2019 Print: ISBN 978-92-9474-605-4, doi:10.2811/615718 PDF: ISBN 978-92-9474-606-1, doi:10.2811/546219 TK-01-19-330-EN-C (print); TK-01-19-330-EN-N (PDF)Further information: The following FRA publications offer further information relevant to the topic of the paper: • #BigData: Discrimination in data-supported decision making (2018) http://fra.europa.eu/en/publication/2018/big-data-discrimination • Under watchful eyes: biometrics, EU IT systems and fundamental rights (2018) http://fra.europa.eu/en/publication/2018/biometrics-rights-protection • Fundamental rights and the interoperability of EU information systems: borders and security (2017) http://fra.europa.eu/en/publication/2017/fundamental-rights-interoperability • Surveillance by intelligence services: fundamental rights safeguards and remedies in the EU - Volume II: field perspectives and legal update (2017) http://fra.europa.eu/en/publication/2017/surveillance-intelligence-socio-lega • Surveillance by intelligence services: fundamental rights safeguards and remedies in the European Union - Mapping Member States’ legal frameworks (2015) http://fra.europa.eu/en/publication/2015/surveillance-intelligence-services • The impact on fundamental rights of the proposed Regulation on the European Travel Information and Authorisation System (ETIAS) (2017) http://fra.europa.eu/en/opinion/2017/etias-impact • Handbook on European data protection law - 2018 edition (2018) https://fra.europa.eu/en/publication/2018/handbook-european-data-protection-law • Handbook on European law relating to access to justice (2016) https://fra.europa.eu/en/publication/2016/handbook-european-law-relating-access-justice • Handbook on European non-discrimination law – 2018 edition (2018) http://fra.europa.eu/en/publication/2018/handbook-european-law-non-discrimination (extracted from326.txt)
20 UK, High Court of Justice (Queens’ Bench Division – Divisional Court Cardiff), The Queen (OTAO) Bridges and Chief Constable of South Wales Police and others , [2019] EWCH 2341 (Admin), 4 September 2019. (extracted from327.txt)
The Data Protection Commissioner of Hamburg ( Hamburgische Beauftragte für Datenschutz und Informa tionsfreiheit ) issued a report about the use of facial recognition technologies at the G20 and found that the use of the technology did not comply with data 50 UK, High Court of Justice (Queens’ Bench Division – Divisional Court Cardiff), The Queen (OTAO) Bridges and Chief Constable of South Wales Police and others , [2019] EWCH 2341 (Admin), 4 September 2019, para. (extracted from327.txt)
As an additional safeguard, the Agency for the Operational Management of Large-Scale Informa tion Technology Systems (eu-LISA)62 is responsible for quality assurance safeguards and reports regularly 62 For an overview of the role and tasks of eu-LISA, see Chapter II of Regulation (EU) 2018/1726 of the European Parliament and of the Council of 14 November 2018 on the European Union Agency for the Operational Management of LargeScale IT Systems in the Area of Freedom, Security and Justice (eu-LISA), and amending Regulation (EC) No 1987/2006 and Council Decision 2007/533/JHA and repealing Regulation (EU) No 1077/2011, OJ L 295, 21.11.2018, pp. (extracted from327.txt)
This research project will examine “how facial recognition is currently being used for the investigation of crime across EU Mem ber States.” It will also give particular consideration to the potential for implementing the exchange of facial images within the Prüm framework.72 The pro ject is implemented by the Forensics Departments of Finland, Latvia, Sweden and the Netherlands, under the leadership of the Estonian Ministry of Justice. (extracted from327.txt)
The Court of Justice of the EU (CJEU) has confirmed in its case law that the fundamental right to dignity is part of EU law.78 Biometric data, including facial images, must be pro cessed in a manner that respects human dignity. (extracted from327.txt)
150 International Justice and Public Safety Network (2011), Privacy Impact Assessment Report for the Utilization of Facial Recognition Technologies to Identify Subjects in the Field , 30 June 2011, p. (extracted from327.txt)
Minister for Justice, Equality and Law Reform, Ireland, Attorney General , 8 May 2014, para. (extracted from327.txt)
See also: FRA and CoE (2016), Handbook on European law relating to access to justice , Luxembourg, Publications Office, June 2016, p. (extracted from327.txt)
Case 36: Automated credit scoring is not qualifying ADM if a human ultimately decides whether to grant a loan or not In anearly pre-GDPR ruling from 2014, the German Federal Court of Justice (Bundesgerichtsoft ) stated that “credit-scoring only amounts to an automated individual decision where the responsible body takes a decision with a legal consequence for the person concerned or a decision that has a significant impact on the person concerned, solely on the basis of a score result without further examination of the content. (extracted from333.txt)
Article 2 from the initial version of the law stated that “ Aucune décision de justice impliquant une appréciation sur un comportement humain ne peut avoir pour fondement un traitement automatisé d’informations donnant une définition du profil ou de la personnalité de l’intéressé. (extracted from333.txt)
150 CJEU, Order of the President of the Court of Justice “Deletion” in Case C-552/21, January 25, 2022, ECLI:EU:C:2022:105. (extracted from333.txt)
Advisory statement on human ethics in artificial intelligence and big data research (2017) - National Research Council Canada Skip to main content Skip to "About government" Language selection Language selection Françaisfr / Gouvernement du Canada Search Search Menu Main Menu Home Jobs and the workplace Immigration and citizenship Travel and tourism Business and industry Benefits Health Taxes Environment and natural resources National security and defence Culture, history and sport Policing, justice and emergencies Transport and infrastructure Canada and the world Money and finances Science and innovation You are here Canada.ca National Research Council Canada Corporate Values and ethics Research involving human participants Advisory statement on human ethics in artificial intelligence and big data research (2017) 1. (extracted from482.txt)
Related links Research involving human participants From: National Research Council Canada Date modified: 2019-03-26 About this site National Research Council Canada Contact the NRC Order products Directory of science professionals Government of Canada All Contacts Departments and agencies About government Themes and topics Jobs Immigration and citizenship Travel and tourism Business Benefits Health Taxes Environment and natural resources National security and defence Culture, history and sport Policing, justice and emergencies Transport and infrastructure Canada and the world Money and finance Science and innovation Indigenous peoples Veterans and military Youth Government of Canada Corporate Social media Mobile applications About Canada.ca Terms and conditions Privacy Top of Page (extracted from482.txt)
Apply EUR-Lex Access to European Union law This document is an excerpt from the EUR-Lex website You are here EUROPA EUR-Lex home EUR-Lex - 52021DC0118 - EN Help Print Menu EU law Treaties Treaties currently in force Founding Treaties Accession Treaties Other treaties and protocols Chronological overview Legal acts Consolidated texts International agreements Preparatory documents EFTA documents Lawmaking procedures Summaries of EU legislation Browse by EU institutions European Parliament European Council Council of the European Union European Commission Court of Justice of the European Union European Central Bank European Court of Auditors European Economic and Social Committee European Committee of the Regions Browse by EuroVoc EU case-law Case-law Reports of cases Directory of case-law Official Journal Access the Official Journal Official Journal L series daily view Official Journal C series daily view Browse the Official Journal Legally binding printed editions Special editions National law and case-law National transposition National case-law JURE case-law Information Themes in focus EUR-Lex developments Statistics ELI register About ELI Technical information ELI implementation overview Resources for implementing ELI ELI highlights ELI testimonials Legislation in schema.org EU budget online Quick search Use quotation marks to search for an "exact phrase". (extracted from125.txt)
Using Green Public Procurement criteria 34 can boost demand for a green digital transformation The digital transformation should also enable modern and efficient justice systems 35 , enforcement of consumer rights and an increased effectiveness of public action including law enforcement and investigation capacities 36 – what is illegal offline is also illegal online, and law enforcement must be best equipped to deal with more and more sophisticated digital crimes. (extracted from125.txt)
The digital principles are rooted in primary EU law, notably the Treaty on European Union (TEU), the Treaty on the Functioning of the European Union (TFEU), the Charter of Fundamental Rights and the case-law of the Court of Justice of the European Union, as well as in secondary legislation 37 . (extracted from125.txt)
(34) https://ec.europa.eu/environment/gpp/eu_gpp_criteria_en.htm (35) Communication from the Commission on the Digitalisation of justice in the European Union A toolbox of opportunities, COM(2020) 710 final. (extracted from125.txt)
Help pages Contact Sitemap Follow us X Legal Legal notice Cookies policy Accessibility Privacy statement Information About EUR-Lex Newsletter Useful links Other services European Data EU tenders EU research results EU Whoiswho EU publications N-Lex EU Law Tracker Discover more on europa.eu Contact the EU Call us 00 800 6 7 8 9 10 11 Use other telephone options Write to us via our contact form Meet us at one of the EU centres Social media Search for EU social media channels Legal Languages on our websites Privacy policy Legal notice Cookies EU institutions European Parliament European Council Council of the European Union European Commission Court of Justice of the European Union (CJEU) European Central Bank (ECB) European Court of Auditors European External Action Service (EEAS) European Economic and Social Committee European Committee of Regions (CoR) European Investment Bank European Ombudsman European Data Protection Supervisor (EDPS) European Data Protection Board European Personnel Selection Office Publications Office of the European Union Agencies Switch to mobile Switch to desktop (extracted from125.txt)
See, e.g., Inioluwa Deborah Raji and others, “Closing the AI accountability gap: defining an end-to-36end framework for internal algorithmic auditing”, 3 January 2020.6 A/HRC/48/31 Artificial intelligence in law enforcement, national security, criminal justice and border management 22. (extracted from657.txt)
States are increasingly integrating AI systems into law enforcement, national security, criminal justice and border management systems. (extracted from657.txt)
18; and Court of Justice of the European Union, Digital Rights Ireland and Others, 76C-293/12 and C-594/12, para. (extracted from657.txt)
See also Court of Justice of the European Union, Maximilan Schrems v. (extracted from657.txt)
The submissions from 82Freedom Online Coalition, Global Network Initiative and Global Partners Digital contain support for risk-based regulation.12 A/HRC/48/31are particularly high, such as law enforcement, national security, criminal justice, social 83protection, employment, health care, education and the financial sector, should have priority. (extracted from657.txt)
These systems affect government approaches to policing and the administration of justice, determine the accessibility of public services, decide who has a chance to be recruited for a job, and affect what information people see and can share online. (extracted from657.txt)
36 Anjanette Raymond, The Dilemma of Private Justice Systems: Big Data Sources, the Cloud and Predictive Analytics , NORTHWESTERN JOURNAL OF INTERNATIONAL LAW & BUSINE SS, FORTHCOMING (2014), http://papers.ssrn.com/sol3/papers.cfm?abstract_id=2469291 (last visited Jul 22, 2015); Kate Crawford, Can an Algorithm be Agonistic? (extracted from119.txt)
56 Jeroen van den Hoven & Emma Rooksby, Distributive justice and the value of information: A (broadly) Rawlsian approach , 376 INFORMATION TECHNOLOGY AND MORAL PHILOSOPHY (2008). (extracted from119.txt)
Models of the (ideal) doctor patient relationship have adapted over time in recognition of the growing importance of patient autonomy and its appropriate balance with other ethical obligations of the doctor towards beneficence, non -maleficence, and justice.124 An influential paper from Emanuel and Emanuel (1992) proposed four models for the doct or-patient relationship: ► Paternalistic Model – This model vests the vast majority of decision -making power in the doctor. (extracted from119.txt)
For example, approaches to care ethics and feminist ethics focus on related goods such as the caring role of t he health professional , relationships and care responsibilities (in contrast to a focus on justice and rights) ,135 tacit knowledge and context -sensitive care that respond s to the interests and needs of patients as unique, socially embedded individuals , and power imbalances and coercion owing to the vulnerable position of the patient. (extracted from119.txt)
NUSSBAUM , FRONTIERS OF JUSTICE DISABILITY , NATIONALITY , SPECIES MEMBERSHIP (OIP): DISABILITY , NATIONALITY , SPECIES MEMBERSHIP (TANNER LECTURES ON HUMAN VALUES ) (New Ed ed. (extracted from119.txt)
As with all practices, phronesis or prudence is a central virtue in medicine, without which other virtues cannot be incorporated into behaviour through virtuous act s.246 Justice, truthfulness and courage are also necessary to protect medicine from the corrupting power of medical institutions, including hospitals, paying organisations and government departments.247 These three core virtues are necessary for continuous revision of standards of excellence and internal goods by practitioners, which requires critical self -reflection on the relationship between one’s actions and the norms of the practice, or the institutional influence on the definition and realisation of nor ms.248 Justice is defined broadly as “the strict habit of rendering what is due to others,”249 or “the virtue of rewarding desert and of repairing failures in rewarding desert within an already constituted community.”250 To be just, standards for treating people in a community must be “uniform and impersonal,” meaning it is unjust to favour personal acquaintances. (extracted from119.txt)
In social or national healthcare systems, justice can be applied to the distribution of medical resources ( e.g., pharmaceuticals, treatments, clinical encounters) in a manner fair to all stakeholders. (extracted from119.txt)
Justice is not merely a quantitative notion, by which all stakeholders receive an equal share, but instead requires matching resources to the needs of the patient a nd making judgments between the relative importance of different needs. (extracted from119.txt)
AI Now produces interdisciplinary research to help ensure that AI systems are accountable to the communities and contexts they are meant to serve, and that they are applied in ways that promote justice and equity. (extracted from18.txt)
Our most recent publications include: ● Dirty Data, Bad Predictions: How Civil Rights Violations Impact Police Data, Predictive Policing Systems, and Justice​ , an article on how “dirty-policing” practices and policies shape the environment and the methodology by which data is created, raising the risk of creating inaccurate, skewed, or systematically biased “dirty data.” ● Anatomy of an AI System​ , a large-scale map and longform essay produced in partnership with ​ SHARE Lab​ , which investigates the human labor, data, and planetary resources required to operate an Amazon Echo. (extracted from18.txt)
​ Advances in understanding of bias, fairness, and justice in machine learning research make it clear that assessments of risks and harms are imperative. (extracted from18.txt)
This includes examples such as criminal justice advocates working to halt the use of discriminatory predictive policing tools, tenants-rights groups opposing facial recognition in housing, and a coalition of Latinx activists, tech workers, and students exposing and protesting lucrative tech company contracts with military and border agencies. (extracted from18.txt)
Not only are James’ views counter to Google’s stated values, but they are directly counter to the project of ensuring that the development and application of AI prioritizes justice over profit.”​ 73​ Following the backlash, Google dissolved ATEAC after a little over a week.​ 74 Yet even if one believes that corporate AI ethics might help guide better tech practices on some level, it is clear that change in the design, development, and implementation of AI systems largely occurs when there is pressure on companies from workers, the press, and policymakers. (extracted from18.txt)
Barbara Grosz, a professor of natural sciences, imagines a world in which “every time a computer scientist logs on to write an algorithm or build a system, a message will flash across the screen that asks, ‘Have you thought about the ethical implications of what you’re doing?’”​ 80​ The Design Justice Network takes this further, centering justice, not ethics, and calling on developers and designers to center affected communities in the process of creating technology together.​ 81 AI developers and researchers make important determinations that can affect billions of people, and helping them consider whom the technology benefits and harms is important. (extracted from18.txt)
As during the dot-com boom and foreclosure crisis, numerous organizations and collectives formed to organize for housing justice. (extracted from18.txt)
Accordingly, housing justice groups such as Social Housing Now (Căsi Sociala Acum) are in the midst of organizing against evictions and for the development of social housing.​ 97 Back in the North, there have been new forms of international solidarity in the works against AI displacement. (extracted from18.txt)
Thus they have been organizing marches, Google bus blockades, and City Council demonstrations.​ 98​ Much of this has taken place in solidarity with organizers and groups in Berlin such as Google Is Not a Good Neighbor (​ Google ist kein guter Nachbar)​ , which in 2018 collectively blocked Google from launching a new tech campus in the neighborhood of Kreuzberg.​ 99​ Solidarity has also been found among New York City organizers who successfully fought the development of a new Amazon campus in 2019, and with activists in Toronto committed to thwarting gentrification induced by Sidewalk Labs.​ 100 During demonstrations, banners, light projections, video clips, and statements of support have expressed international solidarity, revealing a new trend toward urban justice.​ 101​ Much work remains to link struggles against forms of tech-sector displacement worldwide. (extracted from18.txt)
Because AI technologies are often applied in ways that amplify and exacerbate historical patterns of inequality and discrimination, it is these historical practices—not AI systems alone—to which organizers and communities seeking justice are reacting. (extracted from18.txt)
In July, Mijente joined Media Justice (an organization at the helm of San Francisco’s facial-recognition ban)​ 114​ and Tech Workers Coalition​ 115​ to host Take Back Tech. (extracted from18.txt)
This work helped shed light on lucrative tech company contracts with military and border agencies, and mobilized tech workers and students, while also emphasizing the human cost of a deportation campaign rife with human rights abuses.​ 118​ Protesters catalyzed by the campaign have held regular demonstrations at Palantir’s headquarters in Palo Alto and at its New York City offices.​ 119 Organizations such as Never Again Action,​ 120​ and Jews for Racial and Economic Justice (JFREJ)​ 121​ have also led highly visible actions against Amazon, organizing street protests and sit-ins in Amazon bookstores to protest against the company’s ongoing work providing cloud computing services to ICE.​ 122​ And Immigrant rights groups such as Make the Road New York,​ 123 along with Mijente, JFREJ, and other advocates, have reached out to academics and computer science and technology professionals through petitions, demanding that prominent conferences drop Palantir as a sponsor, given the company’s role in empowering ICE.​ 124​ Community-organized opposition to Palantir’s role in ICE’s detention of immigrants resulted in UC Berkeley’s Privacy Law Scholars Conference,​ 125​ Lesbians Who Tech,​ 126​ and the Grace Hopper Celebration all pulling Palantir as a sponsor.​ 127 Athena, a recently launched coalition, takes this further. (extracted from18.txt)
But they also organized around issues like Amazon’s treatment of warehouse workers and its sale of surveillance tech.​ 129​ Athena expands on this multi-issue approach, recognizing that Amazon is at the heart of a set of interlocking issues, including worker rights at warehouses, climate justice, and mass surveillance. (extracted from18.txt)
The pushback against AI thus builds upon the social justice work that organizers have engaged in for a much longer time. (extracted from18.txt)
Worker organizing around AI is also part of a broader tech-worker movement focused on a broad range of social justice issues, including displacement,​ 145​ two-tiered workforces and the exploitation of contract workers,​ 146​ and climate change. (extracted from18.txt)
Building on the emergence of globally oriented data protection approaches such as the European Union’s General Data Protection Regulation (GDPR), policymakers are moving quickly, driven both by the current sense of urgency to regulate the mass deployment of AI technologies lacking discernible safeguards and by the failure of ethical frameworks to adequately answer the call for accountability and justice. (extracted from18.txt)
Addressing the specific case of forensic algorithms like automated software used to analyze DNA and predict potential suspects, the Justice in Forensic Algorithms Act of 2019​ 246​ prohibits companies from withholding information about their system, such as its source code, from a defendant in a criminal proceeding on trade-secrecy grounds. (extracted from18.txt)
The documents showed that the Federal Bureau of Investigation (FBI) and ICE were using state driver’s license databases as “the bedrock of an unprecedented surveillance infrastructure” that relied on facial-recognition technology.​ 289​ The US Justice Department also recently announced plans to collect DNA data from migrants crossing the border, which could create more invasive monitoring of immigrants without any real limits.​ 290 Outside the US, governments are equally eager to pilot AI systems at border checkpoints. (extracted from18.txt)
Though it declined to provide any details on how it is being used by customers, it indicated retail as a potential use case, illustrating how stores can feed live images of shoppers to detect emotional and demographic trends.​ 401 Employment has also experienced a surge in the use of affect recognition, with companies like HireVue and VCV offering to screen job candidates for qualities like “grit” and to track how often they smile.​ 402​ Call center programs Cogito and Empath use voice-analysis algorithms to monitor the reactions of customers and signal to call agents when they sound distressed.​ 403​ Similar programs have been proposed as an assistive technology for people with autism,​ 404​ while Boston-based company BrainCo is creating headbands that purport to detect and quantify students’ attention levels through brain-activity detection,​ 405​ despite studies that outline significant risks associated with the deployment of emotional AI in the classroom.​ 406 Affect-recognition software has also joined risk assessment as a tool in criminal justice. (extracted from18.txt)
This is particularly concerning in contexts such as employment, education, and criminal justice. (extracted from18.txt)
Despite the fact that social sciences and humanities approaches have a long history in information security and risk management,​ 491​ research that addresses both social and technical dimensions in security is necessary, but still relatively nascent.​ 492​ Central in this challenge is redrawing the boundaries of analysis and design to expand beyond the algorithm,​ 493​ and securing channels for all affected stakeholders to democratically steer system development and to dissent when concerns arise.​ 494 CONCLUSION Despite the growth of ethical frameworks, AI systems continue to be deployed rapidly across domains of considerable social significance—in healthcare, education, employment, criminal justice, and many others—without appropriate safeguards or accountability structures in place. (extracted from18.txt)
See Vidushi Marda, “Introduction” in APC, Article 19, and SIDA, “Artificial Intelligence: Human Rights, Social Justice and Development,” Global Information Watch 2019, November 2019, https://giswatch.org/sites/default/files/gisw2019_artificial_intelligence.pdf​ . (extracted from18.txt)
Design Justice Network Principles, accessed November 24, 2019, https://designjustice.org/read-the-principles​ . (extracted from18.txt)
“The Anti-Eviction Mapping Project: Counter Mapping and Oral History Toward Bay Area Housing Justice.” ​ Annals of the American Association of Geographers​ 108, no. (extracted from18.txt)
Media Justice, accessed November 24, 2019, ​ https://mediajustice.org/ 115. (extracted from18.txt)
Jews for Racial and Economic Justice, accessed November 24, 2019, ​ https://jfrej.org/​ . (extracted from18.txt)
Sasha Costanza Chock, “Design Justice: Towards an Intersectional Feminist Framework for Design Theory and Practice,” ​ Proceedings of the Design Research Society 2018​ , June 3, 2018, https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3189696​ . (extracted from18.txt)
Amazon Employees for Climate Justice, “Open letter to Jeff Bezos and the Amazon Board of Directors,” Medium, April 10, 2019, https://medium.com/@amazonemployeesclimatejustice/public-letter-to-jeff-bezos-and-the-amazon-boardof-directors-82a8405f5e38​ . (extracted from18.txt)
Takano Introduces the Justice in Forensic Algorithms Act to Protect Defendants’ Due Process Rights in the Criminal Justice System,” Takano, September 17, 2019, https://takano.house.gov/newsroom/press-releases/rep-takano-introduces-the-justice-in-forensic-algorith ms-act-to-protect-defendants-due-process-rights-in-the-criminal-justice-system​ . (extracted from18.txt)
Bobby Allyn and Joel Rose, “Justice Department Announces Plan to Collect DNA from Migrants Crossing the Border,” NPR, October 21, 2019, https://www.npr.org/2019/10/21/772035602/justice-department-announces-plan-to-collect-dna-from-migr ants-crossing-the-bord​ . (extracted from18.txt)
Gray and Siddharth Surl, ​ Ghost Work: How to Stop Silicon Valley from Building a New Global Underclass ​ (Boston: Houghton Mifflin Harcourt, 2019); Kate Crawford and Vladan Joler, Anatomy of an AI System, 2018, ​ https://anatomyof.ai​ ; Muqing Zhang, “Colonialism Is Alive in the Exploited Tech Work Force”, Outline​ , June 6, 2019 https://theoutline.com/post/7533/colonialism-is-alive-in-the-exploited-tech-work-force?zd=2&zi=exrbzkaf​ ; APC, Article 19, and SIDA, “GISWatch 2019 - Artificial Intelligence: Human rights, social justice and development,” November 2019, ​ https://giswatch.org/sites/default/files/gisw2019_artificial_intelligence.pdf​ ; AI Now 2019 Report | 87 Noopur Raval, “Developing a Framework for Postcolonial Digital Labor,” unpublished manuscript, 2017, https://www.academia.edu/35413303/Developing_a_framework_for_postcolonial_digital_labor​ . (extracted from18.txt)
care or justice)? (extracted from30.txt)
23 Note to EGE (2018): AI should contribute to global justice and equal access to the benefits and advantages that AI, robotics and ‘autonomous’ systems can bring. (extracted from30.txt)
Erosion of the democratic constitutional state in Europe 26 See: https://www.theguardian.com/news/series/cambridge-analytica-files 27 Note to EGE (2018): Rule of law, access to justice and the right to redress and a fair trial provide the necessary framework for ensuring the observance of human rights standards and potential AI specific regulations. (extracted from30.txt)
Thinking about who products are being built for, who they might unintentionally exclude, how product use and product design can protect vulnerable populations, especially now that we’re in the middle of a racial justice crisis and a pandemic, which are disproportionately affecting people of colour and people of lower incomes. (extracted from723.txt)
1 Algorithms in decision-making Contents Summary 3 1 Introduction 7 Our inquiry 10 2 Applications and bias 11 Data sharing 11 In the health sector 11 In the criminal justice system 13 In the web and social media sector 14 Government data sharing and getting value from its data 15 Bias 18 Training data 19 Insufficient data 20 Correlation without causation 21 Lack of representation in the algorithm development community 22 3 Accountability and transparency 24 Accountability 24 Principles and codes 25 Audit and certification 26 Ethics boards 27 Transparency 27 The right to explanation 29 4 The Centre for Data Ethics & Innovation, research and the regulatory environment 32 ‘Automated’ decisions 33 Consent 35 Data protection impact assessments 36 The Information Commissioner’s powers 37 Sector regulation 39 Algorithms in decision-making 2 Conclusions and recommendations 41 Formal minutes 45 Witnesses 46 Published written evidence 47 List of Reports from the Committee during the current Parliament 50 3 Algorithms in decision-making Summary Algorithms have long been used to aid decision-making, but in the last few years the growth of ‘big data’ and ‘machine learning’ has driven an increase in algorithmic decision-making—in finance, the legal sector, the criminal justice system, education, and healthcare, as well as recruitment decisions, giving loans or targeting adverts on social media, and there are plans for autonomous vehicles to be on public roads in the UK. (extracted from535.txt)
The range of different industries in which machine learning is already being put to use includes finance (including access to loans and insurance), the legal sector, the criminal justice system, education, and healthcare, as well as recruitment decisions and targeting adverts on social media, 12 and there are plans for driverless vehicles to be on public roads in the UK in the near future. (extracted from535.txt)
37 In our current inquiry we examined the way data sharing is affecting three sectors in particular—in healthcare, criminal justice and social media. (extracted from535.txt)
65 In the criminal justice system 18. (extracted from535.txt)
In the criminal justice system, algorithms are being used by some police forces for facial image recognition. (extracted from535.txt)
76 64 Qq232, 234, 235 65 Q230 66 Big Brother Watch, Big Brother Watch Briefing for Short Debate on the use of facial recognition technology in security and policing in the House of Lords (March 2018), p8 67 Letter from Baroness Williams of Trafford to the Committee , 28 March 2018 68 Oxford Internet Institute ( ALG0031 ) 69 UCL Jill Dando Institute of Security and Crime Science ( ALG0048 ) para 5 70 Marion Oswald and Sheena Urwin submission; See also, “ Pre-crime software recruited to track gang of thieves “, New Scientist, 11 March 2015 71 RUSI, Big Data and Policing 2017 (September 2017), p20 72 UCL Jill Dando Institute of Security and Crime Science 73 Sheena Urwin, Head of Criminal Justice, Durham Constabulary ( ADM0032 ) 74 Institute of Mathematics and its Applications ( ALG0028 ) para 19 75 Durham Constabulary ( ALG0041 ) 76 HM Inspectorate of Constabulary, PEEL: Police Effectiveness 2016 (March 2017), p33 Algorithms in decision-making 14 Box 2: Durham Constabulary’s use of algorithms The Harm Assessment Risk Tool (HART), designed as a result of a collaboration between Durham Constabulary and Dr Barnes of University of Cambridge, is a decision support system used to assist officers in deciding whether a suspect is eligible for deferred prosecution based on the future risk of offending. (extracted from535.txt)
80 Professor Louise Amoore questioned whether there is a “place for inference or correlation in the criminal justice system” 81 since, unlike normal evidence, it cannot be cross-examined or questioned. (extracted from535.txt)
82 Jamie Grace from Sheffield Hallam University accepted its use but wanted “a single [independent] oversight body and regulator for the use of police databases and algorithmic analysis in criminal justice”. (extracted from535.txt)
They are also moving into areas where the benefits to those applying them may not be matched by the benefits to those subject to their ‘decisions’—in some aspects of the criminal justice system, for example, and algorithms using social media datasets. (extracted from535.txt)
119 Where algorithms are used in the criminal justice system it is imperative that algorithms are not unfairly discriminatory. (extracted from535.txt)
144 She also questioned the use of inference and correlation in the criminal justice system, and suggested that its use in the US for sentencing “constitutes a violation of due process or overt discrimination”. (extracted from535.txt)
Algorithms in the Criminal Justice System: Assessing the Use of Risk Assessments in Sentencing , accessed 5 April 2018 146 Sheena Urwin, Head of Criminal Justice, Durham Constabulary ( ADM0032 ) 147 Q28 Algorithms in decision-making 22 41. (extracted from535.txt)
In some of our evidence, there was a desire for algorithms within the criminal justice system to be restricted to advisory roles. (extracted from535.txt)
265 256 Q150 257 Sheena Urwin, Head of Criminal Justice, Durham Constabulary ( ADM0032 ) 258 EU GDPR, ‘ GDPR Key Changes ’, accessed 20 March 2018 259 Information Commissioner’s Office ( ALG0038 ) 260 Science and Technology Committee, Fourth Report of Session 2015–16, The big data dilemma , HC 468 261 Q57 262 Q365 263 Science and Technology Committee, Fourth Report of Session 2015–16, The big data dilemma , HC 468, para 66 264 Q66 265 Q245 [Dr Dominic King] Algorithms in decision-making 36 82. (extracted from535.txt)
They are also moving into areas where the benefits to those applying them may not be matched by the benefits to those subject to their ‘decisions’—in some aspects of the criminal justice system, for example, and algorithms using social media datasets. (extracted from535.txt)
Q89–148 Sheena Urwin , Head of Criminal Justice, Durham Constabulary; and Professor Kate Bowers , Academic Director, UCL Jill Dando Institute. (extracted from535.txt)
In addition to strong enforcement of the General Data Protection Regulation (GDPR) and safeguards such as human rights impacts assessments, software transparency and the availability of datasets for public scrutiny, it is vital that the upcoming regulatory proposal establishes in law clear limitations a s to what can be considered lawful uses of AI , to unequivocally address the following issues: • the enabling of biometric m ass surveillance and monitori ng of public spaces; • the exacerbation of structural discrimination , exclusion and collective harms; • the restriction of and discriminatory access to vital services such as health -care and social security; • the surveillance of workers and infringement of workers’ fundamental rights; • the impeding of fair access to justice and procedural rights; • the use of systems which make inferences and predictions about our most sensitive characteristics, beha viours and thoughts; • and, crucially, the manipulation or control of human behaviour and associated threats to human dignity, agency, and collective democracy. (extracted from253.txt)
Use of risk assessment tools in the criminal justice system and pre -trial context The use of algorithms in criminal justice matters to profile individuals within legal decision -making processes presents seve re threats to fundamental rights. (extracted from253.txt)
In addition, substantial evidence has shown that the introduction of such systems in criminal justice systems in Europe and elsewhere has resulted in unjust and discriminator y outcomes. (extracted from253.txt)
We argue that legal limits must be imposed on AI risk assessment systems in the criminal justice context. (extracted from253.txt)
Legal restrictions or legislative red -lines on the uses which contravene fundamental rights, including, but not limited to, uses of AI at the border, predictive policing, systems which restrict access to social rights and benefits, and risk -assessment tool s in the criminal justice context; 3. (extracted from253.txt)
Yours sincerely, European Digital Rights (EDRi), including: Access Now Bits of Freedom Chaos Computer Club D3 - Defesa dos Direitos Digitais Electronic Privacy Information Center (EPIC) Fitug Hermes Center Homo Digitalis IT-Pol Denmark Iuridicum Remedium Metamorphosis Foundation Panoptykon Foundation Privacy International Statewatch Other signatories: AI Now Institute, NYU Algorithm Watch Amnesty International App Drivers and Couriers Union (ADCU) Associazione Certi Diritti Associazione Luca Coscioni Associazione per gli Studi Giuridici sull'Immigrazione Big Brother Watch Center for Intersectional Justice (CIJ) Democratic Society Digitale Freiheit Dutch Section - International Commission of Jurists (NJCM) Each One Teach One (EOTO) e.V. (extracted from253.txt)
REPORT with recommendations to the Commission on Civil Law Rules on Robotics | A8-0005/2017 | European ParliamentAccess to page content (press "Enter")Direct access to language menu (press "Enter")EN - English BG - български ES - español CS - čeština DA - dansk DE - Deutsch ET - eesti keel EL - ελληνικά EN - English FR - français GA - Gaeilge HR - hrvatski IT - italiano LV - latviešu valoda LT - lietuvių kalba HU - magyar MT - Malti NL - Nederlands PL - polski PT - português RO - română SK - slovenčina SL - slovenščina FI - suomi SV - svenska NewsTopicsMEPsAbout ParliamentPlenaryCommitteesDelegationsEU budgetOther websitesView other websitesNewsTopicsMEPsAbout ParliamentPlenaryCommitteesDelegationsMultimedia CentrePresidencySecretariat-generalElectionsThink tankEP NewshubAt your serviceVisitsLegislative ObservatoryLegislative trainContracts and GrantsRegisterOpen Data PortalLiaison officesReport - A8-0005/2017ReportA8-0005/2017European ParliamentDownloadA-8-2017-0005_EN (PDF - 573 KB)A-8-2017-0005_EN (DOC - 118 KB) European Parliament REPORT with recommendations to the Commission on Civil Law Rules on Robotics27.1.2017 - (2015/2103(INL))Committee on Legal AffairsRapporteur: Mady Delvaux(Initiative – Rule 46 of the Rules of Procedure)Rapporteurs for the opinions (*):Georg Mayer, Committee on Transport and Tourism Michał Boni, Committee on Civil Liberties, Justice and Home Affairs(*) Associated committees – Rule 54 of the Rules of ProcedureAmendments001-001 (PDF - 10 KB)001-001 (DOC - 48 KB)002-003/REV1 (PDF - 100 KB)002-003/REV1 (DOC - 50 KB)004-008 (PDF - 134 KB)004-008 (DOC - 59 KB)009-010/REV1 (PDF - 102 KB)009-010/REV1 (DOC - 51 KB)009-013 (PDF - 114 KB)009-013 (DOC - 14 KB)012-013/REV1 (PDF - 102 KB)012-013/REV1 (DOC - 50 KB) Procedure : 2015/2103(INL)Document stages in plenaryDocument selected : A8-0005/2017Texts tabled : A8-0005/2017 Debates : PV 15/02/2017 - 14 CRE 15/02/2017 - 14 Votes : PV 16/02/2017 - 6.9 Explanations of votes Texts adopted : P8_TA(2017)0051 MOTION FOR A EUROPEAN PARLIAMENT RESOLUTION ANNEX TO THE MOTION FOR A RESOLUTION:DETAILED RECOMMENDATIONS AS TO THE CONTENT OF THE PROPOSAL REQUESTED EXPLANATORY STATEMENT OPINION of the Committee on Transport and Tourism (*) OPINION of the Committee on Civil Liberties, Justice and Home Affairs (*) OPINION of the Committee on Employment and Social Affairs OPINION of the Committee on the Environment, Public Health and Food Safety OPINION of the Committee on Industry, Research and Energy OPINION of the Committee on the Internal Market and Consumer Protection RESULT OF FINAL VOTE IN COMMITTEE RESPONSIBLEMOTION FOR A EUROPEAN PARLIAMENT RESOLUTION with recommendations to the Commission on Civil Law Rules on Robotics (2015/2103(INL)) The European Parliament, – having regard to Article 225 of the Treaty on the Functioning of the European Union, – having regard to the Product Liability Directive 85/374/EEC, – having regard to Rules 46 and 52 of its Rules of Procedure, – having regard to the report of the Committee on Legal Affairs and the opinions of the Committee on Transport and Tourism, the Committee on Civil Liberties, Justice and Home Affairs, the Committee on Employment and Social Affairs, the Committee on the Environment, Public Health and Food Safety, the Committee on Industry, Research and Energy and the Committee on the Internal Market and Consumer Protection (A8-0005/2017), Introduction A. (extracted from290.txt)
Points out that the guiding ethical framework should be based on the principles of beneficence, non-maleficence, autonomy and justice, on the principles and values enshrined in Article 2 of the Treaty on European Union and in the Charter of Fundamental Rights, such as human dignity, equality, justice and equity, non-discrimination, informed consent, private and family life and data protection, as well as on other underlying principles and values of the Union law, such as non-stigmatisation, transparency, autonomy, individual responsibility and social responsibility, and on existing ethical practices and codes; 14. (extracted from290.txt)
Researchers in the field of robotics should commit themselves to the highest ethical and professional conduct and abide by the following principles: Beneficence – robots should act in the best interests of humans; Non-maleficence – the doctrine of ‘first, do no harm’, whereby robots should not harm a human; Autonomy – the capacity to make an informed, un-coerced decision about the terms of interaction with robots; Justice – fair distribution of the benefits associated with robotics and affordability of homecare and healthcare robots in particular. (extracted from290.txt)
LICENCE FOR DESIGNERS – You should take into account the European values of dignity, autonomy and self-determination, freedom and justice before, during and after the process of design, development and delivery of such technologies including the need not to harm, injure, deceive or exploit (vulnerable) users. (extracted from290.txt)
OPINION of the Committee on Civil Liberties, Justice and Home Affairs (*) (23.11.2016)for the Committee on Legal Affairswith recommendations to the Commission on Civil Law Rules on Robotics(2015/2103(INL))Rapporteur: Michał Boni (Initiative – Rule 46 of the Rules of Procedure) (*) Associated committee – Rule 54 of the Rules of Procedure SUGGESTIONS The Committee on Civil Liberties, Justice and Home Affairs calls on the Committee on Legal Affairs, as the committee responsible: – to incorporate the following suggestions into its motion for a resolution: A. (extracted from290.txt)
Whereas a number of third countries have adopted guidelines and legislation on robotics and some Member States have launched specific reflections in this area; whereas a regulatory framework that governs at Union level the development and the use of robotics and artificial intelligence and builds on existing rules such as the Union’s General Data Protection Regulation[1] could prevent a fragmentation of rules in the single market and further safeguard the protection of the fundamental rights of all EU citizens to human dignity, privacy and family life, the protection of personal data and intellectual property, freedom of expression and information, equality and non-discrimination, solidarity, and citizens’ rights and justice, as well as security and safety, while being subject to the principle of proportionality; Ethical principles 1. (extracted from290.txt)
Believes that robotics and artificial intelligence, especially those with built-in autonomy, including the capability to independently extract, collect and share sensitive information with various stakeholders, and the possibility of self-learning or even evolving to self-modify, should be subject to robust conceptual laws or principles, such as that a robot may not kill or harm a human being and that it must obey and be controlled by a human being; that the process by which robots and artificial intelligence collect, use and process personal data must be transparent and comprehensible; believes that these principles should be technology neutral and based on empirical research; supports the development of an ethics-by-default framework for researchers, academia and engineers which ensures that these technological solutions will not hinder research and technological developments but will be in compliance with existing Union and national ethical practices and codes as well as with the rights and principles enshrined in the CFR, in particular human dignity, the respect for and protection of private and family life, security and safety, the protection of personal data, protection of intellectual property, the freedom of expression and information, equality and non-discrimination, solidarity, and citizens’ rights and justice, and should be subject to proportionality; 3. (extracted from290.txt)
RECs and the Commission are encouraged to start a reflection in order to develop a code of conduct for researchers/designers and users of medical CPS, that should be based on the principles enshrined in the Union’s Charter of Fundamental Rights (such as human dignity and human rights, equality, justice and equity, benefit and harm, dignity, non-discrimination and non-stigmatisation, autonomy and individual responsibility, informed consent, privacy and social responsibility as well as the rights of the elderly, the integration of persons with disabilities, the right to healthcare, and the right to consumer protection) and on existing ethical practices and codes. (extracted from290.txt)
